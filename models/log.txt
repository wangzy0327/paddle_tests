WARNING: Logging before InitGoogleLogging() is written to STDERR
I1018 08:53:49.440793 4097533 dynamic_loader.cc:170] Set paddle lib path : /home/wzy/.local/lib/python3.9/site-packages/paddle/libs
I1018 08:53:50.028918 4097533 init.cc:98] Before Parse: argc is 2, Init commandline: dummy --tryfromenv=use_cuda_managed_memory,use_pinned_memory,enable_pir_in_executor,gpugraph_enable_gpu_direct_access,dataloader_use_file_descriptor,graph_get_neighbor_id,curand_dir,auto_growth_chunk_size_in_mb,add_dependency_for_communication_op,log_memory_stats,initial_gpu_memory_in_mb,enable_interpretercore_launch_cinn,mklml_dir,use_system_allocator,fuse_parameter_memory_size,gpugraph_enable_print_op_debug,async_trace_count,cusparselt_dir,max_inplace_grad_add,allocator_strategy,pir_apply_shape_optimization_pass,prim_forward,eager_delete_scope,gpugraph_parallel_copyer_split_maxsize,free_idle_chunk,local_exe_sub_scope_limit,tracer_onednn_ops_on,enable_record_memory,cache_inference_while_scope,host_trace_level,enable_pe_launch_cinn,fraction_of_cpu_memory_to_use,new_executor_use_inplace,print_ir,enable_neighbor_list_use_uva,pir_apply_inplace_pass,allreduce_record_one_event,enable_unused_var_check,gpugraph_offload_param_stat,graph_embedding_split_infer_mode,enable_graph_multi_node_sampling,call_stack_level,jit_engine_type,enable_pir_in_executor_trace_run,enable_tracker_all2all,low_precision_op_list,enable_dependency_builder_debug_info,enable_pir_api,cuda_dir,ir_inplace_kernel_blacklist,paddle_num_threads,enable_pir_with_pt_in_dy2st,trt_ibuilder_cache,tensorrt_dir,use_shm_cache,free_when_no_cache_hit,fraction_of_gpu_memory_to_use,gpu_memory_limit_mb,einsum_opt,enable_exit_when_partial_worker,allow_cinn_ops,fraction_of_cuda_pinned_memory_to_use,fast_eager_deletion_mode,executor_log_deps_every_microseconds,sync_after_alloc,check_nan_inf,multi_node_sample_use_gpu_table,gpugraph_force_device_batch_num_equal,all_blocks_convert_trt,prim_enabled,use_auto_growth_v2,enable_all2all_use_fp16,gpugraph_offload_gather_copy_maxsize,check_nan_inf_level,dygraph_debug,convert_all_blocks,use_virtual_memory_auto_growth,gpu_allocator_retry_time,cudnn_dir,gpugraph_enable_hbm_table_collision_stat,enable_api_kernel_fallback,cpu_deterministic,query_dest_rank_by_multi_node,run_kp_kernel,graph_load_in_parallel,deny_cinn_ops,sort_sum_gradient,use_stream_safe_cuda_allocator,auto_free_cudagraph_allocations_on_launch,enable_cinn_auto_tune,use_cinn,search_cache_max_number,dist_threadpool_size,initial_cpu_memory_in_mb,gpugraph_hbm_table_load_factor,gpugraph_merge_grads_segment_size,init_allocated_mem,new_executor_use_cuda_graph,use_auto_growth_pinned_allocator,new_executor_sequential_run,new_executor_serial_run,mkl_dir,pir_subgraph_saving_dir,enable_dump_main_program,static_executor_perfstat_filepath,tracer_profile_fname,fuse_parameter_groups_size,prim_forward_blacklist,pe_profile_fname,gpugraph_dedup_pull_push_mode,reallocate_gpu_memory_in_mb,graph_metapath_split_opt,nccl_dir,new_executor_static_build,gpugraph_offload_param_extends,prim_skip_dynamic,gpugraph_slot_feasign_max_num,alloc_fill_value,prim_all,get_host_by_name_time,cupti_dir,use_stride_kernel,print_sub_graph_dir,enable_adjust_op_order,inner_op_parallelism,cusolver_dir,benchmark,enable_async_trace,gpugraph_debug_gpu_memory,gpugraph_storage_mode,apply_pass_to_program,lapack_dir,cusparse_dir,eager_delete_tensor_gb,static_runtime_data_save_path,gpugraph_parallel_stream_num,reader_queue_speed_test_mode,set_to_1d,enable_auto_rdma_trans,npu_storage_format,prim_check_ops,memory_fraction_of_eager_deletion,multiple_of_cupti_buffer_size,check_infer_symbolic,use_autotune,print_allocator_trace_info,cinn_subgraph_graphviz_dir,enable_auto_detect_gpu_topo,tracer_onednn_ops_off,use_mkldnn,prim_backward,gpugraph_sparse_table_storage_mode,fleet_executor_with_standalone,cublas_dir,gpugraph_load_node_list_into_hbm,tensor_operants_mode,enable_sparse_inner_gather,graph_neighbor_size_percent,custom_device_mem_record,save_static_runtime_data,enable_opt_get_features,op_dir,use_cuda_malloc_async_allocator,new_executor_use_local_scope,gpugraph_enable_segment_merge_grads 
I1018 08:53:50.029026 4097533 init.cc:106] After Parse: argc is 2
I1018 08:53:50.029140 4097533 os_info.cc:117] SetCurrentThreadName MainThread
I1018 08:53:50.029151 4097533 init.cc:234] ENV [CUSTOM_DEVICE_ROOT]=/home/wzy/.local/lib/python3.9/site-packages/paddle_custom_device
I1018 08:53:50.029155 4097533 init.cc:143] Try loading custom device libs from: [/home/wzy/.local/lib/python3.9/site-packages/paddle_custom_device]
I1018 08:53:50.029294 4097533 device_manager.cc:741] Found lib: /home/wzy/.local/lib/python3.9/site-packages/paddle_custom_device/libpaddle-custom-mlu.so
I1018 08:53:50.177063 4097533 device_manager.cc:255] Register Device - mlu
I1018 08:53:50.177115 4097533 device_manager.cc:266] GetDeviceCount is 1
I1018 08:53:50.177121 4097533 custom_device.cc:1099] Succeed in loading custom runtime in lib: /home/wzy/.local/lib/python3.9/site-packages/paddle_custom_device/libpaddle-custom-mlu.so
I1018 08:53:50.177127 4097533 custom_kernel.cc:36] Size of custom_kernel_map: 253
I1018 08:53:50.177153 4097533 custom_kernel.cc:58] Succeed in registering kernel [rsqrt:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.177161 4097533 custom_kernel.cc:58] Succeed in registering kernel [rsqrt:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.177196 4097533 custom_kernel.cc:58] Succeed in registering kernel [multiply_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.177199 4097533 custom_kernel.cc:58] Succeed in registering kernel [multiply_grad:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.177204 4097533 custom_kernel.cc:58] Succeed in registering kernel [multiply_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.177224 4097533 custom_kernel.cc:58] Succeed in registering kernel [divide_raw:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.177258 4097533 custom_kernel.cc:58] Succeed in registering kernel [divide_raw:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.177268 4097533 custom_kernel.cc:58] Succeed in registering kernel [flash_attn_unpadded:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.177277 4097533 custom_kernel.cc:58] Succeed in registering kernel [arange_tensor:(mlu, Undefined(AnyLayout), float64)] to Paddle. It will be used like native ones.
I1018 08:53:50.177282 4097533 custom_kernel.cc:58] Succeed in registering kernel [arange_tensor:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.177285 4097533 custom_kernel.cc:58] Succeed in registering kernel [arange_tensor:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.177290 4097533 custom_kernel.cc:58] Succeed in registering kernel [arange_tensor:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.177295 4097533 custom_kernel.cc:58] Succeed in registering kernel [flash_attn:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.177304 4097533 custom_kernel.cc:58] Succeed in registering kernel [tile:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.177340 4097533 custom_kernel.cc:58] Succeed in registering kernel [tile:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.177345 4097533 custom_kernel.cc:58] Succeed in registering kernel [tile:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.177347 4097533 custom_kernel.cc:58] Succeed in registering kernel [tile:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.177351 4097533 custom_kernel.cc:58] Succeed in registering kernel [tile:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.177372 4097533 custom_kernel.cc:58] Succeed in registering kernel [split:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.177378 4097533 custom_kernel.cc:58] Succeed in registering kernel [split:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.177384 4097533 custom_kernel.cc:58] Succeed in registering kernel [split:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.177390 4097533 custom_kernel.cc:58] Succeed in registering kernel [split:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.177438 4097533 custom_kernel.cc:58] Succeed in registering kernel [split:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.177448 4097533 custom_kernel.cc:58] Succeed in registering kernel [squared_l2_norm_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.177452 4097533 custom_kernel.cc:58] Succeed in registering kernel [squared_l2_norm_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.177464 4097533 custom_kernel.cc:58] Succeed in registering kernel [check_finite_and_unscale:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.177469 4097533 custom_kernel.cc:58] Succeed in registering kernel [check_finite_and_unscale:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.177476 4097533 custom_kernel.cc:58] Succeed in registering kernel [exp_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.177484 4097533 custom_kernel.cc:58] Succeed in registering kernel [exp_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.177495 4097533 custom_kernel.cc:58] Succeed in registering kernel [adam:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.177498 4097533 custom_kernel.cc:58] Succeed in registering kernel [adam:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.177508 4097533 custom_kernel.cc:58] Succeed in registering kernel [atan_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.177512 4097533 custom_kernel.cc:58] Succeed in registering kernel [atan_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.177518 4097533 custom_kernel.cc:58] Succeed in registering kernel [logical_xor:(mlu, Undefined(AnyLayout), uint8)] to Paddle. It will be used like native ones.
I1018 08:53:50.177524 4097533 custom_kernel.cc:58] Succeed in registering kernel [logical_xor:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.177531 4097533 custom_kernel.cc:58] Succeed in registering kernel [logical_xor:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.177536 4097533 custom_kernel.cc:58] Succeed in registering kernel [logical_xor:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.177538 4097533 custom_kernel.cc:58] Succeed in registering kernel [logical_xor:(mlu, Undefined(AnyLayout), int16)] to Paddle. It will be used like native ones.
I1018 08:53:50.177544 4097533 custom_kernel.cc:58] Succeed in registering kernel [logical_xor:(mlu, Undefined(AnyLayout), int8)] to Paddle. It will be used like native ones.
I1018 08:53:50.177548 4097533 custom_kernel.cc:58] Succeed in registering kernel [logical_xor:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.177597 4097533 custom_kernel.cc:58] Succeed in registering kernel [logical_xor:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.177613 4097533 custom_kernel.cc:58] Succeed in registering kernel [where:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.177618 4097533 custom_kernel.cc:58] Succeed in registering kernel [where:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.177620 4097533 custom_kernel.cc:58] Succeed in registering kernel [where:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.177624 4097533 custom_kernel.cc:58] Succeed in registering kernel [where:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.177635 4097533 custom_kernel.cc:58] Succeed in registering kernel [p_norm_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.177639 4097533 custom_kernel.cc:58] Succeed in registering kernel [p_norm_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.177645 4097533 custom_kernel.cc:58] Succeed in registering kernel [less_than_raw:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.177651 4097533 custom_kernel.cc:58] Succeed in registering kernel [less_than_raw:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.177656 4097533 custom_kernel.cc:58] Succeed in registering kernel [less_than_raw:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.177660 4097533 custom_kernel.cc:58] Succeed in registering kernel [less_than_raw:(mlu, Undefined(AnyLayout), int16)] to Paddle. It will be used like native ones.
I1018 08:53:50.177664 4097533 custom_kernel.cc:58] Succeed in registering kernel [less_than_raw:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.177672 4097533 custom_kernel.cc:58] Succeed in registering kernel [less_than_raw:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.177701 4097533 custom_kernel.cc:58] Succeed in registering kernel [triu_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.177706 4097533 custom_kernel.cc:58] Succeed in registering kernel [triu_grad:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.177708 4097533 custom_kernel.cc:58] Succeed in registering kernel [triu_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.177719 4097533 custom_kernel.cc:58] Succeed in registering kernel [hardsigmoid_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.177723 4097533 custom_kernel.cc:58] Succeed in registering kernel [hardsigmoid_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.177728 4097533 custom_kernel.cc:58] Succeed in registering kernel [gather_nd:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.177734 4097533 custom_kernel.cc:58] Succeed in registering kernel [gather_nd:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.177745 4097533 custom_kernel.cc:58] Succeed in registering kernel [huber_loss:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.177749 4097533 custom_kernel.cc:58] Succeed in registering kernel [huber_loss:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.177760 4097533 custom_kernel.cc:58] Succeed in registering kernel [one_hot:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.177764 4097533 custom_kernel.cc:58] Succeed in registering kernel [one_hot:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.177770 4097533 custom_kernel.cc:58] Succeed in registering kernel [equal_raw:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.177778 4097533 custom_kernel.cc:58] Succeed in registering kernel [equal_raw:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.177784 4097533 custom_kernel.cc:58] Succeed in registering kernel [equal_raw:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.177788 4097533 custom_kernel.cc:58] Succeed in registering kernel [equal_raw:(mlu, Undefined(AnyLayout), int16)] to Paddle. It will be used like native ones.
I1018 08:53:50.177793 4097533 custom_kernel.cc:58] Succeed in registering kernel [equal_raw:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.177798 4097533 custom_kernel.cc:58] Succeed in registering kernel [equal_raw:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.177816 4097533 custom_kernel.cc:58] Succeed in registering kernel [depthwise_conv2d:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.177819 4097533 custom_kernel.cc:58] Succeed in registering kernel [depthwise_conv2d:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.177829 4097533 custom_kernel.cc:58] Succeed in registering kernel [mean_raw:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.177856 4097533 custom_kernel.cc:58] Succeed in registering kernel [mean_raw:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.177865 4097533 custom_kernel.cc:58] Succeed in registering kernel [gather_nd_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.177870 4097533 custom_kernel.cc:58] Succeed in registering kernel [gather_nd_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.177878 4097533 custom_kernel.cc:58] Succeed in registering kernel [index_sample_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.177882 4097533 custom_kernel.cc:58] Succeed in registering kernel [index_sample_grad:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.177917 4097533 custom_kernel.cc:58] Succeed in registering kernel [index_sample_grad:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.177922 4097533 custom_kernel.cc:58] Succeed in registering kernel [index_sample_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.177932 4097533 custom_kernel.cc:58] Succeed in registering kernel [pool2d_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.177937 4097533 custom_kernel.cc:58] Succeed in registering kernel [pool2d_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.177942 4097533 custom_kernel.cc:58] Succeed in registering kernel [exp:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.177973 4097533 custom_kernel.cc:58] Succeed in registering kernel [exp:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.177982 4097533 custom_kernel.cc:58] Succeed in registering kernel [sigmoid_cross_entropy_with_logits:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.177986 4097533 custom_kernel.cc:58] Succeed in registering kernel [sigmoid_cross_entropy_with_logits:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.177999 4097533 custom_kernel.cc:58] Succeed in registering kernel [scatter:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.178002 4097533 custom_kernel.cc:58] Succeed in registering kernel [scatter:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178038 4097533 custom_kernel.cc:58] Succeed in registering kernel [log:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.178042 4097533 custom_kernel.cc:58] Succeed in registering kernel [log:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178049 4097533 custom_kernel.cc:58] Succeed in registering kernel [embedding:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.178081 4097533 custom_kernel.cc:58] Succeed in registering kernel [embedding:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178084 4097533 custom_kernel.cc:58] Succeed in registering kernel [embedding:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178094 4097533 custom_kernel.cc:58] Succeed in registering kernel [momentum:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.178098 4097533 custom_kernel.cc:58] Succeed in registering kernel [momentum:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178112 4097533 custom_kernel.cc:58] Succeed in registering kernel [clip_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.178117 4097533 custom_kernel.cc:58] Succeed in registering kernel [clip_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178122 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy_h2d:(mlu, Undefined(AnyLayout), uint8)] to Paddle. It will be used like native ones.
I1018 08:53:50.178128 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy_h2d:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.178131 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy_h2d:(mlu, Undefined(AnyLayout), complex128)] to Paddle. It will be used like native ones.
I1018 08:53:50.178138 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy_h2d:(mlu, Undefined(AnyLayout), float64)] to Paddle. It will be used like native ones.
I1018 08:53:50.178186 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy_h2d:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.178190 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy_h2d:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178193 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy_h2d:(mlu, Undefined(AnyLayout), int16)] to Paddle. It will be used like native ones.
I1018 08:53:50.178197 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy_h2d:(mlu, Undefined(AnyLayout), int8)] to Paddle. It will be used like native ones.
I1018 08:53:50.178200 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy_h2d:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.178205 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy_h2d:(mlu, Undefined(AnyLayout), bfloat16)] to Paddle. It will be used like native ones.
I1018 08:53:50.178208 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy_h2d:(mlu, Undefined(AnyLayout), complex64)] to Paddle. It will be used like native ones.
I1018 08:53:50.178211 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy_h2d:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178223 4097533 custom_kernel.cc:58] Succeed in registering kernel [cross_entropy_with_softmax_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.178227 4097533 custom_kernel.cc:58] Succeed in registering kernel [cross_entropy_with_softmax_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178237 4097533 custom_kernel.cc:58] Succeed in registering kernel [nonzero:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178251 4097533 custom_kernel.cc:58] Succeed in registering kernel [nonzero:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.178256 4097533 custom_kernel.cc:58] Succeed in registering kernel [nonzero:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178263 4097533 custom_kernel.cc:58] Succeed in registering kernel [any:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.178275 4097533 custom_kernel.cc:58] Succeed in registering kernel [randperm:(mlu, Undefined(AnyLayout), float64)] to Paddle. It will be used like native ones.
I1018 08:53:50.178279 4097533 custom_kernel.cc:58] Succeed in registering kernel [randperm:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.178283 4097533 custom_kernel.cc:58] Succeed in registering kernel [randperm:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178287 4097533 custom_kernel.cc:58] Succeed in registering kernel [randperm:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178296 4097533 custom_kernel.cc:58] Succeed in registering kernel [relu6:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.178300 4097533 custom_kernel.cc:58] Succeed in registering kernel [relu6:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178306 4097533 custom_kernel.cc:58] Succeed in registering kernel [argmax:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.178314 4097533 custom_kernel.cc:58] Succeed in registering kernel [argmax:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178416 4097533 custom_kernel.cc:58] Succeed in registering kernel [strided_slice_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.178421 4097533 custom_kernel.cc:58] Succeed in registering kernel [strided_slice_grad:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.178424 4097533 custom_kernel.cc:58] Succeed in registering kernel [strided_slice_grad:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178428 4097533 custom_kernel.cc:58] Succeed in registering kernel [strided_slice_grad:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.178431 4097533 custom_kernel.cc:58] Succeed in registering kernel [strided_slice_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178445 4097533 custom_kernel.cc:58] Succeed in registering kernel [max:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.178449 4097533 custom_kernel.cc:58] Succeed in registering kernel [max:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.178453 4097533 custom_kernel.cc:58] Succeed in registering kernel [max:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178457 4097533 custom_kernel.cc:58] Succeed in registering kernel [max:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.178462 4097533 custom_kernel.cc:58] Succeed in registering kernel [max:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178475 4097533 custom_kernel.cc:58] Succeed in registering kernel [uniform:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178485 4097533 custom_kernel.cc:58] Succeed in registering kernel [gaussian:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.178493 4097533 custom_kernel.cc:58] Succeed in registering kernel [gaussian:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178503 4097533 custom_kernel.cc:58] Succeed in registering kernel [not_equal:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.178507 4097533 custom_kernel.cc:58] Succeed in registering kernel [not_equal:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.178514 4097533 custom_kernel.cc:58] Succeed in registering kernel [not_equal:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178516 4097533 custom_kernel.cc:58] Succeed in registering kernel [not_equal:(mlu, Undefined(AnyLayout), int16)] to Paddle. It will be used like native ones.
I1018 08:53:50.178522 4097533 custom_kernel.cc:58] Succeed in registering kernel [not_equal:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.178529 4097533 custom_kernel.cc:58] Succeed in registering kernel [not_equal:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178537 4097533 custom_kernel.cc:58] Succeed in registering kernel [square_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.178543 4097533 custom_kernel.cc:58] Succeed in registering kernel [square_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178575 4097533 custom_kernel.cc:58] Succeed in registering kernel [subtract_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.178579 4097533 custom_kernel.cc:58] Succeed in registering kernel [subtract_grad:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178583 4097533 custom_kernel.cc:58] Succeed in registering kernel [subtract_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178591 4097533 custom_kernel.cc:58] Succeed in registering kernel [minimum:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.178597 4097533 custom_kernel.cc:58] Succeed in registering kernel [minimum:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178603 4097533 custom_kernel.cc:58] Succeed in registering kernel [minimum:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178611 4097533 custom_kernel.cc:58] Succeed in registering kernel [matmul:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.178615 4097533 custom_kernel.cc:58] Succeed in registering kernel [matmul:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178623 4097533 custom_kernel.cc:58] Succeed in registering kernel [full:(mlu, Undefined(AnyLayout), uint8)] to Paddle. It will be used like native ones.
I1018 08:53:50.178628 4097533 custom_kernel.cc:58] Succeed in registering kernel [full:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.178634 4097533 custom_kernel.cc:58] Succeed in registering kernel [full:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.178639 4097533 custom_kernel.cc:58] Succeed in registering kernel [full:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178687 4097533 custom_kernel.cc:58] Succeed in registering kernel [full:(mlu, Undefined(AnyLayout), int16)] to Paddle. It will be used like native ones.
I1018 08:53:50.178691 4097533 custom_kernel.cc:58] Succeed in registering kernel [full:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.178699 4097533 custom_kernel.cc:58] Succeed in registering kernel [full:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178706 4097533 custom_kernel.cc:58] Succeed in registering kernel [masked_select:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.178711 4097533 custom_kernel.cc:58] Succeed in registering kernel [masked_select:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178716 4097533 custom_kernel.cc:58] Succeed in registering kernel [masked_select:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.178721 4097533 custom_kernel.cc:58] Succeed in registering kernel [masked_select:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178730 4097533 custom_kernel.cc:58] Succeed in registering kernel [bitwise_not:(mlu, Undefined(AnyLayout), uint8)] to Paddle. It will be used like native ones.
I1018 08:53:50.178735 4097533 custom_kernel.cc:58] Succeed in registering kernel [bitwise_not:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178745 4097533 custom_kernel.cc:58] Succeed in registering kernel [bitwise_not:(mlu, Undefined(AnyLayout), int16)] to Paddle. It will be used like native ones.
I1018 08:53:50.178750 4097533 custom_kernel.cc:58] Succeed in registering kernel [bitwise_not:(mlu, Undefined(AnyLayout), int8)] to Paddle. It will be used like native ones.
I1018 08:53:50.178753 4097533 custom_kernel.cc:58] Succeed in registering kernel [bitwise_not:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.178763 4097533 custom_kernel.cc:58] Succeed in registering kernel [multinomial:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.178766 4097533 custom_kernel.cc:58] Succeed in registering kernel [multinomial:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178773 4097533 custom_kernel.cc:58] Succeed in registering kernel [strided_slice_raw_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.178779 4097533 custom_kernel.cc:58] Succeed in registering kernel [strided_slice_raw_grad:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.178786 4097533 custom_kernel.cc:58] Succeed in registering kernel [strided_slice_raw_grad:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178793 4097533 custom_kernel.cc:58] Succeed in registering kernel [strided_slice_raw_grad:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.178800 4097533 custom_kernel.cc:58] Succeed in registering kernel [strided_slice_raw_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178812 4097533 custom_kernel.cc:58] Succeed in registering kernel [max_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.178817 4097533 custom_kernel.cc:58] Succeed in registering kernel [max_grad:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.178820 4097533 custom_kernel.cc:58] Succeed in registering kernel [max_grad:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178823 4097533 custom_kernel.cc:58] Succeed in registering kernel [max_grad:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.178848 4097533 custom_kernel.cc:58] Succeed in registering kernel [max_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178854 4097533 custom_kernel.cc:58] Succeed in registering kernel [concat:(mlu, Undefined(AnyLayout), uint8)] to Paddle. It will be used like native ones.
I1018 08:53:50.178861 4097533 custom_kernel.cc:58] Succeed in registering kernel [concat:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.178867 4097533 custom_kernel.cc:58] Succeed in registering kernel [concat:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.178872 4097533 custom_kernel.cc:58] Succeed in registering kernel [concat:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178927 4097533 custom_kernel.cc:58] Succeed in registering kernel [concat:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.178931 4097533 custom_kernel.cc:58] Succeed in registering kernel [concat:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178937 4097533 custom_kernel.cc:58] Succeed in registering kernel [sum_raw:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.178946 4097533 custom_kernel.cc:58] Succeed in registering kernel [sum_raw:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178952 4097533 custom_kernel.cc:58] Succeed in registering kernel [sum_raw:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178963 4097533 custom_kernel.cc:58] Succeed in registering kernel [sigmoid_cross_entropy_with_logits_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.178967 4097533 custom_kernel.cc:58] Succeed in registering kernel [sigmoid_cross_entropy_with_logits_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178973 4097533 custom_kernel.cc:58] Succeed in registering kernel [divide_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.178980 4097533 custom_kernel.cc:58] Succeed in registering kernel [divide_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.178988 4097533 custom_kernel.cc:58] Succeed in registering kernel [squeeze:(mlu, Undefined(AnyLayout), uint8)] to Paddle. It will be used like native ones.
I1018 08:53:50.178993 4097533 custom_kernel.cc:58] Succeed in registering kernel [squeeze:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.179000 4097533 custom_kernel.cc:58] Succeed in registering kernel [squeeze:(mlu, Undefined(AnyLayout), float64)] to Paddle. It will be used like native ones.
I1018 08:53:50.179008 4097533 custom_kernel.cc:58] Succeed in registering kernel [squeeze:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.179013 4097533 custom_kernel.cc:58] Succeed in registering kernel [squeeze:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179018 4097533 custom_kernel.cc:58] Succeed in registering kernel [squeeze:(mlu, Undefined(AnyLayout), int8)] to Paddle. It will be used like native ones.
I1018 08:53:50.179023 4097533 custom_kernel.cc:58] Succeed in registering kernel [squeeze:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.179028 4097533 custom_kernel.cc:58] Succeed in registering kernel [squeeze:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179042 4097533 custom_kernel.cc:58] Succeed in registering kernel [triu:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.179046 4097533 custom_kernel.cc:58] Succeed in registering kernel [triu:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179049 4097533 custom_kernel.cc:58] Succeed in registering kernel [triu:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179059 4097533 custom_kernel.cc:58] Succeed in registering kernel [unsqueeze_grad:(mlu, Undefined(AnyLayout), uint8)] to Paddle. It will be used like native ones.
I1018 08:53:50.179067 4097533 custom_kernel.cc:58] Succeed in registering kernel [unsqueeze_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.179073 4097533 custom_kernel.cc:58] Succeed in registering kernel [unsqueeze_grad:(mlu, Undefined(AnyLayout), complex128)] to Paddle. It will be used like native ones.
I1018 08:53:50.179081 4097533 custom_kernel.cc:58] Succeed in registering kernel [unsqueeze_grad:(mlu, Undefined(AnyLayout), float64)] to Paddle. It will be used like native ones.
I1018 08:53:50.179088 4097533 custom_kernel.cc:58] Succeed in registering kernel [unsqueeze_grad:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.179095 4097533 custom_kernel.cc:58] Succeed in registering kernel [unsqueeze_grad:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179102 4097533 custom_kernel.cc:58] Succeed in registering kernel [unsqueeze_grad:(mlu, Undefined(AnyLayout), int16)] to Paddle. It will be used like native ones.
I1018 08:53:50.179108 4097533 custom_kernel.cc:58] Succeed in registering kernel [unsqueeze_grad:(mlu, Undefined(AnyLayout), int8)] to Paddle. It will be used like native ones.
I1018 08:53:50.179114 4097533 custom_kernel.cc:58] Succeed in registering kernel [unsqueeze_grad:(mlu, Undefined(AnyLayout), bfloat16)] to Paddle. It will be used like native ones.
I1018 08:53:50.179119 4097533 custom_kernel.cc:58] Succeed in registering kernel [unsqueeze_grad:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.179124 4097533 custom_kernel.cc:58] Succeed in registering kernel [unsqueeze_grad:(mlu, Undefined(AnyLayout), complex64)] to Paddle. It will be used like native ones.
I1018 08:53:50.179132 4097533 custom_kernel.cc:58] Succeed in registering kernel [unsqueeze_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179139 4097533 custom_kernel.cc:58] Succeed in registering kernel [coalesce_tensor:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.179149 4097533 custom_kernel.cc:58] Succeed in registering kernel [coalesce_tensor:(mlu, Undefined(AnyLayout), float64)] to Paddle. It will be used like native ones.
I1018 08:53:50.179153 4097533 custom_kernel.cc:58] Succeed in registering kernel [coalesce_tensor:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179158 4097533 custom_kernel.cc:58] Succeed in registering kernel [coalesce_tensor:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179191 4097533 custom_kernel.cc:58] Succeed in registering kernel [subtract:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.179195 4097533 custom_kernel.cc:58] Succeed in registering kernel [subtract:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179199 4097533 custom_kernel.cc:58] Succeed in registering kernel [subtract:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179205 4097533 custom_kernel.cc:58] Succeed in registering kernel [roi_align:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179219 4097533 custom_kernel.cc:58] Succeed in registering kernel [arange:(mlu, Undefined(AnyLayout), float64)] to Paddle. It will be used like native ones.
I1018 08:53:50.179224 4097533 custom_kernel.cc:58] Succeed in registering kernel [arange:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179227 4097533 custom_kernel.cc:58] Succeed in registering kernel [arange:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.179231 4097533 custom_kernel.cc:58] Succeed in registering kernel [arange:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179240 4097533 custom_kernel.cc:58] Succeed in registering kernel [squeeze_grad:(mlu, Undefined(AnyLayout), uint8)] to Paddle. It will be used like native ones.
I1018 08:53:50.179247 4097533 custom_kernel.cc:58] Succeed in registering kernel [squeeze_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.179253 4097533 custom_kernel.cc:58] Succeed in registering kernel [squeeze_grad:(mlu, Undefined(AnyLayout), float64)] to Paddle. It will be used like native ones.
I1018 08:53:50.179260 4097533 custom_kernel.cc:58] Succeed in registering kernel [squeeze_grad:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.179265 4097533 custom_kernel.cc:58] Succeed in registering kernel [squeeze_grad:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179270 4097533 custom_kernel.cc:58] Succeed in registering kernel [squeeze_grad:(mlu, Undefined(AnyLayout), int8)] to Paddle. It will be used like native ones.
I1018 08:53:50.179276 4097533 custom_kernel.cc:58] Succeed in registering kernel [squeeze_grad:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.179281 4097533 custom_kernel.cc:58] Succeed in registering kernel [squeeze_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179287 4097533 custom_kernel.cc:58] Succeed in registering kernel [unstack:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.179294 4097533 custom_kernel.cc:58] Succeed in registering kernel [unstack:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179301 4097533 custom_kernel.cc:58] Succeed in registering kernel [scatter_nd_add:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.179306 4097533 custom_kernel.cc:58] Succeed in registering kernel [scatter_nd_add:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179311 4097533 custom_kernel.cc:58] Succeed in registering kernel [scatter_nd_add:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179322 4097533 custom_kernel.cc:58] Succeed in registering kernel [pool2d:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.179325 4097533 custom_kernel.cc:58] Succeed in registering kernel [pool2d:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179337 4097533 custom_kernel.cc:58] Succeed in registering kernel [max_raw:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.179340 4097533 custom_kernel.cc:58] Succeed in registering kernel [max_raw:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.179344 4097533 custom_kernel.cc:58] Succeed in registering kernel [max_raw:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179348 4097533 custom_kernel.cc:58] Succeed in registering kernel [max_raw:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.179369 4097533 custom_kernel.cc:58] Succeed in registering kernel [max_raw:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179380 4097533 custom_kernel.cc:58] Succeed in registering kernel [batch_norm_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.179384 4097533 custom_kernel.cc:58] Succeed in registering kernel [batch_norm_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179390 4097533 custom_kernel.cc:58] Succeed in registering kernel [stack:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.179401 4097533 custom_kernel.cc:58] Succeed in registering kernel [stack:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179406 4097533 custom_kernel.cc:58] Succeed in registering kernel [stack:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.179411 4097533 custom_kernel.cc:58] Succeed in registering kernel [stack:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179441 4097533 custom_kernel.cc:58] Succeed in registering kernel [multiply:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.179445 4097533 custom_kernel.cc:58] Succeed in registering kernel [multiply:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179448 4097533 custom_kernel.cc:58] Succeed in registering kernel [multiply:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179454 4097533 custom_kernel.cc:58] Succeed in registering kernel [maximum_raw:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.179459 4097533 custom_kernel.cc:58] Succeed in registering kernel [maximum_raw:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179466 4097533 custom_kernel.cc:58] Succeed in registering kernel [maximum_raw:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179482 4097533 custom_kernel.cc:58] Succeed in registering kernel [tril_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.179486 4097533 custom_kernel.cc:58] Succeed in registering kernel [tril_grad:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179490 4097533 custom_kernel.cc:58] Succeed in registering kernel [tril_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179497 4097533 custom_kernel.cc:58] Succeed in registering kernel [abs_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.179504 4097533 custom_kernel.cc:58] Succeed in registering kernel [abs_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179510 4097533 custom_kernel.cc:58] Succeed in registering kernel [bitwise_or:(mlu, Undefined(AnyLayout), uint8)] to Paddle. It will be used like native ones.
I1018 08:53:50.179517 4097533 custom_kernel.cc:58] Succeed in registering kernel [bitwise_or:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179544 4097533 custom_kernel.cc:58] Succeed in registering kernel [bitwise_or:(mlu, Undefined(AnyLayout), int16)] to Paddle. It will be used like native ones.
I1018 08:53:50.179548 4097533 custom_kernel.cc:58] Succeed in registering kernel [bitwise_or:(mlu, Undefined(AnyLayout), int8)] to Paddle. It will be used like native ones.
I1018 08:53:50.179551 4097533 custom_kernel.cc:58] Succeed in registering kernel [bitwise_or:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.179562 4097533 custom_kernel.cc:58] Succeed in registering kernel [prod_infer:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.179566 4097533 custom_kernel.cc:58] Succeed in registering kernel [prod_infer:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179570 4097533 custom_kernel.cc:58] Succeed in registering kernel [prod_infer:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179581 4097533 custom_kernel.cc:58] Succeed in registering kernel [bce_loss:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.179584 4097533 custom_kernel.cc:58] Succeed in registering kernel [bce_loss:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179595 4097533 custom_kernel.cc:58] Succeed in registering kernel [huber_loss_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.179600 4097533 custom_kernel.cc:58] Succeed in registering kernel [huber_loss_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179607 4097533 custom_kernel.cc:58] Succeed in registering kernel [tile_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.179633 4097533 custom_kernel.cc:58] Succeed in registering kernel [tile_grad:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.179636 4097533 custom_kernel.cc:58] Succeed in registering kernel [tile_grad:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179641 4097533 custom_kernel.cc:58] Succeed in registering kernel [tile_grad:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.179643 4097533 custom_kernel.cc:58] Succeed in registering kernel [tile_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179653 4097533 custom_kernel.cc:58] Succeed in registering kernel [swish_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.179657 4097533 custom_kernel.cc:58] Succeed in registering kernel [swish_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179664 4097533 custom_kernel.cc:58] Succeed in registering kernel [sync_batch_norm_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.179669 4097533 custom_kernel.cc:58] Succeed in registering kernel [sync_batch_norm_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179700 4097533 custom_kernel.cc:58] Succeed in registering kernel [subtract_raw:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.179704 4097533 custom_kernel.cc:58] Succeed in registering kernel [subtract_raw:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179708 4097533 custom_kernel.cc:58] Succeed in registering kernel [subtract_raw:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179718 4097533 custom_kernel.cc:58] Succeed in registering kernel [rsqrt_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.179721 4097533 custom_kernel.cc:58] Succeed in registering kernel [rsqrt_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179733 4097533 custom_kernel.cc:58] Succeed in registering kernel [conv2d_transpose_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.179738 4097533 custom_kernel.cc:58] Succeed in registering kernel [conv2d_transpose_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179749 4097533 custom_kernel.cc:58] Succeed in registering kernel [sqrt:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.179751 4097533 custom_kernel.cc:58] Succeed in registering kernel [sqrt:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179759 4097533 custom_kernel.cc:58] Succeed in registering kernel [elementwise_pow_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.179764 4097533 custom_kernel.cc:58] Succeed in registering kernel [elementwise_pow_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179776 4097533 custom_kernel.cc:58] Succeed in registering kernel [where_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.179783 4097533 custom_kernel.cc:58] Succeed in registering kernel [where_grad:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179788 4097533 custom_kernel.cc:58] Succeed in registering kernel [where_grad:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.179792 4097533 custom_kernel.cc:58] Succeed in registering kernel [where_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179800 4097533 custom_kernel.cc:58] Succeed in registering kernel [assign_array:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.179805 4097533 custom_kernel.cc:58] Succeed in registering kernel [assign_array:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.179811 4097533 custom_kernel.cc:58] Succeed in registering kernel [assign_array:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179816 4097533 custom_kernel.cc:58] Succeed in registering kernel [assign_array:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.179868 4097533 custom_kernel.cc:58] Succeed in registering kernel [assign_array:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179876 4097533 custom_kernel.cc:58] Succeed in registering kernel [bitwise_and:(mlu, Undefined(AnyLayout), uint8)] to Paddle. It will be used like native ones.
I1018 08:53:50.179881 4097533 custom_kernel.cc:58] Succeed in registering kernel [bitwise_and:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179890 4097533 custom_kernel.cc:58] Succeed in registering kernel [bitwise_and:(mlu, Undefined(AnyLayout), int16)] to Paddle. It will be used like native ones.
I1018 08:53:50.179894 4097533 custom_kernel.cc:58] Succeed in registering kernel [bitwise_and:(mlu, Undefined(AnyLayout), int8)] to Paddle. It will be used like native ones.
I1018 08:53:50.179898 4097533 custom_kernel.cc:58] Succeed in registering kernel [bitwise_and:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.179931 4097533 custom_kernel.cc:58] Succeed in registering kernel [add_n:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.179935 4097533 custom_kernel.cc:58] Succeed in registering kernel [add_n:(mlu, Undefined(AnyLayout), float64)] to Paddle. It will be used like native ones.
I1018 08:53:50.179939 4097533 custom_kernel.cc:58] Succeed in registering kernel [add_n:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179949 4097533 custom_kernel.cc:58] Succeed in registering kernel [tanh_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.179953 4097533 custom_kernel.cc:58] Succeed in registering kernel [tanh_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179963 4097533 custom_kernel.cc:58] Succeed in registering kernel [dropout:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.179967 4097533 custom_kernel.cc:58] Succeed in registering kernel [dropout:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179973 4097533 custom_kernel.cc:58] Succeed in registering kernel [greater_than:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.179980 4097533 custom_kernel.cc:58] Succeed in registering kernel [greater_than:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.179984 4097533 custom_kernel.cc:58] Succeed in registering kernel [greater_than:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.179991 4097533 custom_kernel.cc:58] Succeed in registering kernel [greater_than:(mlu, Undefined(AnyLayout), int16)] to Paddle. It will be used like native ones.
I1018 08:53:50.179998 4097533 custom_kernel.cc:58] Succeed in registering kernel [greater_than:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.180003 4097533 custom_kernel.cc:58] Succeed in registering kernel [greater_than:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180016 4097533 custom_kernel.cc:58] Succeed in registering kernel [min:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180019 4097533 custom_kernel.cc:58] Succeed in registering kernel [min:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180023 4097533 custom_kernel.cc:58] Succeed in registering kernel [min:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180032 4097533 custom_kernel.cc:58] Succeed in registering kernel [unstack_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180037 4097533 custom_kernel.cc:58] Succeed in registering kernel [unstack_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180043 4097533 custom_kernel.cc:58] Succeed in registering kernel [flash_attn_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180050 4097533 custom_kernel.cc:58] Succeed in registering kernel [conv2d:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180054 4097533 custom_kernel.cc:58] Succeed in registering kernel [conv2d:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180075 4097533 custom_kernel.cc:58] Succeed in registering kernel [cos_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180078 4097533 custom_kernel.cc:58] Succeed in registering kernel [cos_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180087 4097533 custom_kernel.cc:58] Succeed in registering kernel [conv2d_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180092 4097533 custom_kernel.cc:58] Succeed in registering kernel [conv2d_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180101 4097533 custom_kernel.cc:58] Succeed in registering kernel [batch_norm:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180105 4097533 custom_kernel.cc:58] Succeed in registering kernel [batch_norm:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180112 4097533 custom_kernel.cc:58] Succeed in registering kernel [logical_or:(mlu, Undefined(AnyLayout), uint8)] to Paddle. It will be used like native ones.
I1018 08:53:50.180117 4097533 custom_kernel.cc:58] Succeed in registering kernel [logical_or:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180125 4097533 custom_kernel.cc:58] Succeed in registering kernel [logical_or:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.180130 4097533 custom_kernel.cc:58] Succeed in registering kernel [logical_or:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180133 4097533 custom_kernel.cc:58] Succeed in registering kernel [logical_or:(mlu, Undefined(AnyLayout), int16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180140 4097533 custom_kernel.cc:58] Succeed in registering kernel [logical_or:(mlu, Undefined(AnyLayout), int8)] to Paddle. It will be used like native ones.
I1018 08:53:50.180143 4097533 custom_kernel.cc:58] Succeed in registering kernel [logical_or:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.180194 4097533 custom_kernel.cc:58] Succeed in registering kernel [logical_or:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180207 4097533 custom_kernel.cc:58] Succeed in registering kernel [dropout_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180212 4097533 custom_kernel.cc:58] Succeed in registering kernel [dropout_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180218 4097533 custom_kernel.cc:58] Succeed in registering kernel [maximum:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180223 4097533 custom_kernel.cc:58] Succeed in registering kernel [maximum:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180231 4097533 custom_kernel.cc:58] Succeed in registering kernel [maximum:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180241 4097533 custom_kernel.cc:58] Succeed in registering kernel [deformable_conv:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180248 4097533 custom_kernel.cc:58] Succeed in registering kernel [expand_as:(mlu, Undefined(AnyLayout), uint8)] to Paddle. It will be used like native ones.
I1018 08:53:50.180253 4097533 custom_kernel.cc:58] Succeed in registering kernel [expand_as:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180258 4097533 custom_kernel.cc:58] Succeed in registering kernel [expand_as:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.180284 4097533 custom_kernel.cc:58] Succeed in registering kernel [expand_as:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180287 4097533 custom_kernel.cc:58] Succeed in registering kernel [expand_as:(mlu, Undefined(AnyLayout), int8)] to Paddle. It will be used like native ones.
I1018 08:53:50.180291 4097533 custom_kernel.cc:58] Succeed in registering kernel [expand_as:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.180294 4097533 custom_kernel.cc:58] Succeed in registering kernel [expand_as:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180303 4097533 custom_kernel.cc:58] Succeed in registering kernel [cross_entropy_with_softmax:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180307 4097533 custom_kernel.cc:58] Succeed in registering kernel [cross_entropy_with_softmax:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180315 4097533 custom_kernel.cc:58] Succeed in registering kernel [greater_equal:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180320 4097533 custom_kernel.cc:58] Succeed in registering kernel [greater_equal:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.180323 4097533 custom_kernel.cc:58] Succeed in registering kernel [greater_equal:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180328 4097533 custom_kernel.cc:58] Succeed in registering kernel [greater_equal:(mlu, Undefined(AnyLayout), int16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180334 4097533 custom_kernel.cc:58] Succeed in registering kernel [greater_equal:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.180341 4097533 custom_kernel.cc:58] Succeed in registering kernel [greater_equal:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180351 4097533 custom_kernel.cc:58] Succeed in registering kernel [generate_proposals:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180364 4097533 custom_kernel.cc:58] Succeed in registering kernel [relu_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180368 4097533 custom_kernel.cc:58] Succeed in registering kernel [relu_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180375 4097533 custom_kernel.cc:58] Succeed in registering kernel [bilinear_interp:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180380 4097533 custom_kernel.cc:58] Succeed in registering kernel [bilinear_interp:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180392 4097533 custom_kernel.cc:58] Succeed in registering kernel [sigmoid:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180397 4097533 custom_kernel.cc:58] Succeed in registering kernel [sigmoid:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180403 4097533 custom_kernel.cc:58] Succeed in registering kernel [not_equal_raw:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180408 4097533 custom_kernel.cc:58] Succeed in registering kernel [not_equal_raw:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.180413 4097533 custom_kernel.cc:58] Succeed in registering kernel [not_equal_raw:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180418 4097533 custom_kernel.cc:58] Succeed in registering kernel [not_equal_raw:(mlu, Undefined(AnyLayout), int16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180423 4097533 custom_kernel.cc:58] Succeed in registering kernel [not_equal_raw:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.180428 4097533 custom_kernel.cc:58] Succeed in registering kernel [not_equal_raw:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180450 4097533 custom_kernel.cc:58] Succeed in registering kernel [nearest_interp_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180454 4097533 custom_kernel.cc:58] Succeed in registering kernel [nearest_interp_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180460 4097533 custom_kernel.cc:58] Succeed in registering kernel [split_with_num:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180469 4097533 custom_kernel.cc:58] Succeed in registering kernel [split_with_num:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.180472 4097533 custom_kernel.cc:58] Succeed in registering kernel [split_with_num:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180480 4097533 custom_kernel.cc:58] Succeed in registering kernel [split_with_num:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.180485 4097533 custom_kernel.cc:58] Succeed in registering kernel [split_with_num:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180513 4097533 custom_kernel.cc:58] Succeed in registering kernel [matmul_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180517 4097533 custom_kernel.cc:58] Succeed in registering kernel [matmul_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180527 4097533 custom_kernel.cc:58] Succeed in registering kernel [matmul_with_flatten:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180531 4097533 custom_kernel.cc:58] Succeed in registering kernel [matmul_with_flatten:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180544 4097533 custom_kernel.cc:58] Succeed in registering kernel [floor:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180548 4097533 custom_kernel.cc:58] Succeed in registering kernel [floor:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180554 4097533 custom_kernel.cc:58] Succeed in registering kernel [set_value_with_tensor:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180560 4097533 custom_kernel.cc:58] Succeed in registering kernel [set_value_with_tensor:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.180567 4097533 custom_kernel.cc:58] Succeed in registering kernel [set_value_with_tensor:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.180572 4097533 custom_kernel.cc:58] Succeed in registering kernel [set_value_with_tensor:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180579 4097533 custom_kernel.cc:58] Succeed in registering kernel [gelu:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180583 4097533 custom_kernel.cc:58] Succeed in registering kernel [gelu:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180604 4097533 custom_kernel.cc:58] Succeed in registering kernel [silu:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180608 4097533 custom_kernel.cc:58] Succeed in registering kernel [silu:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180629 4097533 custom_kernel.cc:58] Succeed in registering kernel [sigmoid_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180634 4097533 custom_kernel.cc:58] Succeed in registering kernel [sigmoid_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180639 4097533 custom_kernel.cc:58] Succeed in registering kernel [transpose_grad:(mlu, Undefined(AnyLayout), uint8)] to Paddle. It will be used like native ones.
I1018 08:53:50.180646 4097533 custom_kernel.cc:58] Succeed in registering kernel [transpose_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180652 4097533 custom_kernel.cc:58] Succeed in registering kernel [transpose_grad:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180658 4097533 custom_kernel.cc:58] Succeed in registering kernel [transpose_grad:(mlu, Undefined(AnyLayout), int16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180665 4097533 custom_kernel.cc:58] Succeed in registering kernel [transpose_grad:(mlu, Undefined(AnyLayout), int8)] to Paddle. It will be used like native ones.
I1018 08:53:50.180670 4097533 custom_kernel.cc:58] Succeed in registering kernel [transpose_grad:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.180676 4097533 custom_kernel.cc:58] Succeed in registering kernel [transpose_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180682 4097533 custom_kernel.cc:58] Succeed in registering kernel [divide:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180689 4097533 custom_kernel.cc:58] Succeed in registering kernel [divide:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180698 4097533 custom_kernel.cc:58] Succeed in registering kernel [layer_norm_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180702 4097533 custom_kernel.cc:58] Succeed in registering kernel [layer_norm_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180711 4097533 custom_kernel.cc:58] Succeed in registering kernel [uniform_raw:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180719 4097533 custom_kernel.cc:58] Succeed in registering kernel [strided_slice:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180727 4097533 custom_kernel.cc:58] Succeed in registering kernel [strided_slice:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.180732 4097533 custom_kernel.cc:58] Succeed in registering kernel [strided_slice:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180737 4097533 custom_kernel.cc:58] Succeed in registering kernel [strided_slice:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.180743 4097533 custom_kernel.cc:58] Succeed in registering kernel [strided_slice:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180763 4097533 custom_kernel.cc:58] Succeed in registering kernel [hardswish_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180766 4097533 custom_kernel.cc:58] Succeed in registering kernel [hardswish_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180773 4097533 custom_kernel.cc:58] Succeed in registering kernel [square:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180779 4097533 custom_kernel.cc:58] Succeed in registering kernel [square:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180786 4097533 custom_kernel.cc:58] Succeed in registering kernel [bce_loss_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180790 4097533 custom_kernel.cc:58] Succeed in registering kernel [bce_loss_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180801 4097533 custom_kernel.cc:58] Succeed in registering kernel [log_softmax_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180805 4097533 custom_kernel.cc:58] Succeed in registering kernel [log_softmax_grad:(mlu, Undefined(AnyLayout), float64)] to Paddle. It will be used like native ones.
I1018 08:53:50.180811 4097533 custom_kernel.cc:58] Succeed in registering kernel [log_softmax_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180819 4097533 custom_kernel.cc:58] Succeed in registering kernel [flash_attn_unpadded_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180827 4097533 custom_kernel.cc:58] Succeed in registering kernel [roi_align_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180850 4097533 custom_kernel.cc:58] Succeed in registering kernel [pow:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180853 4097533 custom_kernel.cc:58] Succeed in registering kernel [pow:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180859 4097533 custom_kernel.cc:58] Succeed in registering kernel [less_equal_raw:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180866 4097533 custom_kernel.cc:58] Succeed in registering kernel [less_equal_raw:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.180869 4097533 custom_kernel.cc:58] Succeed in registering kernel [less_equal_raw:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180876 4097533 custom_kernel.cc:58] Succeed in registering kernel [less_equal_raw:(mlu, Undefined(AnyLayout), int16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180884 4097533 custom_kernel.cc:58] Succeed in registering kernel [less_equal_raw:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.180889 4097533 custom_kernel.cc:58] Succeed in registering kernel [less_equal_raw:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180896 4097533 custom_kernel.cc:58] Succeed in registering kernel [logical_and:(mlu, Undefined(AnyLayout), uint8)] to Paddle. It will be used like native ones.
I1018 08:53:50.180902 4097533 custom_kernel.cc:58] Succeed in registering kernel [logical_and:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180907 4097533 custom_kernel.cc:58] Succeed in registering kernel [logical_and:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.180910 4097533 custom_kernel.cc:58] Succeed in registering kernel [logical_and:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180915 4097533 custom_kernel.cc:58] Succeed in registering kernel [logical_and:(mlu, Undefined(AnyLayout), int16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180919 4097533 custom_kernel.cc:58] Succeed in registering kernel [logical_and:(mlu, Undefined(AnyLayout), int8)] to Paddle. It will be used like native ones.
I1018 08:53:50.180924 4097533 custom_kernel.cc:58] Succeed in registering kernel [logical_and:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.180971 4097533 custom_kernel.cc:58] Succeed in registering kernel [logical_and:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180981 4097533 custom_kernel.cc:58] Succeed in registering kernel [label_smooth:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180985 4097533 custom_kernel.cc:58] Succeed in registering kernel [label_smooth:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.180994 4097533 custom_kernel.cc:58] Succeed in registering kernel [merged_momentum:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.180999 4097533 custom_kernel.cc:58] Succeed in registering kernel [merged_momentum:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181003 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy_d2h_multi_io:(mlu, Undefined(AnyLayout), uint8)] to Paddle. It will be used like native ones.
I1018 08:53:50.181010 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy_d2h_multi_io:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.181012 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy_d2h_multi_io:(mlu, Undefined(AnyLayout), complex128)] to Paddle. It will be used like native ones.
I1018 08:53:50.181018 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy_d2h_multi_io:(mlu, Undefined(AnyLayout), float64)] to Paddle. It will be used like native ones.
I1018 08:53:50.181066 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy_d2h_multi_io:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.181069 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy_d2h_multi_io:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181073 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy_d2h_multi_io:(mlu, Undefined(AnyLayout), int16)] to Paddle. It will be used like native ones.
I1018 08:53:50.181077 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy_d2h_multi_io:(mlu, Undefined(AnyLayout), int8)] to Paddle. It will be used like native ones.
I1018 08:53:50.181080 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy_d2h_multi_io:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.181087 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy_d2h_multi_io:(mlu, Undefined(AnyLayout), bfloat16)] to Paddle. It will be used like native ones.
I1018 08:53:50.181090 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy_d2h_multi_io:(mlu, Undefined(AnyLayout), complex64)] to Paddle. It will be used like native ones.
I1018 08:53:50.181094 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy_d2h_multi_io:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181100 4097533 custom_kernel.cc:58] Succeed in registering kernel [maximum_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.181106 4097533 custom_kernel.cc:58] Succeed in registering kernel [maximum_grad:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181113 4097533 custom_kernel.cc:58] Succeed in registering kernel [maximum_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181119 4097533 custom_kernel.cc:58] Succeed in registering kernel [expand:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.181125 4097533 custom_kernel.cc:58] Succeed in registering kernel [expand:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.181130 4097533 custom_kernel.cc:58] Succeed in registering kernel [expand:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181138 4097533 custom_kernel.cc:58] Succeed in registering kernel [expand:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.181195 4097533 custom_kernel.cc:58] Succeed in registering kernel [expand:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181202 4097533 custom_kernel.cc:58] Succeed in registering kernel [numel:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.181231 4097533 custom_kernel.cc:58] Succeed in registering kernel [numel:(mlu, Undefined(AnyLayout), float64)] to Paddle. It will be used like native ones.
I1018 08:53:50.181236 4097533 custom_kernel.cc:58] Succeed in registering kernel [numel:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.181242 4097533 custom_kernel.cc:58] Succeed in registering kernel [numel:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181248 4097533 custom_kernel.cc:58] Succeed in registering kernel [numel:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.181254 4097533 custom_kernel.cc:58] Succeed in registering kernel [numel:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181260 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy:(mlu, Undefined(AnyLayout), uint8)] to Paddle. It will be used like native ones.
I1018 08:53:50.181264 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.181270 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy:(mlu, Undefined(AnyLayout), complex128)] to Paddle. It will be used like native ones.
I1018 08:53:50.181274 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy:(mlu, Undefined(AnyLayout), float64)] to Paddle. It will be used like native ones.
I1018 08:53:50.181326 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.181330 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181337 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy:(mlu, Undefined(AnyLayout), int16)] to Paddle. It will be used like native ones.
I1018 08:53:50.181341 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy:(mlu, Undefined(AnyLayout), int8)] to Paddle. It will be used like native ones.
I1018 08:53:50.181344 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.181349 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy:(mlu, Undefined(AnyLayout), bfloat16)] to Paddle. It will be used like native ones.
I1018 08:53:50.181351 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy:(mlu, Undefined(AnyLayout), complex64)] to Paddle. It will be used like native ones.
I1018 08:53:50.181355 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181363 4097533 custom_kernel.cc:58] Succeed in registering kernel [print_kernel:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.181368 4097533 custom_kernel.cc:58] Succeed in registering kernel [print_kernel:(mlu, Undefined(AnyLayout), float64)] to Paddle. It will be used like native ones.
I1018 08:53:50.181372 4097533 custom_kernel.cc:58] Succeed in registering kernel [print_kernel:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.181378 4097533 custom_kernel.cc:58] Succeed in registering kernel [print_kernel:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181385 4097533 custom_kernel.cc:58] Succeed in registering kernel [print_kernel:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.181391 4097533 custom_kernel.cc:58] Succeed in registering kernel [print_kernel:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181399 4097533 custom_kernel.cc:58] Succeed in registering kernel [equal:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.181407 4097533 custom_kernel.cc:58] Succeed in registering kernel [equal:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.181411 4097533 custom_kernel.cc:58] Succeed in registering kernel [equal:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181414 4097533 custom_kernel.cc:58] Succeed in registering kernel [equal:(mlu, Undefined(AnyLayout), int16)] to Paddle. It will be used like native ones.
I1018 08:53:50.181422 4097533 custom_kernel.cc:58] Succeed in registering kernel [equal:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.181427 4097533 custom_kernel.cc:58] Succeed in registering kernel [equal:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181437 4097533 custom_kernel.cc:58] Succeed in registering kernel [layer_norm:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.181442 4097533 custom_kernel.cc:58] Succeed in registering kernel [layer_norm:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181449 4097533 custom_kernel.cc:58] Succeed in registering kernel [less_equal:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.181454 4097533 custom_kernel.cc:58] Succeed in registering kernel [less_equal:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.181458 4097533 custom_kernel.cc:58] Succeed in registering kernel [less_equal:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181463 4097533 custom_kernel.cc:58] Succeed in registering kernel [less_equal:(mlu, Undefined(AnyLayout), int16)] to Paddle. It will be used like native ones.
I1018 08:53:50.181473 4097533 custom_kernel.cc:58] Succeed in registering kernel [less_equal:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.181478 4097533 custom_kernel.cc:58] Succeed in registering kernel [less_equal:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181488 4097533 custom_kernel.cc:58] Succeed in registering kernel [kldiv_loss_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.181491 4097533 custom_kernel.cc:58] Succeed in registering kernel [kldiv_loss_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181497 4097533 custom_kernel.cc:58] Succeed in registering kernel [unsqueeze_infer:(mlu, Undefined(AnyLayout), uint8)] to Paddle. It will be used like native ones.
I1018 08:53:50.181504 4097533 custom_kernel.cc:58] Succeed in registering kernel [unsqueeze_infer:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.181509 4097533 custom_kernel.cc:58] Succeed in registering kernel [unsqueeze_infer:(mlu, Undefined(AnyLayout), complex128)] to Paddle. It will be used like native ones.
I1018 08:53:50.181515 4097533 custom_kernel.cc:58] Succeed in registering kernel [unsqueeze_infer:(mlu, Undefined(AnyLayout), float64)] to Paddle. It will be used like native ones.
I1018 08:53:50.181520 4097533 custom_kernel.cc:58] Succeed in registering kernel [unsqueeze_infer:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.181526 4097533 custom_kernel.cc:58] Succeed in registering kernel [unsqueeze_infer:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181531 4097533 custom_kernel.cc:58] Succeed in registering kernel [unsqueeze_infer:(mlu, Undefined(AnyLayout), int16)] to Paddle. It will be used like native ones.
I1018 08:53:50.181538 4097533 custom_kernel.cc:58] Succeed in registering kernel [unsqueeze_infer:(mlu, Undefined(AnyLayout), int8)] to Paddle. It will be used like native ones.
I1018 08:53:50.181545 4097533 custom_kernel.cc:58] Succeed in registering kernel [unsqueeze_infer:(mlu, Undefined(AnyLayout), bfloat16)] to Paddle. It will be used like native ones.
I1018 08:53:50.181550 4097533 custom_kernel.cc:58] Succeed in registering kernel [unsqueeze_infer:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.181556 4097533 custom_kernel.cc:58] Succeed in registering kernel [unsqueeze_infer:(mlu, Undefined(AnyLayout), complex64)] to Paddle. It will be used like native ones.
I1018 08:53:50.181563 4097533 custom_kernel.cc:58] Succeed in registering kernel [unsqueeze_infer:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181573 4097533 custom_kernel.cc:58] Succeed in registering kernel [rnn:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181583 4097533 custom_kernel.cc:58] Succeed in registering kernel [depthwise_conv2d_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.181588 4097533 custom_kernel.cc:58] Succeed in registering kernel [depthwise_conv2d_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181594 4097533 custom_kernel.cc:58] Succeed in registering kernel [concat_grad:(mlu, Undefined(AnyLayout), uint8)] to Paddle. It will be used like native ones.
I1018 08:53:50.181599 4097533 custom_kernel.cc:58] Succeed in registering kernel [concat_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.181605 4097533 custom_kernel.cc:58] Succeed in registering kernel [concat_grad:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.181610 4097533 custom_kernel.cc:58] Succeed in registering kernel [concat_grad:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181619 4097533 custom_kernel.cc:58] Succeed in registering kernel [concat_grad:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.181635 4097533 custom_kernel.cc:58] Succeed in registering kernel [concat_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181641 4097533 custom_kernel.cc:58] Succeed in registering kernel [sync_batch_norm:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.181645 4097533 custom_kernel.cc:58] Succeed in registering kernel [sync_batch_norm:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181653 4097533 custom_kernel.cc:58] Succeed in registering kernel [sum_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.181659 4097533 custom_kernel.cc:58] Succeed in registering kernel [sum_grad:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.181663 4097533 custom_kernel.cc:58] Succeed in registering kernel [sum_grad:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181670 4097533 custom_kernel.cc:58] Succeed in registering kernel [sum_grad:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.181676 4097533 custom_kernel.cc:58] Succeed in registering kernel [sum_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181684 4097533 custom_kernel.cc:58] Succeed in registering kernel [scale:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.181689 4097533 custom_kernel.cc:58] Succeed in registering kernel [scale:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181700 4097533 custom_kernel.cc:58] Succeed in registering kernel [leaky_relu:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.181704 4097533 custom_kernel.cc:58] Succeed in registering kernel [leaky_relu:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181713 4097533 custom_kernel.cc:58] Succeed in registering kernel [adamw:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.181717 4097533 custom_kernel.cc:58] Succeed in registering kernel [adamw:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181723 4097533 custom_kernel.cc:58] Succeed in registering kernel [elementwise_pow:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.181730 4097533 custom_kernel.cc:58] Succeed in registering kernel [elementwise_pow:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181740 4097533 custom_kernel.cc:58] Succeed in registering kernel [prior_box:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181749 4097533 custom_kernel.cc:58] Succeed in registering kernel [p_norm:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.181753 4097533 custom_kernel.cc:58] Succeed in registering kernel [p_norm:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181759 4097533 custom_kernel.cc:58] Succeed in registering kernel [add_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.181766 4097533 custom_kernel.cc:58] Succeed in registering kernel [add_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181772 4097533 custom_kernel.cc:58] Succeed in registering kernel [squeeze_infer:(mlu, Undefined(AnyLayout), uint8)] to Paddle. It will be used like native ones.
I1018 08:53:50.181782 4097533 custom_kernel.cc:58] Succeed in registering kernel [squeeze_infer:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.181787 4097533 custom_kernel.cc:58] Succeed in registering kernel [squeeze_infer:(mlu, Undefined(AnyLayout), float64)] to Paddle. It will be used like native ones.
I1018 08:53:50.181794 4097533 custom_kernel.cc:58] Succeed in registering kernel [squeeze_infer:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.181799 4097533 custom_kernel.cc:58] Succeed in registering kernel [squeeze_infer:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181805 4097533 custom_kernel.cc:58] Succeed in registering kernel [squeeze_infer:(mlu, Undefined(AnyLayout), int8)] to Paddle. It will be used like native ones.
I1018 08:53:50.181811 4097533 custom_kernel.cc:58] Succeed in registering kernel [squeeze_infer:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.181816 4097533 custom_kernel.cc:58] Succeed in registering kernel [squeeze_infer:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181828 4097533 custom_kernel.cc:58] Succeed in registering kernel [bmm_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.181833 4097533 custom_kernel.cc:58] Succeed in registering kernel [bmm_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181846 4097533 custom_kernel.cc:58] Succeed in registering kernel [log10:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.181850 4097533 custom_kernel.cc:58] Succeed in registering kernel [log10:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181859 4097533 custom_kernel.cc:58] Succeed in registering kernel [set_value:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181865 4097533 custom_kernel.cc:58] Succeed in registering kernel [set_value:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181871 4097533 custom_kernel.cc:58] Succeed in registering kernel [bitwise_xor:(mlu, Undefined(AnyLayout), uint8)] to Paddle. It will be used like native ones.
I1018 08:53:50.181876 4097533 custom_kernel.cc:58] Succeed in registering kernel [bitwise_xor:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181887 4097533 custom_kernel.cc:58] Succeed in registering kernel [bitwise_xor:(mlu, Undefined(AnyLayout), int16)] to Paddle. It will be used like native ones.
I1018 08:53:50.181891 4097533 custom_kernel.cc:58] Succeed in registering kernel [bitwise_xor:(mlu, Undefined(AnyLayout), int8)] to Paddle. It will be used like native ones.
I1018 08:53:50.181895 4097533 custom_kernel.cc:58] Succeed in registering kernel [bitwise_xor:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.181924 4097533 custom_kernel.cc:58] Succeed in registering kernel [tril_triu_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.181928 4097533 custom_kernel.cc:58] Succeed in registering kernel [tril_triu_grad:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181932 4097533 custom_kernel.cc:58] Succeed in registering kernel [tril_triu_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181939 4097533 custom_kernel.cc:58] Succeed in registering kernel [greater_equal_raw:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.181946 4097533 custom_kernel.cc:58] Succeed in registering kernel [greater_equal_raw:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.181949 4097533 custom_kernel.cc:58] Succeed in registering kernel [greater_equal_raw:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181959 4097533 custom_kernel.cc:58] Succeed in registering kernel [greater_equal_raw:(mlu, Undefined(AnyLayout), int16)] to Paddle. It will be used like native ones.
I1018 08:53:50.181964 4097533 custom_kernel.cc:58] Succeed in registering kernel [greater_equal_raw:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.181970 4097533 custom_kernel.cc:58] Succeed in registering kernel [greater_equal_raw:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181977 4097533 custom_kernel.cc:58] Succeed in registering kernel [slice:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.181982 4097533 custom_kernel.cc:58] Succeed in registering kernel [slice:(mlu, Undefined(AnyLayout), float64)] to Paddle. It will be used like native ones.
I1018 08:53:50.181988 4097533 custom_kernel.cc:58] Succeed in registering kernel [slice:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.181993 4097533 custom_kernel.cc:58] Succeed in registering kernel [slice:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.181998 4097533 custom_kernel.cc:58] Succeed in registering kernel [slice:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.182004 4097533 custom_kernel.cc:58] Succeed in registering kernel [slice:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182010 4097533 custom_kernel.cc:58] Succeed in registering kernel [full_like:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182016 4097533 custom_kernel.cc:58] Succeed in registering kernel [full_like:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182020 4097533 custom_kernel.cc:58] Succeed in registering kernel [full_like:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.182026 4097533 custom_kernel.cc:58] Succeed in registering kernel [full_like:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182039 4097533 custom_kernel.cc:58] Succeed in registering kernel [meshgrid:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182042 4097533 custom_kernel.cc:58] Succeed in registering kernel [meshgrid:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182046 4097533 custom_kernel.cc:58] Succeed in registering kernel [meshgrid:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.182049 4097533 custom_kernel.cc:58] Succeed in registering kernel [meshgrid:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182061 4097533 custom_kernel.cc:58] Succeed in registering kernel [sin:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182065 4097533 custom_kernel.cc:58] Succeed in registering kernel [sin:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182072 4097533 custom_kernel.cc:58] Succeed in registering kernel [add:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182080 4097533 custom_kernel.cc:58] Succeed in registering kernel [add:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182086 4097533 custom_kernel.cc:58] Succeed in registering kernel [add_raw:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182091 4097533 custom_kernel.cc:58] Succeed in registering kernel [add_raw:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182106 4097533 custom_kernel.cc:58] Succeed in registering kernel [reciprocal_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182109 4097533 custom_kernel.cc:58] Succeed in registering kernel [reciprocal_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182116 4097533 custom_kernel.cc:58] Succeed in registering kernel [mean_all_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182121 4097533 custom_kernel.cc:58] Succeed in registering kernel [mean_all_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182132 4097533 custom_kernel.cc:58] Succeed in registering kernel [min_raw:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182135 4097533 custom_kernel.cc:58] Succeed in registering kernel [min_raw:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182139 4097533 custom_kernel.cc:58] Succeed in registering kernel [min_raw:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182161 4097533 custom_kernel.cc:58] Succeed in registering kernel [cos:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182165 4097533 custom_kernel.cc:58] Succeed in registering kernel [cos:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182171 4097533 custom_kernel.cc:58] Succeed in registering kernel [embedding_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182178 4097533 custom_kernel.cc:58] Succeed in registering kernel [embedding_grad:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182204 4097533 custom_kernel.cc:58] Succeed in registering kernel [embedding_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182210 4097533 custom_kernel.cc:58] Succeed in registering kernel [masked_select_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182216 4097533 custom_kernel.cc:58] Succeed in registering kernel [masked_select_grad:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182222 4097533 custom_kernel.cc:58] Succeed in registering kernel [masked_select_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182256 4097533 custom_kernel.cc:58] Succeed in registering kernel [assign_value:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182260 4097533 custom_kernel.cc:58] Succeed in registering kernel [assign_value:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.182264 4097533 custom_kernel.cc:58] Succeed in registering kernel [assign_value:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.182267 4097533 custom_kernel.cc:58] Succeed in registering kernel [assign_value:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182277 4097533 custom_kernel.cc:58] Succeed in registering kernel [bilinear_interp_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182281 4097533 custom_kernel.cc:58] Succeed in registering kernel [bilinear_interp_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182315 4097533 custom_kernel.cc:58] Succeed in registering kernel [log2:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182319 4097533 custom_kernel.cc:58] Succeed in registering kernel [log2:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182332 4097533 custom_kernel.cc:58] Succeed in registering kernel [tanh:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182335 4097533 custom_kernel.cc:58] Succeed in registering kernel [tanh:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182348 4097533 custom_kernel.cc:58] Succeed in registering kernel [prod:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182351 4097533 custom_kernel.cc:58] Succeed in registering kernel [prod:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182355 4097533 custom_kernel.cc:58] Succeed in registering kernel [prod:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182364 4097533 custom_kernel.cc:58] Succeed in registering kernel [accuracy:(mlu, Undefined(AnyLayout), uint8)] to Paddle. It will be used like native ones.
I1018 08:53:50.182368 4097533 custom_kernel.cc:58] Succeed in registering kernel [accuracy:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182372 4097533 custom_kernel.cc:58] Succeed in registering kernel [accuracy:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.182376 4097533 custom_kernel.cc:58] Succeed in registering kernel [accuracy:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182380 4097533 custom_kernel.cc:58] Succeed in registering kernel [accuracy:(mlu, Undefined(AnyLayout), int16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182384 4097533 custom_kernel.cc:58] Succeed in registering kernel [accuracy:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182390 4097533 custom_kernel.cc:58] Succeed in registering kernel [less_than:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182395 4097533 custom_kernel.cc:58] Succeed in registering kernel [less_than:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.182401 4097533 custom_kernel.cc:58] Succeed in registering kernel [less_than:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182405 4097533 custom_kernel.cc:58] Succeed in registering kernel [less_than:(mlu, Undefined(AnyLayout), int16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182412 4097533 custom_kernel.cc:58] Succeed in registering kernel [less_than:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.182417 4097533 custom_kernel.cc:58] Succeed in registering kernel [less_than:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182430 4097533 custom_kernel.cc:58] Succeed in registering kernel [atan:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182435 4097533 custom_kernel.cc:58] Succeed in registering kernel [atan:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182444 4097533 custom_kernel.cc:58] Succeed in registering kernel [gelu_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182447 4097533 custom_kernel.cc:58] Succeed in registering kernel [gelu_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182453 4097533 custom_kernel.cc:58] Succeed in registering kernel [unsqueeze:(mlu, Undefined(AnyLayout), uint8)] to Paddle. It will be used like native ones.
I1018 08:53:50.182458 4097533 custom_kernel.cc:58] Succeed in registering kernel [unsqueeze:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182463 4097533 custom_kernel.cc:58] Succeed in registering kernel [unsqueeze:(mlu, Undefined(AnyLayout), complex128)] to Paddle. It will be used like native ones.
I1018 08:53:50.182471 4097533 custom_kernel.cc:58] Succeed in registering kernel [unsqueeze:(mlu, Undefined(AnyLayout), float64)] to Paddle. It will be used like native ones.
I1018 08:53:50.182476 4097533 custom_kernel.cc:58] Succeed in registering kernel [unsqueeze:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.182482 4097533 custom_kernel.cc:58] Succeed in registering kernel [unsqueeze:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182487 4097533 custom_kernel.cc:58] Succeed in registering kernel [unsqueeze:(mlu, Undefined(AnyLayout), int16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182493 4097533 custom_kernel.cc:58] Succeed in registering kernel [unsqueeze:(mlu, Undefined(AnyLayout), int8)] to Paddle. It will be used like native ones.
I1018 08:53:50.182499 4097533 custom_kernel.cc:58] Succeed in registering kernel [unsqueeze:(mlu, Undefined(AnyLayout), bfloat16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182503 4097533 custom_kernel.cc:58] Succeed in registering kernel [unsqueeze:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.182510 4097533 custom_kernel.cc:58] Succeed in registering kernel [unsqueeze:(mlu, Undefined(AnyLayout), complex64)] to Paddle. It will be used like native ones.
I1018 08:53:50.182515 4097533 custom_kernel.cc:58] Succeed in registering kernel [unsqueeze:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182536 4097533 custom_kernel.cc:58] Succeed in registering kernel [argsort_grad:(mlu, Undefined(AnyLayout), uint8)] to Paddle. It will be used like native ones.
I1018 08:53:50.182540 4097533 custom_kernel.cc:58] Succeed in registering kernel [argsort_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182544 4097533 custom_kernel.cc:58] Succeed in registering kernel [argsort_grad:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182547 4097533 custom_kernel.cc:58] Succeed in registering kernel [argsort_grad:(mlu, Undefined(AnyLayout), int16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182569 4097533 custom_kernel.cc:58] Succeed in registering kernel [argsort_grad:(mlu, Undefined(AnyLayout), int8)] to Paddle. It will be used like native ones.
I1018 08:53:50.182571 4097533 custom_kernel.cc:58] Succeed in registering kernel [argsort_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182579 4097533 custom_kernel.cc:58] Succeed in registering kernel [merged_adam:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182582 4097533 custom_kernel.cc:58] Succeed in registering kernel [merged_adam:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182592 4097533 custom_kernel.cc:58] Succeed in registering kernel [log_softmax:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182596 4097533 custom_kernel.cc:58] Succeed in registering kernel [log_softmax:(mlu, Undefined(AnyLayout), float64)] to Paddle. It will be used like native ones.
I1018 08:53:50.182600 4097533 custom_kernel.cc:58] Succeed in registering kernel [log_softmax:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182608 4097533 custom_kernel.cc:58] Succeed in registering kernel [cast:(mlu, Undefined(AnyLayout), uint8)] to Paddle. It will be used like native ones.
I1018 08:53:50.182612 4097533 custom_kernel.cc:58] Succeed in registering kernel [cast:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182617 4097533 custom_kernel.cc:58] Succeed in registering kernel [cast:(mlu, Undefined(AnyLayout), float64)] to Paddle. It will be used like native ones.
I1018 08:53:50.182622 4097533 custom_kernel.cc:58] Succeed in registering kernel [cast:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.182675 4097533 custom_kernel.cc:58] Succeed in registering kernel [cast:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182679 4097533 custom_kernel.cc:58] Succeed in registering kernel [cast:(mlu, Undefined(AnyLayout), int16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182682 4097533 custom_kernel.cc:58] Succeed in registering kernel [cast:(mlu, Undefined(AnyLayout), int8)] to Paddle. It will be used like native ones.
I1018 08:53:50.182686 4097533 custom_kernel.cc:58] Succeed in registering kernel [cast:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.182690 4097533 custom_kernel.cc:58] Succeed in registering kernel [cast:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182700 4097533 custom_kernel.cc:58] Succeed in registering kernel [matmul_with_flatten_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182704 4097533 custom_kernel.cc:58] Succeed in registering kernel [matmul_with_flatten_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182710 4097533 custom_kernel.cc:58] Succeed in registering kernel [topk:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182718 4097533 custom_kernel.cc:58] Succeed in registering kernel [topk:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182727 4097533 custom_kernel.cc:58] Succeed in registering kernel [sqrt_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182731 4097533 custom_kernel.cc:58] Succeed in registering kernel [sqrt_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182740 4097533 custom_kernel.cc:58] Succeed in registering kernel [hardsigmoid:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182745 4097533 custom_kernel.cc:58] Succeed in registering kernel [hardsigmoid:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182757 4097533 custom_kernel.cc:58] Succeed in registering kernel [tril:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182761 4097533 custom_kernel.cc:58] Succeed in registering kernel [tril:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182765 4097533 custom_kernel.cc:58] Succeed in registering kernel [tril:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.182770 4097533 custom_kernel.cc:58] Succeed in registering kernel [tril:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182780 4097533 custom_kernel.cc:58] Succeed in registering kernel [argsort:(mlu, Undefined(AnyLayout), uint8)] to Paddle. It will be used like native ones.
I1018 08:53:50.182785 4097533 custom_kernel.cc:58] Succeed in registering kernel [argsort:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182788 4097533 custom_kernel.cc:58] Succeed in registering kernel [argsort:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182792 4097533 custom_kernel.cc:58] Succeed in registering kernel [argsort:(mlu, Undefined(AnyLayout), int16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182816 4097533 custom_kernel.cc:58] Succeed in registering kernel [argsort:(mlu, Undefined(AnyLayout), int8)] to Paddle. It will be used like native ones.
I1018 08:53:50.182818 4097533 custom_kernel.cc:58] Succeed in registering kernel [argsort:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182832 4097533 custom_kernel.cc:58] Succeed in registering kernel [sgd:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182840 4097533 custom_kernel.cc:58] Succeed in registering kernel [sgd:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182860 4097533 custom_kernel.cc:58] Succeed in registering kernel [clip:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182864 4097533 custom_kernel.cc:58] Succeed in registering kernel [clip:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182874 4097533 custom_kernel.cc:58] Succeed in registering kernel [conv2d_transpose:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182878 4097533 custom_kernel.cc:58] Succeed in registering kernel [conv2d_transpose:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182884 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy_d2h:(mlu, Undefined(AnyLayout), uint8)] to Paddle. It will be used like native ones.
I1018 08:53:50.182888 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy_d2h:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182893 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy_d2h:(mlu, Undefined(AnyLayout), complex128)] to Paddle. It will be used like native ones.
I1018 08:53:50.182897 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy_d2h:(mlu, Undefined(AnyLayout), float64)] to Paddle. It will be used like native ones.
I1018 08:53:50.182950 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy_d2h:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.182953 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy_d2h:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182956 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy_d2h:(mlu, Undefined(AnyLayout), int16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182960 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy_d2h:(mlu, Undefined(AnyLayout), int8)] to Paddle. It will be used like native ones.
I1018 08:53:50.182963 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy_d2h:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.182967 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy_d2h:(mlu, Undefined(AnyLayout), bfloat16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182971 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy_d2h:(mlu, Undefined(AnyLayout), complex64)] to Paddle. It will be used like native ones.
I1018 08:53:50.182974 4097533 custom_kernel.cc:58] Succeed in registering kernel [memcpy_d2h:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182981 4097533 custom_kernel.cc:58] Succeed in registering kernel [mean_all:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.182987 4097533 custom_kernel.cc:58] Succeed in registering kernel [mean_all:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.182993 4097533 custom_kernel.cc:58] Succeed in registering kernel [transpose:(mlu, Undefined(AnyLayout), uint8)] to Paddle. It will be used like native ones.
I1018 08:53:50.182998 4097533 custom_kernel.cc:58] Succeed in registering kernel [transpose:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.183004 4097533 custom_kernel.cc:58] Succeed in registering kernel [transpose:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183009 4097533 custom_kernel.cc:58] Succeed in registering kernel [transpose:(mlu, Undefined(AnyLayout), int16)] to Paddle. It will be used like native ones.
I1018 08:53:50.183018 4097533 custom_kernel.cc:58] Succeed in registering kernel [transpose:(mlu, Undefined(AnyLayout), int8)] to Paddle. It will be used like native ones.
I1018 08:53:50.183023 4097533 custom_kernel.cc:58] Succeed in registering kernel [transpose:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.183029 4097533 custom_kernel.cc:58] Succeed in registering kernel [transpose:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183040 4097533 custom_kernel.cc:58] Succeed in registering kernel [reciprocal:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.183044 4097533 custom_kernel.cc:58] Succeed in registering kernel [reciprocal:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183073 4097533 custom_kernel.cc:58] Succeed in registering kernel [multiply_raw:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.183077 4097533 custom_kernel.cc:58] Succeed in registering kernel [multiply_raw:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183080 4097533 custom_kernel.cc:58] Succeed in registering kernel [multiply_raw:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183089 4097533 custom_kernel.cc:58] Succeed in registering kernel [abs:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.183094 4097533 custom_kernel.cc:58] Succeed in registering kernel [abs:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183104 4097533 custom_kernel.cc:58] Succeed in registering kernel [softmax_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.183109 4097533 custom_kernel.cc:58] Succeed in registering kernel [softmax_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183115 4097533 custom_kernel.cc:58] Succeed in registering kernel [minimum_raw:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.183121 4097533 custom_kernel.cc:58] Succeed in registering kernel [minimum_raw:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183127 4097533 custom_kernel.cc:58] Succeed in registering kernel [minimum_raw:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183137 4097533 custom_kernel.cc:58] Succeed in registering kernel [truncated_gaussian_random:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183146 4097533 custom_kernel.cc:58] Succeed in registering kernel [kldiv_loss:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.183151 4097533 custom_kernel.cc:58] Succeed in registering kernel [kldiv_loss:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183157 4097533 custom_kernel.cc:58] Succeed in registering kernel [cumsum:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.183162 4097533 custom_kernel.cc:58] Succeed in registering kernel [cumsum:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183171 4097533 custom_kernel.cc:58] Succeed in registering kernel [cumsum:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183176 4097533 custom_kernel.cc:58] Succeed in registering kernel [slice_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.183182 4097533 custom_kernel.cc:58] Succeed in registering kernel [slice_grad:(mlu, Undefined(AnyLayout), float64)] to Paddle. It will be used like native ones.
I1018 08:53:50.183192 4097533 custom_kernel.cc:58] Succeed in registering kernel [slice_grad:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.183197 4097533 custom_kernel.cc:58] Succeed in registering kernel [slice_grad:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183203 4097533 custom_kernel.cc:58] Succeed in registering kernel [slice_grad:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.183208 4097533 custom_kernel.cc:58] Succeed in registering kernel [slice_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183216 4097533 custom_kernel.cc:58] Succeed in registering kernel [sum:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.183223 4097533 custom_kernel.cc:58] Succeed in registering kernel [sum:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.183228 4097533 custom_kernel.cc:58] Succeed in registering kernel [sum:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183235 4097533 custom_kernel.cc:58] Succeed in registering kernel [sum:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.183285 4097533 custom_kernel.cc:58] Succeed in registering kernel [sum:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183296 4097533 custom_kernel.cc:58] Succeed in registering kernel [sin_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.183300 4097533 custom_kernel.cc:58] Succeed in registering kernel [sin_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183306 4097533 custom_kernel.cc:58] Succeed in registering kernel [strided_slice_raw:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.183312 4097533 custom_kernel.cc:58] Succeed in registering kernel [strided_slice_raw:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.183318 4097533 custom_kernel.cc:58] Succeed in registering kernel [strided_slice_raw:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183326 4097533 custom_kernel.cc:58] Succeed in registering kernel [strided_slice_raw:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.183332 4097533 custom_kernel.cc:58] Succeed in registering kernel [strided_slice_raw:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183342 4097533 custom_kernel.cc:58] Succeed in registering kernel [leaky_relu_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.183346 4097533 custom_kernel.cc:58] Succeed in registering kernel [leaky_relu_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183352 4097533 custom_kernel.cc:58] Succeed in registering kernel [assign_raw:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.183359 4097533 custom_kernel.cc:58] Succeed in registering kernel [assign_raw:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.183363 4097533 custom_kernel.cc:58] Succeed in registering kernel [assign_raw:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183370 4097533 custom_kernel.cc:58] Succeed in registering kernel [assign_raw:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.183418 4097533 custom_kernel.cc:58] Succeed in registering kernel [assign_raw:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183423 4097533 custom_kernel.cc:58] Succeed in registering kernel [full_batch_size_like:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.183434 4097533 custom_kernel.cc:58] Succeed in registering kernel [full_batch_size_like:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183439 4097533 custom_kernel.cc:58] Succeed in registering kernel [full_batch_size_like:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183449 4097533 custom_kernel.cc:58] Succeed in registering kernel [mean:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.183459 4097533 custom_kernel.cc:58] Succeed in registering kernel [mean:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183466 4097533 custom_kernel.cc:58] Succeed in registering kernel [assign:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.183471 4097533 custom_kernel.cc:58] Succeed in registering kernel [assign:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.183475 4097533 custom_kernel.cc:58] Succeed in registering kernel [assign:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183481 4097533 custom_kernel.cc:58] Succeed in registering kernel [assign:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.183531 4097533 custom_kernel.cc:58] Succeed in registering kernel [assign:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183537 4097533 custom_kernel.cc:58] Succeed in registering kernel [greater_than_raw:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.183543 4097533 custom_kernel.cc:58] Succeed in registering kernel [greater_than_raw:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.183548 4097533 custom_kernel.cc:58] Succeed in registering kernel [greater_than_raw:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183552 4097533 custom_kernel.cc:58] Succeed in registering kernel [greater_than_raw:(mlu, Undefined(AnyLayout), int16)] to Paddle. It will be used like native ones.
I1018 08:53:50.183557 4097533 custom_kernel.cc:58] Succeed in registering kernel [greater_than_raw:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.183564 4097533 custom_kernel.cc:58] Succeed in registering kernel [greater_than_raw:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183576 4097533 custom_kernel.cc:58] Succeed in registering kernel [tril_triu:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.183580 4097533 custom_kernel.cc:58] Succeed in registering kernel [tril_triu:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183584 4097533 custom_kernel.cc:58] Succeed in registering kernel [tril_triu:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183595 4097533 custom_kernel.cc:58] Succeed in registering kernel [one_hot_raw:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183598 4097533 custom_kernel.cc:58] Succeed in registering kernel [one_hot_raw:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.183609 4097533 custom_kernel.cc:58] Succeed in registering kernel [batch_norm_infer:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.183614 4097533 custom_kernel.cc:58] Succeed in registering kernel [batch_norm_infer:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183625 4097533 custom_kernel.cc:58] Succeed in registering kernel [deformable_conv_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183640 4097533 custom_kernel.cc:58] Succeed in registering kernel [squared_l2_norm:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.183643 4097533 custom_kernel.cc:58] Succeed in registering kernel [squared_l2_norm:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183658 4097533 custom_kernel.cc:58] Succeed in registering kernel [gather_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.183662 4097533 custom_kernel.cc:58] Succeed in registering kernel [gather_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183672 4097533 custom_kernel.cc:58] Succeed in registering kernel [relu:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.183677 4097533 custom_kernel.cc:58] Succeed in registering kernel [relu:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183684 4097533 custom_kernel.cc:58] Succeed in registering kernel [mean_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.183712 4097533 custom_kernel.cc:58] Succeed in registering kernel [mean_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183720 4097533 custom_kernel.cc:58] Succeed in registering kernel [hardswish:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.183724 4097533 custom_kernel.cc:58] Succeed in registering kernel [hardswish:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183732 4097533 custom_kernel.cc:58] Succeed in registering kernel [bmm:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.183737 4097533 custom_kernel.cc:58] Succeed in registering kernel [bmm:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183745 4097533 custom_kernel.cc:58] Succeed in registering kernel [yolo_box:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183753 4097533 custom_kernel.cc:58] Succeed in registering kernel [swish:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.183758 4097533 custom_kernel.cc:58] Succeed in registering kernel [swish:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183769 4097533 custom_kernel.cc:58] Succeed in registering kernel [silu_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.183773 4097533 custom_kernel.cc:58] Succeed in registering kernel [silu_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183780 4097533 custom_kernel.cc:58] Succeed in registering kernel [nearest_interp:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.183811 4097533 custom_kernel.cc:58] Succeed in registering kernel [nearest_interp:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183838 4097533 custom_kernel.cc:58] Succeed in registering kernel [gather:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.183842 4097533 custom_kernel.cc:58] Succeed in registering kernel [gather:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183848 4097533 custom_kernel.cc:58] Succeed in registering kernel [minimum_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.183854 4097533 custom_kernel.cc:58] Succeed in registering kernel [minimum_grad:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183866 4097533 custom_kernel.cc:58] Succeed in registering kernel [minimum_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183872 4097533 custom_kernel.cc:58] Succeed in registering kernel [elementwise_pow_raw:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.183879 4097533 custom_kernel.cc:58] Succeed in registering kernel [elementwise_pow_raw:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183887 4097533 custom_kernel.cc:58] Succeed in registering kernel [softmax:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.183892 4097533 custom_kernel.cc:58] Succeed in registering kernel [softmax:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183902 4097533 custom_kernel.cc:58] Succeed in registering kernel [grid_sample:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.183905 4097533 custom_kernel.cc:58] Succeed in registering kernel [grid_sample:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183916 4097533 custom_kernel.cc:58] Succeed in registering kernel [pow_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.183920 4097533 custom_kernel.cc:58] Succeed in registering kernel [pow_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183925 4097533 custom_kernel.cc:58] Succeed in registering kernel [index_sample:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.183931 4097533 custom_kernel.cc:58] Succeed in registering kernel [index_sample:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183960 4097533 custom_kernel.cc:58] Succeed in registering kernel [index_sample:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.183964 4097533 custom_kernel.cc:58] Succeed in registering kernel [index_sample:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183970 4097533 custom_kernel.cc:58] Succeed in registering kernel [logical_not:(mlu, Undefined(AnyLayout), uint8)] to Paddle. It will be used like native ones.
I1018 08:53:50.183976 4097533 custom_kernel.cc:58] Succeed in registering kernel [logical_not:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.183981 4097533 custom_kernel.cc:58] Succeed in registering kernel [logical_not:(mlu, Undefined(AnyLayout), int64)] to Paddle. It will be used like native ones.
I1018 08:53:50.183987 4097533 custom_kernel.cc:58] Succeed in registering kernel [logical_not:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.183991 4097533 custom_kernel.cc:58] Succeed in registering kernel [logical_not:(mlu, Undefined(AnyLayout), int16)] to Paddle. It will be used like native ones.
I1018 08:53:50.183995 4097533 custom_kernel.cc:58] Succeed in registering kernel [logical_not:(mlu, Undefined(AnyLayout), int8)] to Paddle. It will be used like native ones.
I1018 08:53:50.184001 4097533 custom_kernel.cc:58] Succeed in registering kernel [logical_not:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.184046 4097533 custom_kernel.cc:58] Succeed in registering kernel [logical_not:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.184055 4097533 custom_kernel.cc:58] Succeed in registering kernel [rnn_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.184062 4097533 custom_kernel.cc:58] Succeed in registering kernel [flip:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.184078 4097533 custom_kernel.cc:58] Succeed in registering kernel [flip:(mlu, Undefined(AnyLayout), int32)] to Paddle. It will be used like native ones.
I1018 08:53:50.184082 4097533 custom_kernel.cc:58] Succeed in registering kernel [flip:(mlu, Undefined(AnyLayout), bool)] to Paddle. It will be used like native ones.
I1018 08:53:50.184087 4097533 custom_kernel.cc:58] Succeed in registering kernel [flip:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.184094 4097533 custom_kernel.cc:58] Succeed in registering kernel [relu6_grad:(mlu, Undefined(AnyLayout), float16)] to Paddle. It will be used like native ones.
I1018 08:53:50.184099 4097533 custom_kernel.cc:58] Succeed in registering kernel [relu6_grad:(mlu, Undefined(AnyLayout), float32)] to Paddle. It will be used like native ones.
I1018 08:53:50.184103 4097533 custom_kernel.cc:63] Succeed in loading 253 custom kernel(s) from loaded lib(s), will be used like native ones.
I1018 08:53:50.184172 4097533 init.cc:155] Finished in LoadCustomDevice with libs_path: [/home/wzy/.local/lib/python3.9/site-packages/paddle_custom_device]
I1018 08:53:50.184199 4097533 init.cc:240] CustomDevice: mlu, visible devices count: 1
I1018 08:53:50.187755 4097533 pybind.cc:2141] Initialize tensor operants successfully
I1018 08:53:50.313648 4097533 amp_auto_cast.cc:114] -- The size of all_ops: 1184 --
I1018 08:53:50.313685 4097533 amp_auto_cast.cc:115] -- The size of supported_ops: 126 --
I1018 08:53:50.313694 4097533 amp_auto_cast.cc:116] -- The size of unsupported_ops: 1058 --
I1018 08:53:51.236827 4097533 imperative.cc:726] Tracer(0x55b77202ea90) set expected place Place(mlu:0)
I1018 08:53:52.890652 4097533 eager.cc:118] Tensor(conv2d_0.w_0) have not GradNode, add GradNodeAccumulation0x55b77381dd90 for it.
I1018 08:53:52.891885 4097533 layout_autotune.cc:32] Already exists in Layout OP: instance_norm
I1018 08:53:52.893112 4097533 layout_autotune.cc:32] Already exists in Layout OP: transpose2
I1018 08:53:52.894011 4097533 layout_autotune.cc:32] Already exists in Layout OP: reshape2
I1018 08:53:52.895280 4097533 layout_autotune.cc:32] Already exists in Layout OP: batch_norm
I1018 08:53:52.898748 4097533 layout_autotune.cc:32] Already exists in Layout OP: transpose
I1018 08:53:52.899718 4097533 layout_autotune.cc:32] Already exists in Layout OP: softmax
I1018 08:53:52.899897 4097533 layout_autotune.cc:84] The number of layout agnostic OPs: 593, heavily layout sensitive OPs: 37, lightly layout sensitive OPs: 132
I1018 08:53:52.900054 4097533 eager_op_function.cc:33102] CurrentDeviceId: 0 from 0
I1018 08:53:52.900074 4097533 dygraph_functions.cc:65227] Running AD API: gaussian
I1018 08:53:52.900079 4097533 dygraph_functions.cc:65247] { Input: []} 
I1018 08:53:52.900149 4097533 custom_device_op_list.cc:46] Custom Device Black List: 
I1018 08:53:56.647384 4097533 stream.cc:67] Init Stream: 0x55b7735d3eb0, place: Place(mlu:0), priority: 2, flag:0
I1018 08:53:56.647542 4097533 allocator_facade.cc:205] selected allocator strategy:1
I1018 08:53:56.647601 4097533 allocator_facade.cc:1376] FLAGS_auto_growth_chunk_size_in_mb is 0
I1018 08:53:56.647624 4097533 auto_growth_best_fit_allocator.cc:63] chunk_size_:512
I1018 08:53:56.647734 4097533 generator.cc:181] Generator Random state device id: -1, seed: 1062065029425644, offset: 0, cpu_engine: 0x55b7bd3d9100
I1018 08:53:56.647753 4097533 generator.cc:181] Generator Random state device id: -1, seed: 3869631341775763, offset: 0, cpu_engine: 0x55b7b796aef0
I1018 08:53:56.647855 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 37888(0x30080ff5c030000), and remaining 0
I1018 08:53:56.647962 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1181184(0x30080ff5c03a000), and remaining 0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1018 08:53:56.648029 4097533 mlu_baseop.cc:85] device_id: 0, initial seed: 502334669
I1018 08:53:56.648085 4097533 dygraph_functions.cc:65263] Finish AD API: gaussian
I1018 08:53:56.648125 4097533 dygraph_functions.cc:65277] { Input: [],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6fdcbf0 }]), ] } 
I1018 08:53:56.649471 4097533 eager.cc:118] Tensor(batch_norm2d_0.w_0) have not GradNode, add GradNodeAccumulation0x55b77ed32260 for it.
I1018 08:53:56.649667 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.649685 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.649703 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.649713 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_0.w_0, Initialized: 0, Ptr: 0x55b77346a070 }]), ]} 
I1018 08:53:56.649780 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 512(0x30080ff5c15c000), and remaining 0
I1018 08:53:56.649852 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.649868 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_0.w_0, Initialized: 1, Ptr: 0x55b77346a070 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_0.w_0, Initialized: 1, Ptr: 0x55b77346a070 }]), ] } 
I1018 08:53:56.649996 4097533 eager.cc:118] Tensor(batch_norm2d_0.b_0) have not GradNode, add GradNodeAccumulation0x55b77f0e4620 for it.
I1018 08:53:56.650025 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.650032 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.650036 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.650041 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_0.b_0, Initialized: 0, Ptr: 0x55b7b6f6d4b0 }]), ]} 
I1018 08:53:56.650059 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 512(0x30080ff5c15e000), and remaining 0
I1018 08:53:56.650075 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.650086 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_0.b_0, Initialized: 1, Ptr: 0x55b7b6f6d4b0 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_0.b_0, Initialized: 1, Ptr: 0x55b7b6f6d4b0 }]), ] } 
I1018 08:53:56.650331 4097533 eager.cc:118] Tensor(batch_norm2d_0.w_1) have not GradNode, add GradNodeAccumulation0x55b7817f2c00 for it.
I1018 08:53:56.650359 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.650365 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.650369 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.650375 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_0.w_1, Initialized: 0, Ptr: 0x55b7c4ee9a30 }]), ]} 
I1018 08:53:56.650393 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 512(0x30080ff5c160000), and remaining 0
I1018 08:53:56.650408 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.650418 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_0.w_1, Initialized: 1, Ptr: 0x55b7c4ee9a30 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_0.w_1, Initialized: 1, Ptr: 0x55b7c4ee9a30 }]), ] } 
I1018 08:53:56.650568 4097533 eager.cc:118] Tensor(batch_norm2d_0.w_2) have not GradNode, add GradNodeAccumulation0x55b773982330 for it.
I1018 08:53:56.650590 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.650596 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.650599 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.650604 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_0.w_2, Initialized: 0, Ptr: 0x55b7ceb07ff0 }]), ]} 
I1018 08:53:56.650619 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 512(0x30080ff5c162000), and remaining 0
I1018 08:53:56.650632 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.650643 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_0.w_2, Initialized: 1, Ptr: 0x55b7ceb07ff0 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_0.w_2, Initialized: 1, Ptr: 0x55b7ceb07ff0 }]), ] } 
I1018 08:53:56.651441 4097533 eager.cc:118] Tensor(conv2d_1.w_0) have not GradNode, add GradNodeAccumulation0x55b773982760 for it.
I1018 08:53:56.651506 4097533 eager_op_function.cc:33102] CurrentDeviceId: 0 from 0
I1018 08:53:56.651511 4097533 dygraph_functions.cc:65227] Running AD API: gaussian
I1018 08:53:56.651515 4097533 dygraph_functions.cc:65247] { Input: []} 
I1018 08:53:56.651535 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 147968(0x30080ff5c164000), and remaining 0
I1018 08:53:56.651554 4097533 dygraph_functions.cc:65263] Finish AD API: gaussian
I1018 08:53:56.651571 4097533 dygraph_functions.cc:65277] { Input: [],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cc234050 }]), ] } 
I1018 08:53:56.651757 4097533 eager.cc:118] Tensor(batch_norm2d_1.w_0) have not GradNode, add GradNodeAccumulation0x55b773982b90 for it.
I1018 08:53:56.651784 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.651790 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.651794 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.651800 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_1.w_0, Initialized: 0, Ptr: 0x55b7b68c8170 }]), ]} 
I1018 08:53:56.651818 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 512(0x30080ff5c18a000), and remaining 0
I1018 08:53:56.651832 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.651844 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_1.w_0, Initialized: 1, Ptr: 0x55b7b68c8170 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_1.w_0, Initialized: 1, Ptr: 0x55b7b68c8170 }]), ] } 
I1018 08:53:56.651913 4097533 eager.cc:118] Tensor(batch_norm2d_1.b_0) have not GradNode, add GradNodeAccumulation0x55b773f00630 for it.
I1018 08:53:56.651932 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.651935 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.651939 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.651944 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_1.b_0, Initialized: 0, Ptr: 0x55b7b76da540 }]), ]} 
I1018 08:53:56.651957 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 512(0x30080ff5c18c000), and remaining 0
I1018 08:53:56.651968 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.651978 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_1.b_0, Initialized: 1, Ptr: 0x55b7b76da540 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_1.b_0, Initialized: 1, Ptr: 0x55b7b76da540 }]), ] } 
I1018 08:53:56.652091 4097533 eager.cc:118] Tensor(batch_norm2d_1.w_1) have not GradNode, add GradNodeAccumulation0x55b7b64bb640 for it.
I1018 08:53:56.652110 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.652114 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.652117 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.652122 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_1.w_1, Initialized: 0, Ptr: 0x55b7b64bb520 }]), ]} 
I1018 08:53:56.652135 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 512(0x30080ff5c18e000), and remaining 0
I1018 08:53:56.652148 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.652158 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_1.w_1, Initialized: 1, Ptr: 0x55b7b64bb520 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_1.w_1, Initialized: 1, Ptr: 0x55b7b64bb520 }]), ] } 
I1018 08:53:56.652264 4097533 eager.cc:118] Tensor(batch_norm2d_1.w_2) have not GradNode, add GradNodeAccumulation0x55b7b68ca3f0 for it.
I1018 08:53:56.652281 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.652287 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.652289 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.652294 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_1.w_2, Initialized: 0, Ptr: 0x55b7b68ca2d0 }]), ]} 
I1018 08:53:56.652312 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 512(0x30080ff5c190000), and remaining 0
I1018 08:53:56.652324 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.652333 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_1.w_2, Initialized: 1, Ptr: 0x55b7b68ca2d0 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_1.w_2, Initialized: 1, Ptr: 0x55b7b68ca2d0 }]), ] } 
I1018 08:53:56.652637 4097533 eager.cc:118] Tensor(conv2d_2.w_0) have not GradNode, add GradNodeAccumulation0x55b7bd3da270 for it.
I1018 08:53:56.652669 4097533 eager_op_function.cc:33102] CurrentDeviceId: 0 from 0
I1018 08:53:56.652673 4097533 dygraph_functions.cc:65227] Running AD API: gaussian
I1018 08:53:56.652675 4097533 dygraph_functions.cc:65247] { Input: []} 
I1018 08:53:56.652693 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 147968(0x30080ff5c192000), and remaining 0
I1018 08:53:56.652709 4097533 dygraph_functions.cc:65263] Finish AD API: gaussian
I1018 08:53:56.652720 4097533 dygraph_functions.cc:65277] { Input: [],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c3448450 }]), ] } 
I1018 08:53:56.652897 4097533 eager.cc:118] Tensor(batch_norm2d_2.w_0) have not GradNode, add GradNodeAccumulation0x55b7c4ee5b40 for it.
I1018 08:53:56.652920 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.652926 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.652930 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.652935 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_2.w_0, Initialized: 0, Ptr: 0x55b7bd3da150 }]), ]} 
I1018 08:53:56.652951 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 512(0x30080ff5c1b8000), and remaining 0
I1018 08:53:56.652966 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.652976 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_2.w_0, Initialized: 1, Ptr: 0x55b7bd3da150 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_2.w_0, Initialized: 1, Ptr: 0x55b7bd3da150 }]), ] } 
I1018 08:53:56.653041 4097533 eager.cc:118] Tensor(batch_norm2d_2.b_0) have not GradNode, add GradNodeAccumulation0x55b7bd3d87e0 for it.
I1018 08:53:56.653059 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.653064 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.653066 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.653071 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_2.b_0, Initialized: 0, Ptr: 0x55b7bd3d86c0 }]), ]} 
I1018 08:53:56.653084 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 512(0x30080ff5c1ba000), and remaining 0
I1018 08:53:56.653095 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.653105 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_2.b_0, Initialized: 1, Ptr: 0x55b7bd3d86c0 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_2.b_0, Initialized: 1, Ptr: 0x55b7bd3d86c0 }]), ] } 
I1018 08:53:56.653229 4097533 eager.cc:118] Tensor(batch_norm2d_2.w_1) have not GradNode, add GradNodeAccumulation0x55b7b76e6f30 for it.
I1018 08:53:56.653249 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.653254 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.653256 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.653261 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_2.w_1, Initialized: 0, Ptr: 0x55b7b76e6e10 }]), ]} 
I1018 08:53:56.653275 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 512(0x30080ff5c1bc000), and remaining 0
I1018 08:53:56.653287 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.653298 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_2.w_1, Initialized: 1, Ptr: 0x55b7b76e6e10 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_2.w_1, Initialized: 1, Ptr: 0x55b7b76e6e10 }]), ] } 
I1018 08:53:56.653414 4097533 eager.cc:118] Tensor(batch_norm2d_2.w_2) have not GradNode, add GradNodeAccumulation0x55b7c2207c50 for it.
I1018 08:53:56.653432 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.653436 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.653440 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.653445 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_2.w_2, Initialized: 0, Ptr: 0x55b7c2207b30 }]), ]} 
I1018 08:53:56.653458 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 512(0x30080ff5c1be000), and remaining 0
I1018 08:53:56.653471 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.653481 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_2.w_2, Initialized: 1, Ptr: 0x55b7c2207b30 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_2.w_2, Initialized: 1, Ptr: 0x55b7c2207b30 }]), ] } 
I1018 08:53:56.653775 4097533 eager.cc:118] Tensor(conv2d_3.w_0) have not GradNode, add GradNodeAccumulation0x55b7b76e5550 for it.
I1018 08:53:56.653806 4097533 eager_op_function.cc:33102] CurrentDeviceId: 0 from 0
I1018 08:53:56.653810 4097533 dygraph_functions.cc:65227] Running AD API: gaussian
I1018 08:53:56.653812 4097533 dygraph_functions.cc:65247] { Input: []} 
I1018 08:53:56.653828 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 147968(0x30080ff5c1c0000), and remaining 0
I1018 08:53:56.653842 4097533 dygraph_functions.cc:65263] Finish AD API: gaussian
I1018 08:53:56.653852 4097533 dygraph_functions.cc:65277] { Input: [],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b76e5cd0 }]), ] } 
I1018 08:53:56.654016 4097533 eager.cc:118] Tensor(batch_norm2d_3.w_0) have not GradNode, add GradNodeAccumulation0x55b7b76e6360 for it.
I1018 08:53:56.654036 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.654042 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.654044 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.654050 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_3.w_0, Initialized: 0, Ptr: 0x55b7bd3daab0 }]), ]} 
I1018 08:53:56.654065 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 512(0x30080ff5c1e6000), and remaining 0
I1018 08:53:56.654080 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.654090 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_3.w_0, Initialized: 1, Ptr: 0x55b7bd3daab0 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_3.w_0, Initialized: 1, Ptr: 0x55b7bd3daab0 }]), ] } 
I1018 08:53:56.654155 4097533 eager.cc:118] Tensor(batch_norm2d_3.b_0) have not GradNode, add GradNodeAccumulation0x55b7b68c8390 for it.
I1018 08:53:56.654170 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.654175 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.654178 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.654183 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_3.b_0, Initialized: 0, Ptr: 0x55b7b87810f0 }]), ]} 
I1018 08:53:56.654194 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 512(0x30080ff5c1e8000), and remaining 0
I1018 08:53:56.654206 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.654215 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_3.b_0, Initialized: 1, Ptr: 0x55b7b87810f0 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_3.b_0, Initialized: 1, Ptr: 0x55b7b87810f0 }]), ] } 
I1018 08:53:56.654321 4097533 eager.cc:118] Tensor(batch_norm2d_3.w_1) have not GradNode, add GradNodeAccumulation0x55b7b68c9010 for it.
I1018 08:53:56.654341 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.654351 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.654354 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.654359 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_3.w_1, Initialized: 0, Ptr: 0x55b7b778e800 }]), ]} 
I1018 08:53:56.654372 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 512(0x30080ff5c1ea000), and remaining 0
I1018 08:53:56.654385 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.654394 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_3.w_1, Initialized: 1, Ptr: 0x55b7b778e800 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_3.w_1, Initialized: 1, Ptr: 0x55b7b778e800 }]), ] } 
I1018 08:53:56.654805 4097533 eager.cc:118] Tensor(batch_norm2d_3.w_2) have not GradNode, add GradNodeAccumulation0x55b7b3f225c0 for it.
I1018 08:53:56.654831 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.654837 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.654841 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.654847 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_3.w_2, Initialized: 0, Ptr: 0x55b7b3f224a0 }]), ]} 
I1018 08:53:56.654865 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 512(0x30080ff5c1ec000), and remaining 0
I1018 08:53:56.654881 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.654892 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_3.w_2, Initialized: 1, Ptr: 0x55b7b3f224a0 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_3.w_2, Initialized: 1, Ptr: 0x55b7b3f224a0 }]), ] } 
I1018 08:53:56.655177 4097533 eager.cc:118] Tensor(conv2d_4.w_0) have not GradNode, add GradNodeAccumulation0x55b7b68c6d90 for it.
I1018 08:53:56.655205 4097533 eager_op_function.cc:33102] CurrentDeviceId: 0 from 0
I1018 08:53:56.655210 4097533 dygraph_functions.cc:65227] Running AD API: gaussian
I1018 08:53:56.655211 4097533 dygraph_functions.cc:65247] { Input: []} 
I1018 08:53:56.655227 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 147968(0x30080ff5c1ee000), and remaining 0
I1018 08:53:56.655242 4097533 dygraph_functions.cc:65263] Finish AD API: gaussian
I1018 08:53:56.655251 4097533 dygraph_functions.cc:65277] { Input: [],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b68c74e0 }]), ] } 
I1018 08:53:56.655416 4097533 eager.cc:118] Tensor(batch_norm2d_4.w_0) have not GradNode, add GradNodeAccumulation0x55b7b4b49ae0 for it.
I1018 08:53:56.655437 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.655442 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.655447 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.655452 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_4.w_0, Initialized: 0, Ptr: 0x55b7b68c6c70 }]), ]} 
I1018 08:53:56.655467 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 512(0x30080ff5c214000), and remaining 0
I1018 08:53:56.655481 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.655493 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_4.w_0, Initialized: 1, Ptr: 0x55b7b68c6c70 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_4.w_0, Initialized: 1, Ptr: 0x55b7b68c6c70 }]), ] } 
I1018 08:53:56.655555 4097533 eager.cc:118] Tensor(batch_norm2d_4.b_0) have not GradNode, add GradNodeAccumulation0x55b7b4b4a8e0 for it.
I1018 08:53:56.655575 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.655579 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.655582 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.655587 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_4.b_0, Initialized: 0, Ptr: 0x55b7b4b4a7c0 }]), ]} 
I1018 08:53:56.655601 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 512(0x30080ff5c216000), and remaining 0
I1018 08:53:56.655617 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.655627 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_4.b_0, Initialized: 1, Ptr: 0x55b7b4b4a7c0 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_4.b_0, Initialized: 1, Ptr: 0x55b7b4b4a7c0 }]), ] } 
I1018 08:53:56.655733 4097533 eager.cc:118] Tensor(batch_norm2d_4.w_1) have not GradNode, add GradNodeAccumulation0x55b7c2209db0 for it.
I1018 08:53:56.655750 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.655755 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.655758 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.655763 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_4.w_1, Initialized: 0, Ptr: 0x55b7c2209c90 }]), ]} 
I1018 08:53:56.655776 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 512(0x30080ff5c218000), and remaining 0
I1018 08:53:56.655788 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.655798 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_4.w_1, Initialized: 1, Ptr: 0x55b7c2209c90 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_4.w_1, Initialized: 1, Ptr: 0x55b7c2209c90 }]), ] } 
I1018 08:53:56.655901 4097533 eager.cc:118] Tensor(batch_norm2d_4.w_2) have not GradNode, add GradNodeAccumulation0x55b7c220ac70 for it.
I1018 08:53:56.655917 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.655922 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.655925 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.655930 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_4.w_2, Initialized: 0, Ptr: 0x55b7c220ab50 }]), ]} 
I1018 08:53:56.655942 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 512(0x30080ff5c21a000), and remaining 0
I1018 08:53:56.655954 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.655964 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_4.w_2, Initialized: 1, Ptr: 0x55b7c220ab50 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_4.w_2, Initialized: 1, Ptr: 0x55b7c220ab50 }]), ] } 
I1018 08:53:56.656267 4097533 eager.cc:118] Tensor(conv2d_5.w_0) have not GradNode, add GradNodeAccumulation0x55b7b85db7c0 for it.
I1018 08:53:56.656296 4097533 eager_op_function.cc:33102] CurrentDeviceId: 0 from 0
I1018 08:53:56.656301 4097533 dygraph_functions.cc:65227] Running AD API: gaussian
I1018 08:53:56.656302 4097533 dygraph_functions.cc:65247] { Input: []} 
I1018 08:53:56.656318 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 33280(0x30080ff5c21c000), and remaining 0
I1018 08:53:56.656333 4097533 dygraph_functions.cc:65263] Finish AD API: gaussian
I1018 08:53:56.656343 4097533 dygraph_functions.cc:65277] { Input: [],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b85dbf10 }]), ] } 
I1018 08:53:56.656497 4097533 eager.cc:118] Tensor(batch_norm2d_5.w_0) have not GradNode, add GradNodeAccumulation0x55b7b85dc690 for it.
I1018 08:53:56.656517 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.656522 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.656524 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.656530 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_5.w_0, Initialized: 0, Ptr: 0x55b7b3f22b10 }]), ]} 
I1018 08:53:56.656544 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1024(0x30080ff5c226000), and remaining 0
I1018 08:53:56.656559 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.656569 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_5.w_0, Initialized: 1, Ptr: 0x55b7b3f22b10 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_5.w_0, Initialized: 1, Ptr: 0x55b7b3f22b10 }]), ] } 
I1018 08:53:56.656637 4097533 eager.cc:118] Tensor(batch_norm2d_5.b_0) have not GradNode, add GradNodeAccumulation0x55b7b85dd540 for it.
I1018 08:53:56.656654 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.656659 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.656662 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.656667 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_5.b_0, Initialized: 0, Ptr: 0x55b7b85dd420 }]), ]} 
I1018 08:53:56.656680 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1024(0x30080ff5c228000), and remaining 0
I1018 08:53:56.656692 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.656701 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_5.b_0, Initialized: 1, Ptr: 0x55b7b85dd420 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_5.b_0, Initialized: 1, Ptr: 0x55b7b85dd420 }]), ] } 
I1018 08:53:56.656805 4097533 eager.cc:118] Tensor(batch_norm2d_5.w_1) have not GradNode, add GradNodeAccumulation0x55b7bb903270 for it.
I1018 08:53:56.656826 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.656831 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.656833 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.656839 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_5.w_1, Initialized: 0, Ptr: 0x55b7bb903150 }]), ]} 
I1018 08:53:56.656852 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1024(0x30080ff5c22a000), and remaining 0
I1018 08:53:56.656863 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.656873 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_5.w_1, Initialized: 1, Ptr: 0x55b7bb903150 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_5.w_1, Initialized: 1, Ptr: 0x55b7bb903150 }]), ] } 
I1018 08:53:56.656975 4097533 eager.cc:118] Tensor(batch_norm2d_5.w_2) have not GradNode, add GradNodeAccumulation0x55b7bb904360 for it.
I1018 08:53:56.656992 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.656997 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.657001 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.657006 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_5.w_2, Initialized: 0, Ptr: 0x55b7bb904240 }]), ]} 
I1018 08:53:56.657017 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1024(0x30080ff5c22c000), and remaining 0
I1018 08:53:56.657029 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.657039 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_5.w_2, Initialized: 1, Ptr: 0x55b7bb904240 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_5.w_2, Initialized: 1, Ptr: 0x55b7bb904240 }]), ] } 
I1018 08:53:56.657387 4097533 eager.cc:118] Tensor(conv2d_6.w_0) have not GradNode, add GradNodeAccumulation0x55b7c4ee2d80 for it.
I1018 08:53:56.657419 4097533 eager_op_function.cc:33102] CurrentDeviceId: 0 from 0
I1018 08:53:56.657423 4097533 dygraph_functions.cc:65227] Running AD API: gaussian
I1018 08:53:56.657425 4097533 dygraph_functions.cc:65247] { Input: []} 
I1018 08:53:56.657442 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 295424(0x30080ff5c22e000), and remaining 0
I1018 08:53:56.657457 4097533 dygraph_functions.cc:65263] Finish AD API: gaussian
I1018 08:53:56.657467 4097533 dygraph_functions.cc:65277] { Input: [],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4ee3500 }]), ] } 
I1018 08:53:56.657634 4097533 eager.cc:118] Tensor(batch_norm2d_6.w_0) have not GradNode, add GradNodeAccumulation0x55b7c4ee3e00 for it.
I1018 08:53:56.657655 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.657660 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.657663 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.657670 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_6.w_0, Initialized: 0, Ptr: 0x55b7c4ee3ce0 }]), ]} 
I1018 08:53:56.657691 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1024(0x30080ff5c278000), and remaining 0
I1018 08:53:56.657706 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.657716 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_6.w_0, Initialized: 1, Ptr: 0x55b7c4ee3ce0 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_6.w_0, Initialized: 1, Ptr: 0x55b7c4ee3ce0 }]), ] } 
I1018 08:53:56.657781 4097533 eager.cc:118] Tensor(batch_norm2d_6.b_0) have not GradNode, add GradNodeAccumulation0x55b7c4ee4cb0 for it.
I1018 08:53:56.657802 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.657807 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.657810 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.657815 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_6.b_0, Initialized: 0, Ptr: 0x55b7c4ee4b90 }]), ]} 
I1018 08:53:56.657828 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1024(0x30080ff5c27a000), and remaining 0
I1018 08:53:56.657840 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.657850 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_6.b_0, Initialized: 1, Ptr: 0x55b7c4ee4b90 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_6.b_0, Initialized: 1, Ptr: 0x55b7c4ee4b90 }]), ] } 
I1018 08:53:56.657956 4097533 eager.cc:118] Tensor(batch_norm2d_6.w_1) have not GradNode, add GradNodeAccumulation0x55b7cc394a90 for it.
I1018 08:53:56.657976 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.657981 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.657984 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.657989 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_6.w_1, Initialized: 0, Ptr: 0x55b7cc394970 }]), ]} 
I1018 08:53:56.658001 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1024(0x30080ff5c27c000), and remaining 0
I1018 08:53:56.658013 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.658023 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_6.w_1, Initialized: 1, Ptr: 0x55b7cc394970 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_6.w_1, Initialized: 1, Ptr: 0x55b7cc394970 }]), ] } 
I1018 08:53:56.658126 4097533 eager.cc:118] Tensor(batch_norm2d_6.w_2) have not GradNode, add GradNodeAccumulation0x55b7cc395b80 for it.
I1018 08:53:56.658144 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.658149 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.658152 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.658157 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_6.w_2, Initialized: 0, Ptr: 0x55b7cc395a60 }]), ]} 
I1018 08:53:56.658169 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1024(0x30080ff5c27e000), and remaining 0
I1018 08:53:56.658180 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.658190 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_6.w_2, Initialized: 1, Ptr: 0x55b7cc395a60 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_6.w_2, Initialized: 1, Ptr: 0x55b7cc395a60 }]), ] } 
I1018 08:53:56.658465 4097533 eager.cc:118] Tensor(conv2d_7.w_0) have not GradNode, add GradNodeAccumulation0x55b7cc396a70 for it.
I1018 08:53:56.658492 4097533 eager_op_function.cc:33102] CurrentDeviceId: 0 from 0
I1018 08:53:56.658496 4097533 dygraph_functions.cc:65227] Running AD API: gaussian
I1018 08:53:56.658499 4097533 dygraph_functions.cc:65247] { Input: []} 
I1018 08:53:56.658514 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 590336(0x30080ff5c280000), and remaining 0
I1018 08:53:56.658528 4097533 dygraph_functions.cc:65263] Finish AD API: gaussian
I1018 08:53:56.658543 4097533 dygraph_functions.cc:65277] { Input: [],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4d63770 }]), ] } 
I1018 08:53:56.658704 4097533 eager.cc:118] Tensor(batch_norm2d_7.w_0) have not GradNode, add GradNodeAccumulation0x55b7c4d63ea0 for it.
I1018 08:53:56.658723 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.658728 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.658732 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.658738 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_7.w_0, Initialized: 0, Ptr: 0x55b7cc396950 }]), ]} 
I1018 08:53:56.658752 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1024(0x30080ff5c312000), and remaining 0
I1018 08:53:56.658767 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.658778 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_7.w_0, Initialized: 1, Ptr: 0x55b7cc396950 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_7.w_0, Initialized: 1, Ptr: 0x55b7cc396950 }]), ] } 
I1018 08:53:56.658845 4097533 eager.cc:118] Tensor(batch_norm2d_7.b_0) have not GradNode, add GradNodeAccumulation0x55b7c4d64d50 for it.
I1018 08:53:56.658865 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.658870 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.658874 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.658878 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_7.b_0, Initialized: 0, Ptr: 0x55b7c4d64c30 }]), ]} 
I1018 08:53:56.658891 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1024(0x30080ff5c314000), and remaining 0
I1018 08:53:56.658903 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.658912 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_7.b_0, Initialized: 1, Ptr: 0x55b7c4d64c30 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_7.b_0, Initialized: 1, Ptr: 0x55b7c4d64c30 }]), ] } 
I1018 08:53:56.659019 4097533 eager.cc:118] Tensor(batch_norm2d_7.w_1) have not GradNode, add GradNodeAccumulation0x55b7b778b6e0 for it.
I1018 08:53:56.659037 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.659041 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.659044 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.659050 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_7.w_1, Initialized: 0, Ptr: 0x55b7c4d65af0 }]), ]} 
I1018 08:53:56.659062 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1024(0x30080ff5c316000), and remaining 0
I1018 08:53:56.659075 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.659085 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_7.w_1, Initialized: 1, Ptr: 0x55b7c4d65af0 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_7.w_1, Initialized: 1, Ptr: 0x55b7c4d65af0 }]), ] } 
I1018 08:53:56.659188 4097533 eager.cc:118] Tensor(batch_norm2d_7.w_2) have not GradNode, add GradNodeAccumulation0x55b7b778c5a0 for it.
I1018 08:53:56.659204 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.659209 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.659212 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.659219 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_7.w_2, Initialized: 0, Ptr: 0x55b7b778c480 }]), ]} 
I1018 08:53:56.659229 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1024(0x30080ff5c318000), and remaining 0
I1018 08:53:56.659241 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.659250 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_7.w_2, Initialized: 1, Ptr: 0x55b7b778c480 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_7.w_2, Initialized: 1, Ptr: 0x55b7b778c480 }]), ] } 
I1018 08:53:56.659538 4097533 eager.cc:118] Tensor(conv2d_8.w_0) have not GradNode, add GradNodeAccumulation0x55b7b778d7d0 for it.
I1018 08:53:56.659565 4097533 eager_op_function.cc:33102] CurrentDeviceId: 0 from 0
I1018 08:53:56.659569 4097533 dygraph_functions.cc:65227] Running AD API: gaussian
I1018 08:53:56.659571 4097533 dygraph_functions.cc:65247] { Input: []} 
I1018 08:53:56.659588 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 590336(0x30080ff5c31a000), and remaining 0
I1018 08:53:56.659602 4097533 dygraph_functions.cc:65263] Finish AD API: gaussian
I1018 08:53:56.659613 4097533 dygraph_functions.cc:65277] { Input: [],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b778df50 }]), ] } 
I1018 08:53:56.659771 4097533 eager.cc:118] Tensor(batch_norm2d_8.w_0) have not GradNode, add GradNodeAccumulation0x55b7c5cf0810 for it.
I1018 08:53:56.659793 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.659799 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.659802 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.659808 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_8.w_0, Initialized: 0, Ptr: 0x55b7bb905130 }]), ]} 
I1018 08:53:56.659823 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1024(0x30080ff5c3ac000), and remaining 0
I1018 08:53:56.659838 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.659849 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_8.w_0, Initialized: 1, Ptr: 0x55b7bb905130 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_8.w_0, Initialized: 1, Ptr: 0x55b7bb905130 }]), ] } 
I1018 08:53:56.659914 4097533 eager.cc:118] Tensor(batch_norm2d_8.b_0) have not GradNode, add GradNodeAccumulation0x55b7c5cf1730 for it.
I1018 08:53:56.659934 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.659938 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.659941 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.659946 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_8.b_0, Initialized: 0, Ptr: 0x55b7c5cf1610 }]), ]} 
I1018 08:53:56.659960 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1024(0x30080ff5c3ae000), and remaining 0
I1018 08:53:56.659971 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.659981 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_8.b_0, Initialized: 1, Ptr: 0x55b7c5cf1610 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_8.b_0, Initialized: 1, Ptr: 0x55b7c5cf1610 }]), ] } 
I1018 08:53:56.660084 4097533 eager.cc:118] Tensor(batch_norm2d_8.w_1) have not GradNode, add GradNodeAccumulation0x55b7c5cf25f0 for it.
I1018 08:53:56.660100 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.660104 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.660107 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.660113 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_8.w_1, Initialized: 0, Ptr: 0x55b7c5cf24d0 }]), ]} 
I1018 08:53:56.660125 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1024(0x30080ff5c3b0000), and remaining 0
I1018 08:53:56.660136 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.660146 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_8.w_1, Initialized: 1, Ptr: 0x55b7c5cf24d0 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_8.w_1, Initialized: 1, Ptr: 0x55b7c5cf24d0 }]), ] } 
I1018 08:53:56.660247 4097533 eager.cc:118] Tensor(batch_norm2d_8.w_2) have not GradNode, add GradNodeAccumulation0x55b7cc6c2360 for it.
I1018 08:53:56.660264 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.660269 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.660271 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.660281 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_8.w_2, Initialized: 0, Ptr: 0x55b7c5cf3390 }]), ]} 
I1018 08:53:56.660293 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1024(0x30080ff5c3b2000), and remaining 0
I1018 08:53:56.660305 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.660315 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_8.w_2, Initialized: 1, Ptr: 0x55b7c5cf3390 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_8.w_2, Initialized: 1, Ptr: 0x55b7c5cf3390 }]), ] } 
I1018 08:53:56.660593 4097533 eager.cc:118] Tensor(conv2d_9.w_0) have not GradNode, add GradNodeAccumulation0x55b7cc6c36b0 for it.
I1018 08:53:56.660619 4097533 eager_op_function.cc:33102] CurrentDeviceId: 0 from 0
I1018 08:53:56.660622 4097533 dygraph_functions.cc:65227] Running AD API: gaussian
I1018 08:53:56.660625 4097533 dygraph_functions.cc:65247] { Input: []} 
I1018 08:53:56.660640 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 590336(0x30080ff5c3b4000), and remaining 0
I1018 08:53:56.660655 4097533 dygraph_functions.cc:65263] Finish AD API: gaussian
I1018 08:53:56.660665 4097533 dygraph_functions.cc:65277] { Input: [],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cc6c3e30 }]), ] } 
I1018 08:53:56.660820 4097533 eager.cc:118] Tensor(batch_norm2d_9.w_0) have not GradNode, add GradNodeAccumulation0x55b7cc6c4610 for it.
I1018 08:53:56.660840 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.660845 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.660848 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.660854 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_9.w_0, Initialized: 0, Ptr: 0x55b7cc6c3590 }]), ]} 
I1018 08:53:56.660869 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1024(0x30080ff5c446000), and remaining 0
I1018 08:53:56.660883 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.660894 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_9.w_0, Initialized: 1, Ptr: 0x55b7cc6c3590 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_9.w_0, Initialized: 1, Ptr: 0x55b7cc6c3590 }]), ] } 
I1018 08:53:56.660956 4097533 eager.cc:118] Tensor(batch_norm2d_9.b_0) have not GradNode, add GradNodeAccumulation0x55b7b7056210 for it.
I1018 08:53:56.660974 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.660977 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.660980 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.660985 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_9.b_0, Initialized: 0, Ptr: 0x55b7b70560f0 }]), ]} 
I1018 08:53:56.660997 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1024(0x30080ff5c448000), and remaining 0
I1018 08:53:56.661008 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.661018 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_9.b_0, Initialized: 1, Ptr: 0x55b7b70560f0 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_9.b_0, Initialized: 1, Ptr: 0x55b7b70560f0 }]), ] } 
I1018 08:53:56.661123 4097533 eager.cc:118] Tensor(batch_norm2d_9.w_1) have not GradNode, add GradNodeAccumulation0x55b7b70570d0 for it.
I1018 08:53:56.661141 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.661146 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.661149 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.661154 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_9.w_1, Initialized: 0, Ptr: 0x55b7b7056fb0 }]), ]} 
I1018 08:53:56.661166 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1024(0x30080ff5c44a000), and remaining 0
I1018 08:53:56.661178 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.661192 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_9.w_1, Initialized: 1, Ptr: 0x55b7b7056fb0 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_9.w_1, Initialized: 1, Ptr: 0x55b7b7056fb0 }]), ] } 
I1018 08:53:56.661306 4097533 eager.cc:118] Tensor(batch_norm2d_9.w_2) have not GradNode, add GradNodeAccumulation0x55b7b7057f90 for it.
I1018 08:53:56.661325 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.661330 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.661334 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.661339 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_9.w_2, Initialized: 0, Ptr: 0x55b7b7057e70 }]), ]} 
I1018 08:53:56.661351 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1024(0x30080ff5c44c000), and remaining 0
I1018 08:53:56.661365 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.661374 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_9.w_2, Initialized: 1, Ptr: 0x55b7b7057e70 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_9.w_2, Initialized: 1, Ptr: 0x55b7b7057e70 }]), ] } 
I1018 08:53:56.661654 4097533 eager.cc:118] Tensor(conv2d_10.w_0) have not GradNode, add GradNodeAccumulation0x55b7b7058e80 for it.
I1018 08:53:56.661682 4097533 eager_op_function.cc:33102] CurrentDeviceId: 0 from 0
I1018 08:53:56.661686 4097533 dygraph_functions.cc:65227] Running AD API: gaussian
I1018 08:53:56.661688 4097533 dygraph_functions.cc:65247] { Input: []} 
I1018 08:53:56.661705 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 131584(0x30080ff5c44e000), and remaining 0
I1018 08:53:56.661718 4097533 dygraph_functions.cc:65263] Finish AD API: gaussian
I1018 08:53:56.661728 4097533 dygraph_functions.cc:65277] { Input: [],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b7059600 }]), ] } 
I1018 08:53:56.661885 4097533 eager.cc:118] Tensor(batch_norm2d_10.w_0) have not GradNode, add GradNodeAccumulation0x55b7b780df50 for it.
I1018 08:53:56.661904 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.661911 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.661913 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.661919 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_10.w_0, Initialized: 0, Ptr: 0x55b7b7058d60 }]), ]} 
I1018 08:53:56.661934 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1536(0x30080ff5c470000), and remaining 0
I1018 08:53:56.661948 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.661959 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_10.w_0, Initialized: 1, Ptr: 0x55b7b7058d60 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_10.w_0, Initialized: 1, Ptr: 0x55b7b7058d60 }]), ] } 
I1018 08:53:56.662022 4097533 eager.cc:118] Tensor(batch_norm2d_10.b_0) have not GradNode, add GradNodeAccumulation0x55b7b780ee00 for it.
I1018 08:53:56.662039 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.662043 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.662046 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.662051 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_10.b_0, Initialized: 0, Ptr: 0x55b7b780ece0 }]), ]} 
I1018 08:53:56.662063 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1536(0x30080ff5c472000), and remaining 0
I1018 08:53:56.662074 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.662084 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_10.b_0, Initialized: 1, Ptr: 0x55b7b780ece0 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_10.b_0, Initialized: 1, Ptr: 0x55b7b780ece0 }]), ] } 
I1018 08:53:56.662191 4097533 eager.cc:118] Tensor(batch_norm2d_10.w_1) have not GradNode, add GradNodeAccumulation0x55b7b780fcc0 for it.
I1018 08:53:56.662214 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.662218 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.662221 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.662227 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_10.w_1, Initialized: 0, Ptr: 0x55b7b780fba0 }]), ]} 
I1018 08:53:56.662240 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1536(0x30080ff5c474000), and remaining 0
I1018 08:53:56.662251 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.662261 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_10.w_1, Initialized: 1, Ptr: 0x55b7b780fba0 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_10.w_1, Initialized: 1, Ptr: 0x55b7b780fba0 }]), ] } 
I1018 08:53:56.662365 4097533 eager.cc:118] Tensor(batch_norm2d_10.w_2) have not GradNode, add GradNodeAccumulation0x55b7b7810b80 for it.
I1018 08:53:56.662382 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.662386 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.662389 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.662395 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_10.w_2, Initialized: 0, Ptr: 0x55b7b7810a60 }]), ]} 
I1018 08:53:56.662407 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1536(0x30080ff5c476000), and remaining 0
I1018 08:53:56.662420 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.662428 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_10.w_2, Initialized: 1, Ptr: 0x55b7b7810a60 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_10.w_2, Initialized: 1, Ptr: 0x55b7b7810a60 }]), ] } 
I1018 08:53:56.662740 4097533 eager.cc:118] Tensor(conv2d_11.w_0) have not GradNode, add GradNodeAccumulation0x55b7c4edec60 for it.
I1018 08:53:56.662766 4097533 eager_op_function.cc:33102] CurrentDeviceId: 0 from 0
I1018 08:53:56.662770 4097533 dygraph_functions.cc:65227] Running AD API: gaussian
I1018 08:53:56.662772 4097533 dygraph_functions.cc:65247] { Input: []} 
I1018 08:53:56.662788 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1180160(0x30080ff5c478000), and remaining 0
I1018 08:53:56.662802 4097533 dygraph_functions.cc:65263] Finish AD API: gaussian
I1018 08:53:56.662812 4097533 dygraph_functions.cc:65277] { Input: [],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4edf3e0 }]), ] } 
I1018 08:53:56.662964 4097533 eager.cc:118] Tensor(batch_norm2d_11.w_0) have not GradNode, add GradNodeAccumulation0x55b7c4edfbc0 for it.
I1018 08:53:56.662986 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.662992 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.662994 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.663000 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_11.w_0, Initialized: 0, Ptr: 0x55b7b4b498d0 }]), ]} 
I1018 08:53:56.663015 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1536(0x30080ff5c59a000), and remaining 0
I1018 08:53:56.663029 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.663040 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_11.w_0, Initialized: 1, Ptr: 0x55b7b4b498d0 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_11.w_0, Initialized: 1, Ptr: 0x55b7b4b498d0 }]), ] } 
I1018 08:53:56.663102 4097533 eager.cc:118] Tensor(batch_norm2d_11.b_0) have not GradNode, add GradNodeAccumulation0x55b7c4ee0a70 for it.
I1018 08:53:56.663120 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.663123 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.663126 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.663131 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_11.b_0, Initialized: 0, Ptr: 0x55b7c4ee0950 }]), ]} 
I1018 08:53:56.663149 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1536(0x30080ff5c59c000), and remaining 0
I1018 08:53:56.663161 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.663172 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_11.b_0, Initialized: 1, Ptr: 0x55b7c4ee0950 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_11.b_0, Initialized: 1, Ptr: 0x55b7c4ee0950 }]), ] } 
I1018 08:53:56.663278 4097533 eager.cc:118] Tensor(batch_norm2d_11.w_1) have not GradNode, add GradNodeAccumulation0x55b7c5cec9a0 for it.
I1018 08:53:56.663298 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.663303 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.663306 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.663311 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_11.w_1, Initialized: 0, Ptr: 0x55b7b7059750 }]), ]} 
I1018 08:53:56.663323 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1536(0x30080ff5c59e000), and remaining 0
I1018 08:53:56.663336 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.663345 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_11.w_1, Initialized: 1, Ptr: 0x55b7b7059750 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_11.w_1, Initialized: 1, Ptr: 0x55b7b7059750 }]), ] } 
I1018 08:53:56.663448 4097533 eager.cc:118] Tensor(batch_norm2d_11.w_2) have not GradNode, add GradNodeAccumulation0x55b7c5ced860 for it.
I1018 08:53:56.663465 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.663470 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.663473 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.663478 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_11.w_2, Initialized: 0, Ptr: 0x55b7c5ced740 }]), ]} 
I1018 08:53:56.663491 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1536(0x30080ff5c5a0000), and remaining 0
I1018 08:53:56.663501 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.663511 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_11.w_2, Initialized: 1, Ptr: 0x55b7c5ced740 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_11.w_2, Initialized: 1, Ptr: 0x55b7c5ced740 }]), ] } 
I1018 08:53:56.663823 4097533 eager.cc:118] Tensor(conv2d_12.w_0) have not GradNode, add GradNodeAccumulation0x55b7c5cee750 for it.
I1018 08:53:56.663856 4097533 eager_op_function.cc:33102] CurrentDeviceId: 0 from 0
I1018 08:53:56.663861 4097533 dygraph_functions.cc:65227] Running AD API: gaussian
I1018 08:53:56.663862 4097533 dygraph_functions.cc:65247] { Input: []} 
I1018 08:53:56.663879 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2359808(0x30080ff5c5a2000), and remaining 0
I1018 08:53:56.663895 4097533 dygraph_functions.cc:65263] Finish AD API: gaussian
I1018 08:53:56.663905 4097533 dygraph_functions.cc:65277] { Input: [],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c5ceeed0 }]), ] } 
I1018 08:53:56.664060 4097533 eager.cc:118] Tensor(batch_norm2d_12.w_0) have not GradNode, add GradNodeAccumulation0x55b7c5cef6b0 for it.
I1018 08:53:56.664083 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.664088 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.664091 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.664098 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_12.w_0, Initialized: 0, Ptr: 0x55b7c5cee630 }]), ]} 
I1018 08:53:56.664112 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1536(0x30080ff5c7e4000), and remaining 0
I1018 08:53:56.664127 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.664139 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_12.w_0, Initialized: 1, Ptr: 0x55b7c5cee630 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_12.w_0, Initialized: 1, Ptr: 0x55b7c5cee630 }]), ] } 
I1018 08:53:56.664207 4097533 eager.cc:118] Tensor(batch_norm2d_12.b_0) have not GradNode, add GradNodeAccumulation0x55b7cc3975d0 for it.
I1018 08:53:56.664224 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.664229 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.664232 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.664237 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_12.b_0, Initialized: 0, Ptr: 0x55b7cc3974b0 }]), ]} 
I1018 08:53:56.664249 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1536(0x30080ff5c7e6000), and remaining 0
I1018 08:53:56.664263 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.664271 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_12.b_0, Initialized: 1, Ptr: 0x55b7cc3974b0 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_12.b_0, Initialized: 1, Ptr: 0x55b7cc3974b0 }]), ] } 
I1018 08:53:56.664377 4097533 eager.cc:118] Tensor(batch_norm2d_12.w_1) have not GradNode, add GradNodeAccumulation0x55b7cc398490 for it.
I1018 08:53:56.664394 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.664399 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.664402 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.664407 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_12.w_1, Initialized: 0, Ptr: 0x55b7cc398370 }]), ]} 
I1018 08:53:56.664419 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1536(0x30080ff5c7e8000), and remaining 0
I1018 08:53:56.664431 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.664440 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_12.w_1, Initialized: 1, Ptr: 0x55b7cc398370 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_12.w_1, Initialized: 1, Ptr: 0x55b7cc398370 }]), ] } 
I1018 08:53:56.664543 4097533 eager.cc:118] Tensor(batch_norm2d_12.w_2) have not GradNode, add GradNodeAccumulation0x55b7cc399350 for it.
I1018 08:53:56.664559 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.664564 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.664567 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.664572 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_12.w_2, Initialized: 0, Ptr: 0x55b7cc399230 }]), ]} 
I1018 08:53:56.664584 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1536(0x30080ff5c7ea000), and remaining 0
I1018 08:53:56.664595 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.664605 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_12.w_2, Initialized: 1, Ptr: 0x55b7cc399230 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_12.w_2, Initialized: 1, Ptr: 0x55b7cc399230 }]), ] } 
I1018 08:53:56.664872 4097533 eager.cc:118] Tensor(conv2d_13.w_0) have not GradNode, add GradNodeAccumulation0x55b7cc39a490 for it.
I1018 08:53:56.664901 4097533 eager_op_function.cc:33102] CurrentDeviceId: 0 from 0
I1018 08:53:56.664904 4097533 dygraph_functions.cc:65227] Running AD API: gaussian
I1018 08:53:56.664907 4097533 dygraph_functions.cc:65247] { Input: []} 
I1018 08:53:56.664923 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2359808(0x30080ff5c7ec000), and remaining 0
I1018 08:53:56.664937 4097533 dygraph_functions.cc:65263] Finish AD API: gaussian
I1018 08:53:56.664947 4097533 dygraph_functions.cc:65277] { Input: [],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cc39ac10 }]), ] } 
I1018 08:53:56.665107 4097533 eager.cc:118] Tensor(batch_norm2d_13.w_0) have not GradNode, add GradNodeAccumulation0x55b7cc4bd4b0 for it.
I1018 08:53:56.665127 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.665136 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.665139 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.665145 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_13.w_0, Initialized: 0, Ptr: 0x55b7cc39a370 }]), ]} 
I1018 08:53:56.665161 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1536(0x30080ff5ca2e000), and remaining 0
I1018 08:53:56.665175 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.665186 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_13.w_0, Initialized: 1, Ptr: 0x55b7cc39a370 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_13.w_0, Initialized: 1, Ptr: 0x55b7cc39a370 }]), ] } 
I1018 08:53:56.665258 4097533 eager.cc:118] Tensor(batch_norm2d_13.b_0) have not GradNode, add GradNodeAccumulation0x55b7cc4be370 for it.
I1018 08:53:56.665277 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.665282 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.665285 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.665290 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_13.b_0, Initialized: 0, Ptr: 0x55b7cc4be250 }]), ]} 
I1018 08:53:56.665302 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1536(0x30080ff5ca30000), and remaining 0
I1018 08:53:56.665315 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.665324 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_13.b_0, Initialized: 1, Ptr: 0x55b7cc4be250 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_13.b_0, Initialized: 1, Ptr: 0x55b7cc4be250 }]), ] } 
I1018 08:53:56.665429 4097533 eager.cc:118] Tensor(batch_norm2d_13.w_1) have not GradNode, add GradNodeAccumulation0x55b7cc4bf230 for it.
I1018 08:53:56.665447 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.665452 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.665455 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.665460 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_13.w_1, Initialized: 0, Ptr: 0x55b7cc4bf110 }]), ]} 
I1018 08:53:56.665472 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1536(0x30080ff5ca32000), and remaining 0
I1018 08:53:56.665484 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.665494 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_13.w_1, Initialized: 1, Ptr: 0x55b7cc4bf110 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_13.w_1, Initialized: 1, Ptr: 0x55b7cc4bf110 }]), ] } 
I1018 08:53:56.665596 4097533 eager.cc:118] Tensor(batch_norm2d_13.w_2) have not GradNode, add GradNodeAccumulation0x55b7cc4c00f0 for it.
I1018 08:53:56.665613 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.665617 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.665621 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.665627 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_13.w_2, Initialized: 0, Ptr: 0x55b7cc4bffd0 }]), ]} 
I1018 08:53:56.665637 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1536(0x30080ff5ca34000), and remaining 0
I1018 08:53:56.665649 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.665658 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_13.w_2, Initialized: 1, Ptr: 0x55b7cc4bffd0 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_13.w_2, Initialized: 1, Ptr: 0x55b7cc4bffd0 }]), ] } 
I1018 08:53:56.665917 4097533 eager.cc:118] Tensor(conv2d_14.w_0) have not GradNode, add GradNodeAccumulation0x55b7b68cc840 for it.
I1018 08:53:56.665943 4097533 eager_op_function.cc:33102] CurrentDeviceId: 0 from 0
I1018 08:53:56.665947 4097533 dygraph_functions.cc:65227] Running AD API: gaussian
I1018 08:53:56.665949 4097533 dygraph_functions.cc:65247] { Input: []} 
I1018 08:53:56.665971 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2359808(0x30080ff5ca36000), and remaining 0
I1018 08:53:56.665984 4097533 dygraph_functions.cc:65263] Finish AD API: gaussian
I1018 08:53:56.665994 4097533 dygraph_functions.cc:65277] { Input: [],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b68ccfc0 }]), ] } 
I1018 08:53:56.666149 4097533 eager.cc:118] Tensor(batch_norm2d_14.w_0) have not GradNode, add GradNodeAccumulation0x55b7b68cd7a0 for it.
I1018 08:53:56.666168 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.666173 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.666177 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.666182 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_14.w_0, Initialized: 0, Ptr: 0x55b7cc4c0e90 }]), ]} 
I1018 08:53:56.666198 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1536(0x30080ff5cc78000), and remaining 0
I1018 08:53:56.666213 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.666222 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_14.w_0, Initialized: 1, Ptr: 0x55b7cc4c0e90 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_14.w_0, Initialized: 1, Ptr: 0x55b7cc4c0e90 }]), ] } 
I1018 08:53:56.666285 4097533 eager.cc:118] Tensor(batch_norm2d_14.b_0) have not GradNode, add GradNodeAccumulation0x55b7b68ce650 for it.
I1018 08:53:56.666301 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.666306 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.666309 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.666314 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_14.b_0, Initialized: 0, Ptr: 0x55b7b68ce530 }]), ]} 
I1018 08:53:56.666326 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1536(0x30080ff5cc7a000), and remaining 0
I1018 08:53:56.666337 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.666347 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_14.b_0, Initialized: 1, Ptr: 0x55b7b68ce530 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_14.b_0, Initialized: 1, Ptr: 0x55b7b68ce530 }]), ] } 
I1018 08:53:56.666453 4097533 eager.cc:118] Tensor(batch_norm2d_14.w_1) have not GradNode, add GradNodeAccumulation0x55b7b68cf510 for it.
I1018 08:53:56.666471 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.666474 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.666478 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.666483 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_14.w_1, Initialized: 0, Ptr: 0x55b7b68cf3f0 }]), ]} 
I1018 08:53:56.666496 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1536(0x30080ff5cc7c000), and remaining 0
I1018 08:53:56.666507 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.666517 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_14.w_1, Initialized: 1, Ptr: 0x55b7b68cf3f0 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_14.w_1, Initialized: 1, Ptr: 0x55b7b68cf3f0 }]), ] } 
I1018 08:53:56.666618 4097533 eager.cc:118] Tensor(batch_norm2d_14.w_2) have not GradNode, add GradNodeAccumulation0x55b7b68d03d0 for it.
I1018 08:53:56.666635 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.666639 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.666642 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.666648 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_14.w_2, Initialized: 0, Ptr: 0x55b7b68d02b0 }]), ]} 
I1018 08:53:56.666659 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1536(0x30080ff5cc7e000), and remaining 0
I1018 08:53:56.666671 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.666685 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_14.w_2, Initialized: 1, Ptr: 0x55b7b68d02b0 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_14.w_2, Initialized: 1, Ptr: 0x55b7b68d02b0 }]), ] } 
I1018 08:53:56.666954 4097533 eager.cc:118] Tensor(conv2d_15.w_0) have not GradNode, add GradNodeAccumulation0x55b7b759fa30 for it.
I1018 08:53:56.666980 4097533 eager_op_function.cc:33102] CurrentDeviceId: 0 from 0
I1018 08:53:56.666983 4097533 dygraph_functions.cc:65227] Running AD API: gaussian
I1018 08:53:56.666985 4097533 dygraph_functions.cc:65247] { Input: []} 
I1018 08:53:56.667001 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 524800(0x30080ff5cc80000), and remaining 0
I1018 08:53:56.667016 4097533 dygraph_functions.cc:65263] Finish AD API: gaussian
I1018 08:53:56.667024 4097533 dygraph_functions.cc:65277] { Input: [],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b75a01b0 }]), ] } 
I1018 08:53:56.667176 4097533 eager.cc:118] Tensor(batch_norm2d_15.w_0) have not GradNode, add GradNodeAccumulation0x55b7b3f217c0 for it.
I1018 08:53:56.667196 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.667201 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.667204 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.667209 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_15.w_0, Initialized: 0, Ptr: 0x55b7b759f910 }]), ]} 
I1018 08:53:56.667224 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2560(0x30080ff5cd02000), and remaining 0
I1018 08:53:56.667238 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.667249 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_15.w_0, Initialized: 1, Ptr: 0x55b7b759f910 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_15.w_0, Initialized: 1, Ptr: 0x55b7b759f910 }]), ] } 
I1018 08:53:56.667310 4097533 eager.cc:118] Tensor(batch_norm2d_15.b_0) have not GradNode, add GradNodeAccumulation0x55b7b75a2fe0 for it.
I1018 08:53:56.667330 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.667335 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.667337 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.667342 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_15.b_0, Initialized: 0, Ptr: 0x55b7b75a2ec0 }]), ]} 
I1018 08:53:56.667354 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2560(0x30080ff5cd04000), and remaining 0
I1018 08:53:56.667366 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.667375 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_15.b_0, Initialized: 1, Ptr: 0x55b7b75a2ec0 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_15.b_0, Initialized: 1, Ptr: 0x55b7b75a2ec0 }]), ] } 
I1018 08:53:56.667476 4097533 eager.cc:118] Tensor(batch_norm2d_15.w_1) have not GradNode, add GradNodeAccumulation0x55b7c4d5fa60 for it.
I1018 08:53:56.667495 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.667498 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.667501 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.667507 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_15.w_1, Initialized: 0, Ptr: 0x55b7c4d5f940 }]), ]} 
I1018 08:53:56.667519 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2560(0x30080ff5cd06000), and remaining 0
I1018 08:53:56.667531 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.667541 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_15.w_1, Initialized: 1, Ptr: 0x55b7c4d5f940 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_15.w_1, Initialized: 1, Ptr: 0x55b7c4d5f940 }]), ] } 
I1018 08:53:56.667641 4097533 eager.cc:118] Tensor(batch_norm2d_15.w_2) have not GradNode, add GradNodeAccumulation0x55b7c4d60920 for it.
I1018 08:53:56.667663 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.667668 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.667671 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.667676 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_15.w_2, Initialized: 0, Ptr: 0x55b7c4d60800 }]), ]} 
I1018 08:53:56.667688 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2560(0x30080ff5cd08000), and remaining 0
I1018 08:53:56.667701 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.667709 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_15.w_2, Initialized: 1, Ptr: 0x55b7c4d60800 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_15.w_2, Initialized: 1, Ptr: 0x55b7c4d60800 }]), ] } 
I1018 08:53:56.668012 4097533 eager.cc:118] Tensor(conv2d_16.w_0) have not GradNode, add GradNodeAccumulation0x55b7c4d61a60 for it.
I1018 08:53:56.668040 4097533 eager_op_function.cc:33102] CurrentDeviceId: 0 from 0
I1018 08:53:56.668043 4097533 dygraph_functions.cc:65227] Running AD API: gaussian
I1018 08:53:56.668045 4097533 dygraph_functions.cc:65247] { Input: []} 
I1018 08:53:56.668061 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 4719104(0x30080ff5cd0a000), and remaining 0
I1018 08:53:56.668074 4097533 dygraph_functions.cc:65263] Finish AD API: gaussian
I1018 08:53:56.668084 4097533 dygraph_functions.cc:65277] { Input: [],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4d621e0 }]), ] } 
I1018 08:53:56.668236 4097533 eager.cc:118] Tensor(batch_norm2d_16.w_0) have not GradNode, add GradNodeAccumulation0x55b7c4d629c0 for it.
I1018 08:53:56.668255 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.668262 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.668264 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.668270 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_16.w_0, Initialized: 0, Ptr: 0x55b7c4d61940 }]), ]} 
I1018 08:53:56.668284 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2560(0x30080ff5d18c000), and remaining 0
I1018 08:53:56.668299 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.668309 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_16.w_0, Initialized: 1, Ptr: 0x55b7c4d61940 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_16.w_0, Initialized: 1, Ptr: 0x55b7c4d61940 }]), ] } 
I1018 08:53:56.668376 4097533 eager.cc:118] Tensor(batch_norm2d_16.b_0) have not GradNode, add GradNodeAccumulation0x55b7b796c2f0 for it.
I1018 08:53:56.668393 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.668398 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.668401 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.668406 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_16.b_0, Initialized: 0, Ptr: 0x55b7b796c1d0 }]), ]} 
I1018 08:53:56.668419 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2560(0x30080ff5d18e000), and remaining 0
I1018 08:53:56.668431 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.668442 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_16.b_0, Initialized: 1, Ptr: 0x55b7b796c1d0 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_16.b_0, Initialized: 1, Ptr: 0x55b7b796c1d0 }]), ] } 
I1018 08:53:56.668555 4097533 eager.cc:118] Tensor(batch_norm2d_16.w_1) have not GradNode, add GradNodeAccumulation0x55b7b796d1b0 for it.
I1018 08:53:56.668574 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.668579 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.668582 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.668587 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_16.w_1, Initialized: 0, Ptr: 0x55b7b796d090 }]), ]} 
I1018 08:53:56.668604 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2560(0x30080ff5d190000), and remaining 0
I1018 08:53:56.668617 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.668627 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_16.w_1, Initialized: 1, Ptr: 0x55b7b796d090 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_16.w_1, Initialized: 1, Ptr: 0x55b7b796d090 }]), ] } 
I1018 08:53:56.668730 4097533 eager.cc:118] Tensor(batch_norm2d_16.w_2) have not GradNode, add GradNodeAccumulation0x55b7b796e070 for it.
I1018 08:53:56.668747 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.668752 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.668756 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.668761 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_16.w_2, Initialized: 0, Ptr: 0x55b7b796df50 }]), ]} 
I1018 08:53:56.668772 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2560(0x30080ff5d192000), and remaining 0
I1018 08:53:56.668784 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.668793 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_16.w_2, Initialized: 1, Ptr: 0x55b7b796df50 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_16.w_2, Initialized: 1, Ptr: 0x55b7b796df50 }]), ] } 
I1018 08:53:56.669051 4097533 eager.cc:118] Tensor(conv2d_17.w_0) have not GradNode, add GradNodeAccumulation0x55b7b796ef60 for it.
I1018 08:53:56.669078 4097533 eager_op_function.cc:33102] CurrentDeviceId: 0 from 0
I1018 08:53:56.669082 4097533 dygraph_functions.cc:65227] Running AD API: gaussian
I1018 08:53:56.669085 4097533 dygraph_functions.cc:65247] { Input: []} 
I1018 08:53:56.669100 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 9437696(0x30080ff5d194000), and remaining 0
I1018 08:53:56.669114 4097533 dygraph_functions.cc:65263] Finish AD API: gaussian
I1018 08:53:56.669124 4097533 dygraph_functions.cc:65277] { Input: [],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b796f6e0 }]), ] } 
I1018 08:53:56.669292 4097533 eager.cc:118] Tensor(batch_norm2d_17.w_0) have not GradNode, add GradNodeAccumulation0x55b7b796fe00 for it.
I1018 08:53:56.669313 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.669319 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.669322 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.669328 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_17.w_0, Initialized: 0, Ptr: 0x55b7b796ee40 }]), ]} 
I1018 08:53:56.669344 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2560(0x30080ff5da96000), and remaining 0
I1018 08:53:56.669359 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.669369 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_17.w_0, Initialized: 1, Ptr: 0x55b7b796ee40 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_17.w_0, Initialized: 1, Ptr: 0x55b7b796ee40 }]), ] } 
I1018 08:53:56.669432 4097533 eager.cc:118] Tensor(batch_norm2d_17.b_0) have not GradNode, add GradNodeAccumulation0x55b7bea3a5d0 for it.
I1018 08:53:56.669449 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.669453 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.669456 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.669461 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_17.b_0, Initialized: 0, Ptr: 0x55b7bea3a4b0 }]), ]} 
I1018 08:53:56.669473 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2560(0x30080ff5da98000), and remaining 0
I1018 08:53:56.669485 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.669494 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_17.b_0, Initialized: 1, Ptr: 0x55b7bea3a4b0 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_17.b_0, Initialized: 1, Ptr: 0x55b7bea3a4b0 }]), ] } 
I1018 08:53:56.669600 4097533 eager.cc:118] Tensor(batch_norm2d_17.w_1) have not GradNode, add GradNodeAccumulation0x55b7bea3b490 for it.
I1018 08:53:56.669621 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.669626 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.669629 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.669634 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_17.w_1, Initialized: 0, Ptr: 0x55b7bea3b370 }]), ]} 
I1018 08:53:56.669646 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2560(0x30080ff5da9a000), and remaining 0
I1018 08:53:56.669659 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.669668 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_17.w_1, Initialized: 1, Ptr: 0x55b7bea3b370 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_17.w_1, Initialized: 1, Ptr: 0x55b7bea3b370 }]), ] } 
I1018 08:53:56.669770 4097533 eager.cc:118] Tensor(batch_norm2d_17.w_2) have not GradNode, add GradNodeAccumulation0x55b7bea3c350 for it.
I1018 08:53:56.669786 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.669790 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.669793 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.669798 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_17.w_2, Initialized: 0, Ptr: 0x55b7bea3c230 }]), ]} 
I1018 08:53:56.669811 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2560(0x30080ff5da9c000), and remaining 0
I1018 08:53:56.669821 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.669831 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_17.w_2, Initialized: 1, Ptr: 0x55b7bea3c230 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_17.w_2, Initialized: 1, Ptr: 0x55b7bea3c230 }]), ] } 
I1018 08:53:56.670101 4097533 eager.cc:118] Tensor(conv2d_18.w_0) have not GradNode, add GradNodeAccumulation0x55b7bea3d490 for it.
I1018 08:53:56.670127 4097533 eager_op_function.cc:33102] CurrentDeviceId: 0 from 0
I1018 08:53:56.670131 4097533 dygraph_functions.cc:65227] Running AD API: gaussian
I1018 08:53:56.670133 4097533 dygraph_functions.cc:65247] { Input: []} 
I1018 08:53:56.670352 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 9437696(0x30080ff60000000), and remaining 0
I1018 08:53:56.670367 4097533 dygraph_functions.cc:65263] Finish AD API: gaussian
I1018 08:53:56.670377 4097533 dygraph_functions.cc:65277] { Input: [],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7bea3dc10 }]), ] } 
I1018 08:53:56.670536 4097533 eager.cc:118] Tensor(batch_norm2d_18.w_0) have not GradNode, add GradNodeAccumulation0x55b7c49f9190 for it.
I1018 08:53:56.670555 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.670562 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.670564 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.670570 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_18.w_0, Initialized: 0, Ptr: 0x55b7bea3d370 }]), ]} 
I1018 08:53:56.670585 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2560(0x30080ff60902000), and remaining 0
I1018 08:53:56.670599 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.670610 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_18.w_0, Initialized: 1, Ptr: 0x55b7bea3d370 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_18.w_0, Initialized: 1, Ptr: 0x55b7bea3d370 }]), ] } 
I1018 08:53:56.670673 4097533 eager.cc:118] Tensor(batch_norm2d_18.b_0) have not GradNode, add GradNodeAccumulation0x55b7c49fa040 for it.
I1018 08:53:56.670689 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.670699 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.670702 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.670707 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_18.b_0, Initialized: 0, Ptr: 0x55b7c49f9f20 }]), ]} 
I1018 08:53:56.670720 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2560(0x30080ff60904000), and remaining 0
I1018 08:53:56.670732 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.670742 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_18.b_0, Initialized: 1, Ptr: 0x55b7c49f9f20 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_18.b_0, Initialized: 1, Ptr: 0x55b7c49f9f20 }]), ] } 
I1018 08:53:56.670845 4097533 eager.cc:118] Tensor(batch_norm2d_18.w_1) have not GradNode, add GradNodeAccumulation0x55b7c49faf00 for it.
I1018 08:53:56.670861 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.670866 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.670868 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.670874 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_18.w_1, Initialized: 0, Ptr: 0x55b7c49fade0 }]), ]} 
I1018 08:53:56.670886 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2560(0x30080ff60906000), and remaining 0
I1018 08:53:56.670897 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.670907 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_18.w_1, Initialized: 1, Ptr: 0x55b7c49fade0 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_18.w_1, Initialized: 1, Ptr: 0x55b7c49fade0 }]), ] } 
I1018 08:53:56.671007 4097533 eager.cc:118] Tensor(batch_norm2d_18.w_2) have not GradNode, add GradNodeAccumulation0x55b7c49fbdc0 for it.
I1018 08:53:56.671023 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.671028 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.671031 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.671036 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_18.w_2, Initialized: 0, Ptr: 0x55b7c49fbca0 }]), ]} 
I1018 08:53:56.671048 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2560(0x30080ff60908000), and remaining 0
I1018 08:53:56.671059 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.671069 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_18.w_2, Initialized: 1, Ptr: 0x55b7c49fbca0 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_18.w_2, Initialized: 1, Ptr: 0x55b7c49fbca0 }]), ] } 
I1018 08:53:56.671324 4097533 eager.cc:118] Tensor(conv2d_19.w_0) have not GradNode, add GradNodeAccumulation0x55b7c49fccb0 for it.
I1018 08:53:56.671351 4097533 eager_op_function.cc:33102] CurrentDeviceId: 0 from 0
I1018 08:53:56.671355 4097533 dygraph_functions.cc:65227] Running AD API: gaussian
I1018 08:53:56.671357 4097533 dygraph_functions.cc:65247] { Input: []} 
I1018 08:53:56.671373 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 9437696(0x30080ff6090a000), and remaining 0
I1018 08:53:56.671387 4097533 dygraph_functions.cc:65263] Finish AD API: gaussian
I1018 08:53:56.671397 4097533 dygraph_functions.cc:65277] { Input: [],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c49fd430 }]), ] } 
I1018 08:53:56.671552 4097533 eager.cc:118] Tensor(batch_norm2d_19.w_0) have not GradNode, add GradNodeAccumulation0x55b7b8786540 for it.
I1018 08:53:56.671571 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.671576 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.671581 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.671586 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_19.w_0, Initialized: 0, Ptr: 0x55b7c49fcb90 }]), ]} 
I1018 08:53:56.671600 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2560(0x30080ff6120c000), and remaining 0
I1018 08:53:56.671618 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.671629 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_19.w_0, Initialized: 1, Ptr: 0x55b7c49fcb90 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_19.w_0, Initialized: 1, Ptr: 0x55b7c49fcb90 }]), ] } 
I1018 08:53:56.671691 4097533 eager.cc:118] Tensor(batch_norm2d_19.b_0) have not GradNode, add GradNodeAccumulation0x55b7b87873f0 for it.
I1018 08:53:56.671707 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.671712 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.671715 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.671720 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_19.b_0, Initialized: 0, Ptr: 0x55b7b87872d0 }]), ]} 
I1018 08:53:56.671732 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2560(0x30080ff6120e000), and remaining 0
I1018 08:53:56.671743 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.671752 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_19.b_0, Initialized: 1, Ptr: 0x55b7b87872d0 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_19.b_0, Initialized: 1, Ptr: 0x55b7b87872d0 }]), ] } 
I1018 08:53:56.671852 4097533 eager.cc:118] Tensor(batch_norm2d_19.w_1) have not GradNode, add GradNodeAccumulation0x55b7b87882b0 for it.
I1018 08:53:56.671869 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.671873 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.671876 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.671881 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_19.w_1, Initialized: 0, Ptr: 0x55b7b8788190 }]), ]} 
I1018 08:53:56.671893 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2560(0x30080ff61210000), and remaining 0
I1018 08:53:56.671905 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.671914 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_19.w_1, Initialized: 1, Ptr: 0x55b7b8788190 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_19.w_1, Initialized: 1, Ptr: 0x55b7b8788190 }]), ] } 
I1018 08:53:56.672015 4097533 eager.cc:118] Tensor(batch_norm2d_19.w_2) have not GradNode, add GradNodeAccumulation0x55b7b8789170 for it.
I1018 08:53:56.672032 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.672037 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.672040 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.672045 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: batch_norm2d_19.w_2, Initialized: 0, Ptr: 0x55b7b8789050 }]), ]} 
I1018 08:53:56.672057 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2560(0x30080ff61212000), and remaining 0
I1018 08:53:56.672068 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.672077 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: batch_norm2d_19.w_2, Initialized: 1, Ptr: 0x55b7b8789050 }]), ],  
 Output: [ 
( out , [{ Name: batch_norm2d_19.w_2, Initialized: 1, Ptr: 0x55b7b8789050 }]), ] } 
I1018 08:53:56.672461 4097533 eager.cc:118] Tensor(linear_0.w_0) have not GradNode, add GradNodeAccumulation0x55b7b878a500 for it.
I1018 08:53:56.672554 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.672559 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.672564 4097533 eager_op_function.cc:38682] CurrentDeviceId: 0 from 0
I1018 08:53:56.672580 4097533 dygraph_functions.cc:74634] Running AD API: uniform
I1018 08:53:56.672583 4097533 dygraph_functions.cc:74654] { Input: []} 
I1018 08:53:56.672618 4097533 uniform_kernel.cc:45] 512, 1000
I1018 08:53:56.672636 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2048512(0x30080ff61214000), and remaining 0
I1018 08:53:56.679783 4097533 mlu_funcs.h:39] TensorCopy 512, 1000 from Place(cpu) to Place(mlu:0)
I1018 08:53:56.679800 4097533 mlu_funcs.h:55] src:0x55b7cf90a000, dst:0x30080ff61214000
I1018 08:53:56.680014 4097533 dygraph_functions.cc:74670] Finish AD API: uniform
I1018 08:53:56.680027 4097533 dygraph_functions.cc:74684] { Input: [],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f66780 }]), ] } 
I1018 08:53:56.680153 4097533 eager.cc:118] Tensor(linear_0.b_0) have not GradNode, add GradNodeAccumulation0x55b7b6f66d60 for it.
I1018 08:53:56.680179 4097533 eager_utils.cc:2212] type_name: float
I1018 08:53:56.680186 4097533 eager_op_function.cc:32200] CurrentDeviceId: 0 from 0
I1018 08:53:56.680191 4097533 dygraph_functions.cc:63400] Running AD API: full_
I1018 08:53:56.680197 4097533 dygraph_functions.cc:63440] { Input: [ 
( output , [{ Name: linear_0.b_0, Initialized: 0, Ptr: 0x55b7b878a3e0 }]), ]} 
I1018 08:53:56.680217 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 4096(0x30080ff6140a000), and remaining 0
I1018 08:53:56.680234 4097533 dygraph_functions.cc:63456] Finish AD API: full_
I1018 08:53:56.680245 4097533 dygraph_functions.cc:63473] { Input: [ 
( output , [{ Name: linear_0.b_0, Initialized: 1, Ptr: 0x55b7b878a3e0 }]), ],  
 Output: [ 
( out , [{ Name: linear_0.b_0, Initialized: 1, Ptr: 0x55b7b878a3e0 }]), ] } 
I1018 08:53:57.248696 4097533 eager.cc:118] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x55b7b6f77850 for it.
I1018 08:53:57.248741 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.248813 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 37888(0x30080ff6140c000), and remaining 0
I1018 08:53:57.248898 4097533 eager.cc:118] Tensor(generated_tensor_1) have not GradNode, add GradNodeAccumulation0x55b7c22d1da0 for it.
I1018 08:53:57.248903 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.248911 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 512(0x30080ff61416000), and remaining 0
I1018 08:53:57.248942 4097533 eager.cc:118] Tensor(generated_tensor_2) have not GradNode, add GradNodeAccumulation0x55b7c22ce120 for it.
I1018 08:53:57.248946 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.248955 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 512(0x30080ff61418000), and remaining 0
I1018 08:53:57.248979 4097533 eager.cc:118] Tensor(generated_tensor_3) have not GradNode, add GradNodeAccumulation0x55b7c22d0310 for it.
I1018 08:53:57.248982 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.248989 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 512(0x30080ff6141a000), and remaining 0
I1018 08:53:57.249011 4097533 eager.cc:118] Tensor(generated_tensor_4) have not GradNode, add GradNodeAccumulation0x55b7c4ed9d80 for it.
I1018 08:53:57.249015 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.249022 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 512(0x30080ff6141c000), and remaining 0
I1018 08:53:57.249043 4097533 eager.cc:118] Tensor(generated_tensor_5) have not GradNode, add GradNodeAccumulation0x55b7c4e203f0 for it.
I1018 08:53:57.249047 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.249053 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 147968(0x30080ff6141e000), and remaining 0
I1018 08:53:57.249095 4097533 eager.cc:118] Tensor(generated_tensor_6) have not GradNode, add GradNodeAccumulation0x55b7b6f68f10 for it.
I1018 08:53:57.249099 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.249105 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 512(0x30080ff61444000), and remaining 0
I1018 08:53:57.249130 4097533 eager.cc:118] Tensor(generated_tensor_7) have not GradNode, add GradNodeAccumulation0x55b7c4cbbe60 for it.
I1018 08:53:57.249133 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.249140 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 512(0x30080ff61446000), and remaining 0
I1018 08:53:57.249171 4097533 eager.cc:118] Tensor(generated_tensor_8) have not GradNode, add GradNodeAccumulation0x55b7c416d790 for it.
I1018 08:53:57.249176 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.249181 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 512(0x30080ff61448000), and remaining 0
I1018 08:53:57.249203 4097533 eager.cc:118] Tensor(generated_tensor_9) have not GradNode, add GradNodeAccumulation0x55b7c22ce9b0 for it.
I1018 08:53:57.251488 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.251523 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 512(0x30080ff6144a000), and remaining 0
I1018 08:53:57.251578 4097533 eager.cc:118] Tensor(generated_tensor_10) have not GradNode, add GradNodeAccumulation0x55b7b6f6a3e0 for it.
I1018 08:53:57.251581 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.251590 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 147968(0x30080ff6144c000), and remaining 0
I1018 08:53:57.251632 4097533 eager.cc:118] Tensor(generated_tensor_11) have not GradNode, add GradNodeAccumulation0x55b7c4eda840 for it.
I1018 08:53:57.251636 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.251642 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 512(0x30080ff61472000), and remaining 0
I1018 08:53:57.251664 4097533 eager.cc:118] Tensor(generated_tensor_12) have not GradNode, add GradNodeAccumulation0x55b7b6f7d1c0 for it.
I1018 08:53:57.251667 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.251674 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 512(0x30080ff61474000), and remaining 0
I1018 08:53:57.251695 4097533 eager.cc:118] Tensor(generated_tensor_13) have not GradNode, add GradNodeAccumulation0x55b7c4ed9480 for it.
I1018 08:53:57.251698 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.251704 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 512(0x30080ff61476000), and remaining 0
I1018 08:53:57.251725 4097533 eager.cc:118] Tensor(generated_tensor_14) have not GradNode, add GradNodeAccumulation0x55b7b78ee3a0 for it.
I1018 08:53:57.251729 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.251735 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 512(0x30080ff61478000), and remaining 0
I1018 08:53:57.251756 4097533 eager.cc:118] Tensor(generated_tensor_15) have not GradNode, add GradNodeAccumulation0x55b7c1f6e260 for it.
I1018 08:53:57.251760 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.251765 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 147968(0x30080ff6147a000), and remaining 0
I1018 08:53:57.251804 4097533 eager.cc:118] Tensor(generated_tensor_16) have not GradNode, add GradNodeAccumulation0x55b7c4edc9f0 for it.
I1018 08:53:57.251807 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.251816 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 512(0x30080ff614a0000), and remaining 0
I1018 08:53:57.251838 4097533 eager.cc:118] Tensor(generated_tensor_17) have not GradNode, add GradNodeAccumulation0x55b7c4168f20 for it.
I1018 08:53:57.251842 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.251849 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 512(0x30080ff614a2000), and remaining 0
I1018 08:53:57.251870 4097533 eager.cc:118] Tensor(generated_tensor_18) have not GradNode, add GradNodeAccumulation0x55b7b6f7f8d0 for it.
I1018 08:53:57.251873 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.251881 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 512(0x30080ff614a4000), and remaining 0
I1018 08:53:57.251902 4097533 eager.cc:118] Tensor(generated_tensor_19) have not GradNode, add GradNodeAccumulation0x55b7cfe31390 for it.
I1018 08:53:57.251905 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.251912 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 512(0x30080ff614a6000), and remaining 0
I1018 08:53:57.251940 4097533 eager.cc:118] Tensor(generated_tensor_20) have not GradNode, add GradNodeAccumulation0x55b7b6f7e180 for it.
I1018 08:53:57.251943 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.251950 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 147968(0x30080ff614a8000), and remaining 0
I1018 08:53:57.251994 4097533 eager.cc:118] Tensor(generated_tensor_21) have not GradNode, add GradNodeAccumulation0x55b7c4e1c3a0 for it.
I1018 08:53:57.251997 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.252004 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 512(0x30080ff614ce000), and remaining 0
I1018 08:53:57.252029 4097533 eager.cc:118] Tensor(generated_tensor_22) have not GradNode, add GradNodeAccumulation0x55b7b78eb880 for it.
I1018 08:53:57.252032 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.252038 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 512(0x30080ff614d0000), and remaining 0
I1018 08:53:57.252063 4097533 eager.cc:118] Tensor(generated_tensor_23) have not GradNode, add GradNodeAccumulation0x55b7c416a1f0 for it.
I1018 08:53:57.252066 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.252074 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 512(0x30080ff614d2000), and remaining 0
I1018 08:53:57.252097 4097533 eager.cc:118] Tensor(generated_tensor_24) have not GradNode, add GradNodeAccumulation0x55b7c23e32e0 for it.
I1018 08:53:57.252101 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.252108 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 512(0x30080ff614d4000), and remaining 0
I1018 08:53:57.252130 4097533 eager.cc:118] Tensor(generated_tensor_25) have not GradNode, add GradNodeAccumulation0x55b7b70ed580 for it.
I1018 08:53:57.252132 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.252140 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 295424(0x30080ff614d6000), and remaining 0
I1018 08:53:57.252195 4097533 eager.cc:118] Tensor(generated_tensor_26) have not GradNode, add GradNodeAccumulation0x55b7c4edb750 for it.
I1018 08:53:57.252199 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.252205 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1024(0x30080ff61520000), and remaining 0
I1018 08:53:57.252229 4097533 eager.cc:118] Tensor(generated_tensor_27) have not GradNode, add GradNodeAccumulation0x55b7cfe2ff80 for it.
I1018 08:53:57.252233 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.252239 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1024(0x30080ff61522000), and remaining 0
I1018 08:53:57.252260 4097533 eager.cc:118] Tensor(generated_tensor_28) have not GradNode, add GradNodeAccumulation0x55b7b78ea300 for it.
I1018 08:53:57.252264 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.252270 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1024(0x30080ff61524000), and remaining 0
I1018 08:53:57.252290 4097533 eager.cc:118] Tensor(generated_tensor_29) have not GradNode, add GradNodeAccumulation0x55b7c1f6ef90 for it.
I1018 08:53:57.252293 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.252300 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1024(0x30080ff61526000), and remaining 0
I1018 08:53:57.252319 4097533 eager.cc:118] Tensor(generated_tensor_30) have not GradNode, add GradNodeAccumulation0x55b7c416b450 for it.
I1018 08:53:57.252323 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.252329 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 590336(0x30080ff61528000), and remaining 0
I1018 08:53:57.252584 4097533 eager.cc:118] Tensor(generated_tensor_31) have not GradNode, add GradNodeAccumulation0x55b7c416c190 for it.
I1018 08:53:57.252599 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.252620 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1024(0x30080ff615ba000), and remaining 0
I1018 08:53:57.252668 4097533 eager.cc:118] Tensor(generated_tensor_32) have not GradNode, add GradNodeAccumulation0x55b7c23e5100 for it.
I1018 08:53:57.252672 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.252681 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1024(0x30080ff615bc000), and remaining 0
I1018 08:53:57.252704 4097533 eager.cc:118] Tensor(generated_tensor_33) have not GradNode, add GradNodeAccumulation0x55b7c4e1def0 for it.
I1018 08:53:57.252707 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.252714 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1024(0x30080ff615be000), and remaining 0
I1018 08:53:57.252734 4097533 eager.cc:118] Tensor(generated_tensor_34) have not GradNode, add GradNodeAccumulation0x55b7c4e1edc0 for it.
I1018 08:53:57.252737 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.252744 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1024(0x30080ff615c0000), and remaining 0
I1018 08:53:57.252765 4097533 eager.cc:118] Tensor(generated_tensor_35) have not GradNode, add GradNodeAccumulation0x55b7c4cb7430 for it.
I1018 08:53:57.252768 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.252775 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 33280(0x30080ff615c2000), and remaining 0
I1018 08:53:57.252800 4097533 eager.cc:118] Tensor(generated_tensor_36) have not GradNode, add GradNodeAccumulation0x55b7c4cb8290 for it.
I1018 08:53:57.252804 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.252810 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1024(0x30080ff615cc000), and remaining 0
I1018 08:53:57.252831 4097533 eager.cc:118] Tensor(generated_tensor_37) have not GradNode, add GradNodeAccumulation0x55b7c4cb9290 for it.
I1018 08:53:57.252835 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.252840 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1024(0x30080ff615ce000), and remaining 0
I1018 08:53:57.252861 4097533 eager.cc:118] Tensor(generated_tensor_38) have not GradNode, add GradNodeAccumulation0x55b7c4cba1d0 for it.
I1018 08:53:57.252863 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.252869 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1024(0x30080ff615d0000), and remaining 0
I1018 08:53:57.252889 4097533 eager.cc:118] Tensor(generated_tensor_39) have not GradNode, add GradNodeAccumulation0x55b7c4b8c4b0 for it.
I1018 08:53:57.252893 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.252899 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1024(0x30080ff615d2000), and remaining 0
I1018 08:53:57.252919 4097533 eager.cc:118] Tensor(generated_tensor_40) have not GradNode, add GradNodeAccumulation0x55b7c4b8d420 for it.
I1018 08:53:57.252923 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.252929 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 590336(0x30080ff615d4000), and remaining 0
I1018 08:53:57.253121 4097533 eager.cc:118] Tensor(generated_tensor_41) have not GradNode, add GradNodeAccumulation0x55b7c4b8e280 for it.
I1018 08:53:57.253125 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.253132 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1024(0x30080ff61666000), and remaining 0
I1018 08:53:57.253154 4097533 eager.cc:118] Tensor(generated_tensor_42) have not GradNode, add GradNodeAccumulation0x55b7c4b8f440 for it.
I1018 08:53:57.253157 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.253165 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1024(0x30080ff61668000), and remaining 0
I1018 08:53:57.253185 4097533 eager.cc:118] Tensor(generated_tensor_43) have not GradNode, add GradNodeAccumulation0x55b7c4b903b0 for it.
I1018 08:53:57.253187 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.253193 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1024(0x30080ff6166a000), and remaining 0
I1018 08:53:57.253224 4097533 eager.cc:118] Tensor(generated_tensor_44) have not GradNode, add GradNodeAccumulation0x55b7b77cd7c0 for it.
I1018 08:53:57.253229 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.253235 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1024(0x30080ff6166c000), and remaining 0
I1018 08:53:57.253257 4097533 eager.cc:118] Tensor(generated_tensor_45) have not GradNode, add GradNodeAccumulation0x55b7b77ce4a0 for it.
I1018 08:53:57.253259 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.253266 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 590336(0x30080ff6166e000), and remaining 0
I1018 08:53:57.253599 4097533 eager.cc:118] Tensor(generated_tensor_46) have not GradNode, add GradNodeAccumulation0x55b7b77cf300 for it.
I1018 08:53:57.253603 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.253610 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1024(0x30080ff61700000), and remaining 0
I1018 08:53:57.253631 4097533 eager.cc:118] Tensor(generated_tensor_47) have not GradNode, add GradNodeAccumulation0x55b7b77cffe0 for it.
I1018 08:53:57.253635 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.253641 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1024(0x30080ff61702000), and remaining 0
I1018 08:53:57.253664 4097533 eager.cc:118] Tensor(generated_tensor_48) have not GradNode, add GradNodeAccumulation0x55b7b77d0cf0 for it.
I1018 08:53:57.253666 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.253672 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1024(0x30080ff61704000), and remaining 0
I1018 08:53:57.253693 4097533 eager.cc:118] Tensor(generated_tensor_49) have not GradNode, add GradNodeAccumulation0x55b7b77d1930 for it.
I1018 08:53:57.253696 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.253702 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1024(0x30080ff61706000), and remaining 0
I1018 08:53:57.253723 4097533 eager.cc:118] Tensor(generated_tensor_50) have not GradNode, add GradNodeAccumulation0x55b7b77d25b0 for it.
I1018 08:53:57.253726 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.253733 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1180160(0x30080ff61708000), and remaining 0
I1018 08:53:57.253990 4097533 eager.cc:118] Tensor(generated_tensor_51) have not GradNode, add GradNodeAccumulation0x55b7b78e4240 for it.
I1018 08:53:57.253995 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.254001 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1536(0x30080ff6182a000), and remaining 0
I1018 08:53:57.254024 4097533 eager.cc:118] Tensor(generated_tensor_52) have not GradNode, add GradNodeAccumulation0x55b7b78e4f20 for it.
I1018 08:53:57.254027 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.254033 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1536(0x30080ff6182c000), and remaining 0
I1018 08:53:57.254055 4097533 eager.cc:118] Tensor(generated_tensor_53) have not GradNode, add GradNodeAccumulation0x55b7c4ed85d0 for it.
I1018 08:53:57.254057 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.254063 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1536(0x30080ff6182e000), and remaining 0
I1018 08:53:57.254084 4097533 eager.cc:118] Tensor(generated_tensor_54) have not GradNode, add GradNodeAccumulation0x55b7b78e62b0 for it.
I1018 08:53:57.254087 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.254093 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1536(0x30080ff61830000), and remaining 0
I1018 08:53:57.254113 4097533 eager.cc:118] Tensor(generated_tensor_55) have not GradNode, add GradNodeAccumulation0x55b7c22d0770 for it.
I1018 08:53:57.254117 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.254122 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2359808(0x30080ff61832000), and remaining 0
I1018 08:53:57.254370 4097533 eager.cc:118] Tensor(generated_tensor_56) have not GradNode, add GradNodeAccumulation0x55b7b78e77d0 for it.
I1018 08:53:57.254375 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.254382 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1536(0x30080ff61a74000), and remaining 0
I1018 08:53:57.254405 4097533 eager.cc:118] Tensor(generated_tensor_57) have not GradNode, add GradNodeAccumulation0x55b7b78e84b0 for it.
I1018 08:53:57.254407 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.254415 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1536(0x30080ff61a76000), and remaining 0
I1018 08:53:57.254436 4097533 eager.cc:118] Tensor(generated_tensor_58) have not GradNode, add GradNodeAccumulation0x55b7c4e78100 for it.
I1018 08:53:57.254441 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.254446 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1536(0x30080ff61a78000), and remaining 0
I1018 08:53:57.254465 4097533 eager.cc:118] Tensor(generated_tensor_59) have not GradNode, add GradNodeAccumulation0x55b7c4edc350 for it.
I1018 08:53:57.254469 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.254475 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1536(0x30080ff61a7a000), and remaining 0
I1018 08:53:57.254495 4097533 eager.cc:118] Tensor(generated_tensor_60) have not GradNode, add GradNodeAccumulation0x55b7c4e796c0 for it.
I1018 08:53:57.254499 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.254505 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 131584(0x30080ff61a7c000), and remaining 0
I1018 08:53:57.254544 4097533 eager.cc:118] Tensor(generated_tensor_61) have not GradNode, add GradNodeAccumulation0x55b7c4e7a400 for it.
I1018 08:53:57.254549 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.254554 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1536(0x30080ff61a9e000), and remaining 0
I1018 08:53:57.254575 4097533 eager.cc:118] Tensor(generated_tensor_62) have not GradNode, add GradNodeAccumulation0x55b7c4e7b0e0 for it.
I1018 08:53:57.254580 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.254585 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1536(0x30080ff61aa0000), and remaining 0
I1018 08:53:57.254604 4097533 eager.cc:118] Tensor(generated_tensor_63) have not GradNode, add GradNodeAccumulation0x55b7c4e7bdc0 for it.
I1018 08:53:57.254608 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.254614 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1536(0x30080ff61aa2000), and remaining 0
I1018 08:53:57.254634 4097533 eager.cc:118] Tensor(generated_tensor_64) have not GradNode, add GradNodeAccumulation0x55b7b70f04f0 for it.
I1018 08:53:57.254637 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.254645 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1536(0x30080ff61aa4000), and remaining 0
I1018 08:53:57.254666 4097533 eager.cc:118] Tensor(generated_tensor_65) have not GradNode, add GradNodeAccumulation0x55b7c2d5a470 for it.
I1018 08:53:57.254669 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.254676 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2359808(0x30080ff61aa6000), and remaining 0
I1018 08:53:57.255010 4097533 eager.cc:118] Tensor(generated_tensor_66) have not GradNode, add GradNodeAccumulation0x55b7c2d5b1b0 for it.
I1018 08:53:57.255023 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.255050 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1536(0x30080ff61ce8000), and remaining 0
I1018 08:53:57.255072 4097533 eager.cc:118] Tensor(generated_tensor_67) have not GradNode, add GradNodeAccumulation0x55b7c2d5bde0 for it.
I1018 08:53:57.255075 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.255085 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1536(0x30080ff61cea000), and remaining 0
I1018 08:53:57.255105 4097533 eager.cc:118] Tensor(generated_tensor_68) have not GradNode, add GradNodeAccumulation0x55b7c2d5ca10 for it.
I1018 08:53:57.255110 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.255115 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1536(0x30080ff61cec000), and remaining 0
I1018 08:53:57.255136 4097533 eager.cc:118] Tensor(generated_tensor_69) have not GradNode, add GradNodeAccumulation0x55b7c2d5d6f0 for it.
I1018 08:53:57.255138 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.255146 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1536(0x30080ff61cee000), and remaining 0
I1018 08:53:57.255167 4097533 eager.cc:118] Tensor(generated_tensor_70) have not GradNode, add GradNodeAccumulation0x55b7b6f80650 for it.
I1018 08:53:57.255169 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.255175 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2359808(0x30080ff61cf0000), and remaining 0
I1018 08:53:57.255430 4097533 eager.cc:118] Tensor(generated_tensor_71) have not GradNode, add GradNodeAccumulation0x55b7c2d5ebb0 for it.
I1018 08:53:57.255434 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.255441 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1536(0x30080ff61f32000), and remaining 0
I1018 08:53:57.255465 4097533 eager.cc:118] Tensor(generated_tensor_72) have not GradNode, add GradNodeAccumulation0x55b7bacd5e80 for it.
I1018 08:53:57.255468 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.255475 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1536(0x30080ff61f34000), and remaining 0
I1018 08:53:57.255496 4097533 eager.cc:118] Tensor(generated_tensor_73) have not GradNode, add GradNodeAccumulation0x55b7bacd6b90 for it.
I1018 08:53:57.255499 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.255506 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1536(0x30080ff61f36000), and remaining 0
I1018 08:53:57.255527 4097533 eager.cc:118] Tensor(generated_tensor_74) have not GradNode, add GradNodeAccumulation0x55b7b6f80140 for it.
I1018 08:53:57.255529 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.255535 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 1536(0x30080ff61f38000), and remaining 0
I1018 08:53:57.255555 4097533 eager.cc:118] Tensor(generated_tensor_75) have not GradNode, add GradNodeAccumulation0x55b7bacd81c0 for it.
I1018 08:53:57.255559 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.255566 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 4719104(0x30080ff5da9e000), and remaining 0
I1018 08:53:57.255980 4097533 eager.cc:118] Tensor(generated_tensor_76) have not GradNode, add GradNodeAccumulation0x55b7bacd8f00 for it.
I1018 08:53:57.255995 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.256004 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2560(0x30080ff61f3a000), and remaining 0
I1018 08:53:57.256026 4097533 eager.cc:118] Tensor(generated_tensor_77) have not GradNode, add GradNodeAccumulation0x55b7c1f71700 for it.
I1018 08:53:57.256029 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.256035 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2560(0x30080ff61f3c000), and remaining 0
I1018 08:53:57.256057 4097533 eager.cc:118] Tensor(generated_tensor_78) have not GradNode, add GradNodeAccumulation0x55b7bacda620 for it.
I1018 08:53:57.256059 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.256065 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2560(0x30080ff61f3e000), and remaining 0
I1018 08:53:57.256086 4097533 eager.cc:118] Tensor(generated_tensor_79) have not GradNode, add GradNodeAccumulation0x55b7c1f71e10 for it.
I1018 08:53:57.256090 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.256099 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2560(0x30080ff61f40000), and remaining 0
I1018 08:53:57.256119 4097533 eager.cc:118] Tensor(generated_tensor_80) have not GradNode, add GradNodeAccumulation0x55b7c23e2bf0 for it.
I1018 08:53:57.256124 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.256425 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 9437696(0x30080ffe4000000), and remaining 0
I1018 08:53:57.257037 4097533 eager.cc:118] Tensor(generated_tensor_81) have not GradNode, add GradNodeAccumulation0x55b7b779d110 for it.
I1018 08:53:57.257041 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.257050 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2560(0x30080ffe4902000), and remaining 0
I1018 08:53:57.257073 4097533 eager.cc:118] Tensor(generated_tensor_82) have not GradNode, add GradNodeAccumulation0x55b7b779dd50 for it.
I1018 08:53:57.257077 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.257083 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2560(0x30080ffe4904000), and remaining 0
I1018 08:53:57.257103 4097533 eager.cc:118] Tensor(generated_tensor_83) have not GradNode, add GradNodeAccumulation0x55b7c23e5fc0 for it.
I1018 08:53:57.257107 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.257113 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2560(0x30080ffe4906000), and remaining 0
I1018 08:53:57.257133 4097533 eager.cc:118] Tensor(generated_tensor_84) have not GradNode, add GradNodeAccumulation0x55b7c23e4190 for it.
I1018 08:53:57.257136 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.257143 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2560(0x30080ffe4908000), and remaining 0
I1018 08:53:57.257164 4097533 eager.cc:118] Tensor(generated_tensor_85) have not GradNode, add GradNodeAccumulation0x55b7b78eb2b0 for it.
I1018 08:53:57.257166 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.257172 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 524800(0x30080ffe490a000), and remaining 0
I1018 08:53:57.257267 4097533 eager.cc:118] Tensor(generated_tensor_86) have not GradNode, add GradNodeAccumulation0x55b7b779fd60 for it.
I1018 08:53:57.257273 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.257282 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2560(0x30080ffe498c000), and remaining 0
I1018 08:53:57.257305 4097533 eager.cc:118] Tensor(generated_tensor_87) have not GradNode, add GradNodeAccumulation0x55b7b77a0a40 for it.
I1018 08:53:57.257308 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.257316 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2560(0x30080ffe498e000), and remaining 0
I1018 08:53:57.257337 4097533 eager.cc:118] Tensor(generated_tensor_88) have not GradNode, add GradNodeAccumulation0x55b7c23e6590 for it.
I1018 08:53:57.257340 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.257349 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2560(0x30080ffe4990000), and remaining 0
I1018 08:53:57.257368 4097533 eager.cc:118] Tensor(generated_tensor_89) have not GradNode, add GradNodeAccumulation0x55b7b78eddd0 for it.
I1018 08:53:57.257372 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.257378 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2560(0x30080ffe4992000), and remaining 0
I1018 08:53:57.257398 4097533 eager.cc:118] Tensor(generated_tensor_90) have not GradNode, add GradNodeAccumulation0x55b7cfe2f410 for it.
I1018 08:53:57.257402 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.257408 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 9437696(0x30080ffe4994000), and remaining 0
I1018 08:53:57.258165 4097533 eager.cc:118] Tensor(generated_tensor_91) have not GradNode, add GradNodeAccumulation0x55b7b701b970 for it.
I1018 08:53:57.258175 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.258185 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2560(0x30080ffe5296000), and remaining 0
I1018 08:53:57.258214 4097533 eager.cc:118] Tensor(generated_tensor_92) have not GradNode, add GradNodeAccumulation0x55b7b701c650 for it.
I1018 08:53:57.258219 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.258226 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2560(0x30080ffe5298000), and remaining 0
I1018 08:53:57.258252 4097533 eager.cc:118] Tensor(generated_tensor_93) have not GradNode, add GradNodeAccumulation0x55b7c4e1bb70 for it.
I1018 08:53:57.258256 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.258263 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2560(0x30080ffe529a000), and remaining 0
I1018 08:53:57.258291 4097533 eager.cc:118] Tensor(generated_tensor_94) have not GradNode, add GradNodeAccumulation0x55b7c4e1d4c0 for it.
I1018 08:53:57.258294 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.258302 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2560(0x30080ffe529c000), and remaining 0
I1018 08:53:57.258327 4097533 eager.cc:118] Tensor(generated_tensor_95) have not GradNode, add GradNodeAccumulation0x55b7c4e1fce0 for it.
I1018 08:53:57.258332 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.258340 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 9437696(0x30080ffe529e000), and remaining 0
I1018 08:53:57.259027 4097533 eager.cc:118] Tensor(generated_tensor_96) have not GradNode, add GradNodeAccumulation0x55b7b701e720 for it.
I1018 08:53:57.259032 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.259039 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2560(0x30080ffe5ba0000), and remaining 0
I1018 08:53:57.259065 4097533 eager.cc:118] Tensor(generated_tensor_97) have not GradNode, add GradNodeAccumulation0x55b7b701f360 for it.
I1018 08:53:57.259069 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.259078 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2560(0x30080ffe5ba2000), and remaining 0
I1018 08:53:57.259104 4097533 eager.cc:118] Tensor(generated_tensor_98) have not GradNode, add GradNodeAccumulation0x55b7cfe31f30 for it.
I1018 08:53:57.259107 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.259114 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2560(0x30080ffe5ba4000), and remaining 0
I1018 08:53:57.259138 4097533 eager.cc:118] Tensor(generated_tensor_99) have not GradNode, add GradNodeAccumulation0x55b7cfe32750 for it.
I1018 08:53:57.259142 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.259150 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2560(0x30080ffe5ba6000), and remaining 0
I1018 08:53:57.259174 4097533 eager.cc:118] Tensor(generated_tensor_100) have not GradNode, add GradNodeAccumulation0x55b7c416ae60 for it.
I1018 08:53:57.259179 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.259186 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 2048512(0x30080ffe5ba8000), and remaining 0
I1018 08:53:57.263293 4097533 eager.cc:118] Tensor(generated_tensor_101) have not GradNode, add GradNodeAccumulation0x55b7b7797550 for it.
I1018 08:53:57.263305 4097533 eager.cc:188] CurrentDeviceId: 0 from 0
I1018 08:53:57.263322 4097533 auto_growth_best_fit_allocator.cc:127] Not found and reallocate 4096(0x30080ffe5d9e000), and remaining 0
I1018 08:53:57.279691 4097533 eager_op_function.cc:33102] CurrentDeviceId: 0 from 0
I1018 08:53:57.279707 4097533 dygraph_functions.cc:65227] Running AD API: gaussian
I1018 08:53:57.279711 4097533 dygraph_functions.cc:65247] { Input: []} 
I1018 08:53:57.279770 4097533 dygraph_functions.cc:65263] Finish AD API: gaussian
I1018 08:53:57.279790 4097533 dygraph_functions.cc:65277] { Input: [],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f78da0 }]), ] } 
I1018 08:53:57.280331 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.280350 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
[2024-10-18 16:53:57] [CNNL] [Warning]: [cnnlGetConvolutionForwardAlgorithm] is deprecated and will be removed in the future release. See cnnlFindConvolutionForwardAlgorithm() API for replacement.
I1018 08:53:57.280848 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b6f80570)  to GradNodeAccumulation (addr: 0x55b77381dd90)
I1018 08:53:57.280946 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.280967 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.280999 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b77398c530 }]),  
( mean , [{ Name: batch_norm2d_0.w_1, Initialized: 1, Ptr: 0x55b7c4ee9a30 }]),  
( variance , [{ Name: batch_norm2d_0.w_2, Initialized: 1, Ptr: 0x55b7ceb07ff0 }]),  
( scale , [{ Name: batch_norm2d_0.w_0, Initialized: 1, Ptr: 0x55b77346a070 }]),  
( bias , [{ Name: batch_norm2d_0.b_0, Initialized: 1, Ptr: 0x55b7b6f6d4b0 }]), ]} 
I1018 08:53:57.281044 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.281050 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.281176 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c4168f10)  to Conv2dGradNodeFinal (addr: 0x55b7b6f80570)
I1018 08:53:57.281183 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c4168f10)  to GradNodeAccumulation (addr: 0x55b77ed32260)
I1018 08:53:57.281186 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c4168f10)  to GradNodeAccumulation (addr: 0x55b77f0e4620)
I1018 08:53:57.281196 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.281246 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b77398c530 }]),  
( mean , [{ Name: batch_norm2d_0.w_1, Initialized: 1, Ptr: 0x55b7c4ee9a30 }]),  
( variance , [{ Name: batch_norm2d_0.w_2, Initialized: 1, Ptr: 0x55b7ceb07ff0 }]),  
( scale , [{ Name: batch_norm2d_0.w_0, Initialized: 1, Ptr: 0x55b77346a070 }]),  
( bias , [{ Name: batch_norm2d_0.b_0, Initialized: 1, Ptr: 0x55b7b6f6d4b0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7736fdb40 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b77370b470 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f67ac0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7edf0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f67930 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c4169780 }]), ] } 
I1018 08:53:57.281301 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.281320 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.281329 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7736fdb40 }]), ]} 
I1018 08:53:57.281425 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b4d8d1e0)  to BatchNormGradNode (addr: 0x55b7c4168f10)
I1018 08:53:57.281432 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.281445 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7736fdb40 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe2fc00 }]), ] } 
I1018 08:53:57.281498 4097533 eager_op_function.cc:35671] CurrentDeviceId: 0 from 0
I1018 08:53:57.281507 4097533 dygraph_functions.cc:69244] Running AD API: pool2d
I1018 08:53:57.281515 4097533 dygraph_functions.cc:69300] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe2fc00 }]), ]} 
I1018 08:53:57.281567 4097533 pool2d_kernel.cc:153] [Pool2d] pooling type: max exclusive: 1 adaptive: 0
I1018 08:53:57.281620 4097533 pool2d_kernel.cc:198] [Pool2d] extra_input_size: 0
I1018 08:53:57.281664 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Pool2dGradNode (addr: 0x55b7c4e78c00)  to ReluGradNode (addr: 0x55b7b4d8d1e0)
I1018 08:53:57.281672 4097533 dygraph_functions.cc:69376] Finish AD API: pool2d
I1018 08:53:57.281684 4097533 dygraph_functions.cc:69390] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe2fc00 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7736fdb40 }]), ] } 
I1018 08:53:57.281718 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.281723 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.281924 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c2d5e1f0)  to Pool2dGradNode (addr: 0x55b7c4e78c00)
I1018 08:53:57.281932 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c2d5e1f0)  to GradNodeAccumulation (addr: 0x55b773982760)
I1018 08:53:57.281976 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.281981 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.282007 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b70eee30 }]),  
( mean , [{ Name: batch_norm2d_1.w_1, Initialized: 1, Ptr: 0x55b7b64bb520 }]),  
( variance , [{ Name: batch_norm2d_1.w_2, Initialized: 1, Ptr: 0x55b7b68ca2d0 }]),  
( scale , [{ Name: batch_norm2d_1.w_0, Initialized: 1, Ptr: 0x55b7b68c8170 }]),  
( bias , [{ Name: batch_norm2d_1.b_0, Initialized: 1, Ptr: 0x55b7b76da540 }]), ]} 
I1018 08:53:57.282024 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.282028 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.282090 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b77cd7b0)  to Conv2dGradNodeFinal (addr: 0x55b7c2d5e1f0)
I1018 08:53:57.282095 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b77cd7b0)  to GradNodeAccumulation (addr: 0x55b773982b90)
I1018 08:53:57.282099 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b77cd7b0)  to GradNodeAccumulation (addr: 0x55b773f00630)
I1018 08:53:57.282107 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.282150 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b70eee30 }]),  
( mean , [{ Name: batch_norm2d_1.w_1, Initialized: 1, Ptr: 0x55b7b64bb520 }]),  
( variance , [{ Name: batch_norm2d_1.w_2, Initialized: 1, Ptr: 0x55b7b68ca2d0 }]),  
( scale , [{ Name: batch_norm2d_1.w_0, Initialized: 1, Ptr: 0x55b7b68c8170 }]),  
( bias , [{ Name: batch_norm2d_1.b_0, Initialized: 1, Ptr: 0x55b7b76da540 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b7798000 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7d8c0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f78370 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7fd20 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b209b9e0 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c4cb9da0 }]), ] } 
I1018 08:53:57.282169 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.282173 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.282181 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b7798000 }]), ]} 
I1018 08:53:57.282207 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b773371db0)  to BatchNormGradNode (addr: 0x55b7b77cd7b0)
I1018 08:53:57.282227 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.282238 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b7798000 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4edb0f0 }]), ] } 
I1018 08:53:57.282260 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.282264 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.282398 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4e7c8c0)  to ReluGradNode (addr: 0x55b773371db0)
I1018 08:53:57.282405 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4e7c8c0)  to GradNodeAccumulation (addr: 0x55b7bd3da270)
I1018 08:53:57.282444 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.282449 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.282472 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b7798000 }]),  
( mean , [{ Name: batch_norm2d_2.w_1, Initialized: 1, Ptr: 0x55b7b76e6e10 }]),  
( variance , [{ Name: batch_norm2d_2.w_2, Initialized: 1, Ptr: 0x55b7c2207b30 }]),  
( scale , [{ Name: batch_norm2d_2.w_0, Initialized: 1, Ptr: 0x55b7bd3da150 }]),  
( bias , [{ Name: batch_norm2d_2.b_0, Initialized: 1, Ptr: 0x55b7bd3d86c0 }]), ]} 
I1018 08:53:57.282486 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.282491 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.282549 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7cfe31d60)  to Conv2dGradNodeFinal (addr: 0x55b7c4e7c8c0)
I1018 08:53:57.282554 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7cfe31d60)  to GradNodeAccumulation (addr: 0x55b7c4ee5b40)
I1018 08:53:57.282557 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7cfe31d60)  to GradNodeAccumulation (addr: 0x55b7bd3d87e0)
I1018 08:53:57.282565 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.282606 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b7798000 }]),  
( mean , [{ Name: batch_norm2d_2.w_1, Initialized: 1, Ptr: 0x55b7b76e6e10 }]),  
( variance , [{ Name: batch_norm2d_2.w_2, Initialized: 1, Ptr: 0x55b7c2207b30 }]),  
( scale , [{ Name: batch_norm2d_2.w_0, Initialized: 1, Ptr: 0x55b7bd3da150 }]),  
( bias , [{ Name: batch_norm2d_2.b_0, Initialized: 1, Ptr: 0x55b7bd3d86c0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f71280 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f6eb70 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b70ee5f0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b773679a80 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b773679ba0 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b70efb70 }]), ] } 
I1018 08:53:57.282639 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.282652 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f71280 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7736fdb40 }]), ]} 
[2024-10-18 16:53:57] [CNNL] [Warning]: When calculating multiplication of complex_float data, it is required to use [cnnlGetOpTensorWorkspaceSize_v2] to apply for workspace.
I1018 08:53:57.282764 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7c416d780)  to BatchNormGradNode (addr: 0x55b7cfe31d60)
I1018 08:53:57.282769 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7c416d780)  to Pool2dGradNode (addr: 0x55b7c4e78c00)
I1018 08:53:57.282774 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.282793 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f71280 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7736fdb40 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701b2e0 }]), ] } 
I1018 08:53:57.282814 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.282817 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.282824 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701b2e0 }]), ]} 
I1018 08:53:57.282845 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7733494c0)  to AddGradNode (addr: 0x55b7c416d780)
I1018 08:53:57.282850 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.282860 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701b2e0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f71280 }]), ] } 
I1018 08:53:57.282883 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.282887 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.283022 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b6f7ff70)  to ReluGradNode (addr: 0x55b7733494c0)
I1018 08:53:57.283030 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b6f7ff70)  to GradNodeAccumulation (addr: 0x55b7b76e5550)
I1018 08:53:57.283066 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.283070 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.283094 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701b2e0 }]),  
( mean , [{ Name: batch_norm2d_3.w_1, Initialized: 1, Ptr: 0x55b7b778e800 }]),  
( variance , [{ Name: batch_norm2d_3.w_2, Initialized: 1, Ptr: 0x55b7b3f224a0 }]),  
( scale , [{ Name: batch_norm2d_3.w_0, Initialized: 1, Ptr: 0x55b7bd3daab0 }]),  
( bias , [{ Name: batch_norm2d_3.b_0, Initialized: 1, Ptr: 0x55b7b87810f0 }]), ]} 
I1018 08:53:57.283108 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.283113 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.283172 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b6f7df70)  to Conv2dGradNodeFinal (addr: 0x55b7b6f7ff70)
I1018 08:53:57.283177 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b6f7df70)  to GradNodeAccumulation (addr: 0x55b7b76e6360)
I1018 08:53:57.283181 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b6f7df70)  to GradNodeAccumulation (addr: 0x55b7b68c8390)
I1018 08:53:57.283188 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.283229 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701b2e0 }]),  
( mean , [{ Name: batch_norm2d_3.w_1, Initialized: 1, Ptr: 0x55b7b778e800 }]),  
( variance , [{ Name: batch_norm2d_3.w_2, Initialized: 1, Ptr: 0x55b7b3f224a0 }]),  
( scale , [{ Name: batch_norm2d_3.w_0, Initialized: 1, Ptr: 0x55b7bd3daab0 }]),  
( bias , [{ Name: batch_norm2d_3.b_0, Initialized: 1, Ptr: 0x55b7b87810f0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701ab90 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779ef10 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779f490 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd9a60 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd9b80 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7bacd9ca0 }]), ] } 
I1018 08:53:57.283247 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.283257 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.283263 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701ab90 }]), ]} 
I1018 08:53:57.283282 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b77334c090)  to BatchNormGradNode (addr: 0x55b7b6f7df70)
I1018 08:53:57.283288 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.283298 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701ab90 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7f9c0 }]), ] } 
I1018 08:53:57.283317 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.283321 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.283442 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b77d2580)  to ReluGradNode (addr: 0x55b77334c090)
I1018 08:53:57.283448 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b77d2580)  to GradNodeAccumulation (addr: 0x55b7b68c6d90)
I1018 08:53:57.283484 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.283489 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.283514 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701ab90 }]),  
( mean , [{ Name: batch_norm2d_4.w_1, Initialized: 1, Ptr: 0x55b7c2209c90 }]),  
( variance , [{ Name: batch_norm2d_4.w_2, Initialized: 1, Ptr: 0x55b7c220ab50 }]),  
( scale , [{ Name: batch_norm2d_4.w_0, Initialized: 1, Ptr: 0x55b7b68c6c70 }]),  
( bias , [{ Name: batch_norm2d_4.b_0, Initialized: 1, Ptr: 0x55b7b4b4a7c0 }]), ]} 
I1018 08:53:57.283529 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.283532 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.283584 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c1f71c80)  to Conv2dGradNodeFinal (addr: 0x55b7b77d2580)
I1018 08:53:57.283589 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c1f71c80)  to GradNodeAccumulation (addr: 0x55b7b4b49ae0)
I1018 08:53:57.283592 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c1f71c80)  to GradNodeAccumulation (addr: 0x55b7b4b4a8e0)
I1018 08:53:57.283599 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.283640 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701ab90 }]),  
( mean , [{ Name: batch_norm2d_4.w_1, Initialized: 1, Ptr: 0x55b7c2209c90 }]),  
( variance , [{ Name: batch_norm2d_4.w_2, Initialized: 1, Ptr: 0x55b7c220ab50 }]),  
( scale , [{ Name: batch_norm2d_4.w_0, Initialized: 1, Ptr: 0x55b7b68c6c70 }]),  
( bias , [{ Name: batch_norm2d_4.b_0, Initialized: 1, Ptr: 0x55b7b4b4a7c0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ed760 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f6a890 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1d340 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c23e3fd0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c23e40f0 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c23e4210 }]), ] } 
I1018 08:53:57.283653 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.283663 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ed760 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f71280 }]), ]} 
I1018 08:53:57.283694 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7c2d5c8e0)  to BatchNormGradNode (addr: 0x55b7c1f71c80)
I1018 08:53:57.283703 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7c2d5c8e0)  to ReluGradNode (addr: 0x55b7733494c0)
I1018 08:53:57.283708 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.283722 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ed760 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f71280 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7d710 }]), ] } 
I1018 08:53:57.283739 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.283743 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.283751 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7d710 }]), ]} 
I1018 08:53:57.283768 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b77324be20)  to AddGradNode (addr: 0x55b7c2d5c8e0)
I1018 08:53:57.283773 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.283783 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7d710 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ed760 }]), ] } 
I1018 08:53:57.283811 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.283815 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.283950 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c2d5eb80)  to ReluGradNode (addr: 0x55b77324be20)
I1018 08:53:57.283957 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c2d5eb80)  to GradNodeAccumulation (addr: 0x55b7c4ee2d80)
I1018 08:53:57.283994 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.283998 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.284022 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c416c660 }]),  
( mean , [{ Name: batch_norm2d_6.w_1, Initialized: 1, Ptr: 0x55b7cc394970 }]),  
( variance , [{ Name: batch_norm2d_6.w_2, Initialized: 1, Ptr: 0x55b7cc395a60 }]),  
( scale , [{ Name: batch_norm2d_6.w_0, Initialized: 1, Ptr: 0x55b7c4ee3ce0 }]),  
( bias , [{ Name: batch_norm2d_6.b_0, Initialized: 1, Ptr: 0x55b7c4ee4b90 }]), ]} 
I1018 08:53:57.284036 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.284039 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.284098 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c4b8f2b0)  to Conv2dGradNodeFinal (addr: 0x55b7c2d5eb80)
I1018 08:53:57.284103 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c4b8f2b0)  to GradNodeAccumulation (addr: 0x55b7c4ee3e00)
I1018 08:53:57.284106 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c4b8f2b0)  to GradNodeAccumulation (addr: 0x55b7c4ee4cb0)
I1018 08:53:57.284113 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.284154 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c416c660 }]),  
( mean , [{ Name: batch_norm2d_6.w_1, Initialized: 1, Ptr: 0x55b7cc394970 }]),  
( variance , [{ Name: batch_norm2d_6.w_2, Initialized: 1, Ptr: 0x55b7cc395a60 }]),  
( scale , [{ Name: batch_norm2d_6.w_0, Initialized: 1, Ptr: 0x55b7c4ee3ce0 }]),  
( bias , [{ Name: batch_norm2d_6.b_0, Initialized: 1, Ptr: 0x55b7c4ee4b90 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f6ee40 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f6ef60 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f6f130 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c2d5f220 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b8fa60 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c4b90730 }]), ] } 
I1018 08:53:57.284176 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.284180 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.284188 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f6ee40 }]), ]} 
I1018 08:53:57.284206 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b76fbc80)  to BatchNormGradNode (addr: 0x55b7c4b8f2b0)
I1018 08:53:57.284211 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.284222 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f6ee40 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b8c500 }]), ] } 
I1018 08:53:57.284240 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.284245 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.284369 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7cfe2fdf0)  to ReluGradNode (addr: 0x55b7b76fbc80)
I1018 08:53:57.284376 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7cfe2fdf0)  to GradNodeAccumulation (addr: 0x55b7cc396a70)
I1018 08:53:57.284411 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.284415 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.284440 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f6ee40 }]),  
( mean , [{ Name: batch_norm2d_7.w_1, Initialized: 1, Ptr: 0x55b7c4d65af0 }]),  
( variance , [{ Name: batch_norm2d_7.w_2, Initialized: 1, Ptr: 0x55b7b778c480 }]),  
( scale , [{ Name: batch_norm2d_7.w_0, Initialized: 1, Ptr: 0x55b7cc396950 }]),  
( bias , [{ Name: batch_norm2d_7.b_0, Initialized: 1, Ptr: 0x55b7c4d64c30 }]), ]} 
I1018 08:53:57.284456 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.284458 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.284512 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7bacda580)  to Conv2dGradNodeFinal (addr: 0x55b7cfe2fdf0)
I1018 08:53:57.284518 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7bacda580)  to GradNodeAccumulation (addr: 0x55b7c4d63ea0)
I1018 08:53:57.284521 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7bacda580)  to GradNodeAccumulation (addr: 0x55b7c4d64d50)
I1018 08:53:57.284528 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.284569 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f6ee40 }]),  
( mean , [{ Name: batch_norm2d_7.w_1, Initialized: 1, Ptr: 0x55b7c4d65af0 }]),  
( variance , [{ Name: batch_norm2d_7.w_2, Initialized: 1, Ptr: 0x55b7b778c480 }]),  
( scale , [{ Name: batch_norm2d_7.w_0, Initialized: 1, Ptr: 0x55b7cc396950 }]),  
( bias , [{ Name: batch_norm2d_7.b_0, Initialized: 1, Ptr: 0x55b7c4d64c30 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1e270 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1e440 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ea180 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c22d0770 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c22d0890 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c22d09b0 }]), ] } 
I1018 08:53:57.284593 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.284598 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.284742 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b77a0b20)  to ReluGradNode (addr: 0x55b77324be20)
I1018 08:53:57.284750 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b77a0b20)  to GradNodeAccumulation (addr: 0x55b7b85db7c0)
I1018 08:53:57.284785 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.284790 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.284813 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c22d0e30 }]),  
( mean , [{ Name: batch_norm2d_5.w_1, Initialized: 1, Ptr: 0x55b7bb903150 }]),  
( variance , [{ Name: batch_norm2d_5.w_2, Initialized: 1, Ptr: 0x55b7bb904240 }]),  
( scale , [{ Name: batch_norm2d_5.w_0, Initialized: 1, Ptr: 0x55b7b3f22b10 }]),  
( bias , [{ Name: batch_norm2d_5.b_0, Initialized: 1, Ptr: 0x55b7b85dd420 }]), ]} 
I1018 08:53:57.284827 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.284830 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.284886 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c2d5d510)  to Conv2dGradNodeFinal (addr: 0x55b7b77a0b20)
I1018 08:53:57.284891 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c2d5d510)  to GradNodeAccumulation (addr: 0x55b7b85dc690)
I1018 08:53:57.284895 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c2d5d510)  to GradNodeAccumulation (addr: 0x55b7b85dd540)
I1018 08:53:57.284902 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.284943 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c22d0e30 }]),  
( mean , [{ Name: batch_norm2d_5.w_1, Initialized: 1, Ptr: 0x55b7bb903150 }]),  
( variance , [{ Name: batch_norm2d_5.w_2, Initialized: 1, Ptr: 0x55b7bb904240 }]),  
( scale , [{ Name: batch_norm2d_5.w_0, Initialized: 1, Ptr: 0x55b7b3f22b10 }]),  
( bias , [{ Name: batch_norm2d_5.b_0, Initialized: 1, Ptr: 0x55b7b85dd420 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b7797380 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b7797550 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b7797720 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c2d5dcc0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779e0e0 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b779e200 }]), ] } 
I1018 08:53:57.284955 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.284966 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1e270 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7b7797380 }]), ]} 
I1018 08:53:57.284996 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7b77d1750)  to BatchNormGradNode (addr: 0x55b7bacda580)
I1018 08:53:57.285001 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7b77d1750)  to BatchNormGradNode (addr: 0x55b7c2d5d510)
I1018 08:53:57.285004 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.285018 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1e270 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7b7797380 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77d1bd0 }]), ] } 
I1018 08:53:57.285035 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.285039 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.285046 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77d1bd0 }]), ]} 
I1018 08:53:57.285068 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b1a58b50)  to AddGradNode (addr: 0x55b7b77d1750)
I1018 08:53:57.285074 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.285084 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77d1bd0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1e270 }]), ] } 
I1018 08:53:57.285107 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.285111 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.285266 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b701b760)  to ReluGradNode (addr: 0x55b7b1a58b50)
I1018 08:53:57.285274 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b701b760)  to GradNodeAccumulation (addr: 0x55b7b778d7d0)
I1018 08:53:57.285313 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.285317 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.285342 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b7797380 }]),  
( mean , [{ Name: batch_norm2d_8.w_1, Initialized: 1, Ptr: 0x55b7c5cf24d0 }]),  
( variance , [{ Name: batch_norm2d_8.w_2, Initialized: 1, Ptr: 0x55b7c5cf3390 }]),  
( scale , [{ Name: batch_norm2d_8.w_0, Initialized: 1, Ptr: 0x55b7bb905130 }]),  
( bias , [{ Name: batch_norm2d_8.b_0, Initialized: 1, Ptr: 0x55b7c5cf1610 }]), ]} 
I1018 08:53:57.285357 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.285362 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.285418 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b779fb50)  to Conv2dGradNodeFinal (addr: 0x55b7b701b760)
I1018 08:53:57.285423 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b779fb50)  to GradNodeAccumulation (addr: 0x55b7c5cf0810)
I1018 08:53:57.285426 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b779fb50)  to GradNodeAccumulation (addr: 0x55b7c5cf1730)
I1018 08:53:57.285434 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.285475 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b7797380 }]),  
( mean , [{ Name: batch_norm2d_8.w_1, Initialized: 1, Ptr: 0x55b7c5cf24d0 }]),  
( variance , [{ Name: batch_norm2d_8.w_2, Initialized: 1, Ptr: 0x55b7c5cf3390 }]),  
( scale , [{ Name: batch_norm2d_8.w_0, Initialized: 1, Ptr: 0x55b7bb905130 }]),  
( bias , [{ Name: batch_norm2d_8.b_0, Initialized: 1, Ptr: 0x55b7c5cf1610 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77d1bd0 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77d1cf0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c22ce8e0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701caf0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77a0300 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b701bef0 }]), ] } 
I1018 08:53:57.285491 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.285495 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.285502 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77d1bd0 }]), ]} 
I1018 08:53:57.285521 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7c4e7a1f0)  to BatchNormGradNode (addr: 0x55b7b779fb50)
I1018 08:53:57.285526 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.285537 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77d1bd0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779d200 }]), ] } 
I1018 08:53:57.285562 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.285565 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.285691 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b77d0bd0)  to ReluGradNode (addr: 0x55b7c4e7a1f0)
I1018 08:53:57.285697 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b77d0bd0)  to GradNodeAccumulation (addr: 0x55b7cc6c36b0)
I1018 08:53:57.285733 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.285737 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.285761 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77d1bd0 }]),  
( mean , [{ Name: batch_norm2d_9.w_1, Initialized: 1, Ptr: 0x55b7b7056fb0 }]),  
( variance , [{ Name: batch_norm2d_9.w_2, Initialized: 1, Ptr: 0x55b7b7057e70 }]),  
( scale , [{ Name: batch_norm2d_9.w_0, Initialized: 1, Ptr: 0x55b7cc6c3590 }]),  
( bias , [{ Name: batch_norm2d_9.b_0, Initialized: 1, Ptr: 0x55b7b70560f0 }]), ]} 
I1018 08:53:57.285773 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.285777 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.285826 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b701e510)  to Conv2dGradNodeFinal (addr: 0x55b7b77d0bd0)
I1018 08:53:57.285831 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b701e510)  to GradNodeAccumulation (addr: 0x55b7cc6c4610)
I1018 08:53:57.285835 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b701e510)  to GradNodeAccumulation (addr: 0x55b7b7056210)
I1018 08:53:57.285840 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.285877 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77d1bd0 }]),  
( mean , [{ Name: batch_norm2d_9.w_1, Initialized: 1, Ptr: 0x55b7b7056fb0 }]),  
( variance , [{ Name: batch_norm2d_9.w_2, Initialized: 1, Ptr: 0x55b7b7057e70 }]),  
( scale , [{ Name: batch_norm2d_9.w_0, Initialized: 1, Ptr: 0x55b7cc6c3590 }]),  
( bias , [{ Name: batch_norm2d_9.b_0, Initialized: 1, Ptr: 0x55b7b70560f0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779d770 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd71f0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c2d5b710 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701ecc0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77ce550 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b77ce670 }]), ] } 
I1018 08:53:57.285888 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.285897 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779d770 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1e270 }]), ]} 
I1018 08:53:57.285925 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7b78e82d0)  to BatchNormGradNode (addr: 0x55b7b701e510)
I1018 08:53:57.285929 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7b78e82d0)  to ReluGradNode (addr: 0x55b7b1a58b50)
I1018 08:53:57.285933 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.285946 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779d770 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1e270 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e8750 }]), ] } 
I1018 08:53:57.285965 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.285969 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.285975 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e8750 }]), ]} 
I1018 08:53:57.285991 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7bacd5c30)  to AddGradNode (addr: 0x55b7b78e82d0)
I1018 08:53:57.285996 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.286005 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e8750 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779d770 }]), ] } 
I1018 08:53:57.286031 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.286034 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.286207 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4edb4a0)  to ReluGradNode (addr: 0x55b7bacd5c30)
I1018 08:53:57.286214 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4edb4a0)  to GradNodeAccumulation (addr: 0x55b7c4edec60)
I1018 08:53:57.286248 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.286252 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.286274 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e8750 }]),  
( mean , [{ Name: batch_norm2d_11.w_1, Initialized: 1, Ptr: 0x55b7b7059750 }]),  
( variance , [{ Name: batch_norm2d_11.w_2, Initialized: 1, Ptr: 0x55b7c5ced740 }]),  
( scale , [{ Name: batch_norm2d_11.w_0, Initialized: 1, Ptr: 0x55b7b4b498d0 }]),  
( bias , [{ Name: batch_norm2d_11.b_0, Initialized: 1, Ptr: 0x55b7c4ee0950 }]), ]} 
I1018 08:53:57.286288 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.286291 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.286345 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c4169f40)  to Conv2dGradNodeFinal (addr: 0x55b7c4edb4a0)
I1018 08:53:57.286348 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c4169f40)  to GradNodeAccumulation (addr: 0x55b7c4edfbc0)
I1018 08:53:57.286352 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c4169f40)  to GradNodeAccumulation (addr: 0x55b7c4ee0a70)
I1018 08:53:57.286358 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.286396 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e8750 }]),  
( mean , [{ Name: batch_norm2d_11.w_1, Initialized: 1, Ptr: 0x55b7b7059750 }]),  
( variance , [{ Name: batch_norm2d_11.w_2, Initialized: 1, Ptr: 0x55b7c5ced740 }]),  
( scale , [{ Name: batch_norm2d_11.w_0, Initialized: 1, Ptr: 0x55b7b4b498d0 }]),  
( bias , [{ Name: batch_norm2d_11.b_0, Initialized: 1, Ptr: 0x55b7c4ee0950 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd6520 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd8700 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4eda410 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4edace0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c416a6f0 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c416a810 }]), ] } 
I1018 08:53:57.286410 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.286413 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.286420 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd6520 }]), ]} 
I1018 08:53:57.286437 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b77cefd0)  to BatchNormGradNode (addr: 0x55b7c4169f40)
I1018 08:53:57.286445 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.286455 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd6520 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701f650 }]), ] } 
I1018 08:53:57.286471 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.286475 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.286586 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4cb7f60)  to ReluGradNode (addr: 0x55b7b77cefd0)
I1018 08:53:57.286592 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4cb7f60)  to GradNodeAccumulation (addr: 0x55b7c5cee750)
I1018 08:53:57.286624 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.286628 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.286650 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd6520 }]),  
( mean , [{ Name: batch_norm2d_12.w_1, Initialized: 1, Ptr: 0x55b7cc398370 }]),  
( variance , [{ Name: batch_norm2d_12.w_2, Initialized: 1, Ptr: 0x55b7cc399230 }]),  
( scale , [{ Name: batch_norm2d_12.w_0, Initialized: 1, Ptr: 0x55b7c5cee630 }]),  
( bias , [{ Name: batch_norm2d_12.b_0, Initialized: 1, Ptr: 0x55b7cc3974b0 }]), ]} 
I1018 08:53:57.286662 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.286665 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.286715 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b78e3f10)  to Conv2dGradNodeFinal (addr: 0x55b7c4cb7f60)
I1018 08:53:57.286720 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b78e3f10)  to GradNodeAccumulation (addr: 0x55b7c5cef6b0)
I1018 08:53:57.286723 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b78e3f10)  to GradNodeAccumulation (addr: 0x55b7cc3975d0)
I1018 08:53:57.286729 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.286767 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd6520 }]),  
( mean , [{ Name: batch_norm2d_12.w_1, Initialized: 1, Ptr: 0x55b7cc398370 }]),  
( variance , [{ Name: batch_norm2d_12.w_2, Initialized: 1, Ptr: 0x55b7cc399230 }]),  
( scale , [{ Name: batch_norm2d_12.w_0, Initialized: 1, Ptr: 0x55b7c5cee630 }]),  
( bias , [{ Name: batch_norm2d_12.b_0, Initialized: 1, Ptr: 0x55b7cc3974b0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe319c0 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b8df60 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b8e130 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe31840 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e46c0 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b78e47e0 }]), ] } 
I1018 08:53:57.286788 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.286792 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.286899 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b78edba0)  to ReluGradNode (addr: 0x55b7bacd5c30)
I1018 08:53:57.286904 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b78edba0)  to GradNodeAccumulation (addr: 0x55b7b7058e80)
I1018 08:53:57.286934 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.286938 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.286964 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1f8d0 }]),  
( mean , [{ Name: batch_norm2d_10.w_1, Initialized: 1, Ptr: 0x55b7b780fba0 }]),  
( variance , [{ Name: batch_norm2d_10.w_2, Initialized: 1, Ptr: 0x55b7b7810a60 }]),  
( scale , [{ Name: batch_norm2d_10.w_0, Initialized: 1, Ptr: 0x55b7b7058d60 }]),  
( bias , [{ Name: batch_norm2d_10.b_0, Initialized: 1, Ptr: 0x55b7b780ece0 }]), ]} 
I1018 08:53:57.286976 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.286981 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.287029 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b78eb040)  to Conv2dGradNodeFinal (addr: 0x55b7b78edba0)
I1018 08:53:57.287034 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b78eb040)  to GradNodeAccumulation (addr: 0x55b7b780df50)
I1018 08:53:57.287037 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b78eb040)  to GradNodeAccumulation (addr: 0x55b7b780ee00)
I1018 08:53:57.287043 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.287078 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1f8d0 }]),  
( mean , [{ Name: batch_norm2d_10.w_1, Initialized: 1, Ptr: 0x55b7b780fba0 }]),  
( variance , [{ Name: batch_norm2d_10.w_2, Initialized: 1, Ptr: 0x55b7b7810a60 }]),  
( scale , [{ Name: batch_norm2d_10.w_0, Initialized: 1, Ptr: 0x55b7b7058d60 }]),  
( bias , [{ Name: batch_norm2d_10.b_0, Initialized: 1, Ptr: 0x55b7b780ece0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4edce00 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f67be0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f67db0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4edcce0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ee740 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b78ee860 }]), ] } 
I1018 08:53:57.287089 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.287099 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe319c0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4edce00 }]), ]} 
I1018 08:53:57.287125 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7c4e1bb60)  to BatchNormGradNode (addr: 0x55b7b78e3f10)
I1018 08:53:57.287129 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7c4e1bb60)  to BatchNormGradNode (addr: 0x55b7b78eb040)
I1018 08:53:57.287132 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.287144 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe319c0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4edce00 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1bfe0 }]), ] } 
I1018 08:53:57.287160 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.287163 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.287170 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1bfe0 }]), ]} 
I1018 08:53:57.287186 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7c4cbad20)  to AddGradNode (addr: 0x55b7c4e1bb60)
I1018 08:53:57.287190 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.287200 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1bfe0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe319c0 }]), ] } 
I1018 08:53:57.287220 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.287228 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.287351 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4cbbd90)  to ReluGradNode (addr: 0x55b7c4cbad20)
I1018 08:53:57.287357 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4cbbd90)  to GradNodeAccumulation (addr: 0x55b7cc39a490)
I1018 08:53:57.287390 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.287395 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.287416 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4edce00 }]),  
( mean , [{ Name: batch_norm2d_13.w_1, Initialized: 1, Ptr: 0x55b7cc4bf110 }]),  
( variance , [{ Name: batch_norm2d_13.w_2, Initialized: 1, Ptr: 0x55b7cc4bffd0 }]),  
( scale , [{ Name: batch_norm2d_13.w_0, Initialized: 1, Ptr: 0x55b7cc39a370 }]),  
( bias , [{ Name: batch_norm2d_13.b_0, Initialized: 1, Ptr: 0x55b7cc4be250 }]), ]} 
I1018 08:53:57.287429 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.287432 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.287478 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b70ec4b0)  to Conv2dGradNodeFinal (addr: 0x55b7c4cbbd90)
I1018 08:53:57.287483 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b70ec4b0)  to GradNodeAccumulation (addr: 0x55b7cc4bd4b0)
I1018 08:53:57.287487 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b70ec4b0)  to GradNodeAccumulation (addr: 0x55b7cc4be370)
I1018 08:53:57.287492 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.287528 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4edce00 }]),  
( mean , [{ Name: batch_norm2d_13.w_1, Initialized: 1, Ptr: 0x55b7cc4bf110 }]),  
( variance , [{ Name: batch_norm2d_13.w_2, Initialized: 1, Ptr: 0x55b7cc4bffd0 }]),  
( scale , [{ Name: batch_norm2d_13.w_0, Initialized: 1, Ptr: 0x55b7cc39a370 }]),  
( bias , [{ Name: batch_norm2d_13.b_0, Initialized: 1, Ptr: 0x55b7cc4be250 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1bfe0 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1c100 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cbb5a0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77a09a0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f68850 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b70ecc60 }]), ] } 
I1018 08:53:57.287544 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.287546 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.287552 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1bfe0 }]), ]} 
I1018 08:53:57.287569 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b757d860)  to BatchNormGradNode (addr: 0x55b7b70ec4b0)
I1018 08:53:57.287573 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.287583 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1bfe0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b70ed190 }]), ] } 
I1018 08:53:57.287600 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.287603 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.287711 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b77987d0)  to ReluGradNode (addr: 0x55b7b757d860)
I1018 08:53:57.287717 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b77987d0)  to GradNodeAccumulation (addr: 0x55b7b68cc840)
I1018 08:53:57.287753 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.287757 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.287779 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1bfe0 }]),  
( mean , [{ Name: batch_norm2d_14.w_1, Initialized: 1, Ptr: 0x55b7b68cf3f0 }]),  
( variance , [{ Name: batch_norm2d_14.w_2, Initialized: 1, Ptr: 0x55b7b68d02b0 }]),  
( scale , [{ Name: batch_norm2d_14.w_0, Initialized: 1, Ptr: 0x55b7cc4c0e90 }]),  
( bias , [{ Name: batch_norm2d_14.b_0, Initialized: 1, Ptr: 0x55b7b68ce530 }]), ]} 
I1018 08:53:57.287791 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.287794 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.287840 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b7799360)  to Conv2dGradNodeFinal (addr: 0x55b7b77987d0)
I1018 08:53:57.287844 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b7799360)  to GradNodeAccumulation (addr: 0x55b7b68cd7a0)
I1018 08:53:57.287847 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b7799360)  to GradNodeAccumulation (addr: 0x55b7b68ce650)
I1018 08:53:57.287854 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.287891 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1bfe0 }]),  
( mean , [{ Name: batch_norm2d_14.w_1, Initialized: 1, Ptr: 0x55b7b68cf3f0 }]),  
( variance , [{ Name: batch_norm2d_14.w_2, Initialized: 1, Ptr: 0x55b7b68d02b0 }]),  
( scale , [{ Name: batch_norm2d_14.w_0, Initialized: 1, Ptr: 0x55b7cc4c0e90 }]),  
( bias , [{ Name: batch_norm2d_14.b_0, Initialized: 1, Ptr: 0x55b7b68ce530 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b757e240 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b757e410 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b7798240 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b757d6f0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b7799b10 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b7799c30 }]), ] } 
I1018 08:53:57.287901 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.287910 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b757e240 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe319c0 }]), ]} 
I1018 08:53:57.287935 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7b877bfe0)  to BatchNormGradNode (addr: 0x55b7b7799360)
I1018 08:53:57.287938 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7b877bfe0)  to ReluGradNode (addr: 0x55b7c4cbad20)
I1018 08:53:57.287942 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.287954 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b757e240 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe319c0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877c460 }]), ] } 
I1018 08:53:57.287968 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.287972 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.287978 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877c460 }]), ]} 
I1018 08:53:57.287993 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b877d050)  to AddGradNode (addr: 0x55b7b877bfe0)
I1018 08:53:57.287998 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.288012 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877c460 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b757e240 }]), ] } 
I1018 08:53:57.288035 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.288039 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.288164 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b877e490)  to ReluGradNode (addr: 0x55b7b877d050)
I1018 08:53:57.288170 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b877e490)  to GradNodeAccumulation (addr: 0x55b7c4d61a60)
I1018 08:53:57.288204 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.288208 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.288229 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877c460 }]),  
( mean , [{ Name: batch_norm2d_16.w_1, Initialized: 1, Ptr: 0x55b7b796d090 }]),  
( variance , [{ Name: batch_norm2d_16.w_2, Initialized: 1, Ptr: 0x55b7b796df50 }]),  
( scale , [{ Name: batch_norm2d_16.w_0, Initialized: 1, Ptr: 0x55b7c4d61940 }]),  
( bias , [{ Name: batch_norm2d_16.b_0, Initialized: 1, Ptr: 0x55b7b796c1d0 }]), ]} 
I1018 08:53:57.288242 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.288245 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.288296 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b877f020)  to Conv2dGradNodeFinal (addr: 0x55b7b877e490)
I1018 08:53:57.288300 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b877f020)  to GradNodeAccumulation (addr: 0x55b7c4d629c0)
I1018 08:53:57.288303 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b877f020)  to GradNodeAccumulation (addr: 0x55b7b796c2f0)
I1018 08:53:57.288311 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.288347 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877c460 }]),  
( mean , [{ Name: batch_norm2d_16.w_1, Initialized: 1, Ptr: 0x55b7b796d090 }]),  
( variance , [{ Name: batch_norm2d_16.w_2, Initialized: 1, Ptr: 0x55b7b796df50 }]),  
( scale , [{ Name: batch_norm2d_16.w_0, Initialized: 1, Ptr: 0x55b7c4d61940 }]),  
( bias , [{ Name: batch_norm2d_16.b_0, Initialized: 1, Ptr: 0x55b7b796c1d0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877d940 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877db10 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877dce0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877bc90 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877f7d0 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b877f8f0 }]), ] } 
I1018 08:53:57.288362 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.288365 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.288372 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877d940 }]), ]} 
I1018 08:53:57.288388 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7c4b92da0)  to BatchNormGradNode (addr: 0x55b7b877f020)
I1018 08:53:57.288393 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.288403 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877d940 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877fe20 }]), ] } 
I1018 08:53:57.288419 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.288422 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.288568 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4b94300)  to ReluGradNode (addr: 0x55b7c4b92da0)
I1018 08:53:57.288574 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4b94300)  to GradNodeAccumulation (addr: 0x55b7b796ef60)
I1018 08:53:57.288609 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.288612 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.288633 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877d940 }]),  
( mean , [{ Name: batch_norm2d_17.w_1, Initialized: 1, Ptr: 0x55b7bea3b370 }]),  
( variance , [{ Name: batch_norm2d_17.w_2, Initialized: 1, Ptr: 0x55b7bea3c230 }]),  
( scale , [{ Name: batch_norm2d_17.w_0, Initialized: 1, Ptr: 0x55b7b796ee40 }]),  
( bias , [{ Name: batch_norm2d_17.b_0, Initialized: 1, Ptr: 0x55b7bea3a4b0 }]), ]} 
I1018 08:53:57.288647 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.288650 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.288699 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c4b94e90)  to Conv2dGradNodeFinal (addr: 0x55b7c4b94300)
I1018 08:53:57.288703 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c4b94e90)  to GradNodeAccumulation (addr: 0x55b7b796fe00)
I1018 08:53:57.288707 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c4b94e90)  to GradNodeAccumulation (addr: 0x55b7bea3a5d0)
I1018 08:53:57.288713 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.288748 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877d940 }]),  
( mean , [{ Name: batch_norm2d_17.w_1, Initialized: 1, Ptr: 0x55b7bea3b370 }]),  
( variance , [{ Name: batch_norm2d_17.w_2, Initialized: 1, Ptr: 0x55b7bea3c230 }]),  
( scale , [{ Name: batch_norm2d_17.w_0, Initialized: 1, Ptr: 0x55b7b796ee40 }]),  
( bias , [{ Name: batch_norm2d_17.b_0, Initialized: 1, Ptr: 0x55b7bea3a4b0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b93780 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b93950 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b93b20 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b94170 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b92c30 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c4b95640 }]), ] } 
I1018 08:53:57.288770 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.288774 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.288885 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7ceeface0)  to ReluGradNode (addr: 0x55b7b877d050)
I1018 08:53:57.288892 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7ceeface0)  to GradNodeAccumulation (addr: 0x55b7b759fa30)
I1018 08:53:57.288921 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.288925 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.288946 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b95ac0 }]),  
( mean , [{ Name: batch_norm2d_15.w_1, Initialized: 1, Ptr: 0x55b7c4d5f940 }]),  
( variance , [{ Name: batch_norm2d_15.w_2, Initialized: 1, Ptr: 0x55b7c4d60800 }]),  
( scale , [{ Name: batch_norm2d_15.w_0, Initialized: 1, Ptr: 0x55b7b759f910 }]),  
( bias , [{ Name: batch_norm2d_15.b_0, Initialized: 1, Ptr: 0x55b7b75a2ec0 }]), ]} 
I1018 08:53:57.288959 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.288967 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.289016 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7ceefb8d0)  to Conv2dGradNodeFinal (addr: 0x55b7ceeface0)
I1018 08:53:57.289021 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7ceefb8d0)  to GradNodeAccumulation (addr: 0x55b7b3f217c0)
I1018 08:53:57.289023 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7ceefb8d0)  to GradNodeAccumulation (addr: 0x55b7b75a2fe0)
I1018 08:53:57.289031 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.289067 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b95ac0 }]),  
( mean , [{ Name: batch_norm2d_15.w_1, Initialized: 1, Ptr: 0x55b7c4d5f940 }]),  
( variance , [{ Name: batch_norm2d_15.w_2, Initialized: 1, Ptr: 0x55b7c4d60800 }]),  
( scale , [{ Name: batch_norm2d_15.w_0, Initialized: 1, Ptr: 0x55b7b759f910 }]),  
( bias , [{ Name: batch_norm2d_15.b_0, Initialized: 1, Ptr: 0x55b7b75a2ec0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceef9770 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceef9940 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceef9b10 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceef9650 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceef9330 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7ceefc080 }]), ] } 
I1018 08:53:57.289077 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.289088 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b93780 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceef9770 }]), ]} 
I1018 08:53:57.289113 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7ceefea30)  to BatchNormGradNode (addr: 0x55b7c4b94e90)
I1018 08:53:57.289117 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7ceefea30)  to BatchNormGradNode (addr: 0x55b7ceefb8d0)
I1018 08:53:57.289120 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.289134 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b93780 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceef9770 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceef9530 }]), ] } 
I1018 08:53:57.289148 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.289151 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.289158 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceef9530 }]), ]} 
I1018 08:53:57.289175 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7c9effdb0)  to AddGradNode (addr: 0x55b7ceefea30)
I1018 08:53:57.289180 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.289189 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceef9530 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b93780 }]), ] } 
I1018 08:53:57.289214 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.289219 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.289355 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c9f01010)  to ReluGradNode (addr: 0x55b7c9effdb0)
I1018 08:53:57.289361 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c9f01010)  to GradNodeAccumulation (addr: 0x55b7bea3d490)
I1018 08:53:57.289394 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.289403 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.289425 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceef9770 }]),  
( mean , [{ Name: batch_norm2d_18.w_1, Initialized: 1, Ptr: 0x55b7c49fade0 }]),  
( variance , [{ Name: batch_norm2d_18.w_2, Initialized: 1, Ptr: 0x55b7c49fbca0 }]),  
( scale , [{ Name: batch_norm2d_18.w_0, Initialized: 1, Ptr: 0x55b7bea3d370 }]),  
( bias , [{ Name: batch_norm2d_18.b_0, Initialized: 1, Ptr: 0x55b7c49f9f20 }]), ]} 
I1018 08:53:57.289439 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.289443 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.289494 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c9f01790)  to Conv2dGradNodeFinal (addr: 0x55b7c9f01010)
I1018 08:53:57.289498 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c9f01790)  to GradNodeAccumulation (addr: 0x55b7c49f9190)
I1018 08:53:57.289501 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c9f01790)  to GradNodeAccumulation (addr: 0x55b7c49fa040)
I1018 08:53:57.289508 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.289544 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceef9770 }]),  
( mean , [{ Name: batch_norm2d_18.w_1, Initialized: 1, Ptr: 0x55b7c49fade0 }]),  
( variance , [{ Name: batch_norm2d_18.w_2, Initialized: 1, Ptr: 0x55b7c49fbca0 }]),  
( scale , [{ Name: batch_norm2d_18.w_0, Initialized: 1, Ptr: 0x55b7bea3d370 }]),  
( bias , [{ Name: batch_norm2d_18.b_0, Initialized: 1, Ptr: 0x55b7c49f9f20 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceef9530 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefeeb0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f00750 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f00f00 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefe6e0 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7ceefa170 }]), ] } 
I1018 08:53:57.289561 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.289563 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.289570 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceef9530 }]), ]} 
I1018 08:53:57.289587 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7c9f041c0)  to BatchNormGradNode (addr: 0x55b7c9f01790)
I1018 08:53:57.289592 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.289600 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceef9530 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f02180 }]), ] } 
I1018 08:53:57.289618 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.289621 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.289742 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c9f05620)  to ReluGradNode (addr: 0x55b7c9f041c0)
I1018 08:53:57.289748 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c9f05620)  to GradNodeAccumulation (addr: 0x55b7c49fccb0)
I1018 08:53:57.289780 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.289784 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.289806 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceef9530 }]),  
( mean , [{ Name: batch_norm2d_19.w_1, Initialized: 1, Ptr: 0x55b7b8788190 }]),  
( variance , [{ Name: batch_norm2d_19.w_2, Initialized: 1, Ptr: 0x55b7b8789050 }]),  
( scale , [{ Name: batch_norm2d_19.w_0, Initialized: 1, Ptr: 0x55b7c49fcb90 }]),  
( bias , [{ Name: batch_norm2d_19.b_0, Initialized: 1, Ptr: 0x55b7b87872d0 }]), ]} 
I1018 08:53:57.289824 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.289826 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.289878 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c9f061b0)  to Conv2dGradNodeFinal (addr: 0x55b7c9f05620)
I1018 08:53:57.289883 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c9f061b0)  to GradNodeAccumulation (addr: 0x55b7b8786540)
I1018 08:53:57.289886 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c9f061b0)  to GradNodeAccumulation (addr: 0x55b7b87873f0)
I1018 08:53:57.289892 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.289928 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceef9530 }]),  
( mean , [{ Name: batch_norm2d_19.w_1, Initialized: 1, Ptr: 0x55b7b8788190 }]),  
( variance , [{ Name: batch_norm2d_19.w_2, Initialized: 1, Ptr: 0x55b7b8789050 }]),  
( scale , [{ Name: batch_norm2d_19.w_0, Initialized: 1, Ptr: 0x55b7c49fcb90 }]),  
( bias , [{ Name: batch_norm2d_19.b_0, Initialized: 1, Ptr: 0x55b7b87872d0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f04ba0 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f04d70 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f04f40 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f05510 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f04050 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c9f06960 }]), ] } 
I1018 08:53:57.289938 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.289948 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f04ba0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b93780 }]), ]} 
I1018 08:53:57.289974 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7cb088d10)  to BatchNormGradNode (addr: 0x55b7c9f061b0)
I1018 08:53:57.289978 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7cb088d10)  to ReluGradNode (addr: 0x55b7c9effdb0)
I1018 08:53:57.289983 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.289995 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f04ba0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b93780 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb089190 }]), ] } 
I1018 08:53:57.290009 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.290012 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.290019 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb089190 }]), ]} 
I1018 08:53:57.290035 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7cb089d10)  to AddGradNode (addr: 0x55b7cb088d10)
I1018 08:53:57.290040 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.290048 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb089190 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f04ba0 }]), ] } 
I1018 08:53:57.290114 4097533 eager_method.cc:2158] Tensor:  set use_gpudnn = 0
I1018 08:53:57.290125 4097533 eager_op_function.cc:35671] CurrentDeviceId: 0 from 0
I1018 08:53:57.290129 4097533 dygraph_functions.cc:69244] Running AD API: pool2d
I1018 08:53:57.290138 4097533 dygraph_functions.cc:69300] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb089190 }]), ]} 
I1018 08:53:57.290163 4097533 pool2d_kernel.cc:153] [Pool2d] pooling type: avg exclusive: 1 adaptive: 1
[2024-10-18 16:53:57] [CNNL] [Warning]: [cnnlAdaptivePoolingForward] is deprecated and will be removed in the future release, please use [cnnlAdaptivePoolingForward_v2] instead.
I1018 08:53:57.290228 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Pool2dGradNode (addr: 0x55b7cb08a5f0)  to ReluGradNode (addr: 0x55b7cb089d10)
I1018 08:53:57.290233 4097533 dygraph_functions.cc:69376] Finish AD API: pool2d
I1018 08:53:57.290244 4097533 dygraph_functions.cc:69390] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb089190 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb0892b0 }]), ] } 
I1018 08:53:57.290289 4097533 eager_op_function.cc:9777] CurrentDeviceId: 0 from 0
I1018 08:53:57.290303 4097533 dygraph_functions.cc:20255] Running AD API: flatten
I1018 08:53:57.290311 4097533 dygraph_functions.cc:20311] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb0892b0 }]), ]} 
I1018 08:53:57.290345 4097533 dygraph_api.cc:208] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.290364 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from FlattenGradNode (addr: 0x55b7cb08b510)  to Pool2dGradNode (addr: 0x55b7cb08a5f0)
I1018 08:53:57.290370 4097533 dygraph_functions.cc:20387] Finish AD API: flatten
I1018 08:53:57.290382 4097533 dygraph_functions.cc:20404] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb0892b0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08ac50 }]),  
( xshape , [{ Name: None, Initialized: 0, Ptr: 0x55b7cb08ae20 }]), ] } 
I1018 08:53:57.290414 4097533 dygraph_functions.cc:67019] Running AD API: matmul
I1018 08:53:57.290426 4097533 dygraph_functions.cc:67081] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08ac50 }]),  
( y , [{ Name: linear_0.w_0, Initialized: 1, Ptr: 0x55b7b6f66780 }]), ]} 
I1018 08:53:57.290503 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from MatmulGradNode (addr: 0x55b7cb08c290)  to FlattenGradNode (addr: 0x55b7cb08b510)
I1018 08:53:57.290508 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from MatmulGradNode (addr: 0x55b7cb08c290)  to GradNodeAccumulation (addr: 0x55b7b878a500)
I1018 08:53:57.290513 4097533 dygraph_functions.cc:67152] Finish AD API: matmul
I1018 08:53:57.290527 4097533 dygraph_functions.cc:67169] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08ac50 }]),  
( y , [{ Name: linear_0.w_0, Initialized: 1, Ptr: 0x55b7b6f66780 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08c710 }]), ] } 
I1018 08:53:57.290530 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.290540 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08c710 }]),  
( y , [{ Name: linear_0.b_0, Initialized: 1, Ptr: 0x55b7b878a3e0 }]), ]} 
I1018 08:53:57.290566 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7cb08d2d0)  to MatmulGradNode (addr: 0x55b7cb08c290)
I1018 08:53:57.290570 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7cb08d2d0)  to GradNodeAccumulation (addr: 0x55b7b6f66d60)
I1018 08:53:57.290573 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.290586 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08c710 }]),  
( y , [{ Name: linear_0.b_0, Initialized: 1, Ptr: 0x55b7b878a3e0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08d990 }]), ] } 
I1018 08:53:57.290854 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.290861 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.291026 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c1f6ee30)  to GradNodeAccumulation (addr: 0x55b77381dd90)
I1018 08:53:57.291072 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.291076 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.291100 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08ab30 }]),  
( mean , [{ Name: batch_norm2d_0.w_1, Initialized: 1, Ptr: 0x55b7c4ee9a30 }]),  
( variance , [{ Name: batch_norm2d_0.w_2, Initialized: 1, Ptr: 0x55b7ceb07ff0 }]),  
( scale , [{ Name: batch_norm2d_0.w_0, Initialized: 1, Ptr: 0x55b77346a070 }]),  
( bias , [{ Name: batch_norm2d_0.b_0, Initialized: 1, Ptr: 0x55b7b6f6d4b0 }]), ]} 
I1018 08:53:57.291116 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.291119 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.291172 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b77cd7b0)  to Conv2dGradNodeFinal (addr: 0x55b7c1f6ee30)
I1018 08:53:57.291177 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b77cd7b0)  to GradNodeAccumulation (addr: 0x55b77ed32260)
I1018 08:53:57.291179 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b77cd7b0)  to GradNodeAccumulation (addr: 0x55b77f0e4620)
I1018 08:53:57.291185 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.291221 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08ab30 }]),  
( mean , [{ Name: batch_norm2d_0.w_1, Initialized: 1, Ptr: 0x55b7c4ee9a30 }]),  
( variance , [{ Name: batch_norm2d_0.w_2, Initialized: 1, Ptr: 0x55b7ceb07ff0 }]),  
( scale , [{ Name: batch_norm2d_0.w_0, Initialized: 1, Ptr: 0x55b77346a070 }]),  
( bias , [{ Name: batch_norm2d_0.b_0, Initialized: 1, Ptr: 0x55b7b6f6d4b0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08ac50 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08d750 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08d870 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08c710 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08c830 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c9f04d70 }]), ] } 
I1018 08:53:57.291239 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.291242 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.291249 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08ac50 }]), ]} 
I1018 08:53:57.291265 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b1a58b50)  to BatchNormGradNode (addr: 0x55b7b77cd7b0)
I1018 08:53:57.291270 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.291280 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08ac50 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c2aa34e0 }]), ] } 
I1018 08:53:57.291303 4097533 eager_op_function.cc:35671] CurrentDeviceId: 0 from 0
I1018 08:53:57.291307 4097533 dygraph_functions.cc:69244] Running AD API: pool2d
I1018 08:53:57.291313 4097533 dygraph_functions.cc:69300] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c2aa34e0 }]), ]} 
I1018 08:53:57.291333 4097533 pool2d_kernel.cc:153] [Pool2d] pooling type: max exclusive: 1 adaptive: 0
I1018 08:53:57.291358 4097533 pool2d_kernel.cc:198] [Pool2d] extra_input_size: 0
I1018 08:53:57.291388 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Pool2dGradNode (addr: 0x55b7c4e7c8c0)  to ReluGradNode (addr: 0x55b7b1a58b50)
I1018 08:53:57.291392 4097533 dygraph_functions.cc:69376] Finish AD API: pool2d
I1018 08:53:57.291404 4097533 dygraph_functions.cc:69390] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c2aa34e0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08ac50 }]), ] } 
I1018 08:53:57.291431 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.291435 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.291549 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c2d5e1f0)  to Pool2dGradNode (addr: 0x55b7c4e7c8c0)
I1018 08:53:57.291555 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c2d5e1f0)  to GradNodeAccumulation (addr: 0x55b773982760)
I1018 08:53:57.291589 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.291594 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.291615 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f70300 }]),  
( mean , [{ Name: batch_norm2d_1.w_1, Initialized: 1, Ptr: 0x55b7b64bb520 }]),  
( variance , [{ Name: batch_norm2d_1.w_2, Initialized: 1, Ptr: 0x55b7b68ca2d0 }]),  
( scale , [{ Name: batch_norm2d_1.w_0, Initialized: 1, Ptr: 0x55b7b68c8170 }]),  
( bias , [{ Name: batch_norm2d_1.b_0, Initialized: 1, Ptr: 0x55b7b76da540 }]), ]} 
I1018 08:53:57.291627 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.291630 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.291678 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c4168f10)  to Conv2dGradNodeFinal (addr: 0x55b7c2d5e1f0)
I1018 08:53:57.291683 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c4168f10)  to GradNodeAccumulation (addr: 0x55b773982b90)
I1018 08:53:57.291687 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c4168f10)  to GradNodeAccumulation (addr: 0x55b773f00630)
I1018 08:53:57.291693 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.291730 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f70300 }]),  
( mean , [{ Name: batch_norm2d_1.w_1, Initialized: 1, Ptr: 0x55b7b64bb520 }]),  
( variance , [{ Name: batch_norm2d_1.w_2, Initialized: 1, Ptr: 0x55b7b68ca2d0 }]),  
( scale , [{ Name: batch_norm2d_1.w_0, Initialized: 1, Ptr: 0x55b7b68c8170 }]),  
( bias , [{ Name: batch_norm2d_1.b_0, Initialized: 1, Ptr: 0x55b7b76da540 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7ce10 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b77370b470 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7edf0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe32db0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b77398c530 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c22cfc40 }]), ] } 
I1018 08:53:57.291745 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.291749 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.291755 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7ce10 }]), ]} 
I1018 08:53:57.291772 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b76fbc80)  to BatchNormGradNode (addr: 0x55b7c4168f10)
I1018 08:53:57.291777 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.291786 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7ce10 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f6f5b0 }]), ] } 
I1018 08:53:57.291803 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.291807 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.291915 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b70ef960)  to ReluGradNode (addr: 0x55b7b76fbc80)
I1018 08:53:57.291926 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b70ef960)  to GradNodeAccumulation (addr: 0x55b7bd3da270)
I1018 08:53:57.291958 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.291962 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.291985 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7ce10 }]),  
( mean , [{ Name: batch_norm2d_2.w_1, Initialized: 1, Ptr: 0x55b7b76e6e10 }]),  
( variance , [{ Name: batch_norm2d_2.w_2, Initialized: 1, Ptr: 0x55b7c2207b30 }]),  
( scale , [{ Name: batch_norm2d_2.w_0, Initialized: 1, Ptr: 0x55b7bd3da150 }]),  
( bias , [{ Name: batch_norm2d_2.b_0, Initialized: 1, Ptr: 0x55b7bd3d86c0 }]), ]} 
I1018 08:53:57.291996 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.291998 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.292047 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7cfe31d60)  to Conv2dGradNodeFinal (addr: 0x55b7b70ef960)
I1018 08:53:57.292052 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7cfe31d60)  to GradNodeAccumulation (addr: 0x55b7c4ee5b40)
I1018 08:53:57.292054 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7cfe31d60)  to GradNodeAccumulation (addr: 0x55b7bd3d87e0)
I1018 08:53:57.292061 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.292097 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7ce10 }]),  
( mean , [{ Name: batch_norm2d_2.w_1, Initialized: 1, Ptr: 0x55b7b76e6e10 }]),  
( variance , [{ Name: batch_norm2d_2.w_2, Initialized: 1, Ptr: 0x55b7c2207b30 }]),  
( scale , [{ Name: batch_norm2d_2.w_0, Initialized: 1, Ptr: 0x55b7bd3da150 }]),  
( bias , [{ Name: batch_norm2d_2.b_0, Initialized: 1, Ptr: 0x55b7bd3d86c0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7fd20 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e39a0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cb9da0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b773923410 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b78239edb0 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b78239eed0 }]), ] } 
I1018 08:53:57.292109 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.292117 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7fd20 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08ac50 }]), ]} 
I1018 08:53:57.292143 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7c4eda640)  to BatchNormGradNode (addr: 0x55b7cfe31d60)
I1018 08:53:57.292147 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7c4eda640)  to Pool2dGradNode (addr: 0x55b7c4e7c8c0)
I1018 08:53:57.292151 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.292164 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7fd20 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08ac50 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b70ee5f0 }]), ] } 
I1018 08:53:57.292178 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.292181 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.292187 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b70ee5f0 }]), ]} 
I1018 08:53:57.292203 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b77324be20)  to AddGradNode (addr: 0x55b7c4eda640)
I1018 08:53:57.292212 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.292222 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b70ee5f0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7fd20 }]), ] } 
I1018 08:53:57.292240 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.292244 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.292359 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4e78c00)  to ReluGradNode (addr: 0x55b77324be20)
I1018 08:53:57.292364 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4e78c00)  to GradNodeAccumulation (addr: 0x55b7b76e5550)
I1018 08:53:57.292397 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.292400 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.292423 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b70ee5f0 }]),  
( mean , [{ Name: batch_norm2d_3.w_1, Initialized: 1, Ptr: 0x55b7b778e800 }]),  
( variance , [{ Name: batch_norm2d_3.w_2, Initialized: 1, Ptr: 0x55b7b3f224a0 }]),  
( scale , [{ Name: batch_norm2d_3.w_0, Initialized: 1, Ptr: 0x55b7bd3daab0 }]),  
( bias , [{ Name: batch_norm2d_3.b_0, Initialized: 1, Ptr: 0x55b7b87810f0 }]), ]} 
I1018 08:53:57.292434 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.292438 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.292484 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c2d5eb80)  to Conv2dGradNodeFinal (addr: 0x55b7c4e78c00)
I1018 08:53:57.292488 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c2d5eb80)  to GradNodeAccumulation (addr: 0x55b7b76e6360)
I1018 08:53:57.292492 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c2d5eb80)  to GradNodeAccumulation (addr: 0x55b7b68c8390)
I1018 08:53:57.292500 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.292536 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b70ee5f0 }]),  
( mean , [{ Name: batch_norm2d_3.w_1, Initialized: 1, Ptr: 0x55b7b778e800 }]),  
( variance , [{ Name: batch_norm2d_3.w_2, Initialized: 1, Ptr: 0x55b7b3f224a0 }]),  
( scale , [{ Name: batch_norm2d_3.w_0, Initialized: 1, Ptr: 0x55b7bd3daab0 }]),  
( bias , [{ Name: batch_norm2d_3.b_0, Initialized: 1, Ptr: 0x55b7b87810f0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f06fb0 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f070d0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b77380c8a0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b7796580 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77966a0 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b78e5a30 }]), ] } 
I1018 08:53:57.292551 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.292554 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.292560 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f06fb0 }]), ]} 
I1018 08:53:57.292577 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b77334c090)  to BatchNormGradNode (addr: 0x55b7c2d5eb80)
I1018 08:53:57.292582 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.292590 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f06fb0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b8e600 }]), ] } 
I1018 08:53:57.292611 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.292615 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.292721 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4b93d50)  to ReluGradNode (addr: 0x55b77334c090)
I1018 08:53:57.292726 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4b93d50)  to GradNodeAccumulation (addr: 0x55b7b68c6d90)
I1018 08:53:57.292758 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.292762 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.292783 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f06fb0 }]),  
( mean , [{ Name: batch_norm2d_4.w_1, Initialized: 1, Ptr: 0x55b7c2209c90 }]),  
( variance , [{ Name: batch_norm2d_4.w_2, Initialized: 1, Ptr: 0x55b7c220ab50 }]),  
( scale , [{ Name: batch_norm2d_4.w_0, Initialized: 1, Ptr: 0x55b7b68c6c70 }]),  
( bias , [{ Name: batch_norm2d_4.b_0, Initialized: 1, Ptr: 0x55b7b4b4a7c0 }]), ]} 
I1018 08:53:57.292795 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.292798 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.292847 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c1f71c80)  to Conv2dGradNodeFinal (addr: 0x55b7c4b93d50)
I1018 08:53:57.292851 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c1f71c80)  to GradNodeAccumulation (addr: 0x55b7b4b49ae0)
I1018 08:53:57.292855 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c1f71c80)  to GradNodeAccumulation (addr: 0x55b7b4b4a8e0)
I1018 08:53:57.292861 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.292897 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f06fb0 }]),  
( mean , [{ Name: batch_norm2d_4.w_1, Initialized: 1, Ptr: 0x55b7c2209c90 }]),  
( variance , [{ Name: batch_norm2d_4.w_2, Initialized: 1, Ptr: 0x55b7c220ab50 }]),  
( scale , [{ Name: batch_norm2d_4.w_0, Initialized: 1, Ptr: 0x55b7b68c6c70 }]),  
( bias , [{ Name: batch_norm2d_4.b_0, Initialized: 1, Ptr: 0x55b7b4b4a7c0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f78500 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779f660 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779ec90 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701e050 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701d900 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c1f70c90 }]), ] } 
I1018 08:53:57.292908 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.292917 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f78500 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7fd20 }]), ]} 
I1018 08:53:57.292943 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7bacd9960)  to BatchNormGradNode (addr: 0x55b7c1f71c80)
I1018 08:53:57.292948 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7bacd9960)  to ReluGradNode (addr: 0x55b77324be20)
I1018 08:53:57.292950 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.292963 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f78500 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7fd20 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd9de0 }]), ] } 
I1018 08:53:57.292977 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.292985 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.292992 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd9de0 }]), ]} 
I1018 08:53:57.293008 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7733494c0)  to AddGradNode (addr: 0x55b7bacd9960)
I1018 08:53:57.293013 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.293022 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd9de0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f78500 }]), ] } 
I1018 08:53:57.293046 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.293051 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.293165 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b6f6a380)  to ReluGradNode (addr: 0x55b7733494c0)
I1018 08:53:57.293171 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b6f6a380)  to GradNodeAccumulation (addr: 0x55b7c4ee2d80)
I1018 08:53:57.293203 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.293213 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.293236 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd9de0 }]),  
( mean , [{ Name: batch_norm2d_6.w_1, Initialized: 1, Ptr: 0x55b7cc394970 }]),  
( variance , [{ Name: batch_norm2d_6.w_2, Initialized: 1, Ptr: 0x55b7cc395a60 }]),  
( scale , [{ Name: batch_norm2d_6.w_0, Initialized: 1, Ptr: 0x55b7c4ee3ce0 }]),  
( bias , [{ Name: batch_norm2d_6.b_0, Initialized: 1, Ptr: 0x55b7c4ee4b90 }]), ]} 
I1018 08:53:57.293247 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.293251 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.293301 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b6f7df70)  to Conv2dGradNodeFinal (addr: 0x55b7b6f6a380)
I1018 08:53:57.293306 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b6f7df70)  to GradNodeAccumulation (addr: 0x55b7c4ee3e00)
I1018 08:53:57.293309 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b6f7df70)  to GradNodeAccumulation (addr: 0x55b7c4ee4cb0)
I1018 08:53:57.293315 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.293352 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd9de0 }]),  
( mean , [{ Name: batch_norm2d_6.w_1, Initialized: 1, Ptr: 0x55b7cc394970 }]),  
( variance , [{ Name: batch_norm2d_6.w_2, Initialized: 1, Ptr: 0x55b7cc395a60 }]),  
( scale , [{ Name: batch_norm2d_6.w_0, Initialized: 1, Ptr: 0x55b7c4ee3ce0 }]),  
( bias , [{ Name: batch_norm2d_6.b_0, Initialized: 1, Ptr: 0x55b7c4ee4b90 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7f8a0 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7fa70 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ed260 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77d2510 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77d2630 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b77d2750 }]), ] } 
I1018 08:53:57.293367 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.293371 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.293377 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7f8a0 }]), ]} 
I1018 08:53:57.293393 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b773371db0)  to BatchNormGradNode (addr: 0x55b7b6f7df70)
I1018 08:53:57.293403 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.293412 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7f8a0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceef9f60 }]), ] } 
I1018 08:53:57.293429 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.293433 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.293543 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b701c560)  to ReluGradNode (addr: 0x55b773371db0)
I1018 08:53:57.293548 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b701c560)  to GradNodeAccumulation (addr: 0x55b7cc396a70)
I1018 08:53:57.293581 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.293584 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.293605 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7f8a0 }]),  
( mean , [{ Name: batch_norm2d_7.w_1, Initialized: 1, Ptr: 0x55b7c4d65af0 }]),  
( variance , [{ Name: batch_norm2d_7.w_2, Initialized: 1, Ptr: 0x55b7b778c480 }]),  
( scale , [{ Name: batch_norm2d_7.w_0, Initialized: 1, Ptr: 0x55b7cc396950 }]),  
( bias , [{ Name: batch_norm2d_7.b_0, Initialized: 1, Ptr: 0x55b7c4d64c30 }]), ]} 
I1018 08:53:57.293617 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.293620 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.293670 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c416bf80)  to Conv2dGradNodeFinal (addr: 0x55b7b701c560)
I1018 08:53:57.293674 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c416bf80)  to GradNodeAccumulation (addr: 0x55b7c4d63ea0)
I1018 08:53:57.293678 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c416bf80)  to GradNodeAccumulation (addr: 0x55b7c4d64d50)
I1018 08:53:57.293684 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.293720 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7f8a0 }]),  
( mean , [{ Name: batch_norm2d_7.w_1, Initialized: 1, Ptr: 0x55b7c4d65af0 }]),  
( variance , [{ Name: batch_norm2d_7.w_2, Initialized: 1, Ptr: 0x55b7b778c480 }]),  
( scale , [{ Name: batch_norm2d_7.w_0, Initialized: 1, Ptr: 0x55b7cc396950 }]),  
( bias , [{ Name: batch_norm2d_7.b_0, Initialized: 1, Ptr: 0x55b7c4d64c30 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08d050 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08d220 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08d3f0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ec860 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08ba20 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7cb08c220 }]), ] } 
I1018 08:53:57.293741 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.293745 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.293851 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b77a08c0)  to ReluGradNode (addr: 0x55b7733494c0)
I1018 08:53:57.293857 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b77a08c0)  to GradNodeAccumulation (addr: 0x55b7b85db7c0)
I1018 08:53:57.293887 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.293891 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.293912 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe32590 }]),  
( mean , [{ Name: batch_norm2d_5.w_1, Initialized: 1, Ptr: 0x55b7bb903150 }]),  
( variance , [{ Name: batch_norm2d_5.w_2, Initialized: 1, Ptr: 0x55b7bb904240 }]),  
( scale , [{ Name: batch_norm2d_5.w_0, Initialized: 1, Ptr: 0x55b7b3f22b10 }]),  
( bias , [{ Name: batch_norm2d_5.b_0, Initialized: 1, Ptr: 0x55b7b85dd420 }]), ]} 
I1018 08:53:57.293928 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.293931 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.293979 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7cfe2fdf0)  to Conv2dGradNodeFinal (addr: 0x55b7b77a08c0)
I1018 08:53:57.293983 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7cfe2fdf0)  to GradNodeAccumulation (addr: 0x55b7b85dc690)
I1018 08:53:57.293987 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7cfe2fdf0)  to GradNodeAccumulation (addr: 0x55b7b85dd540)
I1018 08:53:57.293992 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.294028 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe32590 }]),  
( mean , [{ Name: batch_norm2d_5.w_1, Initialized: 1, Ptr: 0x55b7bb903150 }]),  
( variance , [{ Name: batch_norm2d_5.w_2, Initialized: 1, Ptr: 0x55b7bb904240 }]),  
( scale , [{ Name: batch_norm2d_5.w_0, Initialized: 1, Ptr: 0x55b7b3f22b10 }]),  
( bias , [{ Name: batch_norm2d_5.b_0, Initialized: 1, Ptr: 0x55b7b85dd420 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e78150 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e78320 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e784f0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe305a0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1dd70 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c4e1de90 }]), ] } 
I1018 08:53:57.294039 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.294049 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08d050 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e78150 }]), ]} 
I1018 08:53:57.294073 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7c4cba280)  to BatchNormGradNode (addr: 0x55b7c416bf80)
I1018 08:53:57.294077 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7c4cba280)  to BatchNormGradNode (addr: 0x55b7cfe2fdf0)
I1018 08:53:57.294080 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.294093 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08d050 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e78150 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cba700 }]), ] } 
I1018 08:53:57.294108 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.294111 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.294117 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cba700 }]), ]} 
I1018 08:53:57.294133 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b4d8d1e0)  to AddGradNode (addr: 0x55b7c4cba280)
I1018 08:53:57.294138 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.294147 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cba700 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08d050 }]), ] } 
I1018 08:53:57.294166 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.294170 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.294292 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b77d00e0)  to ReluGradNode (addr: 0x55b7b4d8d1e0)
I1018 08:53:57.294298 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b77d00e0)  to GradNodeAccumulation (addr: 0x55b7b778d7d0)
I1018 08:53:57.294332 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.294335 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.294356 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e78150 }]),  
( mean , [{ Name: batch_norm2d_8.w_1, Initialized: 1, Ptr: 0x55b7c5cf24d0 }]),  
( variance , [{ Name: batch_norm2d_8.w_2, Initialized: 1, Ptr: 0x55b7c5cf3390 }]),  
( scale , [{ Name: batch_norm2d_8.w_0, Initialized: 1, Ptr: 0x55b7bb905130 }]),  
( bias , [{ Name: batch_norm2d_8.b_0, Initialized: 1, Ptr: 0x55b7c5cf1610 }]), ]} 
I1018 08:53:57.294368 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.294371 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.294421 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c22d0760)  to Conv2dGradNodeFinal (addr: 0x55b7b77d00e0)
I1018 08:53:57.294425 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c22d0760)  to GradNodeAccumulation (addr: 0x55b7c5cf0810)
I1018 08:53:57.294428 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c22d0760)  to GradNodeAccumulation (addr: 0x55b7c5cf1730)
I1018 08:53:57.294435 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.294471 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e78150 }]),  
( mean , [{ Name: batch_norm2d_8.w_1, Initialized: 1, Ptr: 0x55b7c5cf24d0 }]),  
( variance , [{ Name: batch_norm2d_8.w_2, Initialized: 1, Ptr: 0x55b7c5cf3390 }]),  
( scale , [{ Name: batch_norm2d_8.w_0, Initialized: 1, Ptr: 0x55b7bb905130 }]),  
( bias , [{ Name: batch_norm2d_8.b_0, Initialized: 1, Ptr: 0x55b7c5cf1610 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cba700 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cba820 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacda990 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77d05f0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c22d0f10 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c4b8d520 }]), ] } 
I1018 08:53:57.294485 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.294489 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.294495 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cba700 }]), ]} 
I1018 08:53:57.294512 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7c23e4f30)  to BatchNormGradNode (addr: 0x55b7c22d0760)
I1018 08:53:57.294517 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.294526 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cba700 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b8d9a0 }]), ] } 
I1018 08:53:57.294543 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.294546 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.294653 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b78e61c0)  to ReluGradNode (addr: 0x55b7c23e4f30)
I1018 08:53:57.294659 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b78e61c0)  to GradNodeAccumulation (addr: 0x55b7cc6c36b0)
I1018 08:53:57.294696 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.294700 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.294721 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cba700 }]),  
( mean , [{ Name: batch_norm2d_9.w_1, Initialized: 1, Ptr: 0x55b7b7056fb0 }]),  
( variance , [{ Name: batch_norm2d_9.w_2, Initialized: 1, Ptr: 0x55b7b7057e70 }]),  
( scale , [{ Name: batch_norm2d_9.w_0, Initialized: 1, Ptr: 0x55b7cc6c3590 }]),  
( bias , [{ Name: batch_norm2d_9.b_0, Initialized: 1, Ptr: 0x55b7b70560f0 }]), ]} 
I1018 08:53:57.294734 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.294736 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.294786 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c2d5d510)  to Conv2dGradNodeFinal (addr: 0x55b7b78e61c0)
I1018 08:53:57.294791 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c2d5d510)  to GradNodeAccumulation (addr: 0x55b7cc6c4610)
I1018 08:53:57.294795 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c2d5d510)  to GradNodeAccumulation (addr: 0x55b7b7056210)
I1018 08:53:57.294800 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.294836 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cba700 }]),  
( mean , [{ Name: batch_norm2d_9.w_1, Initialized: 1, Ptr: 0x55b7b7056fb0 }]),  
( variance , [{ Name: batch_norm2d_9.w_2, Initialized: 1, Ptr: 0x55b7b7057e70 }]),  
( scale , [{ Name: batch_norm2d_9.w_0, Initialized: 1, Ptr: 0x55b7cc6c3590 }]),  
( bias , [{ Name: batch_norm2d_9.b_0, Initialized: 1, Ptr: 0x55b7b70560f0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e7bbf0 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e7bdc0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e7bf90 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c2d5dcc0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e50c0 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b78e51e0 }]), ] } 
I1018 08:53:57.294847 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.294857 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e7bbf0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08d050 }]), ]} 
I1018 08:53:57.294880 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7b701b880)  to BatchNormGradNode (addr: 0x55b7c2d5d510)
I1018 08:53:57.294884 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7b701b880)  to ReluGradNode (addr: 0x55b7b4d8d1e0)
I1018 08:53:57.294888 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.294900 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e7bbf0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08d050 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701bd00 }]), ] } 
I1018 08:53:57.294915 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.294919 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.294924 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701bd00 }]), ]} 
I1018 08:53:57.294940 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b779fb50)  to AddGradNode (addr: 0x55b7b701b880)
I1018 08:53:57.294945 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.294955 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701bd00 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e7bbf0 }]), ] } 
I1018 08:53:57.294983 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.294986 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.295100 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b77ce2c0)  to ReluGradNode (addr: 0x55b7b779fb50)
I1018 08:53:57.295106 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b77ce2c0)  to GradNodeAccumulation (addr: 0x55b7c4edec60)
I1018 08:53:57.295138 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.295142 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.295164 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701bd00 }]),  
( mean , [{ Name: batch_norm2d_11.w_1, Initialized: 1, Ptr: 0x55b7b7059750 }]),  
( variance , [{ Name: batch_norm2d_11.w_2, Initialized: 1, Ptr: 0x55b7c5ced740 }]),  
( scale , [{ Name: batch_norm2d_11.w_0, Initialized: 1, Ptr: 0x55b7b4b498d0 }]),  
( bias , [{ Name: batch_norm2d_11.b_0, Initialized: 1, Ptr: 0x55b7c4ee0950 }]), ]} 
I1018 08:53:57.295176 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.295179 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.295229 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c4e7a1f0)  to Conv2dGradNodeFinal (addr: 0x55b7b77ce2c0)
I1018 08:53:57.295233 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c4e7a1f0)  to GradNodeAccumulation (addr: 0x55b7c4edfbc0)
I1018 08:53:57.295236 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c4e7a1f0)  to GradNodeAccumulation (addr: 0x55b7c4ee0a70)
I1018 08:53:57.295243 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.295280 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701bd00 }]),  
( mean , [{ Name: batch_norm2d_11.w_1, Initialized: 1, Ptr: 0x55b7b7059750 }]),  
( variance , [{ Name: batch_norm2d_11.w_2, Initialized: 1, Ptr: 0x55b7c5ced740 }]),  
( scale , [{ Name: batch_norm2d_11.w_0, Initialized: 1, Ptr: 0x55b7b4b498d0 }]),  
( bias , [{ Name: batch_norm2d_11.b_0, Initialized: 1, Ptr: 0x55b7c4ee0950 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779d6d0 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77d0af0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77d0cc0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e7a9a0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701e7a0 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b701e8c0 }]), ] } 
I1018 08:53:57.295295 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.295298 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.295305 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779d6d0 }]), ]} 
I1018 08:53:57.295321 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7bacd8130)  to BatchNormGradNode (addr: 0x55b7c4e7a1f0)
I1018 08:53:57.295325 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.295334 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779d6d0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701ed40 }]), ] } 
I1018 08:53:57.295351 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.295356 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.295465 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b78e8440)  to ReluGradNode (addr: 0x55b7bacd8130)
I1018 08:53:57.295476 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b78e8440)  to GradNodeAccumulation (addr: 0x55b7c5cee750)
I1018 08:53:57.295508 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.295512 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.295533 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779d6d0 }]),  
( mean , [{ Name: batch_norm2d_12.w_1, Initialized: 1, Ptr: 0x55b7cc398370 }]),  
( variance , [{ Name: batch_norm2d_12.w_2, Initialized: 1, Ptr: 0x55b7cc399230 }]),  
( scale , [{ Name: batch_norm2d_12.w_0, Initialized: 1, Ptr: 0x55b7c5cee630 }]),  
( bias , [{ Name: batch_norm2d_12.b_0, Initialized: 1, Ptr: 0x55b7cc3974b0 }]), ]} 
I1018 08:53:57.295545 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.295548 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.295596 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7cfe31190)  to Conv2dGradNodeFinal (addr: 0x55b7b78e8440)
I1018 08:53:57.295601 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7cfe31190)  to GradNodeAccumulation (addr: 0x55b7c5cef6b0)
I1018 08:53:57.295604 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7cfe31190)  to GradNodeAccumulation (addr: 0x55b7cc3975d0)
I1018 08:53:57.295610 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.295647 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779d6d0 }]),  
( mean , [{ Name: batch_norm2d_12.w_1, Initialized: 1, Ptr: 0x55b7cc398370 }]),  
( variance , [{ Name: batch_norm2d_12.w_2, Initialized: 1, Ptr: 0x55b7cc399230 }]),  
( scale , [{ Name: batch_norm2d_12.w_0, Initialized: 1, Ptr: 0x55b7c5cee630 }]),  
( bias , [{ Name: batch_norm2d_12.b_0, Initialized: 1, Ptr: 0x55b7cc3974b0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e79620 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e797f0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e799c0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe31940 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe31a60 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7bacd5ec0 }]), ] } 
I1018 08:53:57.295668 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.295672 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.295778 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b77cefd0)  to ReluGradNode (addr: 0x55b7b779fb50)
I1018 08:53:57.295783 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b77cefd0)  to GradNodeAccumulation (addr: 0x55b7b7058e80)
I1018 08:53:57.295814 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.295817 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.295838 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd63f0 }]),  
( mean , [{ Name: batch_norm2d_10.w_1, Initialized: 1, Ptr: 0x55b7b780fba0 }]),  
( variance , [{ Name: batch_norm2d_10.w_2, Initialized: 1, Ptr: 0x55b7b7810a60 }]),  
( scale , [{ Name: batch_norm2d_10.w_0, Initialized: 1, Ptr: 0x55b7b7058d60 }]),  
( bias , [{ Name: batch_norm2d_10.b_0, Initialized: 1, Ptr: 0x55b7b780ece0 }]), ]} 
I1018 08:53:57.295850 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.295852 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.295905 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c4cb7f60)  to Conv2dGradNodeFinal (addr: 0x55b7b77cefd0)
I1018 08:53:57.295910 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c4cb7f60)  to GradNodeAccumulation (addr: 0x55b7b780df50)
I1018 08:53:57.295913 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c4cb7f60)  to GradNodeAccumulation (addr: 0x55b7b780ee00)
I1018 08:53:57.295919 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.295956 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd63f0 }]),  
( mean , [{ Name: batch_norm2d_10.w_1, Initialized: 1, Ptr: 0x55b7b780fba0 }]),  
( variance , [{ Name: batch_norm2d_10.w_2, Initialized: 1, Ptr: 0x55b7b7810a60 }]),  
( scale , [{ Name: batch_norm2d_10.w_0, Initialized: 1, Ptr: 0x55b7b7058d60 }]),  
( bias , [{ Name: batch_norm2d_10.b_0, Initialized: 1, Ptr: 0x55b7b780ece0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e74e0 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e76b0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e7880 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701fa80 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cb8710 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c4cb8830 }]), ] } 
I1018 08:53:57.295966 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.295976 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e79620 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e74e0 }]), ]} 
I1018 08:53:57.296001 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7c4edc340)  to BatchNormGradNode (addr: 0x55b7cfe31190)
I1018 08:53:57.296006 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7c4edc340)  to BatchNormGradNode (addr: 0x55b7c4cb7f60)
I1018 08:53:57.296010 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.296021 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e79620 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e74e0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e202d0 }]), ] } 
I1018 08:53:57.296036 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.296041 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.296046 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e202d0 }]), ]} 
I1018 08:53:57.296062 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b6f67ab0)  to AddGradNode (addr: 0x55b7c4edc340)
I1018 08:53:57.296066 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.296077 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e202d0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e79620 }]), ] } 
I1018 08:53:57.296095 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.296099 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.296216 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7ceef9220)  to ReluGradNode (addr: 0x55b7b6f67ab0)
I1018 08:53:57.296222 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7ceef9220)  to GradNodeAccumulation (addr: 0x55b7cc39a490)
I1018 08:53:57.296254 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.296258 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.296280 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e74e0 }]),  
( mean , [{ Name: batch_norm2d_13.w_1, Initialized: 1, Ptr: 0x55b7cc4bf110 }]),  
( variance , [{ Name: batch_norm2d_13.w_2, Initialized: 1, Ptr: 0x55b7cc4bffd0 }]),  
( scale , [{ Name: batch_norm2d_13.w_0, Initialized: 1, Ptr: 0x55b7cc39a370 }]),  
( bias , [{ Name: batch_norm2d_13.b_0, Initialized: 1, Ptr: 0x55b7cc4be250 }]), ]} 
I1018 08:53:57.296298 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.296301 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.296351 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c9effdb0)  to Conv2dGradNodeFinal (addr: 0x55b7ceef9220)
I1018 08:53:57.296355 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c9effdb0)  to GradNodeAccumulation (addr: 0x55b7cc4bd4b0)
I1018 08:53:57.296358 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c9effdb0)  to GradNodeAccumulation (addr: 0x55b7cc4be370)
I1018 08:53:57.296365 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.296401 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e74e0 }]),  
( mean , [{ Name: batch_norm2d_13.w_1, Initialized: 1, Ptr: 0x55b7cc4bf110 }]),  
( variance , [{ Name: batch_norm2d_13.w_2, Initialized: 1, Ptr: 0x55b7cc4bffd0 }]),  
( scale , [{ Name: batch_norm2d_13.w_0, Initialized: 1, Ptr: 0x55b7cc39a370 }]),  
( bias , [{ Name: batch_norm2d_13.b_0, Initialized: 1, Ptr: 0x55b7cc4be250 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e202d0 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7c9d0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f68450 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cb76a0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f68570 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b78e42f0 }]), ] } 
I1018 08:53:57.296416 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.296420 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.296427 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e202d0 }]), ]} 
I1018 08:53:57.296443 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b78edb60)  to BatchNormGradNode (addr: 0x55b7c9effdb0)
I1018 08:53:57.296447 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.296456 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e202d0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f00970 }]), ] } 
I1018 08:53:57.296473 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.296476 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.296583 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4e1bb60)  to ReluGradNode (addr: 0x55b7b78edb60)
I1018 08:53:57.296589 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4e1bb60)  to GradNodeAccumulation (addr: 0x55b7b68cc840)
I1018 08:53:57.296619 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.296623 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.296644 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e202d0 }]),  
( mean , [{ Name: batch_norm2d_14.w_1, Initialized: 1, Ptr: 0x55b7b68cf3f0 }]),  
( variance , [{ Name: batch_norm2d_14.w_2, Initialized: 1, Ptr: 0x55b7b68d02b0 }]),  
( scale , [{ Name: batch_norm2d_14.w_0, Initialized: 1, Ptr: 0x55b7cc4c0e90 }]),  
( bias , [{ Name: batch_norm2d_14.b_0, Initialized: 1, Ptr: 0x55b7b68ce530 }]), ]} 
I1018 08:53:57.296661 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.296664 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.296710 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b78eafc0)  to Conv2dGradNodeFinal (addr: 0x55b7c4e1bb60)
I1018 08:53:57.296715 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b78eafc0)  to GradNodeAccumulation (addr: 0x55b7b68cd7a0)
I1018 08:53:57.296717 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b78eafc0)  to GradNodeAccumulation (addr: 0x55b7b68ce650)
I1018 08:53:57.296723 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.296758 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e202d0 }]),  
( mean , [{ Name: batch_norm2d_14.w_1, Initialized: 1, Ptr: 0x55b7b68cf3f0 }]),  
( variance , [{ Name: batch_norm2d_14.w_2, Initialized: 1, Ptr: 0x55b7b68d02b0 }]),  
( scale , [{ Name: batch_norm2d_14.w_0, Initialized: 1, Ptr: 0x55b7cc4c0e90 }]),  
( bias , [{ Name: batch_norm2d_14.b_0, Initialized: 1, Ptr: 0x55b7b68ce530 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ee4d0 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ee6a0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ee870 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c416b920 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1c700 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c4e1c820 }]), ] } 
I1018 08:53:57.296768 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.296778 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ee4d0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e79620 }]), ]} 
I1018 08:53:57.296802 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7c4cbb6e0)  to BatchNormGradNode (addr: 0x55b7b78eafc0)
I1018 08:53:57.296806 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7c4cbb6e0)  to ReluGradNode (addr: 0x55b7b6f67ab0)
I1018 08:53:57.296809 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.296823 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ee4d0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e79620 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cbbb60 }]), ] } 
I1018 08:53:57.296836 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.296839 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.296845 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cbbb60 }]), ]} 
I1018 08:53:57.296861 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7ceefb130)  to AddGradNode (addr: 0x55b7c4cbb6e0)
I1018 08:53:57.296865 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.296875 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cbbb60 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ee4d0 }]), ] } 
I1018 08:53:57.296898 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.296902 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.297024 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c9f05500)  to ReluGradNode (addr: 0x55b7ceefb130)
I1018 08:53:57.297029 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c9f05500)  to GradNodeAccumulation (addr: 0x55b7c4d61a60)
I1018 08:53:57.297066 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.297070 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.297091 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cbbb60 }]),  
( mean , [{ Name: batch_norm2d_16.w_1, Initialized: 1, Ptr: 0x55b7b796d090 }]),  
( variance , [{ Name: batch_norm2d_16.w_2, Initialized: 1, Ptr: 0x55b7b796df50 }]),  
( scale , [{ Name: batch_norm2d_16.w_0, Initialized: 1, Ptr: 0x55b7c4d61940 }]),  
( bias , [{ Name: batch_norm2d_16.b_0, Initialized: 1, Ptr: 0x55b7b796c1d0 }]), ]} 
I1018 08:53:57.297103 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.297106 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.297154 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c9f06090)  to Conv2dGradNodeFinal (addr: 0x55b7c9f05500)
I1018 08:53:57.297158 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c9f06090)  to GradNodeAccumulation (addr: 0x55b7c4d629c0)
I1018 08:53:57.297161 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c9f06090)  to GradNodeAccumulation (addr: 0x55b7b796c2f0)
I1018 08:53:57.297168 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.297204 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cbbb60 }]),  
( mean , [{ Name: batch_norm2d_16.w_1, Initialized: 1, Ptr: 0x55b7b796d090 }]),  
( variance , [{ Name: batch_norm2d_16.w_2, Initialized: 1, Ptr: 0x55b7b796df50 }]),  
( scale , [{ Name: batch_norm2d_16.w_0, Initialized: 1, Ptr: 0x55b7c4d61940 }]),  
( bias , [{ Name: batch_norm2d_16.b_0, Initialized: 1, Ptr: 0x55b7b796c1d0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefba20 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefbbf0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefbdc0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cbb390 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f06840 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c9f06960 }]), ] } 
I1018 08:53:57.297238 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.297241 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.297248 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefba20 }]), ]} 
I1018 08:53:57.297266 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b757cc50)  to BatchNormGradNode (addr: 0x55b7c9f06090)
I1018 08:53:57.297271 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.297281 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefba20 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b70ec240 }]), ] } 
I1018 08:53:57.297297 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.297302 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.297430 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b757e040)  to ReluGradNode (addr: 0x55b7b757cc50)
I1018 08:53:57.297436 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b757e040)  to GradNodeAccumulation (addr: 0x55b7b796ef60)
I1018 08:53:57.297469 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.297473 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.297495 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefba20 }]),  
( mean , [{ Name: batch_norm2d_17.w_1, Initialized: 1, Ptr: 0x55b7bea3b370 }]),  
( variance , [{ Name: batch_norm2d_17.w_2, Initialized: 1, Ptr: 0x55b7bea3c230 }]),  
( scale , [{ Name: batch_norm2d_17.w_0, Initialized: 1, Ptr: 0x55b7b796ee40 }]),  
( bias , [{ Name: batch_norm2d_17.b_0, Initialized: 1, Ptr: 0x55b7bea3a4b0 }]), ]} 
I1018 08:53:57.297513 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.297515 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.297562 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c4b91ec0)  to Conv2dGradNodeFinal (addr: 0x55b7b757e040)
I1018 08:53:57.297567 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c4b91ec0)  to GradNodeAccumulation (addr: 0x55b7b796fe00)
I1018 08:53:57.297570 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c4b91ec0)  to GradNodeAccumulation (addr: 0x55b7bea3a5d0)
I1018 08:53:57.297575 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.297608 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefba20 }]),  
( mean , [{ Name: batch_norm2d_17.w_1, Initialized: 1, Ptr: 0x55b7bea3b370 }]),  
( variance , [{ Name: batch_norm2d_17.w_2, Initialized: 1, Ptr: 0x55b7bea3c230 }]),  
( scale , [{ Name: batch_norm2d_17.w_0, Initialized: 1, Ptr: 0x55b7b796ee40 }]),  
( bias , [{ Name: batch_norm2d_17.b_0, Initialized: 1, Ptr: 0x55b7bea3a4b0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b757d630 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b757d750 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b757d920 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b757df30 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b757dbd0 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b757cae0 }]), ] } 
I1018 08:53:57.297629 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.297632 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.297727 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c9f04390)  to ReluGradNode (addr: 0x55b7ceefb130)
I1018 08:53:57.297732 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c9f04390)  to GradNodeAccumulation (addr: 0x55b7b759fa30)
I1018 08:53:57.297760 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.297765 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.297782 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b929d0 }]),  
( mean , [{ Name: batch_norm2d_15.w_1, Initialized: 1, Ptr: 0x55b7c4d5f940 }]),  
( variance , [{ Name: batch_norm2d_15.w_2, Initialized: 1, Ptr: 0x55b7c4d60800 }]),  
( scale , [{ Name: batch_norm2d_15.w_0, Initialized: 1, Ptr: 0x55b7b759f910 }]),  
( bias , [{ Name: batch_norm2d_15.b_0, Initialized: 1, Ptr: 0x55b7b75a2ec0 }]), ]} 
I1018 08:53:57.297793 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.297796 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.297843 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b877e2f0)  to Conv2dGradNodeFinal (addr: 0x55b7c9f04390)
I1018 08:53:57.297847 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b877e2f0)  to GradNodeAccumulation (addr: 0x55b7b3f217c0)
I1018 08:53:57.297849 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b877e2f0)  to GradNodeAccumulation (addr: 0x55b7b75a2fe0)
I1018 08:53:57.297859 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.297891 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b929d0 }]),  
( mean , [{ Name: batch_norm2d_15.w_1, Initialized: 1, Ptr: 0x55b7c4d5f940 }]),  
( variance , [{ Name: batch_norm2d_15.w_2, Initialized: 1, Ptr: 0x55b7c4d60800 }]),  
( scale , [{ Name: batch_norm2d_15.w_0, Initialized: 1, Ptr: 0x55b7b759f910 }]),  
( bias , [{ Name: batch_norm2d_15.b_0, Initialized: 1, Ptr: 0x55b7b75a2ec0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f03150 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f03270 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f03440 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f03030 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f02d10 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b877eaa0 }]), ] } 
I1018 08:53:57.297901 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.297910 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b757d630 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f03150 }]), ]} 
I1018 08:53:57.297935 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7b7798ff0)  to BatchNormGradNode (addr: 0x55b7c4b91ec0)
I1018 08:53:57.297938 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7b7798ff0)  to BatchNormGradNode (addr: 0x55b7b877e2f0)
I1018 08:53:57.297941 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.297952 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b757d630 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f03150 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f02f10 }]), ] } 
I1018 08:53:57.297966 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.297969 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.297974 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f02f10 }]), ]} 
I1018 08:53:57.297989 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b7799f40)  to AddGradNode (addr: 0x55b7b7798ff0)
I1018 08:53:57.297993 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.298002 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f02f10 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b757d630 }]), ] } 
I1018 08:53:57.298020 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.298023 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.298147 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b877b0f0)  to ReluGradNode (addr: 0x55b7b7799f40)
I1018 08:53:57.298153 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b877b0f0)  to GradNodeAccumulation (addr: 0x55b7bea3d490)
I1018 08:53:57.298183 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.298187 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.298206 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f03150 }]),  
( mean , [{ Name: batch_norm2d_18.w_1, Initialized: 1, Ptr: 0x55b7c49fade0 }]),  
( variance , [{ Name: batch_norm2d_18.w_2, Initialized: 1, Ptr: 0x55b7c49fbca0 }]),  
( scale , [{ Name: batch_norm2d_18.w_0, Initialized: 1, Ptr: 0x55b7bea3d370 }]),  
( bias , [{ Name: batch_norm2d_18.b_0, Initialized: 1, Ptr: 0x55b7c49f9f20 }]), ]} 
I1018 08:53:57.298216 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.298225 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.298272 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b877b870)  to Conv2dGradNodeFinal (addr: 0x55b7b877b0f0)
I1018 08:53:57.298276 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b877b870)  to GradNodeAccumulation (addr: 0x55b7c49f9190)
I1018 08:53:57.298280 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b877b870)  to GradNodeAccumulation (addr: 0x55b7c49fa040)
I1018 08:53:57.298285 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.298317 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f03150 }]),  
( mean , [{ Name: batch_norm2d_18.w_1, Initialized: 1, Ptr: 0x55b7c49fade0 }]),  
( variance , [{ Name: batch_norm2d_18.w_2, Initialized: 1, Ptr: 0x55b7c49fbca0 }]),  
( scale , [{ Name: batch_norm2d_18.w_0, Initialized: 1, Ptr: 0x55b7bea3d370 }]),  
( bias , [{ Name: batch_norm2d_18.b_0, Initialized: 1, Ptr: 0x55b7c49f9f20 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f02f10 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b7799470 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779a8e0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779b090 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b7798ca0 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b78e47a0 }]), ] } 
I1018 08:53:57.298331 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.298334 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.298341 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f02f10 }]), ]} 
I1018 08:53:57.298355 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7c4b94300)  to BatchNormGradNode (addr: 0x55b7b877b870)
I1018 08:53:57.298359 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.298368 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f02f10 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877c020 }]), ] } 
I1018 08:53:57.298383 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.298386 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.298499 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4b95760)  to ReluGradNode (addr: 0x55b7c4b94300)
I1018 08:53:57.298506 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4b95760)  to GradNodeAccumulation (addr: 0x55b7c49fccb0)
I1018 08:53:57.298534 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.298537 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.298556 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f02f10 }]),  
( mean , [{ Name: batch_norm2d_19.w_1, Initialized: 1, Ptr: 0x55b7b8788190 }]),  
( variance , [{ Name: batch_norm2d_19.w_2, Initialized: 1, Ptr: 0x55b7b8789050 }]),  
( scale , [{ Name: batch_norm2d_19.w_0, Initialized: 1, Ptr: 0x55b7c49fcb90 }]),  
( bias , [{ Name: batch_norm2d_19.b_0, Initialized: 1, Ptr: 0x55b7b87872d0 }]), ]} 
I1018 08:53:57.298568 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.298570 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.298614 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c4b962f0)  to Conv2dGradNodeFinal (addr: 0x55b7c4b95760)
I1018 08:53:57.298622 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c4b962f0)  to GradNodeAccumulation (addr: 0x55b7b8786540)
I1018 08:53:57.298625 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c4b962f0)  to GradNodeAccumulation (addr: 0x55b7b87873f0)
I1018 08:53:57.298630 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.298663 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f02f10 }]),  
( mean , [{ Name: batch_norm2d_19.w_1, Initialized: 1, Ptr: 0x55b7b8788190 }]),  
( variance , [{ Name: batch_norm2d_19.w_2, Initialized: 1, Ptr: 0x55b7b8789050 }]),  
( scale , [{ Name: batch_norm2d_19.w_0, Initialized: 1, Ptr: 0x55b7c49fcb90 }]),  
( bias , [{ Name: batch_norm2d_19.b_0, Initialized: 1, Ptr: 0x55b7b87872d0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b94ce0 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b94eb0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b95080 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b95650 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877de20 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c4b96aa0 }]), ] } 
I1018 08:53:57.298672 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.298681 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b94ce0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7b757d630 }]), ]} 
I1018 08:53:57.298705 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7cb088a90)  to BatchNormGradNode (addr: 0x55b7c4b962f0)
I1018 08:53:57.298709 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7cb088a90)  to ReluGradNode (addr: 0x55b7b7799f40)
I1018 08:53:57.298712 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.298723 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b94ce0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7b757d630 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb088f10 }]), ] } 
I1018 08:53:57.298736 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.298739 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.298745 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb088f10 }]), ]} 
I1018 08:53:57.298759 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7cb089b00)  to AddGradNode (addr: 0x55b7cb088a90)
I1018 08:53:57.298763 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.298772 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb088f10 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b94ce0 }]), ] } 
I1018 08:53:57.298820 4097533 eager_method.cc:2158] Tensor:  set use_gpudnn = 0
I1018 08:53:57.298828 4097533 eager_op_function.cc:35671] CurrentDeviceId: 0 from 0
I1018 08:53:57.298831 4097533 dygraph_functions.cc:69244] Running AD API: pool2d
I1018 08:53:57.298838 4097533 dygraph_functions.cc:69300] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb088f10 }]), ]} 
I1018 08:53:57.298853 4097533 pool2d_kernel.cc:153] [Pool2d] pooling type: avg exclusive: 1 adaptive: 1
I1018 08:53:57.298892 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Pool2dGradNode (addr: 0x55b7ceefc5a0)  to ReluGradNode (addr: 0x55b7cb089b00)
I1018 08:53:57.298897 4097533 dygraph_functions.cc:69376] Finish AD API: pool2d
I1018 08:53:57.298906 4097533 dygraph_functions.cc:69390] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb088f10 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb089030 }]), ] } 
I1018 08:53:57.298921 4097533 eager_op_function.cc:9777] CurrentDeviceId: 0 from 0
I1018 08:53:57.298931 4097533 dygraph_functions.cc:20255] Running AD API: flatten
I1018 08:53:57.298938 4097533 dygraph_functions.cc:20311] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb089030 }]), ]} 
I1018 08:53:57.298944 4097533 dygraph_api.cc:208] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.298952 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from FlattenGradNode (addr: 0x55b7ceefd4c0)  to Pool2dGradNode (addr: 0x55b7ceefc5a0)
I1018 08:53:57.298956 4097533 dygraph_functions.cc:20387] Finish AD API: flatten
I1018 08:53:57.298966 4097533 dygraph_functions.cc:20404] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb089030 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefcc00 }]),  
( xshape , [{ Name: None, Initialized: 0, Ptr: 0x55b7ceefcdd0 }]), ] } 
I1018 08:53:57.298982 4097533 dygraph_functions.cc:67019] Running AD API: matmul
I1018 08:53:57.298991 4097533 dygraph_functions.cc:67081] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefcc00 }]),  
( y , [{ Name: linear_0.w_0, Initialized: 1, Ptr: 0x55b7b6f66780 }]), ]} 
I1018 08:53:57.299015 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from MatmulGradNode (addr: 0x55b7ceefe180)  to FlattenGradNode (addr: 0x55b7ceefd4c0)
I1018 08:53:57.299019 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from MatmulGradNode (addr: 0x55b7ceefe180)  to GradNodeAccumulation (addr: 0x55b7b878a500)
I1018 08:53:57.299022 4097533 dygraph_functions.cc:67152] Finish AD API: matmul
I1018 08:53:57.299034 4097533 dygraph_functions.cc:67169] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefcc00 }]),  
( y , [{ Name: linear_0.w_0, Initialized: 1, Ptr: 0x55b7b6f66780 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefe600 }]), ] } 
I1018 08:53:57.299037 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.299046 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefe600 }]),  
( y , [{ Name: linear_0.b_0, Initialized: 1, Ptr: 0x55b7b878a3e0 }]), ]} 
I1018 08:53:57.299067 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7ceeff1c0)  to MatmulGradNode (addr: 0x55b7ceefe180)
I1018 08:53:57.299072 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7ceeff1c0)  to GradNodeAccumulation (addr: 0x55b7b6f66d60)
I1018 08:53:57.299074 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.299086 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefe600 }]),  
( y , [{ Name: linear_0.b_0, Initialized: 1, Ptr: 0x55b7b878a3e0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceeff880 }]), ] } 
I1018 08:53:57.299280 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.299288 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.299436 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c2d5e1f0)  to GradNodeAccumulation (addr: 0x55b77381dd90)
I1018 08:53:57.299474 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.299477 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.299499 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefcae0 }]),  
( mean , [{ Name: batch_norm2d_0.w_1, Initialized: 1, Ptr: 0x55b7c4ee9a30 }]),  
( variance , [{ Name: batch_norm2d_0.w_2, Initialized: 1, Ptr: 0x55b7ceb07ff0 }]),  
( scale , [{ Name: batch_norm2d_0.w_0, Initialized: 1, Ptr: 0x55b77346a070 }]),  
( bias , [{ Name: batch_norm2d_0.b_0, Initialized: 1, Ptr: 0x55b7b6f6d4b0 }]), ]} 
I1018 08:53:57.299510 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.299518 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.299568 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7cfe31d60)  to Conv2dGradNodeFinal (addr: 0x55b7c2d5e1f0)
I1018 08:53:57.299572 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7cfe31d60)  to GradNodeAccumulation (addr: 0x55b77ed32260)
I1018 08:53:57.299575 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7cfe31d60)  to GradNodeAccumulation (addr: 0x55b77f0e4620)
I1018 08:53:57.299580 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.299614 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefcae0 }]),  
( mean , [{ Name: batch_norm2d_0.w_1, Initialized: 1, Ptr: 0x55b7c4ee9a30 }]),  
( variance , [{ Name: batch_norm2d_0.w_2, Initialized: 1, Ptr: 0x55b7ceb07ff0 }]),  
( scale , [{ Name: batch_norm2d_0.w_0, Initialized: 1, Ptr: 0x55b77346a070 }]),  
( bias , [{ Name: batch_norm2d_0.b_0, Initialized: 1, Ptr: 0x55b7b6f6d4b0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefcc00 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceeff640 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceeff760 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefe600 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefe720 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c4b94eb0 }]), ] } 
I1018 08:53:57.299628 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.299631 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.299638 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefcc00 }]), ]} 
I1018 08:53:57.299654 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b4d8d1e0)  to BatchNormGradNode (addr: 0x55b7cfe31d60)
I1018 08:53:57.299659 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.299666 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefcc00 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b77370b470 }]), ] } 
I1018 08:53:57.299688 4097533 eager_op_function.cc:35671] CurrentDeviceId: 0 from 0
I1018 08:53:57.299691 4097533 dygraph_functions.cc:69244] Running AD API: pool2d
I1018 08:53:57.299697 4097533 dygraph_functions.cc:69300] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b77370b470 }]), ]} 
I1018 08:53:57.299713 4097533 pool2d_kernel.cc:153] [Pool2d] pooling type: max exclusive: 1 adaptive: 0
I1018 08:53:57.299736 4097533 pool2d_kernel.cc:198] [Pool2d] extra_input_size: 0
I1018 08:53:57.299758 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Pool2dGradNode (addr: 0x55b7c1f6edf0)  to ReluGradNode (addr: 0x55b7b4d8d1e0)
I1018 08:53:57.299763 4097533 dygraph_functions.cc:69376] Finish AD API: pool2d
I1018 08:53:57.299772 4097533 dygraph_functions.cc:69390] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b77370b470 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefcc00 }]), ] } 
I1018 08:53:57.299793 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.299796 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.299901 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b70ef960)  to Pool2dGradNode (addr: 0x55b7c1f6edf0)
I1018 08:53:57.299906 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b70ef960)  to GradNodeAccumulation (addr: 0x55b773982760)
I1018 08:53:57.299937 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.299940 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.299965 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7ce10 }]),  
( mean , [{ Name: batch_norm2d_1.w_1, Initialized: 1, Ptr: 0x55b7b64bb520 }]),  
( variance , [{ Name: batch_norm2d_1.w_2, Initialized: 1, Ptr: 0x55b7b68ca2d0 }]),  
( scale , [{ Name: batch_norm2d_1.w_0, Initialized: 1, Ptr: 0x55b7b68c8170 }]),  
( bias , [{ Name: batch_norm2d_1.b_0, Initialized: 1, Ptr: 0x55b7b76da540 }]), ]} 
I1018 08:53:57.299976 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.299979 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.300029 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c4168f10)  to Conv2dGradNodeFinal (addr: 0x55b7b70ef960)
I1018 08:53:57.300032 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c4168f10)  to GradNodeAccumulation (addr: 0x55b773982b90)
I1018 08:53:57.300035 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c4168f10)  to GradNodeAccumulation (addr: 0x55b773f00630)
I1018 08:53:57.300041 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.300076 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7ce10 }]),  
( mean , [{ Name: batch_norm2d_1.w_1, Initialized: 1, Ptr: 0x55b7b64bb520 }]),  
( variance , [{ Name: batch_norm2d_1.w_2, Initialized: 1, Ptr: 0x55b7b68ca2d0 }]),  
( scale , [{ Name: batch_norm2d_1.w_0, Initialized: 1, Ptr: 0x55b7b68c8170 }]),  
( bias , [{ Name: batch_norm2d_1.b_0, Initialized: 1, Ptr: 0x55b7b76da540 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4169780 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b77398c530 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cb7440 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe32db0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e78780 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b70ee1c0 }]), ] } 
I1018 08:53:57.300089 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.300092 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.300098 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4169780 }]), ]} 
I1018 08:53:57.300113 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b773371db0)  to BatchNormGradNode (addr: 0x55b7c4168f10)
I1018 08:53:57.300117 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.300127 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4169780 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe30fa0 }]), ] } 
I1018 08:53:57.300141 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.300145 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.300246 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4e78c00)  to ReluGradNode (addr: 0x55b773371db0)
I1018 08:53:57.300251 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4e78c00)  to GradNodeAccumulation (addr: 0x55b7bd3da270)
I1018 08:53:57.300280 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.300284 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.300303 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4169780 }]),  
( mean , [{ Name: batch_norm2d_2.w_1, Initialized: 1, Ptr: 0x55b7b76e6e10 }]),  
( variance , [{ Name: batch_norm2d_2.w_2, Initialized: 1, Ptr: 0x55b7c2207b30 }]),  
( scale , [{ Name: batch_norm2d_2.w_0, Initialized: 1, Ptr: 0x55b7bd3da150 }]),  
( bias , [{ Name: batch_norm2d_2.b_0, Initialized: 1, Ptr: 0x55b7bd3d86c0 }]), ]} 
I1018 08:53:57.300319 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.300321 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.300370 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b77cd7b0)  to Conv2dGradNodeFinal (addr: 0x55b7c4e78c00)
I1018 08:53:57.300374 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b77cd7b0)  to GradNodeAccumulation (addr: 0x55b7c4ee5b40)
I1018 08:53:57.300377 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b77cd7b0)  to GradNodeAccumulation (addr: 0x55b7bd3d87e0)
I1018 08:53:57.300383 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.300416 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4169780 }]),  
( mean , [{ Name: batch_norm2d_2.w_1, Initialized: 1, Ptr: 0x55b7b76e6e10 }]),  
( variance , [{ Name: batch_norm2d_2.w_2, Initialized: 1, Ptr: 0x55b7c2207b30 }]),  
( scale , [{ Name: batch_norm2d_2.w_0, Initialized: 1, Ptr: 0x55b7bd3da150 }]),  
( bias , [{ Name: batch_norm2d_2.b_0, Initialized: 1, Ptr: 0x55b7bd3d86c0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1d840 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e39a0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b209b9e0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b78239edb0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b78239eed0 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c2aa67d0 }]), ] } 
I1018 08:53:57.300426 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.300436 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1d840 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefcc00 }]), ]} 
I1018 08:53:57.300459 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7b78e41f0)  to BatchNormGradNode (addr: 0x55b7b77cd7b0)
I1018 08:53:57.300463 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7b78e41f0)  to Pool2dGradNode (addr: 0x55b7c1f6edf0)
I1018 08:53:57.300467 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.300478 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1d840 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefcc00 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f71160 }]), ] } 
I1018 08:53:57.300491 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.300494 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.300499 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f71160 }]), ]} 
I1018 08:53:57.300514 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7733494c0)  to AddGradNode (addr: 0x55b7b78e41f0)
I1018 08:53:57.300518 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.300527 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f71160 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1d840 }]), ] } 
I1018 08:53:57.300544 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.300547 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.300650 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4e7c8c0)  to ReluGradNode (addr: 0x55b7733494c0)
I1018 08:53:57.300655 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4e7c8c0)  to GradNodeAccumulation (addr: 0x55b7b76e5550)
I1018 08:53:57.300689 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.300693 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.300712 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f71160 }]),  
( mean , [{ Name: batch_norm2d_3.w_1, Initialized: 1, Ptr: 0x55b7b778e800 }]),  
( variance , [{ Name: batch_norm2d_3.w_2, Initialized: 1, Ptr: 0x55b7b3f224a0 }]),  
( scale , [{ Name: batch_norm2d_3.w_0, Initialized: 1, Ptr: 0x55b7bd3daab0 }]),  
( bias , [{ Name: batch_norm2d_3.b_0, Initialized: 1, Ptr: 0x55b7b87810f0 }]), ]} 
I1018 08:53:57.300724 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.300726 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.300769 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b6f7df70)  to Conv2dGradNodeFinal (addr: 0x55b7c4e7c8c0)
I1018 08:53:57.300773 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b6f7df70)  to GradNodeAccumulation (addr: 0x55b7b76e6360)
I1018 08:53:57.300776 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b6f7df70)  to GradNodeAccumulation (addr: 0x55b7b68c8390)
I1018 08:53:57.300782 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.300814 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f71160 }]),  
( mean , [{ Name: batch_norm2d_3.w_1, Initialized: 1, Ptr: 0x55b7b778e800 }]),  
( variance , [{ Name: batch_norm2d_3.w_2, Initialized: 1, Ptr: 0x55b7b3f224a0 }]),  
( scale , [{ Name: batch_norm2d_3.w_0, Initialized: 1, Ptr: 0x55b7bd3daab0 }]),  
( bias , [{ Name: batch_norm2d_3.b_0, Initialized: 1, Ptr: 0x55b7b87810f0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1d4a0 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08e350 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4edab50 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b7796670 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b7796790 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b773679a80 }]), ] } 
I1018 08:53:57.300828 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.300832 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.300837 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1d4a0 }]), ]} 
I1018 08:53:57.300851 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b77334c090)  to BatchNormGradNode (addr: 0x55b7b6f7df70)
I1018 08:53:57.300856 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.300864 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1d4a0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e5c70 }]), ] } 
I1018 08:53:57.300879 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.300882 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.300981 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4b970e0)  to ReluGradNode (addr: 0x55b77334c090)
I1018 08:53:57.300987 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4b970e0)  to GradNodeAccumulation (addr: 0x55b7b68c6d90)
I1018 08:53:57.301014 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.301018 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.301038 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1d4a0 }]),  
( mean , [{ Name: batch_norm2d_4.w_1, Initialized: 1, Ptr: 0x55b7c2209c90 }]),  
( variance , [{ Name: batch_norm2d_4.w_2, Initialized: 1, Ptr: 0x55b7c220ab50 }]),  
( scale , [{ Name: batch_norm2d_4.w_0, Initialized: 1, Ptr: 0x55b7b68c6c70 }]),  
( bias , [{ Name: batch_norm2d_4.b_0, Initialized: 1, Ptr: 0x55b7b4b4a7c0 }]), ]} 
I1018 08:53:57.301054 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.301056 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.301103 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c416bf80)  to Conv2dGradNodeFinal (addr: 0x55b7c4b970e0)
I1018 08:53:57.301107 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c416bf80)  to GradNodeAccumulation (addr: 0x55b7b4b49ae0)
I1018 08:53:57.301110 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c416bf80)  to GradNodeAccumulation (addr: 0x55b7b4b4a8e0)
I1018 08:53:57.301116 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.301149 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1d4a0 }]),  
( mean , [{ Name: batch_norm2d_4.w_1, Initialized: 1, Ptr: 0x55b7c2209c90 }]),  
( variance , [{ Name: batch_norm2d_4.w_2, Initialized: 1, Ptr: 0x55b7c220ab50 }]),  
( scale , [{ Name: batch_norm2d_4.w_0, Initialized: 1, Ptr: 0x55b7b68c6c70 }]),  
( bias , [{ Name: batch_norm2d_4.b_0, Initialized: 1, Ptr: 0x55b7b4b4a7c0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779e690 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e9f90 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ea160 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b950e0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f70c90 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b701a910 }]), ] } 
I1018 08:53:57.301159 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.301168 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779e690 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1d840 }]), ]} 
I1018 08:53:57.301191 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7c416d780)  to BatchNormGradNode (addr: 0x55b7c416bf80)
I1018 08:53:57.301194 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7c416d780)  to ReluGradNode (addr: 0x55b7733494c0)
I1018 08:53:57.301198 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.301215 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779e690 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1d840 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4eda710 }]), ] } 
I1018 08:53:57.301229 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.301234 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.301239 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4eda710 }]), ]} 
I1018 08:53:57.301254 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b77324be20)  to AddGradNode (addr: 0x55b7c416d780)
I1018 08:53:57.301257 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.301266 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4eda710 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779e690 }]), ] } 
I1018 08:53:57.301287 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.301291 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.301400 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b6f7f6c0)  to ReluGradNode (addr: 0x55b77324be20)
I1018 08:53:57.301405 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b6f7f6c0)  to GradNodeAccumulation (addr: 0x55b7c4ee2d80)
I1018 08:53:57.301435 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.301440 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.301458 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4eda710 }]),  
( mean , [{ Name: batch_norm2d_6.w_1, Initialized: 1, Ptr: 0x55b7cc394970 }]),  
( variance , [{ Name: batch_norm2d_6.w_2, Initialized: 1, Ptr: 0x55b7cc395a60 }]),  
( scale , [{ Name: batch_norm2d_6.w_0, Initialized: 1, Ptr: 0x55b7c4ee3ce0 }]),  
( bias , [{ Name: batch_norm2d_6.b_0, Initialized: 1, Ptr: 0x55b7c4ee4b90 }]), ]} 
I1018 08:53:57.301469 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.301472 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.301514 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c1f71c40)  to Conv2dGradNodeFinal (addr: 0x55b7b6f7f6c0)
I1018 08:53:57.301517 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c1f71c40)  to GradNodeAccumulation (addr: 0x55b7c4ee3e00)
I1018 08:53:57.301520 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c1f71c40)  to GradNodeAccumulation (addr: 0x55b7c4ee4cb0)
I1018 08:53:57.301527 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.301559 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4eda710 }]),  
( mean , [{ Name: batch_norm2d_6.w_1, Initialized: 1, Ptr: 0x55b7cc394970 }]),  
( variance , [{ Name: batch_norm2d_6.w_2, Initialized: 1, Ptr: 0x55b7cc395a60 }]),  
( scale , [{ Name: batch_norm2d_6.w_0, Initialized: 1, Ptr: 0x55b7c4ee3ce0 }]),  
( bias , [{ Name: batch_norm2d_6.b_0, Initialized: 1, Ptr: 0x55b7c4ee4b90 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77d1330 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779b400 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd9a20 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ed4e0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ed600 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b78ed720 }]), ] } 
I1018 08:53:57.301573 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.301575 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.301581 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77d1330 }]), ]} 
I1018 08:53:57.301596 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b76fbc80)  to BatchNormGradNode (addr: 0x55b7c1f71c40)
I1018 08:53:57.301600 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.301609 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77d1330 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f6a6f0 }]), ] } 
I1018 08:53:57.301623 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.301626 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.301725 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7ceefe0e0)  to ReluGradNode (addr: 0x55b7b76fbc80)
I1018 08:53:57.301730 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7ceefe0e0)  to GradNodeAccumulation (addr: 0x55b7cc396a70)
I1018 08:53:57.301764 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.301767 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.301786 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77d1330 }]),  
( mean , [{ Name: batch_norm2d_7.w_1, Initialized: 1, Ptr: 0x55b7c4d65af0 }]),  
( variance , [{ Name: batch_norm2d_7.w_2, Initialized: 1, Ptr: 0x55b7b778c480 }]),  
( scale , [{ Name: batch_norm2d_7.w_0, Initialized: 1, Ptr: 0x55b7cc396950 }]),  
( bias , [{ Name: batch_norm2d_7.b_0, Initialized: 1, Ptr: 0x55b7c4d64c30 }]), ]} 
I1018 08:53:57.301797 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.301800 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.301846 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b6f7cf80)  to Conv2dGradNodeFinal (addr: 0x55b7ceefe0e0)
I1018 08:53:57.301849 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b6f7cf80)  to GradNodeAccumulation (addr: 0x55b7c4d63ea0)
I1018 08:53:57.301852 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b6f7cf80)  to GradNodeAccumulation (addr: 0x55b7c4d64d50)
I1018 08:53:57.301858 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.301891 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77d1330 }]),  
( mean , [{ Name: batch_norm2d_7.w_1, Initialized: 1, Ptr: 0x55b7c4d65af0 }]),  
( variance , [{ Name: batch_norm2d_7.w_2, Initialized: 1, Ptr: 0x55b7b778c480 }]),  
( scale , [{ Name: batch_norm2d_7.w_0, Initialized: 1, Ptr: 0x55b7cc396950 }]),  
( bias , [{ Name: batch_norm2d_7.b_0, Initialized: 1, Ptr: 0x55b7c4d64c30 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cb7ad0 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b70ef0a0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b70ef270 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e78550 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefa290 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7ceefef40 }]), ] } 
I1018 08:53:57.301910 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.301914 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.302011 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b77a08c0)  to ReluGradNode (addr: 0x55b77324be20)
I1018 08:53:57.302016 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b77a08c0)  to GradNodeAccumulation (addr: 0x55b7b85db7c0)
I1018 08:53:57.302042 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.302045 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.302064 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceeff520 }]),  
( mean , [{ Name: batch_norm2d_5.w_1, Initialized: 1, Ptr: 0x55b7bb903150 }]),  
( variance , [{ Name: batch_norm2d_5.w_2, Initialized: 1, Ptr: 0x55b7bb904240 }]),  
( scale , [{ Name: batch_norm2d_5.w_0, Initialized: 1, Ptr: 0x55b7b3f22b10 }]),  
( bias , [{ Name: batch_norm2d_5.b_0, Initialized: 1, Ptr: 0x55b7b85dd420 }]), ]} 
I1018 08:53:57.302075 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.302078 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.302121 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7cfe2fdf0)  to Conv2dGradNodeFinal (addr: 0x55b7b77a08c0)
I1018 08:53:57.302125 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7cfe2fdf0)  to GradNodeAccumulation (addr: 0x55b7b85dc690)
I1018 08:53:57.302132 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7cfe2fdf0)  to GradNodeAccumulation (addr: 0x55b7b85dd540)
I1018 08:53:57.302138 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.302170 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceeff520 }]),  
( mean , [{ Name: batch_norm2d_5.w_1, Initialized: 1, Ptr: 0x55b7bb903150 }]),  
( variance , [{ Name: batch_norm2d_5.w_2, Initialized: 1, Ptr: 0x55b7bb904240 }]),  
( scale , [{ Name: batch_norm2d_5.w_0, Initialized: 1, Ptr: 0x55b7b3f22b10 }]),  
( bias , [{ Name: batch_norm2d_5.b_0, Initialized: 1, Ptr: 0x55b7b85dd420 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c23e43a0 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c23e4570 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c2d5bcc0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c23e4280 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe305a0 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c4b8c610 }]), ] } 
I1018 08:53:57.302179 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.302188 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cb7ad0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c23e43a0 }]), ]} 
I1018 08:53:57.302211 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7c4e1f070)  to BatchNormGradNode (addr: 0x55b7b6f7cf80)
I1018 08:53:57.302215 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7c4e1f070)  to BatchNormGradNode (addr: 0x55b7cfe2fdf0)
I1018 08:53:57.302218 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.302229 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cb7ad0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c23e43a0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cba170 }]), ] } 
I1018 08:53:57.302243 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.302246 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.302253 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cba170 }]), ]} 
I1018 08:53:57.302268 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b1a58b50)  to AddGradNode (addr: 0x55b7c4e1f070)
I1018 08:53:57.302271 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.302279 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cba170 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cb7ad0 }]), ] } 
I1018 08:53:57.302297 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.302301 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.302409 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c22d0760)  to ReluGradNode (addr: 0x55b7b1a58b50)
I1018 08:53:57.302415 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c22d0760)  to GradNodeAccumulation (addr: 0x55b7b778d7d0)
I1018 08:53:57.302444 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.302448 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.302467 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c23e43a0 }]),  
( mean , [{ Name: batch_norm2d_8.w_1, Initialized: 1, Ptr: 0x55b7c5cf24d0 }]),  
( variance , [{ Name: batch_norm2d_8.w_2, Initialized: 1, Ptr: 0x55b7c5cf3390 }]),  
( scale , [{ Name: batch_norm2d_8.w_0, Initialized: 1, Ptr: 0x55b7bb905130 }]),  
( bias , [{ Name: batch_norm2d_8.b_0, Initialized: 1, Ptr: 0x55b7c5cf1610 }]), ]} 
I1018 08:53:57.302482 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.302485 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.302532 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c4b8d290)  to Conv2dGradNodeFinal (addr: 0x55b7c22d0760)
I1018 08:53:57.302536 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c4b8d290)  to GradNodeAccumulation (addr: 0x55b7c5cf0810)
I1018 08:53:57.302539 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c4b8d290)  to GradNodeAccumulation (addr: 0x55b7c5cf1730)
I1018 08:53:57.302546 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.302578 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c23e43a0 }]),  
( mean , [{ Name: batch_norm2d_8.w_1, Initialized: 1, Ptr: 0x55b7c5cf24d0 }]),  
( variance , [{ Name: batch_norm2d_8.w_2, Initialized: 1, Ptr: 0x55b7c5cf3390 }]),  
( scale , [{ Name: batch_norm2d_8.w_0, Initialized: 1, Ptr: 0x55b7bb905130 }]),  
( bias , [{ Name: batch_norm2d_8.b_0, Initialized: 1, Ptr: 0x55b7c5cf1610 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cba170 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cba290 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c22ce580 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1ecb0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c2d5ed00 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c4e1e560 }]), ] } 
I1018 08:53:57.302592 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.302595 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.302600 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cba170 }]), ]} 
I1018 08:53:57.302615 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b78e6190)  to BatchNormGradNode (addr: 0x55b7c4b8d290)
I1018 08:53:57.302619 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.302628 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cba170 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd6b60 }]), ] } 
I1018 08:53:57.302644 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.302646 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.302745 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b7797460)  to ReluGradNode (addr: 0x55b7b78e6190)
I1018 08:53:57.302750 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b7797460)  to GradNodeAccumulation (addr: 0x55b7cc6c36b0)
I1018 08:53:57.302779 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.302783 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.302801 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cba170 }]),  
( mean , [{ Name: batch_norm2d_9.w_1, Initialized: 1, Ptr: 0x55b7b7056fb0 }]),  
( variance , [{ Name: batch_norm2d_9.w_2, Initialized: 1, Ptr: 0x55b7b7057e70 }]),  
( scale , [{ Name: batch_norm2d_9.w_0, Initialized: 1, Ptr: 0x55b7cc6c3590 }]),  
( bias , [{ Name: batch_norm2d_9.b_0, Initialized: 1, Ptr: 0x55b7b70560f0 }]), ]} 
I1018 08:53:57.302812 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.302815 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.302865 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c2d5d510)  to Conv2dGradNodeFinal (addr: 0x55b7b7797460)
I1018 08:53:57.302870 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c2d5d510)  to GradNodeAccumulation (addr: 0x55b7cc6c4610)
I1018 08:53:57.302872 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c2d5d510)  to GradNodeAccumulation (addr: 0x55b7b7056210)
I1018 08:53:57.302878 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.302911 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cba170 }]),  
( mean , [{ Name: batch_norm2d_9.w_1, Initialized: 1, Ptr: 0x55b7b7056fb0 }]),  
( variance , [{ Name: batch_norm2d_9.w_2, Initialized: 1, Ptr: 0x55b7b7057e70 }]),  
( scale , [{ Name: batch_norm2d_9.w_0, Initialized: 1, Ptr: 0x55b7cc6c3590 }]),  
( bias , [{ Name: batch_norm2d_9.b_0, Initialized: 1, Ptr: 0x55b7b70560f0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701c5e0 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701c7b0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701c980 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c2d5dcc0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e7b280 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c4e7b3a0 }]), ] } 
I1018 08:53:57.302920 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.302929 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701c5e0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cb7ad0 }]), ]} 
I1018 08:53:57.302953 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7b779ddb0)  to BatchNormGradNode (addr: 0x55b7c2d5d510)
I1018 08:53:57.302958 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7b779ddb0)  to ReluGradNode (addr: 0x55b7b1a58b50)
I1018 08:53:57.302960 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.302971 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701c5e0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cb7ad0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779e230 }]), ] } 
I1018 08:53:57.302984 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.302987 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.302994 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779e230 }]), ]} 
I1018 08:53:57.303009 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7c4e7a1f0)  to AddGradNode (addr: 0x55b7b779ddb0)
I1018 08:53:57.303012 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.303020 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779e230 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701c5e0 }]), ] } 
I1018 08:53:57.303041 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.303045 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.303148 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b701b900)  to ReluGradNode (addr: 0x55b7c4e7a1f0)
I1018 08:53:57.303153 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b701b900)  to GradNodeAccumulation (addr: 0x55b7c4edec60)
I1018 08:53:57.303182 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.303186 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.303205 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779e230 }]),  
( mean , [{ Name: batch_norm2d_11.w_1, Initialized: 1, Ptr: 0x55b7b7059750 }]),  
( variance , [{ Name: batch_norm2d_11.w_2, Initialized: 1, Ptr: 0x55b7c5ced740 }]),  
( scale , [{ Name: batch_norm2d_11.w_0, Initialized: 1, Ptr: 0x55b7b4b498d0 }]),  
( bias , [{ Name: batch_norm2d_11.b_0, Initialized: 1, Ptr: 0x55b7c4ee0950 }]), ]} 
I1018 08:53:57.303221 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.303223 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.303269 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7bacd8cf0)  to Conv2dGradNodeFinal (addr: 0x55b7b701b900)
I1018 08:53:57.303273 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7bacd8cf0)  to GradNodeAccumulation (addr: 0x55b7c4edfbc0)
I1018 08:53:57.303275 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7bacd8cf0)  to GradNodeAccumulation (addr: 0x55b7c4ee0a70)
I1018 08:53:57.303282 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.303314 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779e230 }]),  
( mean , [{ Name: batch_norm2d_11.w_1, Initialized: 1, Ptr: 0x55b7b7059750 }]),  
( variance , [{ Name: batch_norm2d_11.w_2, Initialized: 1, Ptr: 0x55b7c5ced740 }]),  
( scale , [{ Name: batch_norm2d_11.w_0, Initialized: 1, Ptr: 0x55b7b4b498d0 }]),  
( bias , [{ Name: batch_norm2d_11.b_0, Initialized: 1, Ptr: 0x55b7c4ee0950 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701e520 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701e6f0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701e8c0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd94a0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779d280 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b779d3a0 }]), ] } 
I1018 08:53:57.303328 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.303331 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.303337 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701e520 }]), ]} 
I1018 08:53:57.303352 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7bacd81f0)  to BatchNormGradNode (addr: 0x55b7bacd8cf0)
I1018 08:53:57.303356 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.303364 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701e520 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c2d5a270 }]), ] } 
I1018 08:53:57.303380 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.303382 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.303484 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7bacd5dd0)  to ReluGradNode (addr: 0x55b7bacd81f0)
I1018 08:53:57.303489 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7bacd5dd0)  to GradNodeAccumulation (addr: 0x55b7c5cee750)
I1018 08:53:57.303517 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.303521 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.303540 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701e520 }]),  
( mean , [{ Name: batch_norm2d_12.w_1, Initialized: 1, Ptr: 0x55b7cc398370 }]),  
( variance , [{ Name: batch_norm2d_12.w_2, Initialized: 1, Ptr: 0x55b7cc399230 }]),  
( scale , [{ Name: batch_norm2d_12.w_0, Initialized: 1, Ptr: 0x55b7c5cee630 }]),  
( bias , [{ Name: batch_norm2d_12.b_0, Initialized: 1, Ptr: 0x55b7cc3974b0 }]), ]} 
I1018 08:53:57.303555 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.303558 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.303602 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c416a1c0)  to Conv2dGradNodeFinal (addr: 0x55b7bacd5dd0)
I1018 08:53:57.303606 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c416a1c0)  to GradNodeAccumulation (addr: 0x55b7c5cef6b0)
I1018 08:53:57.303609 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c416a1c0)  to GradNodeAccumulation (addr: 0x55b7cc3975d0)
I1018 08:53:57.303614 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.303648 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701e520 }]),  
( mean , [{ Name: batch_norm2d_12.w_1, Initialized: 1, Ptr: 0x55b7cc398370 }]),  
( variance , [{ Name: batch_norm2d_12.w_2, Initialized: 1, Ptr: 0x55b7cc399230 }]),  
( scale , [{ Name: batch_norm2d_12.w_0, Initialized: 1, Ptr: 0x55b7c5cee630 }]),  
( bias , [{ Name: batch_norm2d_12.b_0, Initialized: 1, Ptr: 0x55b7cc3974b0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e85d0 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e87a0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e8970 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701f190 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701f2b0 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b701f3d0 }]), ] } 
I1018 08:53:57.303668 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.303671 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.303766 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b77cefd0)  to ReluGradNode (addr: 0x55b7c4e7a1f0)
I1018 08:53:57.303771 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b77cefd0)  to GradNodeAccumulation (addr: 0x55b7b7058e80)
I1018 08:53:57.303797 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.303800 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.303819 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701f900 }]),  
( mean , [{ Name: batch_norm2d_10.w_1, Initialized: 1, Ptr: 0x55b7b780fba0 }]),  
( variance , [{ Name: batch_norm2d_10.w_2, Initialized: 1, Ptr: 0x55b7b7810a60 }]),  
( scale , [{ Name: batch_norm2d_10.w_0, Initialized: 1, Ptr: 0x55b7b7058d60 }]),  
( bias , [{ Name: batch_norm2d_10.b_0, Initialized: 1, Ptr: 0x55b7b780ece0 }]), ]} 
I1018 08:53:57.303830 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.303833 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.303877 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c4ed8f40)  to Conv2dGradNodeFinal (addr: 0x55b7b77cefd0)
I1018 08:53:57.303881 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c4ed8f40)  to GradNodeAccumulation (addr: 0x55b7b780df50)
I1018 08:53:57.303884 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c4ed8f40)  to GradNodeAccumulation (addr: 0x55b7b780ee00)
I1018 08:53:57.303890 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.303923 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701f900 }]),  
( mean , [{ Name: batch_norm2d_10.w_1, Initialized: 1, Ptr: 0x55b7b780fba0 }]),  
( variance , [{ Name: batch_norm2d_10.w_2, Initialized: 1, Ptr: 0x55b7b7810a60 }]),  
( scale , [{ Name: batch_norm2d_10.w_0, Initialized: 1, Ptr: 0x55b7b7058d60 }]),  
( bias , [{ Name: batch_norm2d_10.b_0, Initialized: 1, Ptr: 0x55b7b780ece0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe2f250 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe2f420 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe2f5f0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe31800 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe31b20 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c4ed96f0 }]), ] } 
I1018 08:53:57.303936 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.303946 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e85d0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe2f250 }]), ]} 
I1018 08:53:57.303967 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7b6f7c510)  to BatchNormGradNode (addr: 0x55b7c416a1c0)
I1018 08:53:57.303972 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7b6f7c510)  to BatchNormGradNode (addr: 0x55b7c4ed8f40)
I1018 08:53:57.303974 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.303985 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e85d0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe2f250 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7c990 }]), ] } 
I1018 08:53:57.303999 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.304003 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.304008 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7c990 }]), ]} 
I1018 08:53:57.304023 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7ceef9130)  to AddGradNode (addr: 0x55b7b6f7c510)
I1018 08:53:57.304026 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.304034 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7c990 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e85d0 }]), ] } 
I1018 08:53:57.304051 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.304055 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.304163 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b6f80060)  to ReluGradNode (addr: 0x55b7ceef9130)
I1018 08:53:57.304169 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b6f80060)  to GradNodeAccumulation (addr: 0x55b7cc39a490)
I1018 08:53:57.304198 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.304201 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.304221 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe2f250 }]),  
( mean , [{ Name: batch_norm2d_13.w_1, Initialized: 1, Ptr: 0x55b7cc4bf110 }]),  
( variance , [{ Name: batch_norm2d_13.w_2, Initialized: 1, Ptr: 0x55b7cc4bffd0 }]),  
( scale , [{ Name: batch_norm2d_13.w_0, Initialized: 1, Ptr: 0x55b7cc39a370 }]),  
( bias , [{ Name: batch_norm2d_13.b_0, Initialized: 1, Ptr: 0x55b7cc4be250 }]), ]} 
I1018 08:53:57.304232 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.304235 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.304278 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b877dfd0)  to Conv2dGradNodeFinal (addr: 0x55b7b6f80060)
I1018 08:53:57.304287 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b877dfd0)  to GradNodeAccumulation (addr: 0x55b7cc4bd4b0)
I1018 08:53:57.304289 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b877dfd0)  to GradNodeAccumulation (addr: 0x55b7cc4be370)
I1018 08:53:57.304296 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.304328 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe2f250 }]),  
( mean , [{ Name: batch_norm2d_13.w_1, Initialized: 1, Ptr: 0x55b7cc4bf110 }]),  
( variance , [{ Name: batch_norm2d_13.w_2, Initialized: 1, Ptr: 0x55b7cc4bffd0 }]),  
( scale , [{ Name: batch_norm2d_13.w_0, Initialized: 1, Ptr: 0x55b7cc39a370 }]),  
( bias , [{ Name: batch_norm2d_13.b_0, Initialized: 1, Ptr: 0x55b7cc4be250 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7c990 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7cab0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceef9ad0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7c1c0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c2d5f150 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c4edcb80 }]), ] } 
I1018 08:53:57.304342 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.304345 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.304350 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7c990 }]), ]} 
I1018 08:53:57.304366 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7c23e68d0)  to BatchNormGradNode (addr: 0x55b7b877dfd0)
I1018 08:53:57.304370 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.304378 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7c990 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877e8a0 }]), ] } 
I1018 08:53:57.304394 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.304397 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.304493 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4e1bb60)  to ReluGradNode (addr: 0x55b7c23e68d0)
I1018 08:53:57.304498 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4e1bb60)  to GradNodeAccumulation (addr: 0x55b7b68cc840)
I1018 08:53:57.304526 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.304529 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.304548 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7c990 }]),  
( mean , [{ Name: batch_norm2d_14.w_1, Initialized: 1, Ptr: 0x55b7b68cf3f0 }]),  
( variance , [{ Name: batch_norm2d_14.w_2, Initialized: 1, Ptr: 0x55b7b68d02b0 }]),  
( scale , [{ Name: batch_norm2d_14.w_0, Initialized: 1, Ptr: 0x55b7cc4c0e90 }]),  
( bias , [{ Name: batch_norm2d_14.b_0, Initialized: 1, Ptr: 0x55b7b68ce530 }]), ]} 
I1018 08:53:57.304559 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.304561 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.304607 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b877b0f0)  to Conv2dGradNodeFinal (addr: 0x55b7c4e1bb60)
I1018 08:53:57.304611 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b877b0f0)  to GradNodeAccumulation (addr: 0x55b7b68cd7a0)
I1018 08:53:57.304615 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b877b0f0)  to GradNodeAccumulation (addr: 0x55b7b68ce650)
I1018 08:53:57.304623 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.304656 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7c990 }]),  
( mean , [{ Name: batch_norm2d_14.w_1, Initialized: 1, Ptr: 0x55b7b68cf3f0 }]),  
( variance , [{ Name: batch_norm2d_14.w_2, Initialized: 1, Ptr: 0x55b7b68d02b0 }]),  
( scale , [{ Name: batch_norm2d_14.w_0, Initialized: 1, Ptr: 0x55b7cc4c0e90 }]),  
( bias , [{ Name: batch_norm2d_14.b_0, Initialized: 1, Ptr: 0x55b7b68ce530 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ee060 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ee180 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ee350 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78eea00 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c23e6760 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c4e1c570 }]), ] } 
I1018 08:53:57.304665 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.304673 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ee060 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e85d0 }]), ]} 
I1018 08:53:57.304697 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7b78eb910)  to BatchNormGradNode (addr: 0x55b7b877b0f0)
I1018 08:53:57.304701 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7b78eb910)  to ReluGradNode (addr: 0x55b7ceef9130)
I1018 08:53:57.304704 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.304715 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ee060 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e85d0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ebd90 }]), ] } 
I1018 08:53:57.304728 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.304731 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.304737 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ebd90 }]), ]} 
I1018 08:53:57.304751 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b6f682e0)  to AddGradNode (addr: 0x55b7b78eb910)
I1018 08:53:57.304755 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.304764 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ebd90 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ee060 }]), ] } 
I1018 08:53:57.304785 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.304788 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.304893 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4cbb480)  to ReluGradNode (addr: 0x55b7b6f682e0)
I1018 08:53:57.304898 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4cbb480)  to GradNodeAccumulation (addr: 0x55b7c4d61a60)
I1018 08:53:57.304927 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.304930 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.304950 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ebd90 }]),  
( mean , [{ Name: batch_norm2d_16.w_1, Initialized: 1, Ptr: 0x55b7b796d090 }]),  
( variance , [{ Name: batch_norm2d_16.w_2, Initialized: 1, Ptr: 0x55b7b796df50 }]),  
( scale , [{ Name: batch_norm2d_16.w_0, Initialized: 1, Ptr: 0x55b7c4d61940 }]),  
( bias , [{ Name: batch_norm2d_16.b_0, Initialized: 1, Ptr: 0x55b7b796c1d0 }]), ]} 
I1018 08:53:57.304960 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.304967 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.305011 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c4b95640)  to Conv2dGradNodeFinal (addr: 0x55b7c4cbb480)
I1018 08:53:57.305016 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c4b95640)  to GradNodeAccumulation (addr: 0x55b7c4d629c0)
I1018 08:53:57.305018 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c4b95640)  to GradNodeAccumulation (addr: 0x55b7b796c2f0)
I1018 08:53:57.305023 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.305056 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ebd90 }]),  
( mean , [{ Name: batch_norm2d_16.w_1, Initialized: 1, Ptr: 0x55b7b796d090 }]),  
( variance , [{ Name: batch_norm2d_16.w_2, Initialized: 1, Ptr: 0x55b7b796df50 }]),  
( scale , [{ Name: batch_norm2d_16.w_0, Initialized: 1, Ptr: 0x55b7c4d61940 }]),  
( bias , [{ Name: batch_norm2d_16.b_0, Initialized: 1, Ptr: 0x55b7b796c1d0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cbac40 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cbae10 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78eb5c0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cbbe90 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cbbfb0 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c4cbc0d0 }]), ] } 
I1018 08:53:57.305070 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.305073 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.305078 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cbac40 }]), ]} 
I1018 08:53:57.305094 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b70ed890)  to BatchNormGradNode (addr: 0x55b7c4b95640)
I1018 08:53:57.305097 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.305106 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cbac40 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b96190 }]), ] } 
I1018 08:53:57.305121 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.305124 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.305241 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b877d1e0)  to ReluGradNode (addr: 0x55b7b70ed890)
I1018 08:53:57.305248 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b877d1e0)  to GradNodeAccumulation (addr: 0x55b7b796ef60)
I1018 08:53:57.305277 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.305281 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.305300 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cbac40 }]),  
( mean , [{ Name: batch_norm2d_17.w_1, Initialized: 1, Ptr: 0x55b7bea3b370 }]),  
( variance , [{ Name: batch_norm2d_17.w_2, Initialized: 1, Ptr: 0x55b7bea3c230 }]),  
( scale , [{ Name: batch_norm2d_17.w_0, Initialized: 1, Ptr: 0x55b7b796ee40 }]),  
( bias , [{ Name: batch_norm2d_17.b_0, Initialized: 1, Ptr: 0x55b7bea3a4b0 }]), ]} 
I1018 08:53:57.305311 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.305315 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.305357 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b877efc0)  to Conv2dGradNodeFinal (addr: 0x55b7b877d1e0)
I1018 08:53:57.305361 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b877efc0)  to GradNodeAccumulation (addr: 0x55b7b796fe00)
I1018 08:53:57.305369 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b877efc0)  to GradNodeAccumulation (addr: 0x55b7bea3a5d0)
I1018 08:53:57.305374 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.305408 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cbac40 }]),  
( mean , [{ Name: batch_norm2d_17.w_1, Initialized: 1, Ptr: 0x55b7bea3b370 }]),  
( variance , [{ Name: batch_norm2d_17.w_2, Initialized: 1, Ptr: 0x55b7bea3c230 }]),  
( scale , [{ Name: batch_norm2d_17.w_0, Initialized: 1, Ptr: 0x55b7b796ee40 }]),  
( bias , [{ Name: batch_norm2d_17.b_0, Initialized: 1, Ptr: 0x55b7bea3a4b0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877c7d0 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877c8f0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877cac0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877d0d0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b96f00 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b877cd70 }]), ] } 
I1018 08:53:57.305428 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.305431 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.305524 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7ceefa8c0)  to ReluGradNode (addr: 0x55b7b6f682e0)
I1018 08:53:57.305529 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7ceefa8c0)  to GradNodeAccumulation (addr: 0x55b7b759fa30)
I1018 08:53:57.305557 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.305560 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.305579 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877f770 }]),  
( mean , [{ Name: batch_norm2d_15.w_1, Initialized: 1, Ptr: 0x55b7c4d5f940 }]),  
( variance , [{ Name: batch_norm2d_15.w_2, Initialized: 1, Ptr: 0x55b7c4d60800 }]),  
( scale , [{ Name: batch_norm2d_15.w_0, Initialized: 1, Ptr: 0x55b7b759f910 }]),  
( bias , [{ Name: batch_norm2d_15.b_0, Initialized: 1, Ptr: 0x55b7b75a2ec0 }]), ]} 
I1018 08:53:57.305590 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.305593 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.305637 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7ceefb0a0)  to Conv2dGradNodeFinal (addr: 0x55b7ceefa8c0)
I1018 08:53:57.305641 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7ceefb0a0)  to GradNodeAccumulation (addr: 0x55b7b3f217c0)
I1018 08:53:57.305644 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7ceefb0a0)  to GradNodeAccumulation (addr: 0x55b7b75a2fe0)
I1018 08:53:57.305649 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.305681 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877f770 }]),  
( mean , [{ Name: batch_norm2d_15.w_1, Initialized: 1, Ptr: 0x55b7c4d5f940 }]),  
( variance , [{ Name: batch_norm2d_15.w_2, Initialized: 1, Ptr: 0x55b7c4d60800 }]),  
( scale , [{ Name: batch_norm2d_15.w_0, Initialized: 1, Ptr: 0x55b7b759f910 }]),  
( bias , [{ Name: batch_norm2d_15.b_0, Initialized: 1, Ptr: 0x55b7b75a2ec0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08b7b0 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08b980 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08bb50 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08b1a0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefb850 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7ceefb970 }]), ] } 
I1018 08:53:57.305696 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.305704 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877c7d0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08b7b0 }]), ]} 
I1018 08:53:57.305727 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7b757dd10)  to BatchNormGradNode (addr: 0x55b7b877efc0)
I1018 08:53:57.305732 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7b757dd10)  to BatchNormGradNode (addr: 0x55b7ceefb0a0)
I1018 08:53:57.305734 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.305745 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877c7d0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08b7b0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08b3a0 }]), ] } 
I1018 08:53:57.305759 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.305763 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.305768 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08b3a0 }]), ]} 
I1018 08:53:57.305783 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7c4b92100)  to AddGradNode (addr: 0x55b7b757dd10)
I1018 08:53:57.305786 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.305794 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08b3a0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877c7d0 }]), ] } 
I1018 08:53:57.305812 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.305816 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.305934 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4b93360)  to ReluGradNode (addr: 0x55b7c4b92100)
I1018 08:53:57.305940 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4b93360)  to GradNodeAccumulation (addr: 0x55b7bea3d490)
I1018 08:53:57.305969 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.305974 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.305994 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08b7b0 }]),  
( mean , [{ Name: batch_norm2d_18.w_1, Initialized: 1, Ptr: 0x55b7c49fade0 }]),  
( variance , [{ Name: batch_norm2d_18.w_2, Initialized: 1, Ptr: 0x55b7c49fbca0 }]),  
( scale , [{ Name: batch_norm2d_18.w_0, Initialized: 1, Ptr: 0x55b7bea3d370 }]),  
( bias , [{ Name: batch_norm2d_18.b_0, Initialized: 1, Ptr: 0x55b7c49f9f20 }]), ]} 
I1018 08:53:57.306003 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.306006 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.306051 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b7798470)  to Conv2dGradNodeFinal (addr: 0x55b7c4b93360)
I1018 08:53:57.306056 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b7798470)  to GradNodeAccumulation (addr: 0x55b7c49f9190)
I1018 08:53:57.306057 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b7798470)  to GradNodeAccumulation (addr: 0x55b7c49fa040)
I1018 08:53:57.306062 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.306094 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08b7b0 }]),  
( mean , [{ Name: batch_norm2d_18.w_1, Initialized: 1, Ptr: 0x55b7c49fade0 }]),  
( variance , [{ Name: batch_norm2d_18.w_2, Initialized: 1, Ptr: 0x55b7c49fbca0 }]),  
( scale , [{ Name: batch_norm2d_18.w_0, Initialized: 1, Ptr: 0x55b7bea3d370 }]),  
( bias , [{ Name: batch_norm2d_18.b_0, Initialized: 1, Ptr: 0x55b7c49f9f20 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08b3a0 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b757e190 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b92aa0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b93250 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b757e530 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7cb08c4b0 }]), ] } 
I1018 08:53:57.306113 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.306116 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.306121 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08b3a0 }]), ]} 
I1018 08:53:57.306138 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7c9f04200)  to BatchNormGradNode (addr: 0x55b7b7798470)
I1018 08:53:57.306141 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.306150 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08b3a0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b7798e60 }]), ] } 
I1018 08:53:57.306164 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.306169 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.306274 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c9f05660)  to ReluGradNode (addr: 0x55b7c9f04200)
I1018 08:53:57.306279 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c9f05660)  to GradNodeAccumulation (addr: 0x55b7c49fccb0)
I1018 08:53:57.306308 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.306311 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.306330 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08b3a0 }]),  
( mean , [{ Name: batch_norm2d_19.w_1, Initialized: 1, Ptr: 0x55b7b8788190 }]),  
( variance , [{ Name: batch_norm2d_19.w_2, Initialized: 1, Ptr: 0x55b7b8789050 }]),  
( scale , [{ Name: batch_norm2d_19.w_0, Initialized: 1, Ptr: 0x55b7c49fcb90 }]),  
( bias , [{ Name: batch_norm2d_19.b_0, Initialized: 1, Ptr: 0x55b7b87872d0 }]), ]} 
I1018 08:53:57.306340 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.306344 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.306387 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c9f061f0)  to Conv2dGradNodeFinal (addr: 0x55b7c9f05660)
I1018 08:53:57.306391 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c9f061f0)  to GradNodeAccumulation (addr: 0x55b7b8786540)
I1018 08:53:57.306394 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c9f061f0)  to GradNodeAccumulation (addr: 0x55b7b87873f0)
I1018 08:53:57.306399 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.306432 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08b3a0 }]),  
( mean , [{ Name: batch_norm2d_19.w_1, Initialized: 1, Ptr: 0x55b7b8788190 }]),  
( variance , [{ Name: batch_norm2d_19.w_2, Initialized: 1, Ptr: 0x55b7b8789050 }]),  
( scale , [{ Name: batch_norm2d_19.w_0, Initialized: 1, Ptr: 0x55b7c49fcb90 }]),  
( bias , [{ Name: batch_norm2d_19.b_0, Initialized: 1, Ptr: 0x55b7b87872d0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f04be0 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f04db0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f04f80 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f05550 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779ad20 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c9f069a0 }]), ] } 
I1018 08:53:57.306444 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.306453 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f04be0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877c7d0 }]), ]} 
I1018 08:53:57.306476 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7c9f01b90)  to BatchNormGradNode (addr: 0x55b7c9f061f0)
I1018 08:53:57.306479 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7c9f01b90)  to ReluGradNode (addr: 0x55b7c4b92100)
I1018 08:53:57.306483 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.306494 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f04be0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877c7d0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f02010 }]), ] } 
I1018 08:53:57.306507 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.306510 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.306515 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f02010 }]), ]} 
I1018 08:53:57.306529 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7c9f02c00)  to AddGradNode (addr: 0x55b7c9f01b90)
I1018 08:53:57.306533 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.306542 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f02010 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f04be0 }]), ] } 
I1018 08:53:57.306586 4097533 eager_method.cc:2158] Tensor:  set use_gpudnn = 0
I1018 08:53:57.306596 4097533 eager_op_function.cc:35671] CurrentDeviceId: 0 from 0
I1018 08:53:57.306599 4097533 dygraph_functions.cc:69244] Running AD API: pool2d
I1018 08:53:57.306605 4097533 dygraph_functions.cc:69300] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f02010 }]), ]} 
I1018 08:53:57.306622 4097533 pool2d_kernel.cc:153] [Pool2d] pooling type: avg exclusive: 1 adaptive: 1
I1018 08:53:57.306659 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Pool2dGradNode (addr: 0x55b7c9f034e0)  to ReluGradNode (addr: 0x55b7c9f02c00)
I1018 08:53:57.306664 4097533 dygraph_functions.cc:69376] Finish AD API: pool2d
I1018 08:53:57.306674 4097533 dygraph_functions.cc:69390] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f02010 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f02130 }]), ] } 
I1018 08:53:57.306687 4097533 eager_op_function.cc:9777] CurrentDeviceId: 0 from 0
I1018 08:53:57.306691 4097533 dygraph_functions.cc:20255] Running AD API: flatten
I1018 08:53:57.306697 4097533 dygraph_functions.cc:20311] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f02130 }]), ]} 
I1018 08:53:57.306703 4097533 dygraph_api.cc:208] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.306710 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from FlattenGradNode (addr: 0x55b7cb087920)  to Pool2dGradNode (addr: 0x55b7c9f034e0)
I1018 08:53:57.306715 4097533 dygraph_functions.cc:20387] Finish AD API: flatten
I1018 08:53:57.306725 4097533 dygraph_functions.cc:20404] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f02130 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb087060 }]),  
( xshape , [{ Name: None, Initialized: 0, Ptr: 0x55b7cb087230 }]), ] } 
I1018 08:53:57.306746 4097533 dygraph_functions.cc:67019] Running AD API: matmul
I1018 08:53:57.306754 4097533 dygraph_functions.cc:67081] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb087060 }]),  
( y , [{ Name: linear_0.w_0, Initialized: 1, Ptr: 0x55b7b6f66780 }]), ]} 
I1018 08:53:57.306778 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from MatmulGradNode (addr: 0x55b7cb0886a0)  to FlattenGradNode (addr: 0x55b7cb087920)
I1018 08:53:57.306782 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from MatmulGradNode (addr: 0x55b7cb0886a0)  to GradNodeAccumulation (addr: 0x55b7b878a500)
I1018 08:53:57.306785 4097533 dygraph_functions.cc:67152] Finish AD API: matmul
I1018 08:53:57.306797 4097533 dygraph_functions.cc:67169] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb087060 }]),  
( y , [{ Name: linear_0.w_0, Initialized: 1, Ptr: 0x55b7b6f66780 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb088b20 }]), ] } 
I1018 08:53:57.306800 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.306808 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb088b20 }]),  
( y , [{ Name: linear_0.b_0, Initialized: 1, Ptr: 0x55b7b878a3e0 }]), ]} 
I1018 08:53:57.306829 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7cb0896e0)  to MatmulGradNode (addr: 0x55b7cb0886a0)
I1018 08:53:57.306833 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7cb0896e0)  to GradNodeAccumulation (addr: 0x55b7b6f66d60)
I1018 08:53:57.306836 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.306847 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb088b20 }]),  
( y , [{ Name: linear_0.b_0, Initialized: 1, Ptr: 0x55b7b878a3e0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb089da0 }]), ] } 
I1018 08:53:57.307024 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.307029 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.307173 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c2d5e1f0)  to GradNodeAccumulation (addr: 0x55b77381dd90)
I1018 08:53:57.307209 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.307212 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.307233 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb086f40 }]),  
( mean , [{ Name: batch_norm2d_0.w_1, Initialized: 1, Ptr: 0x55b7c4ee9a30 }]),  
( variance , [{ Name: batch_norm2d_0.w_2, Initialized: 1, Ptr: 0x55b7ceb07ff0 }]),  
( scale , [{ Name: batch_norm2d_0.w_0, Initialized: 1, Ptr: 0x55b77346a070 }]),  
( bias , [{ Name: batch_norm2d_0.b_0, Initialized: 1, Ptr: 0x55b7b6f6d4b0 }]), ]} 
I1018 08:53:57.307245 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.307248 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.307296 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7cfe31d60)  to Conv2dGradNodeFinal (addr: 0x55b7c2d5e1f0)
I1018 08:53:57.307299 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7cfe31d60)  to GradNodeAccumulation (addr: 0x55b77ed32260)
I1018 08:53:57.307302 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7cfe31d60)  to GradNodeAccumulation (addr: 0x55b77f0e4620)
I1018 08:53:57.307308 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.307341 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb086f40 }]),  
( mean , [{ Name: batch_norm2d_0.w_1, Initialized: 1, Ptr: 0x55b7c4ee9a30 }]),  
( variance , [{ Name: batch_norm2d_0.w_2, Initialized: 1, Ptr: 0x55b7ceb07ff0 }]),  
( scale , [{ Name: batch_norm2d_0.w_0, Initialized: 1, Ptr: 0x55b77346a070 }]),  
( bias , [{ Name: batch_norm2d_0.b_0, Initialized: 1, Ptr: 0x55b7b6f6d4b0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb087060 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb089b60 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb089c80 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb088b20 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb088c40 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c9f04db0 }]), ] } 
I1018 08:53:57.307361 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.307364 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.307370 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb087060 }]), ]} 
I1018 08:53:57.307386 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b1a58b50)  to BatchNormGradNode (addr: 0x55b7cfe31d60)
I1018 08:53:57.307390 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.307399 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb087060 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7ce10 }]), ] } 
I1018 08:53:57.307420 4097533 eager_op_function.cc:35671] CurrentDeviceId: 0 from 0
I1018 08:53:57.307423 4097533 dygraph_functions.cc:69244] Running AD API: pool2d
I1018 08:53:57.307428 4097533 dygraph_functions.cc:69300] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7ce10 }]), ]} 
I1018 08:53:57.307446 4097533 pool2d_kernel.cc:153] [Pool2d] pooling type: max exclusive: 1 adaptive: 0
I1018 08:53:57.307471 4097533 pool2d_kernel.cc:198] [Pool2d] extra_input_size: 0
I1018 08:53:57.307493 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Pool2dGradNode (addr: 0x55b7b70ef960)  to ReluGradNode (addr: 0x55b7b1a58b50)
I1018 08:53:57.307497 4097533 dygraph_functions.cc:69376] Finish AD API: pool2d
I1018 08:53:57.307507 4097533 dygraph_functions.cc:69390] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7ce10 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb087060 }]), ] } 
I1018 08:53:57.307528 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.307531 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.307632 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4e7c8c0)  to Pool2dGradNode (addr: 0x55b7b70ef960)
I1018 08:53:57.307637 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4e7c8c0)  to GradNodeAccumulation (addr: 0x55b773982760)
I1018 08:53:57.307667 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.307672 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.307691 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4169780 }]),  
( mean , [{ Name: batch_norm2d_1.w_1, Initialized: 1, Ptr: 0x55b7b64bb520 }]),  
( variance , [{ Name: batch_norm2d_1.w_2, Initialized: 1, Ptr: 0x55b7b68ca2d0 }]),  
( scale , [{ Name: batch_norm2d_1.w_0, Initialized: 1, Ptr: 0x55b7b68c8170 }]),  
( bias , [{ Name: batch_norm2d_1.b_0, Initialized: 1, Ptr: 0x55b7b76da540 }]), ]} 
I1018 08:53:57.307703 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.307704 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.307750 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b77cd7b0)  to Conv2dGradNodeFinal (addr: 0x55b7c4e7c8c0)
I1018 08:53:57.307758 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b77cd7b0)  to GradNodeAccumulation (addr: 0x55b773982b90)
I1018 08:53:57.307761 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b77cd7b0)  to GradNodeAccumulation (addr: 0x55b773f00630)
I1018 08:53:57.307768 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.307801 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4169780 }]),  
( mean , [{ Name: batch_norm2d_1.w_1, Initialized: 1, Ptr: 0x55b7b64bb520 }]),  
( variance , [{ Name: batch_norm2d_1.w_2, Initialized: 1, Ptr: 0x55b7b68ca2d0 }]),  
( scale , [{ Name: batch_norm2d_1.w_0, Initialized: 1, Ptr: 0x55b7b68c8170 }]),  
( bias , [{ Name: batch_norm2d_1.b_0, Initialized: 1, Ptr: 0x55b7b76da540 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f70300 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779ae80 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe32db0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b77398c530 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b70ee1c0 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c1f70170 }]), ] } 
I1018 08:53:57.307816 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.307818 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.307824 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f70300 }]), ]} 
I1018 08:53:57.307842 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b76fbc80)  to BatchNormGradNode (addr: 0x55b7b77cd7b0)
I1018 08:53:57.307845 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.307853 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f70300 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe30fa0 }]), ] } 
I1018 08:53:57.307868 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.307873 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.307971 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4e78c00)  to ReluGradNode (addr: 0x55b7b76fbc80)
I1018 08:53:57.307977 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4e78c00)  to GradNodeAccumulation (addr: 0x55b7bd3da270)
I1018 08:53:57.308007 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.308009 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.308028 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f70300 }]),  
( mean , [{ Name: batch_norm2d_2.w_1, Initialized: 1, Ptr: 0x55b7b76e6e10 }]),  
( variance , [{ Name: batch_norm2d_2.w_2, Initialized: 1, Ptr: 0x55b7c2207b30 }]),  
( scale , [{ Name: batch_norm2d_2.w_0, Initialized: 1, Ptr: 0x55b7bd3da150 }]),  
( bias , [{ Name: batch_norm2d_2.b_0, Initialized: 1, Ptr: 0x55b7bd3d86c0 }]), ]} 
I1018 08:53:57.308039 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.308041 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.308087 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c4168f10)  to Conv2dGradNodeFinal (addr: 0x55b7c4e78c00)
I1018 08:53:57.308091 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c4168f10)  to GradNodeAccumulation (addr: 0x55b7c4ee5b40)
I1018 08:53:57.308094 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c4168f10)  to GradNodeAccumulation (addr: 0x55b7bd3d87e0)
I1018 08:53:57.308104 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.308137 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f70300 }]),  
( mean , [{ Name: batch_norm2d_2.w_1, Initialized: 1, Ptr: 0x55b7b76e6e10 }]),  
( variance , [{ Name: batch_norm2d_2.w_2, Initialized: 1, Ptr: 0x55b7c2207b30 }]),  
( scale , [{ Name: batch_norm2d_2.w_0, Initialized: 1, Ptr: 0x55b7bd3da150 }]),  
( bias , [{ Name: batch_norm2d_2.b_0, Initialized: 1, Ptr: 0x55b7bd3d86c0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b85dedb0 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c22d1310 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c22d1750 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1dab0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1dbd0 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b6f6a0b0 }]), ] } 
I1018 08:53:57.308146 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.308156 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b85dedb0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb087060 }]), ]} 
I1018 08:53:57.308179 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7c4b8df50)  to BatchNormGradNode (addr: 0x55b7c4168f10)
I1018 08:53:57.308182 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7c4b8df50)  to Pool2dGradNode (addr: 0x55b7b70ef960)
I1018 08:53:57.308187 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.308197 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b85dedb0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb087060 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f6ebe0 }]), ] } 
I1018 08:53:57.308210 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.308213 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.308219 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f6ebe0 }]), ]} 
I1018 08:53:57.308233 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b77324be20)  to AddGradNode (addr: 0x55b7c4b8df50)
I1018 08:53:57.308238 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.308246 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f6ebe0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b85dedb0 }]), ] } 
I1018 08:53:57.308262 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.308265 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.308367 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c1f6edf0)  to ReluGradNode (addr: 0x55b77324be20)
I1018 08:53:57.308373 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c1f6edf0)  to GradNodeAccumulation (addr: 0x55b7b76e5550)
I1018 08:53:57.308403 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.308405 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.308425 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f6ebe0 }]),  
( mean , [{ Name: batch_norm2d_3.w_1, Initialized: 1, Ptr: 0x55b7b778e800 }]),  
( variance , [{ Name: batch_norm2d_3.w_2, Initialized: 1, Ptr: 0x55b7b3f224a0 }]),  
( scale , [{ Name: batch_norm2d_3.w_0, Initialized: 1, Ptr: 0x55b7bd3daab0 }]),  
( bias , [{ Name: batch_norm2d_3.b_0, Initialized: 1, Ptr: 0x55b7b87810f0 }]), ]} 
I1018 08:53:57.308436 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.308444 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.308490 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b6f7df70)  to Conv2dGradNodeFinal (addr: 0x55b7c1f6edf0)
I1018 08:53:57.308493 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b6f7df70)  to GradNodeAccumulation (addr: 0x55b7b76e6360)
I1018 08:53:57.308496 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b6f7df70)  to GradNodeAccumulation (addr: 0x55b7b68c8390)
I1018 08:53:57.308502 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.308535 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f6ebe0 }]),  
( mean , [{ Name: batch_norm2d_3.w_1, Initialized: 1, Ptr: 0x55b7b778e800 }]),  
( variance , [{ Name: batch_norm2d_3.w_2, Initialized: 1, Ptr: 0x55b7b3f224a0 }]),  
( scale , [{ Name: batch_norm2d_3.w_0, Initialized: 1, Ptr: 0x55b7bd3daab0 }]),  
( bias , [{ Name: batch_norm2d_3.b_0, Initialized: 1, Ptr: 0x55b7b87810f0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd9970 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b7795f10 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f06ff0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08e460 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c2d5f210 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c2d5cbe0 }]), ] } 
I1018 08:53:57.308549 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.308552 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.308558 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd9970 }]), ]} 
I1018 08:53:57.308573 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b77334c090)  to BatchNormGradNode (addr: 0x55b7b6f7df70)
I1018 08:53:57.308578 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.308586 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd9970 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c2d5bf40 }]), ] } 
I1018 08:53:57.308601 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.308604 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.308703 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b779afd0)  to ReluGradNode (addr: 0x55b77334c090)
I1018 08:53:57.308708 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b779afd0)  to GradNodeAccumulation (addr: 0x55b7b68c6d90)
I1018 08:53:57.308737 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.308740 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.308760 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd9970 }]),  
( mean , [{ Name: batch_norm2d_4.w_1, Initialized: 1, Ptr: 0x55b7c2209c90 }]),  
( variance , [{ Name: batch_norm2d_4.w_2, Initialized: 1, Ptr: 0x55b7c220ab50 }]),  
( scale , [{ Name: batch_norm2d_4.w_0, Initialized: 1, Ptr: 0x55b7b68c6c70 }]),  
( bias , [{ Name: batch_norm2d_4.b_0, Initialized: 1, Ptr: 0x55b7b4b4a7c0 }]), ]} 
I1018 08:53:57.308771 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.308773 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.308818 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c416bf80)  to Conv2dGradNodeFinal (addr: 0x55b7b779afd0)
I1018 08:53:57.308822 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c416bf80)  to GradNodeAccumulation (addr: 0x55b7b4b49ae0)
I1018 08:53:57.308830 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c416bf80)  to GradNodeAccumulation (addr: 0x55b7b4b4a8e0)
I1018 08:53:57.308835 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.308869 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd9970 }]),  
( mean , [{ Name: batch_norm2d_4.w_1, Initialized: 1, Ptr: 0x55b7c2209c90 }]),  
( variance , [{ Name: batch_norm2d_4.w_2, Initialized: 1, Ptr: 0x55b7c220ab50 }]),  
( scale , [{ Name: batch_norm2d_4.w_0, Initialized: 1, Ptr: 0x55b7b68c6c70 }]),  
( bias , [{ Name: batch_norm2d_4.b_0, Initialized: 1, Ptr: 0x55b7b4b4a7c0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701ab90 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ea500 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ea6d0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e6dc0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e6ee0 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b78e7000 }]), ] } 
I1018 08:53:57.308878 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.308887 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701ab90 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7b85dedb0 }]), ]} 
I1018 08:53:57.308909 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7c22d23b0)  to BatchNormGradNode (addr: 0x55b7c416bf80)
I1018 08:53:57.308913 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7c22d23b0)  to ReluGradNode (addr: 0x55b77324be20)
I1018 08:53:57.308916 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.308928 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701ab90 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7b85dedb0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c23e7400 }]), ] } 
I1018 08:53:57.308941 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.308944 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.308949 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c23e7400 }]), ]} 
I1018 08:53:57.308964 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7733494c0)  to AddGradNode (addr: 0x55b7c22d23b0)
I1018 08:53:57.308969 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.308976 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c23e7400 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701ab90 }]), ] } 
I1018 08:53:57.308997 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.309001 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.309103 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4eda600)  to ReluGradNode (addr: 0x55b7733494c0)
I1018 08:53:57.309108 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4eda600)  to GradNodeAccumulation (addr: 0x55b7c4ee2d80)
I1018 08:53:57.309137 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.309140 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.309159 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c23e7400 }]),  
( mean , [{ Name: batch_norm2d_6.w_1, Initialized: 1, Ptr: 0x55b7cc394970 }]),  
( variance , [{ Name: batch_norm2d_6.w_2, Initialized: 1, Ptr: 0x55b7cc395a60 }]),  
( scale , [{ Name: batch_norm2d_6.w_0, Initialized: 1, Ptr: 0x55b7c4ee3ce0 }]),  
( bias , [{ Name: batch_norm2d_6.b_0, Initialized: 1, Ptr: 0x55b7c4ee4b90 }]), ]} 
I1018 08:53:57.309175 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.309177 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.309266 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c1f71c40)  to Conv2dGradNodeFinal (addr: 0x55b7c4eda600)
I1018 08:53:57.309271 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c1f71c40)  to GradNodeAccumulation (addr: 0x55b7c4ee3e00)
I1018 08:53:57.309274 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c1f71c40)  to GradNodeAccumulation (addr: 0x55b7c4ee4cb0)
I1018 08:53:57.309280 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.309314 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c23e7400 }]),  
( mean , [{ Name: batch_norm2d_6.w_1, Initialized: 1, Ptr: 0x55b7cc394970 }]),  
( variance , [{ Name: batch_norm2d_6.w_2, Initialized: 1, Ptr: 0x55b7cc395a60 }]),  
( scale , [{ Name: batch_norm2d_6.w_0, Initialized: 1, Ptr: 0x55b7c4ee3ce0 }]),  
( bias , [{ Name: batch_norm2d_6.b_0, Initialized: 1, Ptr: 0x55b7c4ee4b90 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701e050 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701d900 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77980b0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77d1240 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77d1360 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7bacd76a0 }]), ] } 
I1018 08:53:57.309329 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.309334 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.309338 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701e050 }]), ]} 
I1018 08:53:57.309355 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b773371db0)  to BatchNormGradNode (addr: 0x55b7c1f71c40)
I1018 08:53:57.309358 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.309367 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701e050 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd7b20 }]), ] } 
I1018 08:53:57.309382 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.309386 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.309485 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7ceef9d80)  to ReluGradNode (addr: 0x55b773371db0)
I1018 08:53:57.309490 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7ceef9d80)  to GradNodeAccumulation (addr: 0x55b7cc396a70)
I1018 08:53:57.309520 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.309523 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.309542 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701e050 }]),  
( mean , [{ Name: batch_norm2d_7.w_1, Initialized: 1, Ptr: 0x55b7c4d65af0 }]),  
( variance , [{ Name: batch_norm2d_7.w_2, Initialized: 1, Ptr: 0x55b7b778c480 }]),  
( scale , [{ Name: batch_norm2d_7.w_0, Initialized: 1, Ptr: 0x55b7cc396950 }]),  
( bias , [{ Name: batch_norm2d_7.b_0, Initialized: 1, Ptr: 0x55b7c4d64c30 }]), ]} 
I1018 08:53:57.309553 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.309556 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.309603 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7bacd5dd0)  to Conv2dGradNodeFinal (addr: 0x55b7ceef9d80)
I1018 08:53:57.309607 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7bacd5dd0)  to GradNodeAccumulation (addr: 0x55b7c4d63ea0)
I1018 08:53:57.309610 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7bacd5dd0)  to GradNodeAccumulation (addr: 0x55b7c4d64d50)
I1018 08:53:57.309615 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.309648 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701e050 }]),  
( mean , [{ Name: batch_norm2d_7.w_1, Initialized: 1, Ptr: 0x55b7c4d65af0 }]),  
( variance , [{ Name: batch_norm2d_7.w_2, Initialized: 1, Ptr: 0x55b7b778c480 }]),  
( scale , [{ Name: batch_norm2d_7.w_0, Initialized: 1, Ptr: 0x55b7cc396950 }]),  
( bias , [{ Name: batch_norm2d_7.b_0, Initialized: 1, Ptr: 0x55b7c4d64c30 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4ed8770 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4ed8940 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77d2510 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefa290 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f6a390 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c1f71580 }]), ] } 
I1018 08:53:57.309669 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.309671 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.309769 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c23e3fc0)  to ReluGradNode (addr: 0x55b7733494c0)
I1018 08:53:57.309774 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c23e3fc0)  to GradNodeAccumulation (addr: 0x55b7b85db7c0)
I1018 08:53:57.309801 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.309805 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.309823 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f71a00 }]),  
( mean , [{ Name: batch_norm2d_5.w_1, Initialized: 1, Ptr: 0x55b7bb903150 }]),  
( variance , [{ Name: batch_norm2d_5.w_2, Initialized: 1, Ptr: 0x55b7bb904240 }]),  
( scale , [{ Name: batch_norm2d_5.w_0, Initialized: 1, Ptr: 0x55b7b3f22b10 }]),  
( bias , [{ Name: batch_norm2d_5.b_0, Initialized: 1, Ptr: 0x55b7b85dd420 }]), ]} 
I1018 08:53:57.309834 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.309837 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.309880 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b6f7cf80)  to Conv2dGradNodeFinal (addr: 0x55b7c23e3fc0)
I1018 08:53:57.309885 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b6f7cf80)  to GradNodeAccumulation (addr: 0x55b7b85dc690)
I1018 08:53:57.309887 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b6f7cf80)  to GradNodeAccumulation (addr: 0x55b7b85dd540)
I1018 08:53:57.309893 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.309926 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f71a00 }]),  
( mean , [{ Name: batch_norm2d_5.w_1, Initialized: 1, Ptr: 0x55b7bb903150 }]),  
( variance , [{ Name: batch_norm2d_5.w_2, Initialized: 1, Ptr: 0x55b7bb904240 }]),  
( scale , [{ Name: batch_norm2d_5.w_0, Initialized: 1, Ptr: 0x55b7b3f22b10 }]),  
( bias , [{ Name: batch_norm2d_5.b_0, Initialized: 1, Ptr: 0x55b7b85dd420 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1d920 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb087850 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb087a20 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e79840 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe32ba0 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b78e8560 }]), ] } 
I1018 08:53:57.309939 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.309948 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4ed8770 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1d920 }]), ]} 
I1018 08:53:57.309971 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7c4cb9100)  to BatchNormGradNode (addr: 0x55b7bacd5dd0)
I1018 08:53:57.309974 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7c4cb9100)  to BatchNormGradNode (addr: 0x55b7b6f7cf80)
I1018 08:53:57.309978 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.309989 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4ed8770 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1d920 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e79a40 }]), ] } 
I1018 08:53:57.310002 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.310005 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.310011 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e79a40 }]), ]} 
I1018 08:53:57.310026 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b4d8d1e0)  to AddGradNode (addr: 0x55b7c4cb9100)
I1018 08:53:57.310030 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.310039 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e79a40 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4ed8770 }]), ] } 
I1018 08:53:57.310056 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.310060 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.310164 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7cfe2fdf0)  to ReluGradNode (addr: 0x55b7b4d8d1e0)
I1018 08:53:57.310170 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7cfe2fdf0)  to GradNodeAccumulation (addr: 0x55b7b778d7d0)
I1018 08:53:57.310199 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.310204 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.310222 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1d920 }]),  
( mean , [{ Name: batch_norm2d_8.w_1, Initialized: 1, Ptr: 0x55b7c5cf24d0 }]),  
( variance , [{ Name: batch_norm2d_8.w_2, Initialized: 1, Ptr: 0x55b7c5cf3390 }]),  
( scale , [{ Name: batch_norm2d_8.w_0, Initialized: 1, Ptr: 0x55b7bb905130 }]),  
( bias , [{ Name: batch_norm2d_8.b_0, Initialized: 1, Ptr: 0x55b7c5cf1610 }]), ]} 
I1018 08:53:57.310233 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.310236 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.310278 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b78e4d40)  to Conv2dGradNodeFinal (addr: 0x55b7cfe2fdf0)
I1018 08:53:57.310282 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b78e4d40)  to GradNodeAccumulation (addr: 0x55b7c5cf0810)
I1018 08:53:57.310285 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b78e4d40)  to GradNodeAccumulation (addr: 0x55b7c5cf1730)
I1018 08:53:57.310295 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.310328 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1d920 }]),  
( mean , [{ Name: batch_norm2d_8.w_1, Initialized: 1, Ptr: 0x55b7c5cf24d0 }]),  
( variance , [{ Name: batch_norm2d_8.w_2, Initialized: 1, Ptr: 0x55b7c5cf3390 }]),  
( scale , [{ Name: batch_norm2d_8.w_0, Initialized: 1, Ptr: 0x55b7bb905130 }]),  
( bias , [{ Name: batch_norm2d_8.b_0, Initialized: 1, Ptr: 0x55b7c5cf1610 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e79a40 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b8fa70 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1dd70 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e54f0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe30580 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c22d0770 }]), ] } 
I1018 08:53:57.310343 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.310345 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.310350 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e79a40 }]), ]} 
I1018 08:53:57.310366 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7c4e1ebf0)  to BatchNormGradNode (addr: 0x55b7b78e4d40)
I1018 08:53:57.310370 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.310379 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e79a40 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c22d0d50 }]), ] } 
I1018 08:53:57.310393 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.310396 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.310494 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b7797370)  to ReluGradNode (addr: 0x55b7c4e1ebf0)
I1018 08:53:57.310500 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b7797370)  to GradNodeAccumulation (addr: 0x55b7cc6c36b0)
I1018 08:53:57.310528 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.310532 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.310551 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e79a40 }]),  
( mean , [{ Name: batch_norm2d_9.w_1, Initialized: 1, Ptr: 0x55b7b7056fb0 }]),  
( variance , [{ Name: batch_norm2d_9.w_2, Initialized: 1, Ptr: 0x55b7b7057e70 }]),  
( scale , [{ Name: batch_norm2d_9.w_0, Initialized: 1, Ptr: 0x55b7cc6c3590 }]),  
( bias , [{ Name: batch_norm2d_9.b_0, Initialized: 1, Ptr: 0x55b7b70560f0 }]), ]} 
I1018 08:53:57.310561 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.310564 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.310607 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b779db70)  to Conv2dGradNodeFinal (addr: 0x55b7b7797370)
I1018 08:53:57.310612 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b779db70)  to GradNodeAccumulation (addr: 0x55b7cc6c4610)
I1018 08:53:57.310614 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b779db70)  to GradNodeAccumulation (addr: 0x55b7b7056210)
I1018 08:53:57.310621 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.310652 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e79a40 }]),  
( mean , [{ Name: batch_norm2d_9.w_1, Initialized: 1, Ptr: 0x55b7b7056fb0 }]),  
( variance , [{ Name: batch_norm2d_9.w_2, Initialized: 1, Ptr: 0x55b7b7057e70 }]),  
( scale , [{ Name: batch_norm2d_9.w_0, Initialized: 1, Ptr: 0x55b7cc6c3590 }]),  
( bias , [{ Name: batch_norm2d_9.b_0, Initialized: 1, Ptr: 0x55b7b70560f0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b901f0 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b90310 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b904e0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779e320 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e7b190 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c4e7b2b0 }]), ] } 
I1018 08:53:57.310665 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.310674 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b901f0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4ed8770 }]), ]} 
I1018 08:53:57.310698 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7b77d1750)  to BatchNormGradNode (addr: 0x55b7b779db70)
I1018 08:53:57.310701 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7b77d1750)  to ReluGradNode (addr: 0x55b7b4d8d1e0)
I1018 08:53:57.310704 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.310715 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b901f0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4ed8770 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1e020 }]), ] } 
I1018 08:53:57.310729 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.310731 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.310737 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1e020 }]), ]} 
I1018 08:53:57.310751 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7bacd8cf0)  to AddGradNode (addr: 0x55b7b77d1750)
I1018 08:53:57.310755 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.310765 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1e020 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b901f0 }]), ] } 
I1018 08:53:57.310784 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.310788 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.310892 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4e7a1f0)  to ReluGradNode (addr: 0x55b7bacd8cf0)
I1018 08:53:57.310899 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4e7a1f0)  to GradNodeAccumulation (addr: 0x55b7c4edec60)
I1018 08:53:57.310927 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.310930 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.310950 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1e020 }]),  
( mean , [{ Name: batch_norm2d_11.w_1, Initialized: 1, Ptr: 0x55b7b7059750 }]),  
( variance , [{ Name: batch_norm2d_11.w_2, Initialized: 1, Ptr: 0x55b7c5ced740 }]),  
( scale , [{ Name: batch_norm2d_11.w_0, Initialized: 1, Ptr: 0x55b7b4b498d0 }]),  
( bias , [{ Name: batch_norm2d_11.b_0, Initialized: 1, Ptr: 0x55b7c4ee0950 }]), ]} 
I1018 08:53:57.310961 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.310963 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.311007 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b77ce2c0)  to Conv2dGradNodeFinal (addr: 0x55b7c4e7a1f0)
I1018 08:53:57.311017 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b77ce2c0)  to GradNodeAccumulation (addr: 0x55b7c4edfbc0)
I1018 08:53:57.311019 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b77ce2c0)  to GradNodeAccumulation (addr: 0x55b7c4ee0a70)
I1018 08:53:57.311025 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.311058 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1e020 }]),  
( mean , [{ Name: batch_norm2d_11.w_1, Initialized: 1, Ptr: 0x55b7b7059750 }]),  
( variance , [{ Name: batch_norm2d_11.w_2, Initialized: 1, Ptr: 0x55b7c5ced740 }]),  
( scale , [{ Name: batch_norm2d_11.w_0, Initialized: 1, Ptr: 0x55b7b4b498d0 }]),  
( bias , [{ Name: batch_norm2d_11.b_0, Initialized: 1, Ptr: 0x55b7c4ee0950 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77d1f70 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77d0550 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701b770 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701ca30 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77cea70 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c4e7a980 }]), ] } 
I1018 08:53:57.311072 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.311075 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.311081 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77d1f70 }]), ]} 
I1018 08:53:57.311096 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7bacd7fb0)  to BatchNormGradNode (addr: 0x55b7b77ce2c0)
I1018 08:53:57.311100 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.311110 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77d1f70 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779ff00 }]), ] } 
I1018 08:53:57.311123 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.311127 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.311228 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4edb4a0)  to ReluGradNode (addr: 0x55b7bacd7fb0)
I1018 08:53:57.311233 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4edb4a0)  to GradNodeAccumulation (addr: 0x55b7c5cee750)
I1018 08:53:57.311262 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.311265 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.311285 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77d1f70 }]),  
( mean , [{ Name: batch_norm2d_12.w_1, Initialized: 1, Ptr: 0x55b7cc398370 }]),  
( variance , [{ Name: batch_norm2d_12.w_2, Initialized: 1, Ptr: 0x55b7cc399230 }]),  
( scale , [{ Name: batch_norm2d_12.w_0, Initialized: 1, Ptr: 0x55b7c5cee630 }]),  
( bias , [{ Name: batch_norm2d_12.b_0, Initialized: 1, Ptr: 0x55b7cc3974b0 }]), ]} 
I1018 08:53:57.311295 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.311298 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.311343 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b701f180)  to Conv2dGradNodeFinal (addr: 0x55b7c4edb4a0)
I1018 08:53:57.311347 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b701f180)  to GradNodeAccumulation (addr: 0x55b7c5cef6b0)
I1018 08:53:57.311350 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b701f180)  to GradNodeAccumulation (addr: 0x55b7cc3975d0)
I1018 08:53:57.311359 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.311393 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77d1f70 }]),  
( mean , [{ Name: batch_norm2d_12.w_1, Initialized: 1, Ptr: 0x55b7cc398370 }]),  
( variance , [{ Name: batch_norm2d_12.w_2, Initialized: 1, Ptr: 0x55b7cc399230 }]),  
( scale , [{ Name: batch_norm2d_12.w_0, Initialized: 1, Ptr: 0x55b7c5cee630 }]),  
( bias , [{ Name: batch_norm2d_12.b_0, Initialized: 1, Ptr: 0x55b7cc3974b0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c2d5b810 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4169f50 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c416a120 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701f930 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701fa50 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b78e74e0 }]), ] } 
I1018 08:53:57.311412 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.311415 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.311511 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b77cefd0)  to ReluGradNode (addr: 0x55b7bacd8cf0)
I1018 08:53:57.311517 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b77cefd0)  to GradNodeAccumulation (addr: 0x55b7b7058e80)
I1018 08:53:57.311542 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.311547 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.311565 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e7a10 }]),  
( mean , [{ Name: batch_norm2d_10.w_1, Initialized: 1, Ptr: 0x55b7b780fba0 }]),  
( variance , [{ Name: batch_norm2d_10.w_2, Initialized: 1, Ptr: 0x55b7b7810a60 }]),  
( scale , [{ Name: batch_norm2d_10.w_0, Initialized: 1, Ptr: 0x55b7b7058d60 }]),  
( bias , [{ Name: batch_norm2d_10.b_0, Initialized: 1, Ptr: 0x55b7b780ece0 }]), ]} 
I1018 08:53:57.311576 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.311578 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.311620 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b7798470)  to Conv2dGradNodeFinal (addr: 0x55b7b77cefd0)
I1018 08:53:57.311625 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b7798470)  to GradNodeAccumulation (addr: 0x55b7b780df50)
I1018 08:53:57.311626 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b7798470)  to GradNodeAccumulation (addr: 0x55b7b780ee00)
I1018 08:53:57.311632 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.311666 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e7a10 }]),  
( mean , [{ Name: batch_norm2d_10.w_1, Initialized: 1, Ptr: 0x55b7b780fba0 }]),  
( variance , [{ Name: batch_norm2d_10.w_2, Initialized: 1, Ptr: 0x55b7b7810a60 }]),  
( scale , [{ Name: batch_norm2d_10.w_0, Initialized: 1, Ptr: 0x55b7b7058d60 }]),  
( bias , [{ Name: batch_norm2d_10.b_0, Initialized: 1, Ptr: 0x55b7b780ece0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b93a70 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e3f20 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e40f0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b93950 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b93630 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b7798c20 }]), ] } 
I1018 08:53:57.311676 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.311688 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c2d5b810 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b93a70 }]), ]} 
I1018 08:53:57.311712 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7c4ed8f40)  to BatchNormGradNode (addr: 0x55b7b701f180)
I1018 08:53:57.311714 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7c4ed8f40)  to BatchNormGradNode (addr: 0x55b7b7798470)
I1018 08:53:57.311717 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.311729 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c2d5b810 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b93a70 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4ed93c0 }]), ] } 
I1018 08:53:57.311743 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.311746 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.311751 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4ed93c0 }]), ]} 
I1018 08:53:57.311766 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b6f7c6f0)  to AddGradNode (addr: 0x55b7c4ed8f40)
I1018 08:53:57.311770 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.311779 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4ed93c0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c2d5b810 }]), ] } 
I1018 08:53:57.311797 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.311800 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.311905 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4edc2e0)  to ReluGradNode (addr: 0x55b7b6f7c6f0)
I1018 08:53:57.311911 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4edc2e0)  to GradNodeAccumulation (addr: 0x55b7cc39a490)
I1018 08:53:57.311940 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.311944 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.311964 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b93a70 }]),  
( mean , [{ Name: batch_norm2d_13.w_1, Initialized: 1, Ptr: 0x55b7cc4bf110 }]),  
( variance , [{ Name: batch_norm2d_13.w_2, Initialized: 1, Ptr: 0x55b7cc4bffd0 }]),  
( scale , [{ Name: batch_norm2d_13.w_0, Initialized: 1, Ptr: 0x55b7cc39a370 }]),  
( bias , [{ Name: batch_norm2d_13.b_0, Initialized: 1, Ptr: 0x55b7cc4be250 }]), ]} 
I1018 08:53:57.311975 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.311977 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.312021 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b6f7ff70)  to Conv2dGradNodeFinal (addr: 0x55b7c4edc2e0)
I1018 08:53:57.312024 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b6f7ff70)  to GradNodeAccumulation (addr: 0x55b7cc4bd4b0)
I1018 08:53:57.312027 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b6f7ff70)  to GradNodeAccumulation (addr: 0x55b7cc4be370)
I1018 08:53:57.312032 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.312067 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b93a70 }]),  
( mean , [{ Name: batch_norm2d_13.w_1, Initialized: 1, Ptr: 0x55b7cc4bf110 }]),  
( variance , [{ Name: batch_norm2d_13.w_2, Initialized: 1, Ptr: 0x55b7cc4bffd0 }]),  
( scale , [{ Name: batch_norm2d_13.w_0, Initialized: 1, Ptr: 0x55b7cc39a370 }]),  
( bias , [{ Name: batch_norm2d_13.b_0, Initialized: 1, Ptr: 0x55b7cc4be250 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4ed93c0 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4ed94e0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08ce90 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe32790 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4edcb60 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c4edcc80 }]), ] } 
I1018 08:53:57.312084 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.312088 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.312093 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4ed93c0 }]), ]} 
I1018 08:53:57.312108 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7ceefee40)  to BatchNormGradNode (addr: 0x55b7b6f7ff70)
I1018 08:53:57.312114 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.312121 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4ed93c0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f80960 }]), ] } 
I1018 08:53:57.312136 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.312139 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.312237 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7ceef9460)  to ReluGradNode (addr: 0x55b7ceefee40)
I1018 08:53:57.312242 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7ceef9460)  to GradNodeAccumulation (addr: 0x55b7b68cc840)
I1018 08:53:57.312269 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.312273 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.312291 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4ed93c0 }]),  
( mean , [{ Name: batch_norm2d_14.w_1, Initialized: 1, Ptr: 0x55b7b68cf3f0 }]),  
( variance , [{ Name: batch_norm2d_14.w_2, Initialized: 1, Ptr: 0x55b7b68d02b0 }]),  
( scale , [{ Name: batch_norm2d_14.w_0, Initialized: 1, Ptr: 0x55b7cc4c0e90 }]),  
( bias , [{ Name: batch_norm2d_14.b_0, Initialized: 1, Ptr: 0x55b7b68ce530 }]), ]} 
I1018 08:53:57.312302 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.312305 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.312348 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c23e61a0)  to Conv2dGradNodeFinal (addr: 0x55b7ceef9460)
I1018 08:53:57.312352 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c23e61a0)  to GradNodeAccumulation (addr: 0x55b7b68cd7a0)
I1018 08:53:57.312354 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c23e61a0)  to GradNodeAccumulation (addr: 0x55b7b68ce650)
I1018 08:53:57.312361 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.312392 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4ed93c0 }]),  
( mean , [{ Name: batch_norm2d_14.w_1, Initialized: 1, Ptr: 0x55b7b68cf3f0 }]),  
( variance , [{ Name: batch_norm2d_14.w_2, Initialized: 1, Ptr: 0x55b7b68d02b0 }]),  
( scale , [{ Name: batch_norm2d_14.w_0, Initialized: 1, Ptr: 0x55b7cc4c0e90 }]),  
( bias , [{ Name: batch_norm2d_14.b_0, Initialized: 1, Ptr: 0x55b7b68ce530 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceeff7b0 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceeff980 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceef8f50 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c23e6950 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c23e6a70 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c23e6b90 }]), ] } 
I1018 08:53:57.312407 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.312417 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceeff7b0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c2d5b810 }]), ]} 
I1018 08:53:57.312441 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7c416b410)  to BatchNormGradNode (addr: 0x55b7c23e61a0)
I1018 08:53:57.312445 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7c416b410)  to ReluGradNode (addr: 0x55b7b6f7c6f0)
I1018 08:53:57.312448 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.312459 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceeff7b0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c2d5b810 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ee9b0 }]), ] } 
I1018 08:53:57.312472 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.312475 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.312481 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ee9b0 }]), ]} 
I1018 08:53:57.312495 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b78eb540)  to AddGradNode (addr: 0x55b7c416b410)
I1018 08:53:57.312500 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.312507 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ee9b0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceeff7b0 }]), ] } 
I1018 08:53:57.312528 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.312532 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.312637 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b6f680d0)  to ReluGradNode (addr: 0x55b7b78eb540)
I1018 08:53:57.312644 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b6f680d0)  to GradNodeAccumulation (addr: 0x55b7c4d61a60)
I1018 08:53:57.312672 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.312675 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.312695 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ee9b0 }]),  
( mean , [{ Name: batch_norm2d_16.w_1, Initialized: 1, Ptr: 0x55b7b796d090 }]),  
( variance , [{ Name: batch_norm2d_16.w_2, Initialized: 1, Ptr: 0x55b7b796df50 }]),  
( scale , [{ Name: batch_norm2d_16.w_0, Initialized: 1, Ptr: 0x55b7c4d61940 }]),  
( bias , [{ Name: batch_norm2d_16.b_0, Initialized: 1, Ptr: 0x55b7b796c1d0 }]), ]} 
I1018 08:53:57.312705 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.312707 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.312750 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7cb08add0)  to Conv2dGradNodeFinal (addr: 0x55b7b6f680d0)
I1018 08:53:57.312754 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7cb08add0)  to GradNodeAccumulation (addr: 0x55b7c4d629c0)
I1018 08:53:57.312757 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7cb08add0)  to GradNodeAccumulation (addr: 0x55b7b796c2f0)
I1018 08:53:57.312762 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.312795 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ee9b0 }]),  
( mean , [{ Name: batch_norm2d_16.w_1, Initialized: 1, Ptr: 0x55b7b796d090 }]),  
( variance , [{ Name: batch_norm2d_16.w_2, Initialized: 1, Ptr: 0x55b7b796df50 }]),  
( scale , [{ Name: batch_norm2d_16.w_0, Initialized: 1, Ptr: 0x55b7c4d61940 }]),  
( bias , [{ Name: batch_norm2d_16.b_0, Initialized: 1, Ptr: 0x55b7b796c1d0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ebe30 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f67930 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f67b00 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c416b0c0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08b580 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7cb08b6a0 }]), ] } 
I1018 08:53:57.312813 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.312816 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.312822 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ebe30 }]), ]} 
I1018 08:53:57.312839 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7c4cbb450)  to BatchNormGradNode (addr: 0x55b7cb08add0)
I1018 08:53:57.312842 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.312850 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ebe30 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08bc80 }]), ] } 
I1018 08:53:57.312865 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.312868 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.312985 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7ceefa730)  to ReluGradNode (addr: 0x55b7c4cbb450)
I1018 08:53:57.312990 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7ceefa730)  to GradNodeAccumulation (addr: 0x55b7b796ef60)
I1018 08:53:57.313019 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.313023 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.313042 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ebe30 }]),  
( mean , [{ Name: batch_norm2d_17.w_1, Initialized: 1, Ptr: 0x55b7bea3b370 }]),  
( variance , [{ Name: batch_norm2d_17.w_2, Initialized: 1, Ptr: 0x55b7bea3c230 }]),  
( scale , [{ Name: batch_norm2d_17.w_0, Initialized: 1, Ptr: 0x55b7b796ee40 }]),  
( bias , [{ Name: batch_norm2d_17.b_0, Initialized: 1, Ptr: 0x55b7bea3a4b0 }]), ]} 
I1018 08:53:57.313053 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.313055 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.313099 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7ceefb1d0)  to Conv2dGradNodeFinal (addr: 0x55b7ceefa730)
I1018 08:53:57.313103 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7ceefb1d0)  to GradNodeAccumulation (addr: 0x55b7b796fe00)
I1018 08:53:57.313107 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7ceefb1d0)  to GradNodeAccumulation (addr: 0x55b7bea3a5d0)
I1018 08:53:57.313112 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.313144 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ebe30 }]),  
( mean , [{ Name: batch_norm2d_17.w_1, Initialized: 1, Ptr: 0x55b7bea3b370 }]),  
( variance , [{ Name: batch_norm2d_17.w_2, Initialized: 1, Ptr: 0x55b7bea3c230 }]),  
( scale , [{ Name: batch_norm2d_17.w_0, Initialized: 1, Ptr: 0x55b7b796ee40 }]),  
( bias , [{ Name: batch_norm2d_17.b_0, Initialized: 1, Ptr: 0x55b7bea3a4b0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cbbe30 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cbbf50 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cbc120 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefa620 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cbc240 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c4cbb2e0 }]), ] } 
I1018 08:53:57.313169 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.313171 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.313270 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b7799020)  to ReluGradNode (addr: 0x55b7b78eb540)
I1018 08:53:57.313277 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b7799020)  to GradNodeAccumulation (addr: 0x55b7b759fa30)
I1018 08:53:57.313303 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.313308 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.313326 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f05550 }]),  
( mean , [{ Name: batch_norm2d_15.w_1, Initialized: 1, Ptr: 0x55b7c4d5f940 }]),  
( variance , [{ Name: batch_norm2d_15.w_2, Initialized: 1, Ptr: 0x55b7c4d60800 }]),  
( scale , [{ Name: batch_norm2d_15.w_0, Initialized: 1, Ptr: 0x55b7b759f910 }]),  
( bias , [{ Name: batch_norm2d_15.b_0, Initialized: 1, Ptr: 0x55b7b75a2ec0 }]), ]} 
I1018 08:53:57.313337 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.313340 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.313381 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b7799c10)  to Conv2dGradNodeFinal (addr: 0x55b7b7799020)
I1018 08:53:57.313385 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b7799c10)  to GradNodeAccumulation (addr: 0x55b7b3f217c0)
I1018 08:53:57.313387 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b7799c10)  to GradNodeAccumulation (addr: 0x55b7b75a2fe0)
I1018 08:53:57.313393 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.313426 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f05550 }]),  
( mean , [{ Name: batch_norm2d_15.w_1, Initialized: 1, Ptr: 0x55b7c4d5f940 }]),  
( variance , [{ Name: batch_norm2d_15.w_2, Initialized: 1, Ptr: 0x55b7c4d60800 }]),  
( scale , [{ Name: batch_norm2d_15.w_0, Initialized: 1, Ptr: 0x55b7b759f910 }]),  
( bias , [{ Name: batch_norm2d_15.b_0, Initialized: 1, Ptr: 0x55b7b75a2ec0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b70eca60 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b70ecc30 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b70ece00 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b70ec940 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779a3c0 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b779a4e0 }]), ] } 
I1018 08:53:57.313436 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.313444 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cbbe30 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7b70eca60 }]), ]} 
I1018 08:53:57.313467 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7b757c430)  to BatchNormGradNode (addr: 0x55b7ceefb1d0)
I1018 08:53:57.313469 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7b757c430)  to BatchNormGradNode (addr: 0x55b7b7799c10)
I1018 08:53:57.313479 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.313490 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cbbe30 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7b70eca60 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b70ec820 }]), ] } 
I1018 08:53:57.313504 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.313508 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.313513 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b70ec820 }]), ]} 
I1018 08:53:57.313529 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b757cfe0)  to AddGradNode (addr: 0x55b7b757c430)
I1018 08:53:57.313532 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.313540 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b70ec820 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cbbe30 }]), ] } 
I1018 08:53:57.313558 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.313562 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.313678 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4b93d50)  to ReluGradNode (addr: 0x55b7b757cfe0)
I1018 08:53:57.313683 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4b93d50)  to GradNodeAccumulation (addr: 0x55b7bea3d490)
I1018 08:53:57.313714 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.313716 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.313736 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b70eca60 }]),  
( mean , [{ Name: batch_norm2d_18.w_1, Initialized: 1, Ptr: 0x55b7c49fade0 }]),  
( variance , [{ Name: batch_norm2d_18.w_2, Initialized: 1, Ptr: 0x55b7c49fbca0 }]),  
( scale , [{ Name: batch_norm2d_18.w_0, Initialized: 1, Ptr: 0x55b7bea3d370 }]),  
( bias , [{ Name: batch_norm2d_18.b_0, Initialized: 1, Ptr: 0x55b7c49f9f20 }]), ]} 
I1018 08:53:57.313746 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.313750 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.313791 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c4b944d0)  to Conv2dGradNodeFinal (addr: 0x55b7c4b93d50)
I1018 08:53:57.313795 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c4b944d0)  to GradNodeAccumulation (addr: 0x55b7c49f9190)
I1018 08:53:57.313798 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c4b944d0)  to GradNodeAccumulation (addr: 0x55b7c49fa040)
I1018 08:53:57.313803 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.313836 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b70eca60 }]),  
( mean , [{ Name: batch_norm2d_18.w_1, Initialized: 1, Ptr: 0x55b7c49fade0 }]),  
( variance , [{ Name: batch_norm2d_18.w_2, Initialized: 1, Ptr: 0x55b7c49fbca0 }]),  
( scale , [{ Name: batch_norm2d_18.w_0, Initialized: 1, Ptr: 0x55b7bea3d370 }]),  
( bias , [{ Name: batch_norm2d_18.b_0, Initialized: 1, Ptr: 0x55b7c49f9f20 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b70ec820 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefdc80 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b757d980 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b757e250 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefd920 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7cb08c960 }]), ] } 
I1018 08:53:57.313854 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.313858 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.313863 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b70ec820 }]), ]} 
I1018 08:53:57.313879 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7c4b96c00)  to BatchNormGradNode (addr: 0x55b7c4b944d0)
I1018 08:53:57.313884 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.313891 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b70ec820 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b94c80 }]), ] } 
I1018 08:53:57.313906 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.313910 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.314018 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c9f00790)  to ReluGradNode (addr: 0x55b7c4b96c00)
I1018 08:53:57.314023 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c9f00790)  to GradNodeAccumulation (addr: 0x55b7c49fccb0)
I1018 08:53:57.314050 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.314054 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.314074 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b70ec820 }]),  
( mean , [{ Name: batch_norm2d_19.w_1, Initialized: 1, Ptr: 0x55b7b8788190 }]),  
( variance , [{ Name: batch_norm2d_19.w_2, Initialized: 1, Ptr: 0x55b7b8789050 }]),  
( scale , [{ Name: batch_norm2d_19.w_0, Initialized: 1, Ptr: 0x55b7c49fcb90 }]),  
( bias , [{ Name: batch_norm2d_19.b_0, Initialized: 1, Ptr: 0x55b7b87872d0 }]), ]} 
I1018 08:53:57.314083 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.314086 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.314127 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c9f01320)  to Conv2dGradNodeFinal (addr: 0x55b7c9f00790)
I1018 08:53:57.314131 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c9f01320)  to GradNodeAccumulation (addr: 0x55b7b8786540)
I1018 08:53:57.314133 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c9f01320)  to GradNodeAccumulation (addr: 0x55b7b87873f0)
I1018 08:53:57.314138 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.314170 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b70ec820 }]),  
( mean , [{ Name: batch_norm2d_19.w_1, Initialized: 1, Ptr: 0x55b7b8788190 }]),  
( variance , [{ Name: batch_norm2d_19.w_2, Initialized: 1, Ptr: 0x55b7b8789050 }]),  
( scale , [{ Name: batch_norm2d_19.w_0, Initialized: 1, Ptr: 0x55b7c49fcb90 }]),  
( bias , [{ Name: batch_norm2d_19.b_0, Initialized: 1, Ptr: 0x55b7b87872d0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9effdc0 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9effee0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f000b0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f00680 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b96a90 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c9f01ad0 }]), ] } 
I1018 08:53:57.314179 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.314188 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9effdc0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cbbe30 }]), ]} 
I1018 08:53:57.314209 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7b877b910)  to BatchNormGradNode (addr: 0x55b7c9f01320)
I1018 08:53:57.314217 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7b877b910)  to ReluGradNode (addr: 0x55b7b757cfe0)
I1018 08:53:57.314220 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.314232 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9effdc0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cbbe30 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877bd90 }]), ] } 
I1018 08:53:57.314245 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.314249 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.314253 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877bd90 }]), ]} 
I1018 08:53:57.314268 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b877c980)  to AddGradNode (addr: 0x55b7b877b910)
I1018 08:53:57.314272 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.314281 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877bd90 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9effdc0 }]), ] } 
I1018 08:53:57.314324 4097533 eager_method.cc:2158] Tensor:  set use_gpudnn = 0
I1018 08:53:57.314333 4097533 eager_op_function.cc:35671] CurrentDeviceId: 0 from 0
I1018 08:53:57.314337 4097533 dygraph_functions.cc:69244] Running AD API: pool2d
I1018 08:53:57.314343 4097533 dygraph_functions.cc:69300] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877bd90 }]), ]} 
I1018 08:53:57.314357 4097533 pool2d_kernel.cc:153] [Pool2d] pooling type: avg exclusive: 1 adaptive: 1
I1018 08:53:57.314395 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Pool2dGradNode (addr: 0x55b7b877d260)  to ReluGradNode (addr: 0x55b7b877c980)
I1018 08:53:57.314399 4097533 dygraph_functions.cc:69376] Finish AD API: pool2d
I1018 08:53:57.314409 4097533 dygraph_functions.cc:69390] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877bd90 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877beb0 }]), ] } 
I1018 08:53:57.314424 4097533 eager_op_function.cc:9777] CurrentDeviceId: 0 from 0
I1018 08:53:57.314426 4097533 dygraph_functions.cc:20255] Running AD API: flatten
I1018 08:53:57.314432 4097533 dygraph_functions.cc:20311] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877beb0 }]), ]} 
I1018 08:53:57.314438 4097533 dygraph_api.cc:208] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.314445 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from FlattenGradNode (addr: 0x55b7b877e180)  to Pool2dGradNode (addr: 0x55b7b877d260)
I1018 08:53:57.314450 4097533 dygraph_functions.cc:20387] Finish AD API: flatten
I1018 08:53:57.314460 4097533 dygraph_functions.cc:20404] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877beb0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877d8c0 }]),  
( xshape , [{ Name: None, Initialized: 0, Ptr: 0x55b7b877da90 }]), ] } 
I1018 08:53:57.314473 4097533 dygraph_functions.cc:67019] Running AD API: matmul
I1018 08:53:57.314482 4097533 dygraph_functions.cc:67081] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877d8c0 }]),  
( y , [{ Name: linear_0.w_0, Initialized: 1, Ptr: 0x55b7b6f66780 }]), ]} 
I1018 08:53:57.314505 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from MatmulGradNode (addr: 0x55b7b877ee40)  to FlattenGradNode (addr: 0x55b7b877e180)
I1018 08:53:57.314509 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from MatmulGradNode (addr: 0x55b7b877ee40)  to GradNodeAccumulation (addr: 0x55b7b878a500)
I1018 08:53:57.314512 4097533 dygraph_functions.cc:67152] Finish AD API: matmul
I1018 08:53:57.314523 4097533 dygraph_functions.cc:67169] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877d8c0 }]),  
( y , [{ Name: linear_0.w_0, Initialized: 1, Ptr: 0x55b7b6f66780 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877f2c0 }]), ] } 
I1018 08:53:57.314531 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.314539 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877f2c0 }]),  
( y , [{ Name: linear_0.b_0, Initialized: 1, Ptr: 0x55b7b878a3e0 }]), ]} 
I1018 08:53:57.314560 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7b877fe20)  to MatmulGradNode (addr: 0x55b7b877ee40)
I1018 08:53:57.314563 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7b877fe20)  to GradNodeAccumulation (addr: 0x55b7b6f66d60)
I1018 08:53:57.314567 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.314579 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877f2c0 }]),  
( y , [{ Name: linear_0.b_0, Initialized: 1, Ptr: 0x55b7b878a3e0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b87804e0 }]), ] } 
I1018 08:53:57.314754 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.314759 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.314898 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b78e4320)  to GradNodeAccumulation (addr: 0x55b77381dd90)
I1018 08:53:57.314934 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.314937 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.314958 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877d7a0 }]),  
( mean , [{ Name: batch_norm2d_0.w_1, Initialized: 1, Ptr: 0x55b7c4ee9a30 }]),  
( variance , [{ Name: batch_norm2d_0.w_2, Initialized: 1, Ptr: 0x55b7ceb07ff0 }]),  
( scale , [{ Name: batch_norm2d_0.w_0, Initialized: 1, Ptr: 0x55b77346a070 }]),  
( bias , [{ Name: batch_norm2d_0.b_0, Initialized: 1, Ptr: 0x55b7b6f6d4b0 }]), ]} 
I1018 08:53:57.314970 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.314973 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.315018 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7cfe31d60)  to Conv2dGradNodeFinal (addr: 0x55b7b78e4320)
I1018 08:53:57.315022 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7cfe31d60)  to GradNodeAccumulation (addr: 0x55b77ed32260)
I1018 08:53:57.315025 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7cfe31d60)  to GradNodeAccumulation (addr: 0x55b77f0e4620)
I1018 08:53:57.315030 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.315064 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877d7a0 }]),  
( mean , [{ Name: batch_norm2d_0.w_1, Initialized: 1, Ptr: 0x55b7c4ee9a30 }]),  
( variance , [{ Name: batch_norm2d_0.w_2, Initialized: 1, Ptr: 0x55b7ceb07ff0 }]),  
( scale , [{ Name: batch_norm2d_0.w_0, Initialized: 1, Ptr: 0x55b77346a070 }]),  
( bias , [{ Name: batch_norm2d_0.b_0, Initialized: 1, Ptr: 0x55b7b6f6d4b0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877d8c0 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b87802a0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b87803c0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877f2c0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877f3e0 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7736fdb00 }]), ] } 
I1018 08:53:57.315079 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.315088 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.315093 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877d8c0 }]), ]} 
I1018 08:53:57.315109 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b4d8d1e0)  to BatchNormGradNode (addr: 0x55b7cfe31d60)
I1018 08:53:57.315114 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.315121 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877d8c0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7ce10 }]), ] } 
I1018 08:53:57.315141 4097533 eager_op_function.cc:35671] CurrentDeviceId: 0 from 0
I1018 08:53:57.315145 4097533 dygraph_functions.cc:69244] Running AD API: pool2d
I1018 08:53:57.315151 4097533 dygraph_functions.cc:69300] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7ce10 }]), ]} 
I1018 08:53:57.315165 4097533 pool2d_kernel.cc:153] [Pool2d] pooling type: max exclusive: 1 adaptive: 0
I1018 08:53:57.315187 4097533 pool2d_kernel.cc:198] [Pool2d] extra_input_size: 0
I1018 08:53:57.315209 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Pool2dGradNode (addr: 0x55b7c4b8f2b0)  to ReluGradNode (addr: 0x55b7b4d8d1e0)
I1018 08:53:57.315213 4097533 dygraph_functions.cc:69376] Finish AD API: pool2d
I1018 08:53:57.315222 4097533 dygraph_functions.cc:69390] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7ce10 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877d8c0 }]), ] } 
I1018 08:53:57.315243 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.315248 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.315346 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c2d5e1f0)  to Pool2dGradNode (addr: 0x55b7c4b8f2b0)
I1018 08:53:57.315352 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c2d5e1f0)  to GradNodeAccumulation (addr: 0x55b773982760)
I1018 08:53:57.315382 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.315385 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.315405 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4169780 }]),  
( mean , [{ Name: batch_norm2d_1.w_1, Initialized: 1, Ptr: 0x55b7b64bb520 }]),  
( variance , [{ Name: batch_norm2d_1.w_2, Initialized: 1, Ptr: 0x55b7b68ca2d0 }]),  
( scale , [{ Name: batch_norm2d_1.w_0, Initialized: 1, Ptr: 0x55b7b68c8170 }]),  
( bias , [{ Name: batch_norm2d_1.b_0, Initialized: 1, Ptr: 0x55b7b76da540 }]), ]} 
I1018 08:53:57.315415 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.315418 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.315460 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c4168f10)  to Conv2dGradNodeFinal (addr: 0x55b7c2d5e1f0)
I1018 08:53:57.315464 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c4168f10)  to GradNodeAccumulation (addr: 0x55b773982b90)
I1018 08:53:57.315467 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c4168f10)  to GradNodeAccumulation (addr: 0x55b773f00630)
I1018 08:53:57.315474 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.315506 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4169780 }]),  
( mean , [{ Name: batch_norm2d_1.w_1, Initialized: 1, Ptr: 0x55b7b64bb520 }]),  
( variance , [{ Name: batch_norm2d_1.w_2, Initialized: 1, Ptr: 0x55b7b68ca2d0 }]),  
( scale , [{ Name: batch_norm2d_1.w_0, Initialized: 1, Ptr: 0x55b7b68c8170 }]),  
( bias , [{ Name: batch_norm2d_1.b_0, Initialized: 1, Ptr: 0x55b7b76da540 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b77398c530 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe32db0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b8780bf0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cb7f70 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ea900 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b70ed860 }]), ] } 
I1018 08:53:57.315524 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.315528 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.315533 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b77398c530 }]), ]} 
I1018 08:53:57.315549 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b773371db0)  to BatchNormGradNode (addr: 0x55b7c4168f10)
I1018 08:53:57.315553 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.315562 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b77398c530 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b8c100 }]), ] } 
I1018 08:53:57.315577 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.315582 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.315677 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b70ef960)  to ReluGradNode (addr: 0x55b773371db0)
I1018 08:53:57.315682 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b70ef960)  to GradNodeAccumulation (addr: 0x55b7bd3da270)
I1018 08:53:57.315711 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.315714 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.315733 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b77398c530 }]),  
( mean , [{ Name: batch_norm2d_2.w_1, Initialized: 1, Ptr: 0x55b7b76e6e10 }]),  
( variance , [{ Name: batch_norm2d_2.w_2, Initialized: 1, Ptr: 0x55b7c2207b30 }]),  
( scale , [{ Name: batch_norm2d_2.w_0, Initialized: 1, Ptr: 0x55b7bd3da150 }]),  
( bias , [{ Name: batch_norm2d_2.b_0, Initialized: 1, Ptr: 0x55b7bd3d86c0 }]), ]} 
I1018 08:53:57.315743 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.315747 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.315788 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b77cd7b0)  to Conv2dGradNodeFinal (addr: 0x55b7b70ef960)
I1018 08:53:57.315793 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b77cd7b0)  to GradNodeAccumulation (addr: 0x55b7c4ee5b40)
I1018 08:53:57.315795 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b77cd7b0)  to GradNodeAccumulation (addr: 0x55b7bd3d87e0)
I1018 08:53:57.315801 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.315834 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b77398c530 }]),  
( mean , [{ Name: batch_norm2d_2.w_1, Initialized: 1, Ptr: 0x55b7b76e6e10 }]),  
( variance , [{ Name: batch_norm2d_2.w_2, Initialized: 1, Ptr: 0x55b7c2207b30 }]),  
( scale , [{ Name: batch_norm2d_2.w_0, Initialized: 1, Ptr: 0x55b7bd3da150 }]),  
( bias , [{ Name: batch_norm2d_2.b_0, Initialized: 1, Ptr: 0x55b7bd3d86c0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7fe00 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cb9e50 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e3a50 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c2d5c9e0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1e960 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c4e1ea80 }]), ] } 
I1018 08:53:57.315848 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.315857 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7fe00 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877d8c0 }]), ]} 
I1018 08:53:57.315879 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7c2d5eb80)  to BatchNormGradNode (addr: 0x55b7b77cd7b0)
I1018 08:53:57.315883 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7c2d5eb80)  to Pool2dGradNode (addr: 0x55b7c4b8f2b0)
I1018 08:53:57.315886 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.315897 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7fe00 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877d8c0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f6eac0 }]), ] } 
I1018 08:53:57.315912 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.315914 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.315920 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f6eac0 }]), ]} 
I1018 08:53:57.315934 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7733494c0)  to AddGradNode (addr: 0x55b7c2d5eb80)
I1018 08:53:57.315938 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.315948 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f6eac0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7fe00 }]), ] } 
I1018 08:53:57.315964 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.315968 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.316066 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4e78c00)  to ReluGradNode (addr: 0x55b7733494c0)
I1018 08:53:57.316072 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4e78c00)  to GradNodeAccumulation (addr: 0x55b7b76e5550)
I1018 08:53:57.316102 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.316104 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.316123 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f6eac0 }]),  
( mean , [{ Name: batch_norm2d_3.w_1, Initialized: 1, Ptr: 0x55b7b778e800 }]),  
( variance , [{ Name: batch_norm2d_3.w_2, Initialized: 1, Ptr: 0x55b7b3f224a0 }]),  
( scale , [{ Name: batch_norm2d_3.w_0, Initialized: 1, Ptr: 0x55b7bd3daab0 }]),  
( bias , [{ Name: batch_norm2d_3.b_0, Initialized: 1, Ptr: 0x55b7b87810f0 }]), ]} 
I1018 08:53:57.316134 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.316138 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.316179 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c416bf80)  to Conv2dGradNodeFinal (addr: 0x55b7c4e78c00)
I1018 08:53:57.316183 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c416bf80)  to GradNodeAccumulation (addr: 0x55b7b76e6360)
I1018 08:53:57.316186 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c416bf80)  to GradNodeAccumulation (addr: 0x55b7b68c8390)
I1018 08:53:57.316192 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.316226 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f6eac0 }]),  
( mean , [{ Name: batch_norm2d_3.w_1, Initialized: 1, Ptr: 0x55b7b778e800 }]),  
( variance , [{ Name: batch_norm2d_3.w_2, Initialized: 1, Ptr: 0x55b7b3f224a0 }]),  
( scale , [{ Name: batch_norm2d_3.w_0, Initialized: 1, Ptr: 0x55b7bd3daab0 }]),  
( bias , [{ Name: batch_norm2d_3.b_0, Initialized: 1, Ptr: 0x55b7b87810f0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb088590 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c2d5f080 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c2d5f250 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b7796580 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77966a0 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b773679a80 }]), ] } 
I1018 08:53:57.316244 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.316247 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.316253 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb088590 }]), ]} 
I1018 08:53:57.316268 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b77334c090)  to BatchNormGradNode (addr: 0x55b7c416bf80)
I1018 08:53:57.316272 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.316282 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb088590 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b8e670 }]), ] } 
I1018 08:53:57.316295 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.316299 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.316394 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c416a440)  to ReluGradNode (addr: 0x55b77334c090)
I1018 08:53:57.316399 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c416a440)  to GradNodeAccumulation (addr: 0x55b7b68c6d90)
I1018 08:53:57.316427 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.316430 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.316449 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb088590 }]),  
( mean , [{ Name: batch_norm2d_4.w_1, Initialized: 1, Ptr: 0x55b7c2209c90 }]),  
( variance , [{ Name: batch_norm2d_4.w_2, Initialized: 1, Ptr: 0x55b7c220ab50 }]),  
( scale , [{ Name: batch_norm2d_4.w_0, Initialized: 1, Ptr: 0x55b7b68c6c70 }]),  
( bias , [{ Name: batch_norm2d_4.b_0, Initialized: 1, Ptr: 0x55b7b4b4a7c0 }]), ]} 
I1018 08:53:57.316460 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.316462 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.316504 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b6f7df70)  to Conv2dGradNodeFinal (addr: 0x55b7c416a440)
I1018 08:53:57.316507 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b6f7df70)  to GradNodeAccumulation (addr: 0x55b7b4b49ae0)
I1018 08:53:57.316510 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b6f7df70)  to GradNodeAccumulation (addr: 0x55b7b4b4a8e0)
I1018 08:53:57.316515 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.316548 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb088590 }]),  
( mean , [{ Name: batch_norm2d_4.w_1, Initialized: 1, Ptr: 0x55b7c2209c90 }]),  
( variance , [{ Name: batch_norm2d_4.w_2, Initialized: 1, Ptr: 0x55b7c220ab50 }]),  
( scale , [{ Name: batch_norm2d_4.w_0, Initialized: 1, Ptr: 0x55b7b68c6c70 }]),  
( bias , [{ Name: batch_norm2d_4.b_0, Initialized: 1, Ptr: 0x55b7b4b4a7c0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b757e4d0 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77d29d0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e6e70 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e41a0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701ed80 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b701ddd0 }]), ] } 
I1018 08:53:57.316562 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.316571 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b757e4d0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7fe00 }]), ]} 
I1018 08:53:57.316593 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7c4b8df50)  to BatchNormGradNode (addr: 0x55b7b6f7df70)
I1018 08:53:57.316597 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7c4b8df50)  to ReluGradNode (addr: 0x55b7733494c0)
I1018 08:53:57.316601 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.316612 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b757e4d0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7fe00 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779ca80 }]), ] } 
I1018 08:53:57.316624 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.316627 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.316633 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779ca80 }]), ]} 
I1018 08:53:57.316648 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b77324be20)  to AddGradNode (addr: 0x55b7c4b8df50)
I1018 08:53:57.316651 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.316660 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779ca80 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b757e4d0 }]), ] } 
I1018 08:53:57.316680 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.316684 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.316784 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4e7c8c0)  to ReluGradNode (addr: 0x55b77324be20)
I1018 08:53:57.316789 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4e7c8c0)  to GradNodeAccumulation (addr: 0x55b7c4ee2d80)
I1018 08:53:57.316818 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.316821 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.316841 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779ca80 }]),  
( mean , [{ Name: batch_norm2d_6.w_1, Initialized: 1, Ptr: 0x55b7cc394970 }]),  
( variance , [{ Name: batch_norm2d_6.w_2, Initialized: 1, Ptr: 0x55b7cc395a60 }]),  
( scale , [{ Name: batch_norm2d_6.w_0, Initialized: 1, Ptr: 0x55b7c4ee3ce0 }]),  
( bias , [{ Name: batch_norm2d_6.b_0, Initialized: 1, Ptr: 0x55b7c4ee4b90 }]), ]} 
I1018 08:53:57.316852 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.316854 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.316897 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c1f71c40)  to Conv2dGradNodeFinal (addr: 0x55b7c4e7c8c0)
I1018 08:53:57.316901 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c1f71c40)  to GradNodeAccumulation (addr: 0x55b7c4ee3e00)
I1018 08:53:57.316903 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c1f71c40)  to GradNodeAccumulation (addr: 0x55b7c4ee4cb0)
I1018 08:53:57.316913 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.316947 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779ca80 }]),  
( mean , [{ Name: batch_norm2d_6.w_1, Initialized: 1, Ptr: 0x55b7cc394970 }]),  
( variance , [{ Name: batch_norm2d_6.w_2, Initialized: 1, Ptr: 0x55b7cc395a60 }]),  
( scale , [{ Name: batch_norm2d_6.w_0, Initialized: 1, Ptr: 0x55b7c4ee3ce0 }]),  
( bias , [{ Name: batch_norm2d_6.b_0, Initialized: 1, Ptr: 0x55b7c4ee4b90 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ed430 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ed550 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c416d790 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c22d2730 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701e520 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b701e640 }]), ] } 
I1018 08:53:57.316962 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.316964 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.316970 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ed430 }]), ]} 
I1018 08:53:57.316985 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b76fbc80)  to BatchNormGradNode (addr: 0x55b7c1f71c40)
I1018 08:53:57.316989 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.316998 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ed430 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7f6d0 }]), ] } 
I1018 08:53:57.317013 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.317015 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.317111 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7cfe326b0)  to ReluGradNode (addr: 0x55b7b76fbc80)
I1018 08:53:57.317116 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7cfe326b0)  to GradNodeAccumulation (addr: 0x55b7cc396a70)
I1018 08:53:57.317143 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.317147 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.317166 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ed430 }]),  
( mean , [{ Name: batch_norm2d_7.w_1, Initialized: 1, Ptr: 0x55b7c4d65af0 }]),  
( variance , [{ Name: batch_norm2d_7.w_2, Initialized: 1, Ptr: 0x55b7b778c480 }]),  
( scale , [{ Name: batch_norm2d_7.w_0, Initialized: 1, Ptr: 0x55b7cc396950 }]),  
( bias , [{ Name: batch_norm2d_7.b_0, Initialized: 1, Ptr: 0x55b7c4d64c30 }]), ]} 
I1018 08:53:57.317176 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.317179 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.317235 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b78e82d0)  to Conv2dGradNodeFinal (addr: 0x55b7cfe326b0)
I1018 08:53:57.317240 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b78e82d0)  to GradNodeAccumulation (addr: 0x55b7c4d63ea0)
I1018 08:53:57.317243 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b78e82d0)  to GradNodeAccumulation (addr: 0x55b7c4d64d50)
I1018 08:53:57.317250 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.317281 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ed430 }]),  
( mean , [{ Name: batch_norm2d_7.w_1, Initialized: 1, Ptr: 0x55b7c4d65af0 }]),  
( variance , [{ Name: batch_norm2d_7.w_2, Initialized: 1, Ptr: 0x55b7b778c480 }]),  
( scale , [{ Name: batch_norm2d_7.w_0, Initialized: 1, Ptr: 0x55b7cc396950 }]),  
( bias , [{ Name: batch_norm2d_7.b_0, Initialized: 1, Ptr: 0x55b7c4d64c30 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f6a5d0 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f6a7a0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4ed85d0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb2febf0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f71580 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c1f716a0 }]), ] } 
I1018 08:53:57.317307 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.317310 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.317405 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c23e3fc0)  to ReluGradNode (addr: 0x55b77324be20)
I1018 08:53:57.317410 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c23e3fc0)  to GradNodeAccumulation (addr: 0x55b7b85db7c0)
I1018 08:53:57.317438 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.317441 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.317461 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877fc00 }]),  
( mean , [{ Name: batch_norm2d_5.w_1, Initialized: 1, Ptr: 0x55b7bb903150 }]),  
( variance , [{ Name: batch_norm2d_5.w_2, Initialized: 1, Ptr: 0x55b7bb904240 }]),  
( scale , [{ Name: batch_norm2d_5.w_0, Initialized: 1, Ptr: 0x55b7b3f22b10 }]),  
( bias , [{ Name: batch_norm2d_5.b_0, Initialized: 1, Ptr: 0x55b7b85dd420 }]), ]} 
I1018 08:53:57.317471 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.317474 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.317517 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c4e77fd0)  to Conv2dGradNodeFinal (addr: 0x55b7c23e3fc0)
I1018 08:53:57.317521 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c4e77fd0)  to GradNodeAccumulation (addr: 0x55b7b85dc690)
I1018 08:53:57.317524 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c4e77fd0)  to GradNodeAccumulation (addr: 0x55b7b85dd540)
I1018 08:53:57.317529 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.317562 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877fc00 }]),  
( mean , [{ Name: batch_norm2d_5.w_1, Initialized: 1, Ptr: 0x55b7bb903150 }]),  
( variance , [{ Name: batch_norm2d_5.w_2, Initialized: 1, Ptr: 0x55b7bb904240 }]),  
( scale , [{ Name: batch_norm2d_5.w_0, Initialized: 1, Ptr: 0x55b7b3f22b10 }]),  
( bias , [{ Name: batch_norm2d_5.b_0, Initialized: 1, Ptr: 0x55b7b85dd420 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4eda8b0 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1d340 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1d510 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4eda790 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4eda470 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c4e78780 }]), ] } 
I1018 08:53:57.317571 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.317580 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f6a5d0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4eda8b0 }]), ]} 
I1018 08:53:57.317602 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7c4cba040)  to BatchNormGradNode (addr: 0x55b7b78e82d0)
I1018 08:53:57.317610 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7c4cba040)  to BatchNormGradNode (addr: 0x55b7c4e77fd0)
I1018 08:53:57.317613 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.317625 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f6a5d0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4eda8b0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cb96d0 }]), ] } 
I1018 08:53:57.317638 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.317641 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.317647 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cb96d0 }]), ]} 
I1018 08:53:57.317662 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b1a58b50)  to AddGradNode (addr: 0x55b7c4cba040)
I1018 08:53:57.317667 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.317674 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cb96d0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f6a5d0 }]), ] } 
I1018 08:53:57.317692 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.317696 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.317799 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7bacda490)  to ReluGradNode (addr: 0x55b7b1a58b50)
I1018 08:53:57.317804 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7bacda490)  to GradNodeAccumulation (addr: 0x55b7b778d7d0)
I1018 08:53:57.317833 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.317836 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.317855 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4eda8b0 }]),  
( mean , [{ Name: batch_norm2d_8.w_1, Initialized: 1, Ptr: 0x55b7c5cf24d0 }]),  
( variance , [{ Name: batch_norm2d_8.w_2, Initialized: 1, Ptr: 0x55b7c5cf3390 }]),  
( scale , [{ Name: batch_norm2d_8.w_0, Initialized: 1, Ptr: 0x55b7bb905130 }]),  
( bias , [{ Name: batch_norm2d_8.b_0, Initialized: 1, Ptr: 0x55b7c5cf1610 }]), ]} 
I1018 08:53:57.317867 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.317869 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.317909 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c22ce570)  to Conv2dGradNodeFinal (addr: 0x55b7bacda490)
I1018 08:53:57.317912 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c22ce570)  to GradNodeAccumulation (addr: 0x55b7c5cf0810)
I1018 08:53:57.317915 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c22ce570)  to GradNodeAccumulation (addr: 0x55b7c5cf1730)
I1018 08:53:57.317920 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.317955 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4eda8b0 }]),  
( mean , [{ Name: batch_norm2d_8.w_1, Initialized: 1, Ptr: 0x55b7c5cf24d0 }]),  
( variance , [{ Name: batch_norm2d_8.w_2, Initialized: 1, Ptr: 0x55b7c5cf3390 }]),  
( scale , [{ Name: batch_norm2d_8.w_0, Initialized: 1, Ptr: 0x55b7bb905130 }]),  
( bias , [{ Name: batch_norm2d_8.b_0, Initialized: 1, Ptr: 0x55b7c5cf1610 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cb96d0 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cb97f0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1dd70 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c22ced20 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacdac20 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b78e4d50 }]), ] } 
I1018 08:53:57.317973 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.317976 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.317982 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cb96d0 }]), ]} 
I1018 08:53:57.317998 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7bacd6980)  to BatchNormGradNode (addr: 0x55b7c22ce570)
I1018 08:53:57.318002 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.318010 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cb96d0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e5330 }]), ] } 
I1018 08:53:57.318025 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.318029 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.318123 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4e1ebf0)  to ReluGradNode (addr: 0x55b7bacd6980)
I1018 08:53:57.318128 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4e1ebf0)  to GradNodeAccumulation (addr: 0x55b7cc6c36b0)
I1018 08:53:57.318156 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.318161 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.318179 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cb96d0 }]),  
( mean , [{ Name: batch_norm2d_9.w_1, Initialized: 1, Ptr: 0x55b7b7056fb0 }]),  
( variance , [{ Name: batch_norm2d_9.w_2, Initialized: 1, Ptr: 0x55b7b7057e70 }]),  
( scale , [{ Name: batch_norm2d_9.w_0, Initialized: 1, Ptr: 0x55b7cc6c3590 }]),  
( bias , [{ Name: batch_norm2d_9.b_0, Initialized: 1, Ptr: 0x55b7b70560f0 }]), ]} 
I1018 08:53:57.318189 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.318192 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.318233 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c4e7af00)  to Conv2dGradNodeFinal (addr: 0x55b7c4e1ebf0)
I1018 08:53:57.318238 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c4e7af00)  to GradNodeAccumulation (addr: 0x55b7cc6c4610)
I1018 08:53:57.318240 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c4e7af00)  to GradNodeAccumulation (addr: 0x55b7b7056210)
I1018 08:53:57.318245 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.318279 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cb96d0 }]),  
( mean , [{ Name: batch_norm2d_9.w_1, Initialized: 1, Ptr: 0x55b7b7056fb0 }]),  
( variance , [{ Name: batch_norm2d_9.w_2, Initialized: 1, Ptr: 0x55b7b7057e70 }]),  
( scale , [{ Name: batch_norm2d_9.w_0, Initialized: 1, Ptr: 0x55b7cc6c3590 }]),  
( bias , [{ Name: batch_norm2d_9.b_0, Initialized: 1, Ptr: 0x55b7b70560f0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b901f0 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b903c0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b90590 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e7b6b0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b7797600 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b7797720 }]), ] } 
I1018 08:53:57.318287 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.318300 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b901f0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f6a5d0 }]), ]} 
I1018 08:53:57.318323 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7b77a0860)  to BatchNormGradNode (addr: 0x55b7c4e7af00)
I1018 08:53:57.318326 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7b77a0860)  to ReluGradNode (addr: 0x55b7b1a58b50)
I1018 08:53:57.318329 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.318341 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b901f0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f6a5d0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77d0630 }]), ] } 
I1018 08:53:57.318354 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.318357 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.318363 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77d0630 }]), ]} 
I1018 08:53:57.318377 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b779db70)  to AddGradNode (addr: 0x55b7b77a0860)
I1018 08:53:57.318382 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.318389 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77d0630 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b901f0 }]), ] } 
I1018 08:53:57.318410 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.318413 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.318513 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4e7a1f0)  to ReluGradNode (addr: 0x55b7b779db70)
I1018 08:53:57.318518 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4e7a1f0)  to GradNodeAccumulation (addr: 0x55b7c4edec60)
I1018 08:53:57.318547 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.318550 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.318569 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77d0630 }]),  
( mean , [{ Name: batch_norm2d_11.w_1, Initialized: 1, Ptr: 0x55b7b7059750 }]),  
( variance , [{ Name: batch_norm2d_11.w_2, Initialized: 1, Ptr: 0x55b7c5ced740 }]),  
( scale , [{ Name: batch_norm2d_11.w_0, Initialized: 1, Ptr: 0x55b7b4b498d0 }]),  
( bias , [{ Name: batch_norm2d_11.b_0, Initialized: 1, Ptr: 0x55b7c4ee0950 }]), ]} 
I1018 08:53:57.318580 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.318583 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.318624 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b77ce2c0)  to Conv2dGradNodeFinal (addr: 0x55b7c4e7a1f0)
I1018 08:53:57.318627 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b77ce2c0)  to GradNodeAccumulation (addr: 0x55b7c4edfbc0)
I1018 08:53:57.318629 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b77ce2c0)  to GradNodeAccumulation (addr: 0x55b7c4ee0a70)
I1018 08:53:57.318635 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.318667 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77d0630 }]),  
( mean , [{ Name: batch_norm2d_11.w_1, Initialized: 1, Ptr: 0x55b7b7059750 }]),  
( variance , [{ Name: batch_norm2d_11.w_2, Initialized: 1, Ptr: 0x55b7c5ced740 }]),  
( scale , [{ Name: batch_norm2d_11.w_0, Initialized: 1, Ptr: 0x55b7b4b498d0 }]),  
( bias , [{ Name: batch_norm2d_11.b_0, Initialized: 1, Ptr: 0x55b7c4ee0950 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77a1080 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c2d5dc60 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77d1760 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77d02d0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77d1fd0 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b77cea70 }]), ] } 
I1018 08:53:57.318686 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.318688 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.318694 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77a1080 }]), ]} 
I1018 08:53:57.318709 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b701f180)  to BatchNormGradNode (addr: 0x55b7b77ce2c0)
I1018 08:53:57.318713 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.318722 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77a1080 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779fec0 }]), ] } 
I1018 08:53:57.318737 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.318740 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.318835 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c2d5a260)  to ReluGradNode (addr: 0x55b7b701f180)
I1018 08:53:57.318840 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c2d5a260)  to GradNodeAccumulation (addr: 0x55b7c5cee750)
I1018 08:53:57.318868 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.318872 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.318892 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77a1080 }]),  
( mean , [{ Name: batch_norm2d_12.w_1, Initialized: 1, Ptr: 0x55b7cc398370 }]),  
( variance , [{ Name: batch_norm2d_12.w_2, Initialized: 1, Ptr: 0x55b7cc399230 }]),  
( scale , [{ Name: batch_norm2d_12.w_0, Initialized: 1, Ptr: 0x55b7c5cee630 }]),  
( bias , [{ Name: batch_norm2d_12.b_0, Initialized: 1, Ptr: 0x55b7cc3974b0 }]), ]} 
I1018 08:53:57.318902 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.318904 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.318944 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7bacd5c30)  to Conv2dGradNodeFinal (addr: 0x55b7c2d5a260)
I1018 08:53:57.318948 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7bacd5c30)  to GradNodeAccumulation (addr: 0x55b7c5cef6b0)
I1018 08:53:57.318950 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7bacd5c30)  to GradNodeAccumulation (addr: 0x55b7cc3975d0)
I1018 08:53:57.318956 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.318989 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77a1080 }]),  
( mean , [{ Name: batch_norm2d_12.w_1, Initialized: 1, Ptr: 0x55b7cc398370 }]),  
( variance , [{ Name: batch_norm2d_12.w_2, Initialized: 1, Ptr: 0x55b7cc399230 }]),  
( scale , [{ Name: batch_norm2d_12.w_0, Initialized: 1, Ptr: 0x55b7c5cee630 }]),  
( bias , [{ Name: batch_norm2d_12.b_0, Initialized: 1, Ptr: 0x55b7cc3974b0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779d770 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd7fc0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd8190 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd63e0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd6500 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c4edb4b0 }]), ] } 
I1018 08:53:57.319012 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.319016 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.319108 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4e1f8c0)  to ReluGradNode (addr: 0x55b7b779db70)
I1018 08:53:57.319113 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4e1f8c0)  to GradNodeAccumulation (addr: 0x55b7b7058e80)
I1018 08:53:57.319139 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.319142 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.319161 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4edb9e0 }]),  
( mean , [{ Name: batch_norm2d_10.w_1, Initialized: 1, Ptr: 0x55b7b780fba0 }]),  
( variance , [{ Name: batch_norm2d_10.w_2, Initialized: 1, Ptr: 0x55b7b7810a60 }]),  
( scale , [{ Name: batch_norm2d_10.w_0, Initialized: 1, Ptr: 0x55b7b7058d60 }]),  
( bias , [{ Name: batch_norm2d_10.b_0, Initialized: 1, Ptr: 0x55b7b780ece0 }]), ]} 
I1018 08:53:57.319172 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.319175 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.319214 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b779aa00)  to Conv2dGradNodeFinal (addr: 0x55b7c4e1f8c0)
I1018 08:53:57.319218 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b779aa00)  to GradNodeAccumulation (addr: 0x55b7b780df50)
I1018 08:53:57.319221 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b779aa00)  to GradNodeAccumulation (addr: 0x55b7b780ee00)
I1018 08:53:57.319226 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.319259 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4edb9e0 }]),  
( mean , [{ Name: batch_norm2d_10.w_1, Initialized: 1, Ptr: 0x55b7b780fba0 }]),  
( variance , [{ Name: batch_norm2d_10.w_2, Initialized: 1, Ptr: 0x55b7b7810a60 }]),  
( scale , [{ Name: batch_norm2d_10.w_0, Initialized: 1, Ptr: 0x55b7b7058d60 }]),  
( bias , [{ Name: batch_norm2d_10.b_0, Initialized: 1, Ptr: 0x55b7b780ece0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe2fb60 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe311a0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe31370 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779b1b0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779b2d0 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b779b3f0 }]), ] } 
I1018 08:53:57.319268 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.319278 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779d770 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe2fb60 }]), ]} 
I1018 08:53:57.319299 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7b6f7ff70)  to BatchNormGradNode (addr: 0x55b7bacd5c30)
I1018 08:53:57.319303 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7b6f7ff70)  to BatchNormGradNode (addr: 0x55b7b779aa00)
I1018 08:53:57.319305 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.319317 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779d770 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe2fb60 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe2f920 }]), ] } 
I1018 08:53:57.319334 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.319337 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.319343 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe2f920 }]), ]} 
I1018 08:53:57.319358 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7ceefee40)  to AddGradNode (addr: 0x55b7b6f7ff70)
I1018 08:53:57.319362 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.319371 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe2f920 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779d770 }]), ] } 
I1018 08:53:57.319388 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.319392 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.319492 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7ceef8f40)  to ReluGradNode (addr: 0x55b7ceefee40)
I1018 08:53:57.319499 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7ceef8f40)  to GradNodeAccumulation (addr: 0x55b7cc39a490)
I1018 08:53:57.319526 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.319530 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.319550 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe2fb60 }]),  
( mean , [{ Name: batch_norm2d_13.w_1, Initialized: 1, Ptr: 0x55b7cc4bf110 }]),  
( variance , [{ Name: batch_norm2d_13.w_2, Initialized: 1, Ptr: 0x55b7cc4bffd0 }]),  
( scale , [{ Name: batch_norm2d_13.w_0, Initialized: 1, Ptr: 0x55b7cc39a370 }]),  
( bias , [{ Name: batch_norm2d_13.b_0, Initialized: 1, Ptr: 0x55b7cc4be250 }]), ]} 
I1018 08:53:57.319559 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.319562 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.319602 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7cb08d6e0)  to Conv2dGradNodeFinal (addr: 0x55b7ceef8f40)
I1018 08:53:57.319607 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7cb08d6e0)  to GradNodeAccumulation (addr: 0x55b7cc4bd4b0)
I1018 08:53:57.319609 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7cb08d6e0)  to GradNodeAccumulation (addr: 0x55b7cc4be370)
I1018 08:53:57.319614 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.319648 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe2fb60 }]),  
( mean , [{ Name: batch_norm2d_13.w_1, Initialized: 1, Ptr: 0x55b7cc4bf110 }]),  
( variance , [{ Name: batch_norm2d_13.w_2, Initialized: 1, Ptr: 0x55b7cc4bffd0 }]),  
( scale , [{ Name: batch_norm2d_13.w_0, Initialized: 1, Ptr: 0x55b7cc39a370 }]),  
( bias , [{ Name: batch_norm2d_13.b_0, Initialized: 1, Ptr: 0x55b7cc4be250 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe2f920 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4ed9950 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceeff7e0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4edc7b0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701bd40 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b6f80a90 }]), ] } 
I1018 08:53:57.319660 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.319664 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.319669 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe2f920 }]), ]} 
I1018 08:53:57.319689 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b78ee670)  to BatchNormGradNode (addr: 0x55b7cb08d6e0)
I1018 08:53:57.319693 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.319702 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe2f920 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08de90 }]), ] } 
I1018 08:53:57.319717 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.319720 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.319814 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c416abf0)  to ReluGradNode (addr: 0x55b7b78ee670)
I1018 08:53:57.319819 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c416abf0)  to GradNodeAccumulation (addr: 0x55b7b68cc840)
I1018 08:53:57.319847 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.319851 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.319869 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe2f920 }]),  
( mean , [{ Name: batch_norm2d_14.w_1, Initialized: 1, Ptr: 0x55b7b68cf3f0 }]),  
( variance , [{ Name: batch_norm2d_14.w_2, Initialized: 1, Ptr: 0x55b7b68d02b0 }]),  
( scale , [{ Name: batch_norm2d_14.w_0, Initialized: 1, Ptr: 0x55b7cc4c0e90 }]),  
( bias , [{ Name: batch_norm2d_14.b_0, Initialized: 1, Ptr: 0x55b7b68ce530 }]), ]} 
I1018 08:53:57.319880 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.319883 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.319922 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7cb08c5f0)  to Conv2dGradNodeFinal (addr: 0x55b7c416abf0)
I1018 08:53:57.319926 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7cb08c5f0)  to GradNodeAccumulation (addr: 0x55b7b68cd7a0)
I1018 08:53:57.319929 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7cb08c5f0)  to GradNodeAccumulation (addr: 0x55b7b68ce650)
I1018 08:53:57.319934 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.319967 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe2f920 }]),  
( mean , [{ Name: batch_norm2d_14.w_1, Initialized: 1, Ptr: 0x55b7b68cf3f0 }]),  
( variance , [{ Name: batch_norm2d_14.w_2, Initialized: 1, Ptr: 0x55b7b68d02b0 }]),  
( scale , [{ Name: batch_norm2d_14.w_0, Initialized: 1, Ptr: 0x55b7cc4c0e90 }]),  
( bias , [{ Name: batch_norm2d_14.b_0, Initialized: 1, Ptr: 0x55b7b68ce530 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1c200 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1c3d0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1c5a0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ee500 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c416b790 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c416b8b0 }]), ] } 
I1018 08:53:57.319976 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.319985 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1c200 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779d770 }]), ]} 
I1018 08:53:57.320006 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7cb089e70)  to BatchNormGradNode (addr: 0x55b7cb08c5f0)
I1018 08:53:57.320010 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7cb089e70)  to ReluGradNode (addr: 0x55b7ceefee40)
I1018 08:53:57.320019 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.320029 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1c200 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779d770 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08a2f0 }]), ] } 
I1018 08:53:57.320042 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.320046 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.320051 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08a2f0 }]), ]} 
I1018 08:53:57.320066 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b70ecbf0)  to AddGradNode (addr: 0x55b7cb089e70)
I1018 08:53:57.320070 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.320078 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08a2f0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1c200 }]), ] } 
I1018 08:53:57.320099 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.320102 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.320201 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b6f683a0)  to ReluGradNode (addr: 0x55b7b70ecbf0)
I1018 08:53:57.320207 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b6f683a0)  to GradNodeAccumulation (addr: 0x55b7c4d61a60)
I1018 08:53:57.320235 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.320240 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.320258 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08a2f0 }]),  
( mean , [{ Name: batch_norm2d_16.w_1, Initialized: 1, Ptr: 0x55b7b796d090 }]),  
( variance , [{ Name: batch_norm2d_16.w_2, Initialized: 1, Ptr: 0x55b7b796df50 }]),  
( scale , [{ Name: batch_norm2d_16.w_0, Initialized: 1, Ptr: 0x55b7c4d61940 }]),  
( bias , [{ Name: batch_norm2d_16.b_0, Initialized: 1, Ptr: 0x55b7b796c1d0 }]), ]} 
I1018 08:53:57.320268 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.320271 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.320312 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7cb086f30)  to Conv2dGradNodeFinal (addr: 0x55b7b6f683a0)
I1018 08:53:57.320315 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7cb086f30)  to GradNodeAccumulation (addr: 0x55b7c4d629c0)
I1018 08:53:57.320318 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7cb086f30)  to GradNodeAccumulation (addr: 0x55b7b796c2f0)
I1018 08:53:57.320323 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.320358 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08a2f0 }]),  
( mean , [{ Name: batch_norm2d_16.w_1, Initialized: 1, Ptr: 0x55b7b796d090 }]),  
( variance , [{ Name: batch_norm2d_16.w_2, Initialized: 1, Ptr: 0x55b7b796df50 }]),  
( scale , [{ Name: batch_norm2d_16.w_0, Initialized: 1, Ptr: 0x55b7c4d61940 }]),  
( bias , [{ Name: batch_norm2d_16.b_0, Initialized: 1, Ptr: 0x55b7b796c1d0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f67930 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f67b00 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f67cd0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb089b20 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb0876e0 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7cb087800 }]), ] } 
I1018 08:53:57.320375 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.320379 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.320384 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f67930 }]), ]} 
I1018 08:53:57.320399 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7c9f02110)  to BatchNormGradNode (addr: 0x55b7cb086f30)
I1018 08:53:57.320403 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.320412 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f67930 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb087d30 }]), ] } 
I1018 08:53:57.320426 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.320430 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.320537 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c9f00670)  to ReluGradNode (addr: 0x55b7c9f02110)
I1018 08:53:57.320544 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c9f00670)  to GradNodeAccumulation (addr: 0x55b7b796ef60)
I1018 08:53:57.320571 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.320575 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.320595 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f67930 }]),  
( mean , [{ Name: batch_norm2d_17.w_1, Initialized: 1, Ptr: 0x55b7bea3b370 }]),  
( variance , [{ Name: batch_norm2d_17.w_2, Initialized: 1, Ptr: 0x55b7bea3c230 }]),  
( scale , [{ Name: batch_norm2d_17.w_0, Initialized: 1, Ptr: 0x55b7b796ee40 }]),  
( bias , [{ Name: batch_norm2d_17.b_0, Initialized: 1, Ptr: 0x55b7bea3a4b0 }]), ]} 
I1018 08:53:57.320605 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.320608 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.320648 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c9f00b70)  to Conv2dGradNodeFinal (addr: 0x55b7c9f00670)
I1018 08:53:57.320652 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c9f00b70)  to GradNodeAccumulation (addr: 0x55b7b796fe00)
I1018 08:53:57.320655 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c9f00b70)  to GradNodeAccumulation (addr: 0x55b7bea3a5d0)
I1018 08:53:57.320660 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.320693 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f67930 }]),  
( mean , [{ Name: batch_norm2d_17.w_1, Initialized: 1, Ptr: 0x55b7bea3b370 }]),  
( variance , [{ Name: batch_norm2d_17.w_2, Initialized: 1, Ptr: 0x55b7bea3c230 }]),  
( scale , [{ Name: batch_norm2d_17.w_0, Initialized: 1, Ptr: 0x55b7b796ee40 }]),  
( bias , [{ Name: batch_norm2d_17.b_0, Initialized: 1, Ptr: 0x55b7bea3a4b0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f02af0 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f02cc0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f02e90 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f034e0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f01320 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c9f01440 }]), ] } 
I1018 08:53:57.320712 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.320716 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.320806 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b757d5c0)  to ReluGradNode (addr: 0x55b7b70ecbf0)
I1018 08:53:57.320816 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b757d5c0)  to GradNodeAccumulation (addr: 0x55b7b759fa30)
I1018 08:53:57.320842 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.320847 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.320865 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f018c0 }]),  
( mean , [{ Name: batch_norm2d_15.w_1, Initialized: 1, Ptr: 0x55b7c4d5f940 }]),  
( variance , [{ Name: batch_norm2d_15.w_2, Initialized: 1, Ptr: 0x55b7c4d60800 }]),  
( scale , [{ Name: batch_norm2d_15.w_0, Initialized: 1, Ptr: 0x55b7b759f910 }]),  
( bias , [{ Name: batch_norm2d_15.b_0, Initialized: 1, Ptr: 0x55b7b75a2ec0 }]), ]} 
I1018 08:53:57.320876 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.320879 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.320920 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b7798470)  to Conv2dGradNodeFinal (addr: 0x55b7b757d5c0)
I1018 08:53:57.320924 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b7798470)  to GradNodeAccumulation (addr: 0x55b7b3f217c0)
I1018 08:53:57.320926 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b7798470)  to GradNodeAccumulation (addr: 0x55b7b75a2fe0)
I1018 08:53:57.320932 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.320966 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f018c0 }]),  
( mean , [{ Name: batch_norm2d_15.w_1, Initialized: 1, Ptr: 0x55b7c4d5f940 }]),  
( variance , [{ Name: batch_norm2d_15.w_2, Initialized: 1, Ptr: 0x55b7c4d60800 }]),  
( scale , [{ Name: batch_norm2d_15.w_0, Initialized: 1, Ptr: 0x55b7b759f910 }]),  
( bias , [{ Name: batch_norm2d_15.b_0, Initialized: 1, Ptr: 0x55b7b75a2ec0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f01ef0 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b757c440 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b757c610 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08bd60 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b7798c20 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b7798d40 }]), ] } 
I1018 08:53:57.320974 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.320983 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f02af0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f01ef0 }]), ]} 
I1018 08:53:57.321005 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7b877bef0)  to BatchNormGradNode (addr: 0x55b7c9f00b70)
I1018 08:53:57.321009 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7b877bef0)  to BatchNormGradNode (addr: 0x55b7b7798470)
I1018 08:53:57.321012 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.321023 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f02af0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f01ef0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08bf60 }]), ] } 
I1018 08:53:57.321036 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.321039 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.321045 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08bf60 }]), ]} 
I1018 08:53:57.321059 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b877ce40)  to AddGradNode (addr: 0x55b7b877bef0)
I1018 08:53:57.321069 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.321079 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08bf60 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f02af0 }]), ] } 
I1018 08:53:57.321096 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.321099 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.321221 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c9f042d0)  to ReluGradNode (addr: 0x55b7b877ce40)
I1018 08:53:57.321228 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c9f042d0)  to GradNodeAccumulation (addr: 0x55b7bea3d490)
I1018 08:53:57.321257 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.321260 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.321280 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f01ef0 }]),  
( mean , [{ Name: batch_norm2d_18.w_1, Initialized: 1, Ptr: 0x55b7c49fade0 }]),  
( variance , [{ Name: batch_norm2d_18.w_2, Initialized: 1, Ptr: 0x55b7c49fbca0 }]),  
( scale , [{ Name: batch_norm2d_18.w_0, Initialized: 1, Ptr: 0x55b7bea3d370 }]),  
( bias , [{ Name: batch_norm2d_18.b_0, Initialized: 1, Ptr: 0x55b7c49f9f20 }]), ]} 
I1018 08:53:57.321291 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.321295 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.321336 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c9f04cd0)  to Conv2dGradNodeFinal (addr: 0x55b7c9f042d0)
I1018 08:53:57.321339 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c9f04cd0)  to GradNodeAccumulation (addr: 0x55b7c49f9190)
I1018 08:53:57.321342 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c9f04cd0)  to GradNodeAccumulation (addr: 0x55b7c49fa040)
I1018 08:53:57.321347 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.321380 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f01ef0 }]),  
( mean , [{ Name: batch_norm2d_18.w_1, Initialized: 1, Ptr: 0x55b7c49fade0 }]),  
( variance , [{ Name: batch_norm2d_18.w_2, Initialized: 1, Ptr: 0x55b7c49fbca0 }]),  
( scale , [{ Name: batch_norm2d_18.w_0, Initialized: 1, Ptr: 0x55b7bea3d370 }]),  
( bias , [{ Name: batch_norm2d_18.b_0, Initialized: 1, Ptr: 0x55b7c49f9f20 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08bf60 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877c370 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f03a80 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f041c0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877bba0 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b757cff0 }]), ] } 
I1018 08:53:57.321394 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.321398 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.321403 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08bf60 }]), ]} 
I1018 08:53:57.321419 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7ceef9d80)  to BatchNormGradNode (addr: 0x55b7c9f04cd0)
I1018 08:53:57.321422 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.321431 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08bf60 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f056c0 }]), ] } 
I1018 08:53:57.321445 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.321453 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.321560 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7ceefb1e0)  to ReluGradNode (addr: 0x55b7ceef9d80)
I1018 08:53:57.321566 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7ceefb1e0)  to GradNodeAccumulation (addr: 0x55b7c49fccb0)
I1018 08:53:57.321594 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.321599 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.321617 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08bf60 }]),  
( mean , [{ Name: batch_norm2d_19.w_1, Initialized: 1, Ptr: 0x55b7b8788190 }]),  
( variance , [{ Name: batch_norm2d_19.w_2, Initialized: 1, Ptr: 0x55b7b8789050 }]),  
( scale , [{ Name: batch_norm2d_19.w_0, Initialized: 1, Ptr: 0x55b7c49fcb90 }]),  
( bias , [{ Name: batch_norm2d_19.b_0, Initialized: 1, Ptr: 0x55b7b87872d0 }]), ]} 
I1018 08:53:57.321628 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.321630 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.321672 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7ceefbd70)  to Conv2dGradNodeFinal (addr: 0x55b7ceefb1e0)
I1018 08:53:57.321676 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7ceefbd70)  to GradNodeAccumulation (addr: 0x55b7b8786540)
I1018 08:53:57.321678 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7ceefbd70)  to GradNodeAccumulation (addr: 0x55b7b87873f0)
I1018 08:53:57.321683 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.321717 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08bf60 }]),  
( mean , [{ Name: batch_norm2d_19.w_1, Initialized: 1, Ptr: 0x55b7b8788190 }]),  
( variance , [{ Name: batch_norm2d_19.w_2, Initialized: 1, Ptr: 0x55b7b8789050 }]),  
( scale , [{ Name: batch_norm2d_19.w_0, Initialized: 1, Ptr: 0x55b7c49fcb90 }]),  
( bias , [{ Name: batch_norm2d_19.b_0, Initialized: 1, Ptr: 0x55b7b87872d0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefa760 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefa930 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefab00 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefb0d0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefc520 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7ceefc640 }]), ] } 
I1018 08:53:57.321727 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.321734 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefa760 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f02af0 }]), ]} 
I1018 08:53:57.321756 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7c4b919c0)  to BatchNormGradNode (addr: 0x55b7ceefbd70)
I1018 08:53:57.321759 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7c4b919c0)  to ReluGradNode (addr: 0x55b7b877ce40)
I1018 08:53:57.321763 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.321774 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefa760 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f02af0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b91e40 }]), ] } 
I1018 08:53:57.321786 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.321789 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.321795 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b91e40 }]), ]} 
I1018 08:53:57.321815 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7c4b92a30)  to AddGradNode (addr: 0x55b7c4b919c0)
I1018 08:53:57.321818 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.321826 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b91e40 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefa760 }]), ] } 
I1018 08:53:57.321870 4097533 eager_method.cc:2158] Tensor:  set use_gpudnn = 0
I1018 08:53:57.321878 4097533 eager_op_function.cc:35671] CurrentDeviceId: 0 from 0
I1018 08:53:57.321882 4097533 dygraph_functions.cc:69244] Running AD API: pool2d
I1018 08:53:57.321888 4097533 dygraph_functions.cc:69300] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b91e40 }]), ]} 
I1018 08:53:57.321902 4097533 pool2d_kernel.cc:153] [Pool2d] pooling type: avg exclusive: 1 adaptive: 1
I1018 08:53:57.321939 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Pool2dGradNode (addr: 0x55b7c4b93310)  to ReluGradNode (addr: 0x55b7c4b92a30)
I1018 08:53:57.321944 4097533 dygraph_functions.cc:69376] Finish AD API: pool2d
I1018 08:53:57.321954 4097533 dygraph_functions.cc:69390] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b91e40 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b91f60 }]), ] } 
I1018 08:53:57.321967 4097533 eager_op_function.cc:9777] CurrentDeviceId: 0 from 0
I1018 08:53:57.321970 4097533 dygraph_functions.cc:20255] Running AD API: flatten
I1018 08:53:57.321976 4097533 dygraph_functions.cc:20311] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b91f60 }]), ]} 
I1018 08:53:57.321983 4097533 dygraph_api.cc:208] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.321990 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from FlattenGradNode (addr: 0x55b7c4b94230)  to Pool2dGradNode (addr: 0x55b7c4b93310)
I1018 08:53:57.321993 4097533 dygraph_functions.cc:20387] Finish AD API: flatten
I1018 08:53:57.322005 4097533 dygraph_functions.cc:20404] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b91f60 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b93970 }]),  
( xshape , [{ Name: None, Initialized: 0, Ptr: 0x55b7c4b93b40 }]), ] } 
I1018 08:53:57.322017 4097533 dygraph_functions.cc:67019] Running AD API: matmul
I1018 08:53:57.322026 4097533 dygraph_functions.cc:67081] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b93970 }]),  
( y , [{ Name: linear_0.w_0, Initialized: 1, Ptr: 0x55b7b6f66780 }]), ]} 
I1018 08:53:57.322049 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from MatmulGradNode (addr: 0x55b7c4b94ef0)  to FlattenGradNode (addr: 0x55b7c4b94230)
I1018 08:53:57.322053 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from MatmulGradNode (addr: 0x55b7c4b94ef0)  to GradNodeAccumulation (addr: 0x55b7b878a500)
I1018 08:53:57.322057 4097533 dygraph_functions.cc:67152] Finish AD API: matmul
I1018 08:53:57.322068 4097533 dygraph_functions.cc:67169] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b93970 }]),  
( y , [{ Name: linear_0.w_0, Initialized: 1, Ptr: 0x55b7b6f66780 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b95370 }]), ] } 
I1018 08:53:57.322072 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.322079 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b95370 }]),  
( y , [{ Name: linear_0.b_0, Initialized: 1, Ptr: 0x55b7b878a3e0 }]), ]} 
I1018 08:53:57.322100 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7c4b95f30)  to MatmulGradNode (addr: 0x55b7c4b94ef0)
I1018 08:53:57.322103 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7c4b95f30)  to GradNodeAccumulation (addr: 0x55b7b6f66d60)
I1018 08:53:57.322111 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.322122 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b95370 }]),  
( y , [{ Name: linear_0.b_0, Initialized: 1, Ptr: 0x55b7b878a3e0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b965f0 }]), ] } 
I1018 08:53:57.322285 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.322290 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.322427 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b78e4320)  to GradNodeAccumulation (addr: 0x55b77381dd90)
I1018 08:53:57.322464 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.322468 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.322495 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b93850 }]),  
( mean , [{ Name: batch_norm2d_0.w_1, Initialized: 1, Ptr: 0x55b7c4ee9a30 }]),  
( variance , [{ Name: batch_norm2d_0.w_2, Initialized: 1, Ptr: 0x55b7ceb07ff0 }]),  
( scale , [{ Name: batch_norm2d_0.w_0, Initialized: 1, Ptr: 0x55b77346a070 }]),  
( bias , [{ Name: batch_norm2d_0.b_0, Initialized: 1, Ptr: 0x55b7b6f6d4b0 }]), ]} 
I1018 08:53:57.322508 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.322511 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.322563 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7cb2feac0)  to Conv2dGradNodeFinal (addr: 0x55b7b78e4320)
I1018 08:53:57.322567 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7cb2feac0)  to GradNodeAccumulation (addr: 0x55b77ed32260)
I1018 08:53:57.322571 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7cb2feac0)  to GradNodeAccumulation (addr: 0x55b77f0e4620)
I1018 08:53:57.322575 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.322609 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b93850 }]),  
( mean , [{ Name: batch_norm2d_0.w_1, Initialized: 1, Ptr: 0x55b7c4ee9a30 }]),  
( variance , [{ Name: batch_norm2d_0.w_2, Initialized: 1, Ptr: 0x55b7ceb07ff0 }]),  
( scale , [{ Name: batch_norm2d_0.w_0, Initialized: 1, Ptr: 0x55b77346a070 }]),  
( bias , [{ Name: batch_norm2d_0.b_0, Initialized: 1, Ptr: 0x55b7b6f6d4b0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b93970 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b963b0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b964d0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b95370 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b95490 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7ceefa930 }]), ] } 
I1018 08:53:57.322624 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.322628 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.322634 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b93970 }]), ]} 
I1018 08:53:57.322649 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b1a58b50)  to BatchNormGradNode (addr: 0x55b7cb2feac0)
I1018 08:53:57.322654 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.322662 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b93970 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7ce10 }]), ] } 
I1018 08:53:57.322682 4097533 eager_op_function.cc:35671] CurrentDeviceId: 0 from 0
I1018 08:53:57.322686 4097533 dygraph_functions.cc:69244] Running AD API: pool2d
I1018 08:53:57.322697 4097533 dygraph_functions.cc:69300] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7ce10 }]), ]} 
I1018 08:53:57.322712 4097533 pool2d_kernel.cc:153] [Pool2d] pooling type: max exclusive: 1 adaptive: 0
I1018 08:53:57.322733 4097533 pool2d_kernel.cc:198] [Pool2d] extra_input_size: 0
I1018 08:53:57.322755 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Pool2dGradNode (addr: 0x55b7c9f03a70)  to ReluGradNode (addr: 0x55b7b1a58b50)
I1018 08:53:57.322760 4097533 dygraph_functions.cc:69376] Finish AD API: pool2d
I1018 08:53:57.322769 4097533 dygraph_functions.cc:69390] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7ce10 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b93970 }]), ] } 
I1018 08:53:57.322790 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.322793 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.322894 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c2d5e1f0)  to Pool2dGradNode (addr: 0x55b7c9f03a70)
I1018 08:53:57.322901 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c2d5e1f0)  to GradNodeAccumulation (addr: 0x55b773982760)
I1018 08:53:57.322930 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.322934 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.322954 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4169780 }]),  
( mean , [{ Name: batch_norm2d_1.w_1, Initialized: 1, Ptr: 0x55b7b64bb520 }]),  
( variance , [{ Name: batch_norm2d_1.w_2, Initialized: 1, Ptr: 0x55b7b68ca2d0 }]),  
( scale , [{ Name: batch_norm2d_1.w_0, Initialized: 1, Ptr: 0x55b7b68c8170 }]),  
( bias , [{ Name: batch_norm2d_1.b_0, Initialized: 1, Ptr: 0x55b7b76da540 }]), ]} 
I1018 08:53:57.322964 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.322968 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.323009 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7cfe31d60)  to Conv2dGradNodeFinal (addr: 0x55b7c2d5e1f0)
I1018 08:53:57.323014 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7cfe31d60)  to GradNodeAccumulation (addr: 0x55b773982b90)
I1018 08:53:57.323016 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7cfe31d60)  to GradNodeAccumulation (addr: 0x55b773f00630)
I1018 08:53:57.323024 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.323056 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4169780 }]),  
( mean , [{ Name: batch_norm2d_1.w_1, Initialized: 1, Ptr: 0x55b7b64bb520 }]),  
( variance , [{ Name: batch_norm2d_1.w_2, Initialized: 1, Ptr: 0x55b7b68ca2d0 }]),  
( scale , [{ Name: batch_norm2d_1.w_0, Initialized: 1, Ptr: 0x55b7b68c8170 }]),  
( bias , [{ Name: batch_norm2d_1.b_0, Initialized: 1, Ptr: 0x55b7b76da540 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e8bb0 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b77398c530 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701bae0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08c2c0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77982f0 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b70ee1c0 }]), ] } 
I1018 08:53:57.323071 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.323074 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.323081 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e8bb0 }]), ]} 
I1018 08:53:57.323100 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b76fbc80)  to BatchNormGradNode (addr: 0x55b7cfe31d60)
I1018 08:53:57.323105 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.323113 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e8bb0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4169a80 }]), ] } 
I1018 08:53:57.323129 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.323132 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.323230 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c22d23b0)  to ReluGradNode (addr: 0x55b7b76fbc80)
I1018 08:53:57.323236 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c22d23b0)  to GradNodeAccumulation (addr: 0x55b7bd3da270)
I1018 08:53:57.323266 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.323269 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.323288 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e8bb0 }]),  
( mean , [{ Name: batch_norm2d_2.w_1, Initialized: 1, Ptr: 0x55b7b76e6e10 }]),  
( variance , [{ Name: batch_norm2d_2.w_2, Initialized: 1, Ptr: 0x55b7c2207b30 }]),  
( scale , [{ Name: batch_norm2d_2.w_0, Initialized: 1, Ptr: 0x55b7bd3da150 }]),  
( bias , [{ Name: batch_norm2d_2.b_0, Initialized: 1, Ptr: 0x55b7bd3d86c0 }]), ]} 
I1018 08:53:57.323299 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.323302 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.323343 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b77cd7b0)  to Conv2dGradNodeFinal (addr: 0x55b7c22d23b0)
I1018 08:53:57.323347 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b77cd7b0)  to GradNodeAccumulation (addr: 0x55b7c4ee5b40)
I1018 08:53:57.323350 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b77cd7b0)  to GradNodeAccumulation (addr: 0x55b7bd3d87e0)
I1018 08:53:57.323356 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.323390 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e8bb0 }]),  
( mean , [{ Name: batch_norm2d_2.w_1, Initialized: 1, Ptr: 0x55b7b76e6e10 }]),  
( variance , [{ Name: batch_norm2d_2.w_2, Initialized: 1, Ptr: 0x55b7c2207b30 }]),  
( scale , [{ Name: batch_norm2d_2.w_0, Initialized: 1, Ptr: 0x55b7bd3da150 }]),  
( bias , [{ Name: batch_norm2d_2.b_0, Initialized: 1, Ptr: 0x55b7bd3d86c0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7fdd0 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cb9e50 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e3a50 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1e960 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1ea80 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b773923320 }]), ] } 
I1018 08:53:57.323400 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.323408 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7fdd0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b93970 }]), ]} 
I1018 08:53:57.323431 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7bacd7fb0)  to BatchNormGradNode (addr: 0x55b7b77cd7b0)
I1018 08:53:57.323434 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7bacd7fb0)  to Pool2dGradNode (addr: 0x55b7c9f03a70)
I1018 08:53:57.323443 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.323455 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7fdd0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b93970 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77980c0 }]), ] } 
I1018 08:53:57.323468 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.323472 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.323477 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77980c0 }]), ]} 
I1018 08:53:57.323493 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b77324be20)  to AddGradNode (addr: 0x55b7bacd7fb0)
I1018 08:53:57.323496 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.323504 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77980c0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7fdd0 }]), ] } 
I1018 08:53:57.323521 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.323524 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.323624 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b70ef960)  to ReluGradNode (addr: 0x55b77324be20)
I1018 08:53:57.323630 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b70ef960)  to GradNodeAccumulation (addr: 0x55b7b76e5550)
I1018 08:53:57.323659 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.323662 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.323683 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77980c0 }]),  
( mean , [{ Name: batch_norm2d_3.w_1, Initialized: 1, Ptr: 0x55b7b778e800 }]),  
( variance , [{ Name: batch_norm2d_3.w_2, Initialized: 1, Ptr: 0x55b7b3f224a0 }]),  
( scale , [{ Name: batch_norm2d_3.w_0, Initialized: 1, Ptr: 0x55b7bd3daab0 }]),  
( bias , [{ Name: batch_norm2d_3.b_0, Initialized: 1, Ptr: 0x55b7b87810f0 }]), ]} 
I1018 08:53:57.323693 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.323695 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.323740 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c4168f10)  to Conv2dGradNodeFinal (addr: 0x55b7b70ef960)
I1018 08:53:57.323743 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c4168f10)  to GradNodeAccumulation (addr: 0x55b7b76e6360)
I1018 08:53:57.323746 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c4168f10)  to GradNodeAccumulation (addr: 0x55b7b68c8390)
I1018 08:53:57.323752 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.323786 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77980c0 }]),  
( mean , [{ Name: batch_norm2d_3.w_1, Initialized: 1, Ptr: 0x55b7b778e800 }]),  
( variance , [{ Name: batch_norm2d_3.w_2, Initialized: 1, Ptr: 0x55b7b3f224a0 }]),  
( scale , [{ Name: batch_norm2d_3.w_0, Initialized: 1, Ptr: 0x55b7bd3daab0 }]),  
( bias , [{ Name: batch_norm2d_3.b_0, Initialized: 1, Ptr: 0x55b7b87810f0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb088720 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b7795f10 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b77960e0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd9b70 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7803ae590 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7803ae6b0 }]), ] } 
I1018 08:53:57.323804 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.323807 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.323813 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb088720 }]), ]} 
I1018 08:53:57.323828 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b77334c090)  to BatchNormGradNode (addr: 0x55b7c4168f10)
I1018 08:53:57.323832 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.323841 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb088720 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe31370 }]), ] } 
I1018 08:53:57.323855 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.323859 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.323956 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c416d780)  to ReluGradNode (addr: 0x55b77334c090)
I1018 08:53:57.323961 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c416d780)  to GradNodeAccumulation (addr: 0x55b7b68c6d90)
I1018 08:53:57.323989 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.323992 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.324011 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb088720 }]),  
( mean , [{ Name: batch_norm2d_4.w_1, Initialized: 1, Ptr: 0x55b7c2209c90 }]),  
( variance , [{ Name: batch_norm2d_4.w_2, Initialized: 1, Ptr: 0x55b7c220ab50 }]),  
( scale , [{ Name: batch_norm2d_4.w_0, Initialized: 1, Ptr: 0x55b7b68c6c70 }]),  
( bias , [{ Name: batch_norm2d_4.b_0, Initialized: 1, Ptr: 0x55b7b4b4a7c0 }]), ]} 
I1018 08:53:57.324023 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.324025 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.324065 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c416bf80)  to Conv2dGradNodeFinal (addr: 0x55b7c416d780)
I1018 08:53:57.324069 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c416bf80)  to GradNodeAccumulation (addr: 0x55b7b4b49ae0)
I1018 08:53:57.324072 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c416bf80)  to GradNodeAccumulation (addr: 0x55b7b4b4a8e0)
I1018 08:53:57.324077 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.324111 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb088720 }]),  
( mean , [{ Name: batch_norm2d_4.w_1, Initialized: 1, Ptr: 0x55b7c2209c90 }]),  
( variance , [{ Name: batch_norm2d_4.w_2, Initialized: 1, Ptr: 0x55b7c220ab50 }]),  
( scale , [{ Name: batch_norm2d_4.w_0, Initialized: 1, Ptr: 0x55b7b68c6c70 }]),  
( bias , [{ Name: batch_norm2d_4.b_0, Initialized: 1, Ptr: 0x55b7b4b4a7c0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e7040 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779e570 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779e740 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701ed80 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701ab90 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b7796c30 }]), ] } 
I1018 08:53:57.324120 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.324129 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e7040 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7fdd0 }]), ]} 
I1018 08:53:57.324151 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7c4b8df50)  to BatchNormGradNode (addr: 0x55b7c416bf80)
I1018 08:53:57.324159 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7c4b8df50)  to ReluGradNode (addr: 0x55b77324be20)
I1018 08:53:57.324162 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.324174 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e7040 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7fdd0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f77f00 }]), ] } 
I1018 08:53:57.324187 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.324190 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.324195 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f77f00 }]), ]} 
I1018 08:53:57.324210 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7733494c0)  to AddGradNode (addr: 0x55b7c4b8df50)
I1018 08:53:57.324214 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.324223 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f77f00 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e7040 }]), ] } 
I1018 08:53:57.324244 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.324249 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.324348 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4e78c00)  to ReluGradNode (addr: 0x55b7733494c0)
I1018 08:53:57.324354 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4e78c00)  to GradNodeAccumulation (addr: 0x55b7c4ee2d80)
I1018 08:53:57.324383 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.324386 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.324405 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f77f00 }]),  
( mean , [{ Name: batch_norm2d_6.w_1, Initialized: 1, Ptr: 0x55b7cc394970 }]),  
( variance , [{ Name: batch_norm2d_6.w_2, Initialized: 1, Ptr: 0x55b7cc395a60 }]),  
( scale , [{ Name: batch_norm2d_6.w_0, Initialized: 1, Ptr: 0x55b7c4ee3ce0 }]),  
( bias , [{ Name: batch_norm2d_6.b_0, Initialized: 1, Ptr: 0x55b7c4ee4b90 }]), ]} 
I1018 08:53:57.324416 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.324419 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.324461 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b6f7df70)  to Conv2dGradNodeFinal (addr: 0x55b7c4e78c00)
I1018 08:53:57.324465 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b6f7df70)  to GradNodeAccumulation (addr: 0x55b7c4ee3e00)
I1018 08:53:57.324468 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b6f7df70)  to GradNodeAccumulation (addr: 0x55b7c4ee4cb0)
I1018 08:53:57.324473 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.324507 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f77f00 }]),  
( mean , [{ Name: batch_norm2d_6.w_1, Initialized: 1, Ptr: 0x55b7cc394970 }]),  
( variance , [{ Name: batch_norm2d_6.w_2, Initialized: 1, Ptr: 0x55b7cc395a60 }]),  
( scale , [{ Name: batch_norm2d_6.w_0, Initialized: 1, Ptr: 0x55b7c4ee3ce0 }]),  
( bias , [{ Name: batch_norm2d_6.b_0, Initialized: 1, Ptr: 0x55b7c4ee4b90 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c416a620 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c416a740 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e7c8d0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd76a0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd77c0 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7bacd78e0 }]), ] } 
I1018 08:53:57.324525 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.324528 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.324534 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c416a620 }]), ]} 
I1018 08:53:57.324549 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b773371db0)  to BatchNormGradNode (addr: 0x55b7b6f7df70)
I1018 08:53:57.324553 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.324561 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c416a620 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7f8a0 }]), ] } 
I1018 08:53:57.324576 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.324579 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.324676 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4b94cb0)  to ReluGradNode (addr: 0x55b773371db0)
I1018 08:53:57.324681 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4b94cb0)  to GradNodeAccumulation (addr: 0x55b7cc396a70)
I1018 08:53:57.324709 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.324713 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.324733 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c416a620 }]),  
( mean , [{ Name: batch_norm2d_7.w_1, Initialized: 1, Ptr: 0x55b7c4d65af0 }]),  
( variance , [{ Name: batch_norm2d_7.w_2, Initialized: 1, Ptr: 0x55b7b778c480 }]),  
( scale , [{ Name: batch_norm2d_7.w_0, Initialized: 1, Ptr: 0x55b7cc396950 }]),  
( bias , [{ Name: batch_norm2d_7.b_0, Initialized: 1, Ptr: 0x55b7c4d64c30 }]), ]} 
I1018 08:53:57.324743 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.324745 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.324787 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c2d5eb40)  to Conv2dGradNodeFinal (addr: 0x55b7c4b94cb0)
I1018 08:53:57.324791 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c2d5eb40)  to GradNodeAccumulation (addr: 0x55b7c4d63ea0)
I1018 08:53:57.324795 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c2d5eb40)  to GradNodeAccumulation (addr: 0x55b7c4d64d50)
I1018 08:53:57.324800 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.324833 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c416a620 }]),  
( mean , [{ Name: batch_norm2d_7.w_1, Initialized: 1, Ptr: 0x55b7c4d65af0 }]),  
( variance , [{ Name: batch_norm2d_7.w_2, Initialized: 1, Ptr: 0x55b7b778c480 }]),  
( scale , [{ Name: batch_norm2d_7.w_0, Initialized: 1, Ptr: 0x55b7cc396950 }]),  
( bias , [{ Name: batch_norm2d_7.b_0, Initialized: 1, Ptr: 0x55b7c4d64c30 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f702c0 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f71580 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f71750 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78ec950 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78eca70 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b78ecb90 }]), ] } 
I1018 08:53:57.324852 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.324860 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.324954 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b6f7cf80)  to ReluGradNode (addr: 0x55b7733494c0)
I1018 08:53:57.324959 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b6f7cf80)  to GradNodeAccumulation (addr: 0x55b7b85db7c0)
I1018 08:53:57.324986 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.324990 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.325009 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b757c610 }]),  
( mean , [{ Name: batch_norm2d_5.w_1, Initialized: 1, Ptr: 0x55b7bb903150 }]),  
( variance , [{ Name: batch_norm2d_5.w_2, Initialized: 1, Ptr: 0x55b7bb904240 }]),  
( scale , [{ Name: batch_norm2d_5.w_0, Initialized: 1, Ptr: 0x55b7b3f22b10 }]),  
( bias , [{ Name: batch_norm2d_5.b_0, Initialized: 1, Ptr: 0x55b7b85dd420 }]), ]} 
I1018 08:53:57.325019 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.325022 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.325064 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c2d5c8e0)  to Conv2dGradNodeFinal (addr: 0x55b7b6f7cf80)
I1018 08:53:57.325067 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c2d5c8e0)  to GradNodeAccumulation (addr: 0x55b7b85dc690)
I1018 08:53:57.325070 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c2d5c8e0)  to GradNodeAccumulation (addr: 0x55b7b85dd540)
I1018 08:53:57.325076 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.325109 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b757c610 }]),  
( mean , [{ Name: batch_norm2d_5.w_1, Initialized: 1, Ptr: 0x55b7bb903150 }]),  
( variance , [{ Name: batch_norm2d_5.w_2, Initialized: 1, Ptr: 0x55b7bb904240 }]),  
( scale , [{ Name: batch_norm2d_5.w_0, Initialized: 1, Ptr: 0x55b7b3f22b10 }]),  
( bias , [{ Name: batch_norm2d_5.b_0, Initialized: 1, Ptr: 0x55b7b85dd420 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b94160 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b94330 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b94500 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b96290 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c2d5d090 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b78e8470 }]), ] } 
I1018 08:53:57.325119 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.325127 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f702c0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b94160 }]), ]} 
I1018 08:53:57.325150 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7c22d0760)  to BatchNormGradNode (addr: 0x55b7c2d5eb40)
I1018 08:53:57.325153 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7c22d0760)  to BatchNormGradNode (addr: 0x55b7c2d5c8e0)
I1018 08:53:57.325156 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.325168 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f702c0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b94160 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c2d5c420 }]), ] } 
I1018 08:53:57.325181 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.325184 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.325191 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c2d5c420 }]), ]} 
I1018 08:53:57.325223 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b4d8d1e0)  to AddGradNode (addr: 0x55b7c22d0760)
I1018 08:53:57.325228 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.325237 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c2d5c420 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f702c0 }]), ] } 
I1018 08:53:57.325258 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.325260 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.325364 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4b8c320)  to ReluGradNode (addr: 0x55b7b4d8d1e0)
I1018 08:53:57.325369 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4b8c320)  to GradNodeAccumulation (addr: 0x55b7b778d7d0)
I1018 08:53:57.325399 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.325402 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.325421 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b94160 }]),  
( mean , [{ Name: batch_norm2d_8.w_1, Initialized: 1, Ptr: 0x55b7c5cf24d0 }]),  
( variance , [{ Name: batch_norm2d_8.w_2, Initialized: 1, Ptr: 0x55b7c5cf3390 }]),  
( scale , [{ Name: batch_norm2d_8.w_0, Initialized: 1, Ptr: 0x55b7bb905130 }]),  
( bias , [{ Name: batch_norm2d_8.b_0, Initialized: 1, Ptr: 0x55b7c5cf1610 }]), ]} 
I1018 08:53:57.325433 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.325435 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.325476 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c4cb9100)  to Conv2dGradNodeFinal (addr: 0x55b7c4b8c320)
I1018 08:53:57.325480 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c4cb9100)  to GradNodeAccumulation (addr: 0x55b7c5cf0810)
I1018 08:53:57.325484 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c4cb9100)  to GradNodeAccumulation (addr: 0x55b7c5cf1730)
I1018 08:53:57.325489 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.325522 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b94160 }]),  
( mean , [{ Name: batch_norm2d_8.w_1, Initialized: 1, Ptr: 0x55b7c5cf24d0 }]),  
( variance , [{ Name: batch_norm2d_8.w_2, Initialized: 1, Ptr: 0x55b7c5cf3390 }]),  
( scale , [{ Name: batch_norm2d_8.w_0, Initialized: 1, Ptr: 0x55b7bb905130 }]),  
( bias , [{ Name: batch_norm2d_8.b_0, Initialized: 1, Ptr: 0x55b7c5cf1610 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c2d5c420 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cfe30580 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4b8f3e0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f71cc0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cb98b0 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c4b8cab0 }]), ] } 
I1018 08:53:57.325536 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.325538 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.325544 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c2d5c420 }]), ]} 
I1018 08:53:57.325559 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7c4b8d290)  to BatchNormGradNode (addr: 0x55b7c4cb9100)
I1018 08:53:57.325563 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.325572 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c2d5c420 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1e0d0 }]), ] } 
I1018 08:53:57.325591 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.325594 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.325690 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4b90380)  to ReluGradNode (addr: 0x55b7c4b8d290)
I1018 08:53:57.325695 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4b90380)  to GradNodeAccumulation (addr: 0x55b7cc6c36b0)
I1018 08:53:57.325724 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.325727 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.325747 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c2d5c420 }]),  
( mean , [{ Name: batch_norm2d_9.w_1, Initialized: 1, Ptr: 0x55b7b7056fb0 }]),  
( variance , [{ Name: batch_norm2d_9.w_2, Initialized: 1, Ptr: 0x55b7b7057e70 }]),  
( scale , [{ Name: batch_norm2d_9.w_0, Initialized: 1, Ptr: 0x55b7cc6c3590 }]),  
( bias , [{ Name: batch_norm2d_9.b_0, Initialized: 1, Ptr: 0x55b7b70560f0 }]), ]} 
I1018 08:53:57.325757 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.325760 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.325801 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b779db70)  to Conv2dGradNodeFinal (addr: 0x55b7c4b90380)
I1018 08:53:57.325805 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b779db70)  to GradNodeAccumulation (addr: 0x55b7cc6c4610)
I1018 08:53:57.325809 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b779db70)  to GradNodeAccumulation (addr: 0x55b7b7056210)
I1018 08:53:57.325814 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.325846 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c2d5c420 }]),  
( mean , [{ Name: batch_norm2d_9.w_1, Initialized: 1, Ptr: 0x55b7b7056fb0 }]),  
( variance , [{ Name: batch_norm2d_9.w_2, Initialized: 1, Ptr: 0x55b7b7057e70 }]),  
( scale , [{ Name: batch_norm2d_9.w_0, Initialized: 1, Ptr: 0x55b7cc6c3590 }]),  
( bias , [{ Name: batch_norm2d_9.b_0, Initialized: 1, Ptr: 0x55b7b70560f0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1ec00 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1edd0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1efa0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779e320 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c23e51c0 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c23e52e0 }]), ] } 
I1018 08:53:57.325856 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.325865 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1ec00 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f702c0 }]), ]} 
I1018 08:53:57.325887 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7b701c470)  to BatchNormGradNode (addr: 0x55b7b779db70)
I1018 08:53:57.325891 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7b701c470)  to ReluGradNode (addr: 0x55b7b4d8d1e0)
I1018 08:53:57.325893 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.325906 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1ec00 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f702c0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e7c410 }]), ] } 
I1018 08:53:57.325922 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.325927 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.325932 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e7c410 }]), ]} 
I1018 08:53:57.325946 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b77cfe00)  to AddGradNode (addr: 0x55b7b701c470)
I1018 08:53:57.325950 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.325958 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e7c410 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1ec00 }]), ] } 
I1018 08:53:57.325979 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.325984 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.326084 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c22ce660)  to ReluGradNode (addr: 0x55b7b77cfe00)
I1018 08:53:57.326089 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c22ce660)  to GradNodeAccumulation (addr: 0x55b7c4edec60)
I1018 08:53:57.326117 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.326121 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.326140 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e7c410 }]),  
( mean , [{ Name: batch_norm2d_11.w_1, Initialized: 1, Ptr: 0x55b7b7059750 }]),  
( variance , [{ Name: batch_norm2d_11.w_2, Initialized: 1, Ptr: 0x55b7c5ced740 }]),  
( scale , [{ Name: batch_norm2d_11.w_0, Initialized: 1, Ptr: 0x55b7b4b498d0 }]),  
( bias , [{ Name: batch_norm2d_11.b_0, Initialized: 1, Ptr: 0x55b7c4ee0950 }]), ]} 
I1018 08:53:57.326151 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.326153 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.326195 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c2d5afa0)  to Conv2dGradNodeFinal (addr: 0x55b7c22ce660)
I1018 08:53:57.326200 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c2d5afa0)  to GradNodeAccumulation (addr: 0x55b7c4edfbc0)
I1018 08:53:57.326202 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c2d5afa0)  to GradNodeAccumulation (addr: 0x55b7c4ee0a70)
I1018 08:53:57.326207 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.326241 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e7c410 }]),  
( mean , [{ Name: batch_norm2d_11.w_1, Initialized: 1, Ptr: 0x55b7b7059750 }]),  
( variance , [{ Name: batch_norm2d_11.w_2, Initialized: 1, Ptr: 0x55b7c5ced740 }]),  
( scale , [{ Name: batch_norm2d_11.w_0, Initialized: 1, Ptr: 0x55b7b4b498d0 }]),  
( bias , [{ Name: batch_norm2d_11.b_0, Initialized: 1, Ptr: 0x55b7c4ee0950 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701cc90 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c2d5dc60 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e7c0b0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c2d5dd80 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c2d5b750 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c4e7b200 }]), ] } 
I1018 08:53:57.326254 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.326257 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.326263 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701cc90 }]), ]} 
I1018 08:53:57.326283 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7bacd8cf0)  to BatchNormGradNode (addr: 0x55b7c2d5afa0)
I1018 08:53:57.326287 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.326296 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701cc90 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e7b730 }]), ] } 
I1018 08:53:57.326310 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.326314 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.326411 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4e7a1f0)  to ReluGradNode (addr: 0x55b7bacd8cf0)
I1018 08:53:57.326416 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c4e7a1f0)  to GradNodeAccumulation (addr: 0x55b7c5cee750)
I1018 08:53:57.326443 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.326447 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.326467 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701cc90 }]),  
( mean , [{ Name: batch_norm2d_12.w_1, Initialized: 1, Ptr: 0x55b7cc398370 }]),  
( variance , [{ Name: batch_norm2d_12.w_2, Initialized: 1, Ptr: 0x55b7cc399230 }]),  
( scale , [{ Name: batch_norm2d_12.w_0, Initialized: 1, Ptr: 0x55b7c5cee630 }]),  
( bias , [{ Name: batch_norm2d_12.b_0, Initialized: 1, Ptr: 0x55b7cc3974b0 }]), ]} 
I1018 08:53:57.326476 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.326479 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.326519 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7bacd5c30)  to Conv2dGradNodeFinal (addr: 0x55b7c4e7a1f0)
I1018 08:53:57.326524 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7bacd5c30)  to GradNodeAccumulation (addr: 0x55b7c5cef6b0)
I1018 08:53:57.326526 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7bacd5c30)  to GradNodeAccumulation (addr: 0x55b7cc3975d0)
I1018 08:53:57.326531 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.326565 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b701cc90 }]),  
( mean , [{ Name: batch_norm2d_12.w_1, Initialized: 1, Ptr: 0x55b7cc398370 }]),  
( variance , [{ Name: batch_norm2d_12.w_2, Initialized: 1, Ptr: 0x55b7cc399230 }]),  
( scale , [{ Name: batch_norm2d_12.w_0, Initialized: 1, Ptr: 0x55b7c5cee630 }]),  
( bias , [{ Name: batch_norm2d_12.b_0, Initialized: 1, Ptr: 0x55b7cc3974b0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd6990 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd6b60 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd6d30 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd63e0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd6500 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c2d5a4f0 }]), ] } 
I1018 08:53:57.326584 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.326587 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.326679 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7cfe2f700)  to ReluGradNode (addr: 0x55b7b77cfe00)
I1018 08:53:57.326684 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7cfe2f700)  to GradNodeAccumulation (addr: 0x55b7b7058e80)
I1018 08:53:57.326710 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.326714 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.326737 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c2d5a970 }]),  
( mean , [{ Name: batch_norm2d_10.w_1, Initialized: 1, Ptr: 0x55b7b780fba0 }]),  
( variance , [{ Name: batch_norm2d_10.w_2, Initialized: 1, Ptr: 0x55b7b7810a60 }]),  
( scale , [{ Name: batch_norm2d_10.w_0, Initialized: 1, Ptr: 0x55b7b7058d60 }]),  
( bias , [{ Name: batch_norm2d_10.b_0, Initialized: 1, Ptr: 0x55b7b780ece0 }]), ]} 
I1018 08:53:57.326748 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.326750 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.326792 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c4e1f8c0)  to Conv2dGradNodeFinal (addr: 0x55b7cfe2f700)
I1018 08:53:57.326795 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c4e1f8c0)  to GradNodeAccumulation (addr: 0x55b7b780df50)
I1018 08:53:57.326798 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c4e1f8c0)  to GradNodeAccumulation (addr: 0x55b7b780ee00)
I1018 08:53:57.326803 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.326838 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c2d5a970 }]),  
( mean , [{ Name: batch_norm2d_10.w_1, Initialized: 1, Ptr: 0x55b7b780fba0 }]),  
( variance , [{ Name: batch_norm2d_10.w_2, Initialized: 1, Ptr: 0x55b7b7810a60 }]),  
( scale , [{ Name: batch_norm2d_10.w_0, Initialized: 1, Ptr: 0x55b7b7058d60 }]),  
( bias , [{ Name: batch_norm2d_10.b_0, Initialized: 1, Ptr: 0x55b7b780ece0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4edbd60 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e74e0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78e76b0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4edbc40 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4edb920 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c4e20070 }]), ] } 
I1018 08:53:57.326846 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.326855 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd6990 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4edbd60 }]), ]} 
I1018 08:53:57.326876 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7b6f7c1b0)  to BatchNormGradNode (addr: 0x55b7bacd5c30)
I1018 08:53:57.326880 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7b6f7c1b0)  to BatchNormGradNode (addr: 0x55b7c4e1f8c0)
I1018 08:53:57.326884 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.326894 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd6990 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4edbd60 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7c630 }]), ] } 
I1018 08:53:57.326908 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.326911 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.326916 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7c630 }]), ]} 
I1018 08:53:57.326931 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7c4edc7a0)  to AddGradNode (addr: 0x55b7b6f7c1b0)
I1018 08:53:57.326936 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.326944 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7c630 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd6990 }]), ] } 
I1018 08:53:57.326961 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.326968 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.327070 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b7798470)  to ReluGradNode (addr: 0x55b7c4edc7a0)
I1018 08:53:57.327075 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b7798470)  to GradNodeAccumulation (addr: 0x55b7cc39a490)
I1018 08:53:57.327105 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.327108 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.327128 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4edbd60 }]),  
( mean , [{ Name: batch_norm2d_13.w_1, Initialized: 1, Ptr: 0x55b7cc4bf110 }]),  
( variance , [{ Name: batch_norm2d_13.w_2, Initialized: 1, Ptr: 0x55b7cc4bffd0 }]),  
( scale , [{ Name: batch_norm2d_13.w_0, Initialized: 1, Ptr: 0x55b7cc39a370 }]),  
( bias , [{ Name: batch_norm2d_13.b_0, Initialized: 1, Ptr: 0x55b7cc4be250 }]), ]} 
I1018 08:53:57.327138 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.327142 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.327181 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b7798970)  to Conv2dGradNodeFinal (addr: 0x55b7b7798470)
I1018 08:53:57.327185 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b7798970)  to GradNodeAccumulation (addr: 0x55b7cc4bd4b0)
I1018 08:53:57.327188 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b7798970)  to GradNodeAccumulation (addr: 0x55b7cc4be370)
I1018 08:53:57.327193 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.327226 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4edbd60 }]),  
( mean , [{ Name: batch_norm2d_13.w_1, Initialized: 1, Ptr: 0x55b7cc4bf110 }]),  
( variance , [{ Name: batch_norm2d_13.w_2, Initialized: 1, Ptr: 0x55b7cc4bffd0 }]),  
( scale , [{ Name: batch_norm2d_13.w_0, Initialized: 1, Ptr: 0x55b7cc39a370 }]),  
( bias , [{ Name: batch_norm2d_13.b_0, Initialized: 1, Ptr: 0x55b7cc4be250 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7c630 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7c750 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f800a0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b7799120 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c1f720b0 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b6f80a60 }]), ] } 
I1018 08:53:57.327239 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.327242 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.327248 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7c630 }]), ]} 
I1018 08:53:57.327263 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b78ee380)  to BatchNormGradNode (addr: 0x55b7b7798970)
I1018 08:53:57.327266 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.327275 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7c630 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceef9360 }]), ] } 
I1018 08:53:57.327289 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.327293 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.327386 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c416abf0)  to ReluGradNode (addr: 0x55b7b78ee380)
I1018 08:53:57.327395 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7c416abf0)  to GradNodeAccumulation (addr: 0x55b7b68cc840)
I1018 08:53:57.327423 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.327427 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.327446 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7c630 }]),  
( mean , [{ Name: batch_norm2d_14.w_1, Initialized: 1, Ptr: 0x55b7b68cf3f0 }]),  
( variance , [{ Name: batch_norm2d_14.w_2, Initialized: 1, Ptr: 0x55b7b68d02b0 }]),  
( scale , [{ Name: batch_norm2d_14.w_0, Initialized: 1, Ptr: 0x55b7cc4c0e90 }]),  
( bias , [{ Name: batch_norm2d_14.b_0, Initialized: 1, Ptr: 0x55b7b68ce530 }]), ]} 
I1018 08:53:57.327456 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.327459 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.327499 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b757d550)  to Conv2dGradNodeFinal (addr: 0x55b7c416abf0)
I1018 08:53:57.327503 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b757d550)  to GradNodeAccumulation (addr: 0x55b7b68cd7a0)
I1018 08:53:57.327505 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b757d550)  to GradNodeAccumulation (addr: 0x55b7b68ce650)
I1018 08:53:57.327512 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.327544 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f7c630 }]),  
( mean , [{ Name: batch_norm2d_14.w_1, Initialized: 1, Ptr: 0x55b7b68cf3f0 }]),  
( variance , [{ Name: batch_norm2d_14.w_2, Initialized: 1, Ptr: 0x55b7b68d02b0 }]),  
( scale , [{ Name: batch_norm2d_14.w_0, Initialized: 1, Ptr: 0x55b7cc4c0e90 }]),  
( bias , [{ Name: batch_norm2d_14.b_0, Initialized: 1, Ptr: 0x55b7b68ce530 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1bed0 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1c0a0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1c270 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b78eea00 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceef9c10 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b78ee210 }]), ] } 
I1018 08:53:57.327553 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.327562 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1bed0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd6990 }]), ]} 
I1018 08:53:57.327584 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7b877fc80)  to BatchNormGradNode (addr: 0x55b7b757d550)
I1018 08:53:57.327587 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7b877fc80)  to ReluGradNode (addr: 0x55b7c4edc7a0)
I1018 08:53:57.327590 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.327601 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1bed0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7bacd6990 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b8780100 }]), ] } 
I1018 08:53:57.327615 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.327617 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.327623 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b8780100 }]), ]} 
I1018 08:53:57.327636 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b6f67920)  to AddGradNode (addr: 0x55b7b877fc80)
I1018 08:53:57.327641 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.327656 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b8780100 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4e1bed0 }]), ] } 
I1018 08:53:57.327677 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.327679 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.327780 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b70ec230)  to ReluGradNode (addr: 0x55b7b6f67920)
I1018 08:53:57.327785 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b70ec230)  to GradNodeAccumulation (addr: 0x55b7c4d61a60)
I1018 08:53:57.327814 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.327818 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.327837 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b8780100 }]),  
( mean , [{ Name: batch_norm2d_16.w_1, Initialized: 1, Ptr: 0x55b7b796d090 }]),  
( variance , [{ Name: batch_norm2d_16.w_2, Initialized: 1, Ptr: 0x55b7b796df50 }]),  
( scale , [{ Name: batch_norm2d_16.w_0, Initialized: 1, Ptr: 0x55b7c4d61940 }]),  
( bias , [{ Name: batch_norm2d_16.b_0, Initialized: 1, Ptr: 0x55b7b796c1d0 }]), ]} 
I1018 08:53:57.327848 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.327852 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.327893 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b877e5a0)  to Conv2dGradNodeFinal (addr: 0x55b7b70ec230)
I1018 08:53:57.327896 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b877e5a0)  to GradNodeAccumulation (addr: 0x55b7c4d629c0)
I1018 08:53:57.327898 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b877e5a0)  to GradNodeAccumulation (addr: 0x55b7b796c2f0)
I1018 08:53:57.327904 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.327937 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b8780100 }]),  
( mean , [{ Name: batch_norm2d_16.w_1, Initialized: 1, Ptr: 0x55b7b796d090 }]),  
( variance , [{ Name: batch_norm2d_16.w_2, Initialized: 1, Ptr: 0x55b7b796df50 }]),  
( scale , [{ Name: batch_norm2d_16.w_0, Initialized: 1, Ptr: 0x55b7c4d61940 }]),  
( bias , [{ Name: batch_norm2d_16.b_0, Initialized: 1, Ptr: 0x55b7b796c1d0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f68300 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f684d0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f686a0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b70ecec0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7b70ecfe0 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b70ed100 }]), ] } 
I1018 08:53:57.327951 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.327955 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.327960 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f68300 }]), ]} 
I1018 08:53:57.327975 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7c4cbac30)  to BatchNormGradNode (addr: 0x55b7b877e5a0)
I1018 08:53:57.327978 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.327987 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f68300 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877ed50 }]), ] } 
I1018 08:53:57.328001 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.328009 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.328120 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7ceefb0c0)  to ReluGradNode (addr: 0x55b7c4cbac30)
I1018 08:53:57.328126 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7ceefb0c0)  to GradNodeAccumulation (addr: 0x55b7b796ef60)
I1018 08:53:57.328155 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.328158 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.328178 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f68300 }]),  
( mean , [{ Name: batch_norm2d_17.w_1, Initialized: 1, Ptr: 0x55b7bea3b370 }]),  
( variance , [{ Name: batch_norm2d_17.w_2, Initialized: 1, Ptr: 0x55b7bea3c230 }]),  
( scale , [{ Name: batch_norm2d_17.w_0, Initialized: 1, Ptr: 0x55b7b796ee40 }]),  
( bias , [{ Name: batch_norm2d_17.b_0, Initialized: 1, Ptr: 0x55b7bea3a4b0 }]), ]} 
I1018 08:53:57.328189 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.328191 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.328233 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7ceefb840)  to Conv2dGradNodeFinal (addr: 0x55b7ceefb0c0)
I1018 08:53:57.328236 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7ceefb840)  to GradNodeAccumulation (addr: 0x55b7b796fe00)
I1018 08:53:57.328239 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7ceefb840)  to GradNodeAccumulation (addr: 0x55b7bea3a5d0)
I1018 08:53:57.328245 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.328277 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7b6f68300 }]),  
( mean , [{ Name: batch_norm2d_17.w_1, Initialized: 1, Ptr: 0x55b7bea3b370 }]),  
( variance , [{ Name: batch_norm2d_17.w_2, Initialized: 1, Ptr: 0x55b7bea3c230 }]),  
( scale , [{ Name: batch_norm2d_17.w_0, Initialized: 1, Ptr: 0x55b7b796ee40 }]),  
( bias , [{ Name: batch_norm2d_17.b_0, Initialized: 1, Ptr: 0x55b7bea3a4b0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cbb610 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cbb7e0 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cbb9b0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cbc080 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cbbd50 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7ceefbff0 }]), ] } 
I1018 08:53:57.328297 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.328300 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.328392 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7cb089a80)  to ReluGradNode (addr: 0x55b7b6f67920)
I1018 08:53:57.328397 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7cb089a80)  to GradNodeAccumulation (addr: 0x55b7b759fa30)
I1018 08:53:57.328423 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.328426 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.328445 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefc470 }]),  
( mean , [{ Name: batch_norm2d_15.w_1, Initialized: 1, Ptr: 0x55b7c4d5f940 }]),  
( variance , [{ Name: batch_norm2d_15.w_2, Initialized: 1, Ptr: 0x55b7c4d60800 }]),  
( scale , [{ Name: batch_norm2d_15.w_0, Initialized: 1, Ptr: 0x55b7b759f910 }]),  
( bias , [{ Name: batch_norm2d_15.b_0, Initialized: 1, Ptr: 0x55b7b75a2ec0 }]), ]} 
I1018 08:53:57.328455 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.328461 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.328505 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7c4b919c0)  to Conv2dGradNodeFinal (addr: 0x55b7cb089a80)
I1018 08:53:57.328508 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7c4b919c0)  to GradNodeAccumulation (addr: 0x55b7b3f217c0)
I1018 08:53:57.328510 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7c4b919c0)  to GradNodeAccumulation (addr: 0x55b7b75a2fe0)
I1018 08:53:57.328516 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.328549 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefc470 }]),  
( mean , [{ Name: batch_norm2d_15.w_1, Initialized: 1, Ptr: 0x55b7c4d5f940 }]),  
( variance , [{ Name: batch_norm2d_15.w_2, Initialized: 1, Ptr: 0x55b7c4d60800 }]),  
( scale , [{ Name: batch_norm2d_15.w_0, Initialized: 1, Ptr: 0x55b7b759f910 }]),  
( bias , [{ Name: batch_norm2d_15.b_0, Initialized: 1, Ptr: 0x55b7b75a2ec0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08bfb0 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08c180 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb0888b0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08be90 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08bb70 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7c4b92170 }]), ] } 
I1018 08:53:57.328559 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.328567 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cbb610 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08bfb0 }]), ]} 
I1018 08:53:57.328589 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7cb08d810)  to BatchNormGradNode (addr: 0x55b7ceefb840)
I1018 08:53:57.328593 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7cb08d810)  to BatchNormGradNode (addr: 0x55b7c4b919c0)
I1018 08:53:57.328596 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.328608 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cbb610 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08bfb0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08bd70 }]), ] } 
I1018 08:53:57.328621 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.328624 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.328630 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08bd70 }]), ]} 
I1018 08:53:57.328644 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7b77995d0)  to AddGradNode (addr: 0x55b7cb08d810)
I1018 08:53:57.328648 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.328656 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08bd70 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cbb610 }]), ] } 
I1018 08:53:57.328675 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.328678 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.328792 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b779a830)  to ReluGradNode (addr: 0x55b7b77995d0)
I1018 08:53:57.328797 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b779a830)  to GradNodeAccumulation (addr: 0x55b7bea3d490)
I1018 08:53:57.328826 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.328835 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.328855 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08bfb0 }]),  
( mean , [{ Name: batch_norm2d_18.w_1, Initialized: 1, Ptr: 0x55b7c49fade0 }]),  
( variance , [{ Name: batch_norm2d_18.w_2, Initialized: 1, Ptr: 0x55b7c49fbca0 }]),  
( scale , [{ Name: batch_norm2d_18.w_0, Initialized: 1, Ptr: 0x55b7bea3d370 }]),  
( bias , [{ Name: batch_norm2d_18.b_0, Initialized: 1, Ptr: 0x55b7c49f9f20 }]), ]} 
I1018 08:53:57.328866 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.328869 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.328910 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7ceefcc80)  to Conv2dGradNodeFinal (addr: 0x55b7b779a830)
I1018 08:53:57.328914 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7ceefcc80)  to GradNodeAccumulation (addr: 0x55b7c49f9190)
I1018 08:53:57.328917 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7ceefcc80)  to GradNodeAccumulation (addr: 0x55b7c49fa040)
I1018 08:53:57.328922 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.328954 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08bfb0 }]),  
( mean , [{ Name: batch_norm2d_18.w_1, Initialized: 1, Ptr: 0x55b7c49fade0 }]),  
( variance , [{ Name: batch_norm2d_18.w_2, Initialized: 1, Ptr: 0x55b7c49fbca0 }]),  
( scale , [{ Name: batch_norm2d_18.w_0, Initialized: 1, Ptr: 0x55b7bea3d370 }]),  
( bias , [{ Name: batch_norm2d_18.b_0, Initialized: 1, Ptr: 0x55b7c49f9f20 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08bd70 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08dc90 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b7799f70 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779a720 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08d4c0 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7cfe2f580 }]), ] } 
I1018 08:53:57.328969 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.328971 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.328977 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08bd70 }]), ]} 
I1018 08:53:57.328992 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7ceeff0c0)  to BatchNormGradNode (addr: 0x55b7ceefcc80)
I1018 08:53:57.328996 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.329005 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08bd70 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b779b240 }]), ] } 
I1018 08:53:57.329020 4097533 eager_op_function.cc:5423] CurrentDeviceId: 0 from 0
I1018 08:53:57.329022 4097533 conv2d_fwd_function.cc:104] Final State Running: conv2d_ad_func
I1018 08:53:57.329131 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b877b9f0)  to ReluGradNode (addr: 0x55b7ceeff0c0)
I1018 08:53:57.329136 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from Conv2dGradNodeFinal (addr: 0x55b7b877b9f0)  to GradNodeAccumulation (addr: 0x55b7c49fccb0)
I1018 08:53:57.329164 4097533 eager_op_function.cc:28462] CurrentDeviceId: 0 from 0
I1018 08:53:57.329169 4097533 dygraph_functions.cc:57442] Running AD API: batch_norm
I1018 08:53:57.329187 4097533 dygraph_functions.cc:57535] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08bd70 }]),  
( mean , [{ Name: batch_norm2d_19.w_1, Initialized: 1, Ptr: 0x55b7b8788190 }]),  
( variance , [{ Name: batch_norm2d_19.w_2, Initialized: 1, Ptr: 0x55b7b8789050 }]),  
( scale , [{ Name: batch_norm2d_19.w_0, Initialized: 1, Ptr: 0x55b7c49fcb90 }]),  
( bias , [{ Name: batch_norm2d_19.b_0, Initialized: 1, Ptr: 0x55b7b87872d0 }]), ]} 
I1018 08:53:57.329203 4097533 api.cc:36661] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.329211 4097533 api.cc:36665] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.329253 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from BatchNormGradNode (addr: 0x55b7b877c580)  to Conv2dGradNodeFinal (addr: 0x55b7b877b9f0)
I1018 08:53:57.329257 4097533 grad_node_info.cc:393] Add Edges for slot: 3, the Edge is from BatchNormGradNode (addr: 0x55b7b877c580)  to GradNodeAccumulation (addr: 0x55b7b8786540)
I1018 08:53:57.329260 4097533 grad_node_info.cc:393] Add Edges for slot: 4, the Edge is from BatchNormGradNode (addr: 0x55b7b877c580)  to GradNodeAccumulation (addr: 0x55b7b87873f0)
I1018 08:53:57.329265 4097533 dygraph_functions.cc:57660] Finish AD API: batch_norm
I1018 08:53:57.329298 4097533 dygraph_functions.cc:57701] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7cb08bd70 }]),  
( mean , [{ Name: batch_norm2d_19.w_1, Initialized: 1, Ptr: 0x55b7b8788190 }]),  
( variance , [{ Name: batch_norm2d_19.w_2, Initialized: 1, Ptr: 0x55b7b8789050 }]),  
( scale , [{ Name: batch_norm2d_19.w_0, Initialized: 1, Ptr: 0x55b7c49fcb90 }]),  
( bias , [{ Name: batch_norm2d_19.b_0, Initialized: 1, Ptr: 0x55b7b87872d0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceeffaa0 }]),  
( mean_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877b100 }]),  
( variance_out , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877b2d0 }]),  
( saved_mean , [{ Name: None, Initialized: 1, Ptr: 0x55b7b877b8a0 }]),  
( saved_variance , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceefef50 }]),  
( reserve_space , [{ Name: None, Initialized: 0, Ptr: 0x55b7b877cd30 }]), ] } 
I1018 08:53:57.329308 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.329316 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceeffaa0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cbb610 }]), ]} 
I1018 08:53:57.329339 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7c9f053f0)  to BatchNormGradNode (addr: 0x55b7b877c580)
I1018 08:53:57.329342 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7c9f053f0)  to ReluGradNode (addr: 0x55b7b77995d0)
I1018 08:53:57.329345 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.329356 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceeffaa0 }]),  
( y , [{ Name: None, Initialized: 1, Ptr: 0x55b7c4cbb610 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f05870 }]), ] } 
I1018 08:53:57.329370 4097533 eager_op_function.cc:20422] CurrentDeviceId: 0 from 0
I1018 08:53:57.329372 4097533 dygraph_functions.cc:41369] Running AD API: relu
I1018 08:53:57.329378 4097533 dygraph_functions.cc:41425] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f05870 }]), ]} 
I1018 08:53:57.329392 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from ReluGradNode (addr: 0x55b7c9f06460)  to AddGradNode (addr: 0x55b7c9f053f0)
I1018 08:53:57.329396 4097533 dygraph_functions.cc:41492] Finish AD API: relu
I1018 08:53:57.329406 4097533 dygraph_functions.cc:41506] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f05870 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7ceeffaa0 }]), ] } 
I1018 08:53:57.329449 4097533 eager_method.cc:2158] Tensor:  set use_gpudnn = 0
I1018 08:53:57.329458 4097533 eager_op_function.cc:35671] CurrentDeviceId: 0 from 0
I1018 08:53:57.329461 4097533 dygraph_functions.cc:69244] Running AD API: pool2d
I1018 08:53:57.329468 4097533 dygraph_functions.cc:69300] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f05870 }]), ]} 
I1018 08:53:57.329488 4097533 pool2d_kernel.cc:153] [Pool2d] pooling type: avg exclusive: 1 adaptive: 1
I1018 08:53:57.329525 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from Pool2dGradNode (addr: 0x55b7c9f06d40)  to ReluGradNode (addr: 0x55b7c9f06460)
I1018 08:53:57.329530 4097533 dygraph_functions.cc:69376] Finish AD API: pool2d
I1018 08:53:57.329540 4097533 dygraph_functions.cc:69390] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f05870 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f05990 }]), ] } 
I1018 08:53:57.329553 4097533 eager_op_function.cc:9777] CurrentDeviceId: 0 from 0
I1018 08:53:57.329556 4097533 dygraph_functions.cc:20255] Running AD API: flatten
I1018 08:53:57.329562 4097533 dygraph_functions.cc:20311] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f05990 }]), ]} 
I1018 08:53:57.329569 4097533 dygraph_api.cc:208] Perform View between Output and Input Tensor, share allocation and inplace version.
I1018 08:53:57.329576 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from FlattenGradNode (addr: 0x55b7c9f00890)  to Pool2dGradNode (addr: 0x55b7c9f06d40)
I1018 08:53:57.329579 4097533 dygraph_functions.cc:20387] Finish AD API: flatten
I1018 08:53:57.329591 4097533 dygraph_functions.cc:20404] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f05990 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9efffd0 }]),  
( xshape , [{ Name: None, Initialized: 0, Ptr: 0x55b7c9f001a0 }]), ] } 
I1018 08:53:57.329603 4097533 dygraph_functions.cc:67019] Running AD API: matmul
I1018 08:53:57.329612 4097533 dygraph_functions.cc:67081] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9efffd0 }]),  
( y , [{ Name: linear_0.w_0, Initialized: 1, Ptr: 0x55b7b6f66780 }]), ]} 
I1018 08:53:57.329636 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from MatmulGradNode (addr: 0x55b7c9f01550)  to FlattenGradNode (addr: 0x55b7c9f00890)
I1018 08:53:57.329639 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from MatmulGradNode (addr: 0x55b7c9f01550)  to GradNodeAccumulation (addr: 0x55b7b878a500)
I1018 08:53:57.329643 4097533 dygraph_functions.cc:67152] Finish AD API: matmul
I1018 08:53:57.329654 4097533 dygraph_functions.cc:67169] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9efffd0 }]),  
( y , [{ Name: linear_0.w_0, Initialized: 1, Ptr: 0x55b7b6f66780 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f019d0 }]), ] } 
I1018 08:53:57.329658 4097533 dygraph_functions.cc:56007] Running AD API: add
I1018 08:53:57.329665 4097533 dygraph_functions.cc:56079] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f019d0 }]),  
( y , [{ Name: linear_0.b_0, Initialized: 1, Ptr: 0x55b7b878a3e0 }]), ]} 
I1018 08:53:57.329686 4097533 grad_node_info.cc:393] Add Edges for slot: 0, the Edge is from AddGradNode (addr: 0x55b7c9f02590)  to MatmulGradNode (addr: 0x55b7c9f01550)
I1018 08:53:57.329689 4097533 grad_node_info.cc:393] Add Edges for slot: 1, the Edge is from AddGradNode (addr: 0x55b7c9f02590)  to GradNodeAccumulation (addr: 0x55b7b6f66d60)
I1018 08:53:57.329692 4097533 dygraph_functions.cc:56149] Finish AD API: add
I1018 08:53:57.329703 4097533 dygraph_functions.cc:56166] { Input: [ 
( x , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f019d0 }]),  
( y , [{ Name: linear_0.b_0, Initialized: 1, Ptr: 0x55b7b878a3e0 }]), ],  
 Output: [ 
( out , [{ Name: None, Initialized: 1, Ptr: 0x55b7c9f02c50 }]), ] } 
I1018 08:53:57.341797 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.341835 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.343020 4097533 op_desc.cc:1108] CompileTime infer shape on batch_norm
I1018 08:53:57.343055 4097533 batch_norm_op.cc:112] 0
I1018 08:53:57.343057 4097533 batch_norm_op.cc:113] NCHW
I1018 08:53:57.343071 4097533 batch_norm_op.cc:171] 1, 64, 112, 112
I1018 08:53:57.343950 4097533 op_desc.cc:1108] CompileTime infer shape on relu
I1018 08:53:57.343966 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: relu; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.345057 4097533 op_desc.cc:1108] CompileTime infer shape on pool2d
I1018 08:53:57.345080 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: pool2d; inputs: X; attributes: ksize, strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm; outputs: Out
I1018 08:53:57.345887 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.345901 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.346582 4097533 op_desc.cc:1108] CompileTime infer shape on batch_norm
I1018 08:53:57.346602 4097533 batch_norm_op.cc:112] 0
I1018 08:53:57.346606 4097533 batch_norm_op.cc:113] NCHW
I1018 08:53:57.346616 4097533 batch_norm_op.cc:171] 1, 64, 56, 56
I1018 08:53:57.347029 4097533 op_desc.cc:1108] CompileTime infer shape on relu
I1018 08:53:57.347038 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: relu; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.347589 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.347599 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.348213 4097533 op_desc.cc:1108] CompileTime infer shape on batch_norm
I1018 08:53:57.348232 4097533 batch_norm_op.cc:112] 0
I1018 08:53:57.348234 4097533 batch_norm_op.cc:113] NCHW
I1018 08:53:57.348245 4097533 batch_norm_op.cc:171] 1, 64, 56, 56
I1018 08:53:57.348848 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.349288 4097533 op_desc.cc:1108] CompileTime infer shape on relu
I1018 08:53:57.349299 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: relu; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.349874 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.349885 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.350503 4097533 op_desc.cc:1108] CompileTime infer shape on batch_norm
I1018 08:53:57.350521 4097533 batch_norm_op.cc:112] 0
I1018 08:53:57.350523 4097533 batch_norm_op.cc:113] NCHW
I1018 08:53:57.350534 4097533 batch_norm_op.cc:171] 1, 64, 56, 56
I1018 08:53:57.350921 4097533 op_desc.cc:1108] CompileTime infer shape on relu
I1018 08:53:57.350929 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: relu; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.351454 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.351464 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.352077 4097533 op_desc.cc:1108] CompileTime infer shape on batch_norm
I1018 08:53:57.352093 4097533 batch_norm_op.cc:112] 0
I1018 08:53:57.352097 4097533 batch_norm_op.cc:113] NCHW
I1018 08:53:57.352108 4097533 batch_norm_op.cc:171] 1, 64, 56, 56
I1018 08:53:57.352479 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.352882 4097533 op_desc.cc:1108] CompileTime infer shape on relu
I1018 08:53:57.352890 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: relu; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.353494 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.353506 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.354122 4097533 op_desc.cc:1108] CompileTime infer shape on batch_norm
I1018 08:53:57.354140 4097533 batch_norm_op.cc:112] 0
I1018 08:53:57.354142 4097533 batch_norm_op.cc:113] NCHW
I1018 08:53:57.354153 4097533 batch_norm_op.cc:171] 1, 128, 28, 28
I1018 08:53:57.354533 4097533 op_desc.cc:1108] CompileTime infer shape on relu
I1018 08:53:57.354540 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: relu; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.355388 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.355401 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.356015 4097533 op_desc.cc:1108] CompileTime infer shape on batch_norm
I1018 08:53:57.356032 4097533 batch_norm_op.cc:112] 0
I1018 08:53:57.356035 4097533 batch_norm_op.cc:113] NCHW
I1018 08:53:57.356046 4097533 batch_norm_op.cc:171] 1, 128, 28, 28
I1018 08:53:57.356607 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.356618 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.357244 4097533 op_desc.cc:1108] CompileTime infer shape on batch_norm
I1018 08:53:57.357263 4097533 batch_norm_op.cc:112] 0
I1018 08:53:57.357265 4097533 batch_norm_op.cc:113] NCHW
I1018 08:53:57.357275 4097533 batch_norm_op.cc:171] 1, 128, 28, 28
I1018 08:53:57.357654 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.358040 4097533 op_desc.cc:1108] CompileTime infer shape on relu
I1018 08:53:57.358048 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: relu; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.358597 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.358608 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.359256 4097533 op_desc.cc:1108] CompileTime infer shape on batch_norm
I1018 08:53:57.359274 4097533 batch_norm_op.cc:112] 0
I1018 08:53:57.359277 4097533 batch_norm_op.cc:113] NCHW
I1018 08:53:57.359287 4097533 batch_norm_op.cc:171] 1, 128, 28, 28
I1018 08:53:57.359670 4097533 op_desc.cc:1108] CompileTime infer shape on relu
I1018 08:53:57.359678 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: relu; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.360200 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.360211 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.360808 4097533 op_desc.cc:1108] CompileTime infer shape on batch_norm
I1018 08:53:57.360826 4097533 batch_norm_op.cc:112] 0
I1018 08:53:57.360828 4097533 batch_norm_op.cc:113] NCHW
I1018 08:53:57.360839 4097533 batch_norm_op.cc:171] 1, 128, 28, 28
I1018 08:53:57.361233 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.361649 4097533 op_desc.cc:1108] CompileTime infer shape on relu
I1018 08:53:57.361658 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: relu; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.362250 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.362262 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.362869 4097533 op_desc.cc:1108] CompileTime infer shape on batch_norm
I1018 08:53:57.362885 4097533 batch_norm_op.cc:112] 0
I1018 08:53:57.362887 4097533 batch_norm_op.cc:113] NCHW
I1018 08:53:57.362898 4097533 batch_norm_op.cc:171] 1, 256, 14, 14
I1018 08:53:57.363279 4097533 op_desc.cc:1108] CompileTime infer shape on relu
I1018 08:53:57.363287 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: relu; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.363809 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.363819 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.364416 4097533 op_desc.cc:1108] CompileTime infer shape on batch_norm
I1018 08:53:57.364432 4097533 batch_norm_op.cc:112] 0
I1018 08:53:57.364434 4097533 batch_norm_op.cc:113] NCHW
I1018 08:53:57.364445 4097533 batch_norm_op.cc:171] 1, 256, 14, 14
I1018 08:53:57.365016 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.365027 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.365662 4097533 op_desc.cc:1108] CompileTime infer shape on batch_norm
I1018 08:53:57.365679 4097533 batch_norm_op.cc:112] 0
I1018 08:53:57.365682 4097533 batch_norm_op.cc:113] NCHW
I1018 08:53:57.365692 4097533 batch_norm_op.cc:171] 1, 256, 14, 14
I1018 08:53:57.366070 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.366463 4097533 op_desc.cc:1108] CompileTime infer shape on relu
I1018 08:53:57.366472 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: relu; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.367030 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.367041 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.367655 4097533 op_desc.cc:1108] CompileTime infer shape on batch_norm
I1018 08:53:57.367671 4097533 batch_norm_op.cc:112] 0
I1018 08:53:57.367674 4097533 batch_norm_op.cc:113] NCHW
I1018 08:53:57.367684 4097533 batch_norm_op.cc:171] 1, 256, 14, 14
I1018 08:53:57.368069 4097533 op_desc.cc:1108] CompileTime infer shape on relu
I1018 08:53:57.368077 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: relu; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.368599 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.368609 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.369230 4097533 op_desc.cc:1108] CompileTime infer shape on batch_norm
I1018 08:53:57.369248 4097533 batch_norm_op.cc:112] 0
I1018 08:53:57.369251 4097533 batch_norm_op.cc:113] NCHW
I1018 08:53:57.369261 4097533 batch_norm_op.cc:171] 1, 256, 14, 14
I1018 08:53:57.369647 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.370029 4097533 op_desc.cc:1108] CompileTime infer shape on relu
I1018 08:53:57.370038 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: relu; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.370617 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.370628 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.371244 4097533 op_desc.cc:1108] CompileTime infer shape on batch_norm
I1018 08:53:57.371263 4097533 batch_norm_op.cc:112] 0
I1018 08:53:57.371265 4097533 batch_norm_op.cc:113] NCHW
I1018 08:53:57.371276 4097533 batch_norm_op.cc:171] 1, 512, 7, 7
I1018 08:53:57.371661 4097533 op_desc.cc:1108] CompileTime infer shape on relu
I1018 08:53:57.371670 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: relu; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.372198 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.372208 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.372802 4097533 op_desc.cc:1108] CompileTime infer shape on batch_norm
I1018 08:53:57.372819 4097533 batch_norm_op.cc:112] 0
I1018 08:53:57.372821 4097533 batch_norm_op.cc:113] NCHW
I1018 08:53:57.372831 4097533 batch_norm_op.cc:171] 1, 512, 7, 7
I1018 08:53:57.373395 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.373407 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.374017 4097533 op_desc.cc:1108] CompileTime infer shape on batch_norm
I1018 08:53:57.374034 4097533 batch_norm_op.cc:112] 0
I1018 08:53:57.374037 4097533 batch_norm_op.cc:113] NCHW
I1018 08:53:57.374047 4097533 batch_norm_op.cc:171] 1, 512, 7, 7
I1018 08:53:57.374423 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.374807 4097533 op_desc.cc:1108] CompileTime infer shape on relu
I1018 08:53:57.374816 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: relu; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.375375 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.375385 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.375994 4097533 op_desc.cc:1108] CompileTime infer shape on batch_norm
I1018 08:53:57.376013 4097533 batch_norm_op.cc:112] 0
I1018 08:53:57.376014 4097533 batch_norm_op.cc:113] NCHW
I1018 08:53:57.376025 4097533 batch_norm_op.cc:171] 1, 512, 7, 7
I1018 08:53:57.376403 4097533 op_desc.cc:1108] CompileTime infer shape on relu
I1018 08:53:57.376411 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: relu; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.376927 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.376937 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.377535 4097533 op_desc.cc:1108] CompileTime infer shape on batch_norm
I1018 08:53:57.377553 4097533 batch_norm_op.cc:112] 0
I1018 08:53:57.377557 4097533 batch_norm_op.cc:113] NCHW
I1018 08:53:57.377573 4097533 batch_norm_op.cc:171] 1, 512, 7, 7
I1018 08:53:57.377955 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.378350 4097533 op_desc.cc:1108] CompileTime infer shape on relu
I1018 08:53:57.378357 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: relu; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.378814 4097533 op_desc.cc:1108] CompileTime infer shape on pool2d
I1018 08:53:57.378826 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: pool2d; inputs: X; attributes: ksize, strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm; outputs: Out
I1018 08:53:57.379951 4097533 op_desc.cc:1108] CompileTime infer shape on flatten_contiguous_range
I1018 08:53:57.379972 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: flatten; inputs: X; attributes: start_axis, stop_axis; outputs: Out, XShape
I1018 08:53:57.381001 4097533 op_desc.cc:1108] CompileTime infer shape on matmul_v2
I1018 08:53:57.381017 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
I1018 08:53:57.381453 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.395206 4097533 eager.cc:118] Tensor(cuda_graph) have not GradNode, add GradNodeAccumulation0x55b7b6f68f10 for it.
I1018 08:53:57.395255 4097533 amp_auto_cast.cc:160] 0 0 0 0
I1018 08:53:57.417562 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.418097 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.418581 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.419011 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.419030 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.420341 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.420352 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.420926 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.421413 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.421908 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.422310 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.422323 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.422773 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.422783 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.423208 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.423674 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.424074 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.424527 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.424921 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.427275 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.427291 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.427749 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.427760 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.428187 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.428197 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.428587 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.428596 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.429230 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.429706 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.429716 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.430847 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.431309 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.431761 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.432152 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.432164 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.432618 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.432627 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.433094 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.433513 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.433974 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.434362 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.434373 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.434809 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.434818 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.435213 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.435657 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.436049 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.436496 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.436892 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.437283 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.437294 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.437660 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.437669 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.438028 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.438037 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.438395 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.438402 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.438958 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.439389 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.439399 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.440563 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.441030 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.441494 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.441887 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.441900 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.442333 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.442342 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.442812 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.443223 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.443684 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.444072 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.444082 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.444511 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.444520 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.444913 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.445363 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.445753 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.446192 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.446588 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.446965 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.446975 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.447340 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.447350 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.447710 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.447718 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.448076 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.448084 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.449280 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.449726 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.449736 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.450964 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.451421 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.451862 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.452257 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.452270 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.452713 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.452720 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.453192 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.453608 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.454064 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.454452 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.454463 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.454895 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.454903 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.455297 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.455741 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.456130 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.456570 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.456967 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.457353 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.457365 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.457731 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.457741 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.458099 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.458108 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.458464 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.458473 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.459005 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.459424 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.459432 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.460744 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.461225 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.461686 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.462075 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.462087 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.462528 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.462538 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.462997 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.463407 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.463873 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.464263 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.464275 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.464712 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.464721 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.465116 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.465572 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.465973 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.466418 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.466804 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.467177 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.467187 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.467556 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.467566 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.467928 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.467937 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.468299 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.468308 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.469611 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.470049 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.470059 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.471426 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.471892 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.472359 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.472750 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.472764 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.473202 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.473217 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.473680 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.474088 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.474550 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.474939 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.474951 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.475386 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.475394 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.475804 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.476258 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.476648 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.477092 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.477486 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.477875 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.477886 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.478263 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.478272 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.478643 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.478652 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.479015 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.479024 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.479573 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.479997 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.480006 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.481462 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.481936 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.482390 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.482784 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.482795 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.483237 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.483245 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.483703 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.484109 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.484576 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.484968 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.484980 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.485422 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.485432 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.485838 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.486292 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.486681 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.487175 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.487571 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.487949 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.487964 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.488349 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.488359 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.488732 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.488741 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.489105 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.489113 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.490643 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.491103 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.491544 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.491935 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.491946 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.492393 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.492403 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.492872 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.493278 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.493731 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.494114 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.494124 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.494560 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.494568 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.494962 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.495404 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.495797 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.496253 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.496646 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.497026 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.497036 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.497426 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.497437 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.497807 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.497817 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.498181 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.498190 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.499688 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.500139 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.500149 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.501693 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.502154 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.502596 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.502987 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.503000 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.503451 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.503460 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.503926 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.504328 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.504786 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.505179 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.505191 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.505631 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.505640 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.506033 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.506475 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.506860 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.507305 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.507694 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.508076 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.508087 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.508455 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.508463 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.508824 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.508832 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.509192 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.509202 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.509764 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.510190 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.510198 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.511843 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.512315 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.512766 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.513157 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.513175 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.513638 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.513648 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.514115 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.514523 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.514986 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.515381 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.515393 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.515836 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.515843 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.516240 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.516692 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.517084 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.517555 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.517957 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.518342 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.518352 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.518720 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.518729 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.519091 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.519099 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.519464 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.519474 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.521112 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.521564 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.521575 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.523233 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.523694 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.524137 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.524528 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.524539 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.524982 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.524991 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.525481 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.525893 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.526366 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.526762 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.526773 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.527206 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.527215 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.527606 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.528050 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.528443 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.528889 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.529284 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.529671 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.529681 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.530050 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.530058 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.530427 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.530436 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.530798 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.530807 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.531347 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.531776 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.531785 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.533528 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.533991 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.534435 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.534827 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.534840 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.535293 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.535302 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.535781 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.536185 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.536644 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.537039 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.537051 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.537496 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.537506 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.537911 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.538355 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.538748 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.539189 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.539588 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.539968 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.539978 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.540343 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.540352 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.540719 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.540727 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.541085 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.541093 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.542960 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.543440 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.543895 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.544283 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.544296 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.544737 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.544745 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.545239 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.545655 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.546167 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.546566 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.546577 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.547017 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.547026 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.547426 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.547873 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.548256 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.548693 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.549083 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.549481 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.549492 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.549865 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.549873 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.550253 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.550263 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.550626 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.550634 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.552434 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.552863 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.552873 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.554778 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.555253 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.555709 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.556102 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.556113 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.556552 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.556561 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.557027 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.557433 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.557898 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.558288 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.558300 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.558740 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.558749 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.559154 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.559603 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.559989 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.560425 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.560815 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.561190 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.561199 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.561578 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.561587 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.561959 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.561968 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.562327 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.562336 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.562878 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.563299 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.563313 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.565260 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.565742 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.566192 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.566579 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.566591 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.567039 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.567047 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.567514 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.567914 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.568382 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.568778 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.568789 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.569236 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.569245 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.569649 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.570101 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.570489 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.570933 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.571326 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.571707 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.571717 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.572088 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.572096 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.572458 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.572467 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.572826 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.572835 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.574788 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.575223 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.575233 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.577232 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.577708 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.578155 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.578543 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.578555 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.579010 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.579018 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.579485 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.579883 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.580353 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.580749 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.580760 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.581210 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.581220 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.581625 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.582077 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.582464 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.582911 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.583298 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.583673 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.583683 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.584053 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.584061 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.584422 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.584431 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.584795 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.584805 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.585361 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.585789 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.585798 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.587854 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.588320 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.588774 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.589160 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.589172 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.589628 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.589638 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.590106 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.590503 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.590950 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.591351 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.591363 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.591804 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.591814 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.592216 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.592670 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.593060 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.593531 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.593930 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.594316 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.594326 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.594696 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.594705 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.595067 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.595077 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.595441 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.595450 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.597599 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.598071 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.598524 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.598912 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.598924 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.599360 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.599370 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.599828 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.600240 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.600706 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.601096 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.601107 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.601560 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.601570 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.601966 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.602417 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.602806 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.603253 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.603641 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.604025 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.604036 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.604418 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.604429 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.604796 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.604805 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.605165 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.605173 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.607333 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.607779 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.607789 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.610002 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.610458 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.610901 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.611284 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.611296 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.611744 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.611753 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.612216 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.612617 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.613080 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.613474 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.613487 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.613919 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.613927 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.614326 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.614778 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.615167 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.615619 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.616017 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.616401 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.616411 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.616777 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.616786 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.617146 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.617161 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.617530 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.617540 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.618077 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.618525 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.618534 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.620769 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.621243 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.621701 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.622085 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.622097 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.622534 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.622543 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.623020 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.623420 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.623879 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.624266 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.624279 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.624712 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.624722 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.625116 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.625573 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.625962 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.626396 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.626780 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.627169 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.627180 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.627552 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.627561 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.627923 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.627933 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.628288 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.628297 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.630530 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.630956 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.630971 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.633216 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.640628 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.640646 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.640676 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.640687 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.640694 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.640702 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.640708 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.640720 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.640725 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.640738 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.640753 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.640767 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.640779 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.640784 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.640794 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.640797 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.640808 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.640821 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.640832 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.640844 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.640856 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.640867 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.640873 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.640882 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.640885 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.640893 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.640897 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.640904 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.640908 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.640916 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.640924 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.640928 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.640939 4097533 op_desc.cc:1108] CompileTime infer shape on pool2d
I1018 08:53:57.640950 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: pool2d; inputs: X; attributes: ksize, strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm; outputs: Out
I1018 08:53:57.640969 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.640974 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.640990 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.640997 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.641005 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.641012 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.641016 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.641026 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.641031 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.641041 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.641053 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.641065 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.641076 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.641081 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.641090 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.641094 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.641104 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.641116 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.641127 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.641139 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.641150 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.641162 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.641166 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.641175 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.641180 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.641186 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.641191 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.641198 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.641201 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.641222 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.641230 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.641234 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.641248 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.641252 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.641268 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.641275 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.641283 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.641288 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.641294 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.641304 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.641306 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.641319 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.641330 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.641342 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.641352 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.641357 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.641366 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.641369 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.641381 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.641392 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.641403 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.641414 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.641425 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.641436 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.641440 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.641449 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.641453 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.641461 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.641465 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.641472 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.641476 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.641484 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.641496 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.641503 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.641506 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.641516 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.641521 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.641537 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.641546 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.641552 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.641559 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.641564 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.641573 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.641577 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.641588 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.641599 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.641611 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.641621 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.641626 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.641635 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.641638 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.641649 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.641660 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.641671 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.641682 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.641693 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.641705 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.641709 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.641717 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.641721 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.641729 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.641733 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.641741 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.641744 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.641752 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.641759 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.641763 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.641772 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.641777 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.641790 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.641798 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.641808 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.641815 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.641819 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.641829 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.641832 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.641844 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.641855 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.641866 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.641876 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.641880 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.641891 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.641893 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.641903 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.641916 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.641927 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.641937 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.641948 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.641959 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.641963 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.641971 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.641975 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.641983 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.641988 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.641994 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.641999 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.642007 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.642019 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.642025 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.642028 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.642038 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.642042 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.642056 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.642064 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.642071 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.642078 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.642082 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.642094 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.642098 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.642108 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.642120 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.642131 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.642141 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.642146 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.642155 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.642159 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.642169 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.642181 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.642192 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.642203 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.642215 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.642226 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.642230 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.642239 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.642243 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.642251 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.642256 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.642262 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.642266 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.642274 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.642282 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.642285 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.642294 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.642299 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.642313 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.642321 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.642328 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.642334 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.642339 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.642349 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.642352 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.642365 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.642377 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.642388 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.642398 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.642403 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.642412 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.642416 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.642426 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.642438 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.642449 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.642460 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.642472 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.642483 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.642488 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.642495 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.642500 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.642508 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.642511 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.642518 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.642522 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.642530 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.642534 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.642547 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.642555 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.642563 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.642570 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.642575 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.642583 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.642587 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.642598 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.642609 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.642621 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.642632 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.642635 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.642647 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.642652 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.642661 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.642674 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.642685 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.642697 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.642709 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.642719 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.642724 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.642731 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.642736 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.642743 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.642747 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.642755 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.642758 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.642766 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.642778 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.642786 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.642789 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.642798 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.642803 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.642817 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.642824 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.642832 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.642838 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.642843 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.642853 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.642855 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.642866 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.642877 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.642889 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.642899 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.642904 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.642913 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.642916 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.642930 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.642941 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.642952 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.642963 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.642974 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.642985 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.642990 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.642998 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.643002 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.643010 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.643014 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.643021 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.643025 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.643033 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.643041 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.643044 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.643054 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.643059 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.643072 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.643080 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.643087 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.643095 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.643100 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.643108 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.643112 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.643122 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.643134 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.643146 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.643155 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.643159 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.643169 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.643173 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.643183 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.643195 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.643206 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.643220 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.643232 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.643244 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.643247 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.643256 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.643260 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.643267 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.643271 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.643280 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.643283 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.643291 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.643302 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.643309 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.643313 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.643323 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.643327 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.643342 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.643348 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.643355 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.643362 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.643368 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.643376 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.643379 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.643390 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.643402 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.643414 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.643424 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.643429 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.643437 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.643440 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.643451 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.643462 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.643473 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.643484 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.643496 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.643509 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.643514 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.643522 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.643527 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.643534 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.643538 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.643545 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.643549 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.643558 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.643564 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.643568 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.643577 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.643582 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.643595 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.643604 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.643610 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.643617 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.643622 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.643631 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.643635 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.643646 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.643656 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.643668 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.643678 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.643683 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.643692 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.643695 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.643707 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.643718 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.643728 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.643739 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.643750 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.643761 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.643765 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.643779 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.643783 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.643790 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.643795 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.643802 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.643806 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.643813 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.643817 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.643831 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.643838 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.643846 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.643852 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.643857 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.643867 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.643870 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.643880 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.643891 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.643903 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.643913 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.643918 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.643927 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.643930 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.643940 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.643952 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.643963 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.643975 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.643985 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.643996 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.644001 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.644009 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.644013 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.644021 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.644024 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.644032 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.644037 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.644048 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.644060 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.644068 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.644071 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.644081 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.644086 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.644099 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.644107 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.644114 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.644121 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.644126 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.644135 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.644140 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.644150 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.644161 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.644172 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.644182 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.644187 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.644196 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.644201 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.644210 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.644222 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.644232 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.644243 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.644255 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.644266 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.644270 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.644279 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.644284 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.644290 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.644295 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.644302 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.644306 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.644315 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.644321 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.644327 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.644337 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.644342 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.644356 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.644363 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.644371 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.644378 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.644382 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.644392 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.644395 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.644407 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.644418 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.644429 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.644439 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.644444 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.644454 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.644457 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.644467 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.644479 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.644490 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.644501 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.644512 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.644523 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.644528 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.644536 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.644541 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.644547 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.644552 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.644559 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.644563 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.644572 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.644583 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.644590 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.644593 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.644605 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.644610 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.644623 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.644631 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.644639 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.644645 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.644650 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.644659 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.644663 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.644675 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.644686 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.644698 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.644708 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.644713 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.644722 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.644726 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.644737 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.644748 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.644759 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.644770 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.644783 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.644793 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.644798 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.644805 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.644810 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.644817 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.644821 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.644829 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.644833 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.644840 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.644847 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.644851 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.644861 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.644865 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.644881 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.644888 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.644896 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.644902 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.644907 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.644917 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.644920 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.644930 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.644942 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.644953 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.644963 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.644968 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.644977 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.644981 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.644991 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.645004 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.645015 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.645025 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.645036 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.645047 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.645052 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.645061 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.645064 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.645072 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.645076 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.645083 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.645087 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.645094 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.645098 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.645112 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.645120 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.645128 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.645134 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.645138 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.645148 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.645153 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.645164 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.645175 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.645186 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.645196 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.645200 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.645215 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.645218 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.645228 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.645241 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.645251 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.645263 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.645274 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.645285 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.645290 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.645299 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.645303 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.645310 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.645315 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.645323 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.645326 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.645334 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.645346 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.645354 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.645357 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.645367 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.645371 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.645385 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.645393 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.645401 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.645407 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.645412 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.645421 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.645426 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.645438 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.645449 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.645462 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.645471 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.645476 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.645485 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.645489 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.645499 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.645511 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.645522 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.645534 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.645545 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.645555 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.645560 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.645568 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.645572 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.645579 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.645583 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.645591 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.645594 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.645602 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.645609 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.645613 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.645623 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.645627 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.645640 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.645648 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.645655 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.645663 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.645668 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.645676 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.645679 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.645690 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.645701 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.645713 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.645723 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.645730 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.645740 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.645743 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.645753 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.645766 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.645776 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.645787 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.645798 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.645810 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.645814 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.645823 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.645826 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.645834 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.645838 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.645845 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.645849 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.645859 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.645869 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.645876 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.645880 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.645890 4097533 op_desc.cc:1108] CompileTime infer shape on pool2d
I1018 08:53:57.645896 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: pool2d; inputs: X; attributes: ksize, strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm; outputs: Out
I1018 08:53:57.645913 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.645926 4097533 op_desc.cc:1108] CompileTime infer shape on matmul_v2
I1018 08:53:57.645931 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
I1018 08:53:57.645943 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.660971 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.660989 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.661017 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.661028 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.661036 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.661043 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.661049 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.661067 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.661072 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.661087 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.661101 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.661115 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.661126 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.661131 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.661140 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.661144 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.661155 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.661168 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.661180 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.661191 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.661202 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.661226 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.661232 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.661242 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.661245 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.661252 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.661257 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.661264 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.661268 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.661276 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.661284 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.661288 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.661298 4097533 op_desc.cc:1108] CompileTime infer shape on pool2d
I1018 08:53:57.661305 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: pool2d; inputs: X; attributes: ksize, strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm; outputs: Out
I1018 08:53:57.661322 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.661329 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.661343 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.661351 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.661358 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.661365 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.661370 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.661383 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.661387 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.661399 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.661412 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.661423 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.661433 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.661438 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.661448 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.661451 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.661463 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.661473 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.661485 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.661496 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.661509 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.661520 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.661523 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.661532 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.661536 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.661543 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.661547 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.661556 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.661559 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.661567 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.661574 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.661578 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.661587 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.661592 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.661607 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.661614 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.661621 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.661628 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.661633 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.661641 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.661645 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.661660 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.661672 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.661684 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.661693 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.661698 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.661707 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.661711 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.661721 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.661733 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.661744 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.661756 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.661767 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.661778 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.661782 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.661792 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.661795 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.661803 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.661806 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.661814 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.661818 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.661826 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.661837 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.661845 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.661849 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.661859 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.661864 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.661878 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.661886 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.661895 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.661901 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.661906 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.661914 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.661918 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.661929 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.661942 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.661955 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.661965 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.661970 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.661979 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.661983 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.661993 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.662005 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.662016 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.662027 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.662038 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.662050 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.662055 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.662063 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.662067 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.662074 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.662078 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.662086 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.662089 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.662097 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.662106 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.662108 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.662118 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.662122 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.662137 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.662145 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.662153 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.662159 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.662164 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.662173 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.662178 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.662189 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.662199 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.662210 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.662221 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.662225 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.662237 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.662241 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.662251 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.662262 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.662273 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.662285 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.662295 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.662307 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.662312 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.662319 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.662323 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.662330 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.662335 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.662343 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.662346 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.662354 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.662365 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.662372 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.662376 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.662385 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.662390 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.662403 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.662411 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.662418 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.662425 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.662431 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.662439 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.662443 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.662453 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.662465 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.662477 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.662487 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.662490 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.662500 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.662506 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.662518 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.662528 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.662539 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.662551 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.662561 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.662572 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.662577 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.662585 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.662590 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.662596 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.662601 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.662608 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.662612 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.662621 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.662627 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.662631 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.662642 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.662645 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.662659 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.662667 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.662674 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.662680 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.662684 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.662693 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.662698 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.662708 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.662719 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.662731 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.662741 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.662745 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.662755 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.662758 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.662770 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.662784 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.662796 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.662806 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.662817 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.662828 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.662832 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.662842 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.662845 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.662853 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.662856 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.662864 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.662868 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.662875 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.662879 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.662894 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.662900 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.662909 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.662914 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.662919 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.662928 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.662932 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.662942 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.662954 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.662966 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.662976 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.662979 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.662988 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.662992 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.663002 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.663013 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.663024 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.663035 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.663046 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.663057 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.663062 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.663070 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.663076 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.663084 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.663089 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.663095 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.663100 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.663107 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.663118 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.663125 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.663129 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.663139 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.663143 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.663156 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.663164 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.663172 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.663177 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.663182 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.663192 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.663195 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.663206 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.663218 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.663228 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.663239 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.663244 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.663252 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.663256 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.663266 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.663277 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.663288 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.663300 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.663311 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.663321 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.663326 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.663333 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.663337 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.663344 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.663352 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.663358 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.663362 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.663370 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.663378 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.663381 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.663391 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.663395 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.663408 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.663416 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.663424 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.663430 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.663434 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.663443 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.663447 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.663458 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.663470 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.663480 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.663491 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.663494 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.663503 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.663507 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.663517 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.663528 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.663539 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.663550 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.663561 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.663573 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.663576 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.663585 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.663589 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.663596 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.663600 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.663609 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.663614 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.663622 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.663633 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.663641 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.663645 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.663654 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.663658 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.663671 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.663679 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.663686 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.663692 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.663697 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.663707 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.663710 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.663722 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.663733 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.663744 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.663754 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.663758 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.663767 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.663771 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.663781 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.663792 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.663803 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.663815 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.663825 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.663837 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.663841 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.663849 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.663853 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.663861 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.663865 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.663872 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.663877 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.663884 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.663893 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.663897 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.663908 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.663911 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.663924 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.663933 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.663939 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.663946 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.663950 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.663959 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.663964 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.663975 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.663985 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.663996 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.664006 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.664011 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.664019 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.664023 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.664033 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.664044 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.664055 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.664067 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.664077 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.664088 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.664093 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.664100 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.664104 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.664112 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.664116 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.664124 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.664127 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.664134 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.664139 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.664155 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.664162 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.664170 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.664175 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.664180 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.664189 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.664193 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.664204 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.664215 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.664227 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.664237 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.664242 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.664250 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.664254 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.664264 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.664275 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.664286 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.664297 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.664309 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.664319 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.664323 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.664331 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.664335 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.664342 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.664346 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.664355 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.664357 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.664366 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.664376 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.664383 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.664387 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.664397 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.664400 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.664414 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.664422 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.664428 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.664438 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.664443 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.664451 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.664455 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.664465 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.664477 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.664489 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.664498 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.664503 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.664512 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.664515 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.664526 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.664537 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.664548 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.664558 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.664569 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.664580 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.664585 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.664593 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.664597 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.664604 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.664608 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.664615 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.664619 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.664628 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.664634 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.664638 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.664647 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.664651 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.664664 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.664671 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.664680 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.664685 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.664690 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.664703 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.664706 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.664717 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.664729 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.664741 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.664750 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.664754 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.664764 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.664767 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.664777 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.664789 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.664800 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.664811 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.664821 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.664832 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.664837 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.664845 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.664849 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.664856 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.664860 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.664868 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.664872 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.664880 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.664891 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.664898 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.664902 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.664911 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.664916 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.664929 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.664937 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.664944 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.664952 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.664955 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.664964 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.664968 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.664981 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.664992 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.665004 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.665014 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.665019 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.665028 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.665032 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.665042 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.665053 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.665064 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.665076 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.665087 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.665097 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.665102 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.665110 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.665114 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.665122 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.665127 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.665133 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.665138 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.665145 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.665153 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.665155 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.665165 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.665169 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.665184 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.665190 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.665198 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.665205 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.665213 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.665223 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.665226 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.665237 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.665248 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.665262 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.665273 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.665277 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.665287 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.665290 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.665302 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.665313 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.665323 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.665335 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.665345 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.665356 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.665361 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.665369 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.665374 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.665380 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.665385 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.665392 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.665396 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.665403 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.665407 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.665421 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.665428 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.665436 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.665442 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.665446 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.665455 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.665459 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.665469 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.665481 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.665493 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.665503 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.665506 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.665516 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.665519 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.665530 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.665544 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.665555 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.665565 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.665577 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.665588 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.665592 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.665601 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.665606 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.665612 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.665616 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.665623 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.665627 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.665635 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.665647 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.665654 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.665657 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.665668 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.665671 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.665685 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.665692 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.665700 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.665707 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.665711 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.665720 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.665724 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.665735 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.665746 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.665757 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.665768 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.665772 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.665782 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.665786 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.665796 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.665807 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.665819 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.665832 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.665843 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.665855 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.665859 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.665868 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.665872 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.665879 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.665884 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.665890 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.665894 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.665902 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.665910 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.665913 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.665923 4097533 op_desc.cc:1108] CompileTime infer shape on conv2d
I1018 08:53:57.665927 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.665940 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.665948 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.665956 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.665962 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.665967 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.665977 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.665980 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.665990 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.666002 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_sub
I1018 08:53:57.666014 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.666024 4097533 op_desc.cc:1108] CompileTime infer shape on scale
I1018 08:53:57.666028 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
I1018 08:53:57.666038 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_pow
I1018 08:53:57.666041 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: elementwise_pow_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.666051 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.666064 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.666074 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_mul
I1018 08:53:57.666085 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.666096 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.666107 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.666111 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.666122 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.666126 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.666134 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.666138 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.666146 4097533 op_desc.cc:1108] CompileTime infer shape on assign
I1018 08:53:57.666149 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_raw; inputs: X; attributes: ; outputs: Out
I1018 08:53:57.666157 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.666168 4097533 op_desc.cc:1108] CompileTime infer shape on fill_constant
I1018 08:53:57.666175 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_max
I1018 08:53:57.666179 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: maximum_raw; inputs: X, Y; attributes: axis; outputs: Out
I1018 08:53:57.666189 4097533 op_desc.cc:1108] CompileTime infer shape on pool2d
I1018 08:53:57.666194 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: pool2d; inputs: X; attributes: ksize, strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm; outputs: Out
I1018 08:53:57.666211 4097533 op_desc.cc:1108] CompileTime infer shape on reshape2
I1018 08:53:57.666224 4097533 op_desc.cc:1108] CompileTime infer shape on matmul_v2
I1018 08:53:57.666229 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
I1018 08:53:57.666240 4097533 op_desc.cc:1108] CompileTime infer shape on elementwise_add
I1018 08:53:57.673743 4097533 graph.cc:151] create OpNode by conv2d
I1018 08:53:57.673786 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.673803 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.673817 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.673830 4097533 graph.cc:151] create OpNode by scale
I1018 08:53:57.673841 4097533 graph.cc:151] create OpNode by elementwise_pow
I1018 08:53:57.673856 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.673871 4097533 graph.cc:151] create OpNode by elementwise_sub
I1018 08:53:57.673884 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.673900 4097533 graph.cc:151] create OpNode by scale
I1018 08:53:57.673913 4097533 graph.cc:151] create OpNode by elementwise_pow
I1018 08:53:57.673926 4097533 graph.cc:151] create OpNode by elementwise_mul
I1018 08:53:57.673938 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.673952 4097533 graph.cc:151] create OpNode by elementwise_mul
I1018 08:53:57.673964 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.673978 4097533 graph.cc:151] create OpNode by elementwise_add
I1018 08:53:57.673990 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.674002 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.674014 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.674026 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.674036 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.674048 4097533 graph.cc:151] create OpNode by elementwise_max
I1018 08:53:57.674062 4097533 graph.cc:151] create OpNode by pool2d
I1018 08:53:57.674073 4097533 graph.cc:151] create OpNode by conv2d
I1018 08:53:57.674088 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.674103 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.674115 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.674132 4097533 graph.cc:151] create OpNode by scale
I1018 08:53:57.674144 4097533 graph.cc:151] create OpNode by elementwise_pow
I1018 08:53:57.674156 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.674171 4097533 graph.cc:151] create OpNode by elementwise_sub
I1018 08:53:57.674183 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.674198 4097533 graph.cc:151] create OpNode by scale
I1018 08:53:57.674211 4097533 graph.cc:151] create OpNode by elementwise_pow
I1018 08:53:57.674223 4097533 graph.cc:151] create OpNode by elementwise_mul
I1018 08:53:57.674234 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.674248 4097533 graph.cc:151] create OpNode by elementwise_mul
I1018 08:53:57.674260 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.674274 4097533 graph.cc:151] create OpNode by elementwise_add
I1018 08:53:57.674286 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.674297 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.674309 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.674319 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.674328 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.674340 4097533 graph.cc:151] create OpNode by elementwise_max
I1018 08:53:57.674351 4097533 graph.cc:151] create OpNode by conv2d
I1018 08:53:57.674365 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.674378 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.674391 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.674400 4097533 graph.cc:151] create OpNode by scale
I1018 08:53:57.674412 4097533 graph.cc:151] create OpNode by elementwise_pow
I1018 08:53:57.674424 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.674438 4097533 graph.cc:151] create OpNode by elementwise_sub
I1018 08:53:57.674453 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.674468 4097533 graph.cc:151] create OpNode by scale
I1018 08:53:57.674481 4097533 graph.cc:151] create OpNode by elementwise_pow
I1018 08:53:57.674494 4097533 graph.cc:151] create OpNode by elementwise_mul
I1018 08:53:57.674505 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.674520 4097533 graph.cc:151] create OpNode by elementwise_mul
I1018 08:53:57.674531 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.674546 4097533 graph.cc:151] create OpNode by elementwise_add
I1018 08:53:57.674557 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.674567 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.674578 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.674590 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.674600 4097533 graph.cc:151] create OpNode by elementwise_add
I1018 08:53:57.674611 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.674623 4097533 graph.cc:151] create OpNode by elementwise_max
I1018 08:53:57.674634 4097533 graph.cc:151] create OpNode by conv2d
I1018 08:53:57.674646 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.674657 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.674669 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.674680 4097533 graph.cc:151] create OpNode by scale
I1018 08:53:57.674692 4097533 graph.cc:151] create OpNode by elementwise_pow
I1018 08:53:57.674703 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.674718 4097533 graph.cc:151] create OpNode by elementwise_sub
I1018 08:53:57.674729 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.674744 4097533 graph.cc:151] create OpNode by scale
I1018 08:53:57.674755 4097533 graph.cc:151] create OpNode by elementwise_pow
I1018 08:53:57.674767 4097533 graph.cc:151] create OpNode by elementwise_mul
I1018 08:53:57.674778 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.674790 4097533 graph.cc:151] create OpNode by elementwise_mul
I1018 08:53:57.674801 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.674819 4097533 graph.cc:151] create OpNode by elementwise_add
I1018 08:53:57.674832 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.674844 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.674854 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.674865 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.674875 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.674886 4097533 graph.cc:151] create OpNode by elementwise_max
I1018 08:53:57.674898 4097533 graph.cc:151] create OpNode by conv2d
I1018 08:53:57.674911 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.674923 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.674935 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.674947 4097533 graph.cc:151] create OpNode by scale
I1018 08:53:57.674957 4097533 graph.cc:151] create OpNode by elementwise_pow
I1018 08:53:57.674969 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.674983 4097533 graph.cc:151] create OpNode by elementwise_sub
I1018 08:53:57.674994 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.675009 4097533 graph.cc:151] create OpNode by scale
I1018 08:53:57.675020 4097533 graph.cc:151] create OpNode by elementwise_pow
I1018 08:53:57.675032 4097533 graph.cc:151] create OpNode by elementwise_mul
I1018 08:53:57.675045 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.675057 4097533 graph.cc:151] create OpNode by elementwise_mul
I1018 08:53:57.675069 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.675084 4097533 graph.cc:151] create OpNode by elementwise_add
I1018 08:53:57.675101 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.675114 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.675125 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.675136 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.675146 4097533 graph.cc:151] create OpNode by elementwise_add
I1018 08:53:57.675156 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.675168 4097533 graph.cc:151] create OpNode by elementwise_max
I1018 08:53:57.675179 4097533 graph.cc:151] create OpNode by conv2d
I1018 08:53:57.675192 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.675204 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.675215 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.675225 4097533 graph.cc:151] create OpNode by scale
I1018 08:53:57.675237 4097533 graph.cc:151] create OpNode by elementwise_pow
I1018 08:53:57.675248 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.675261 4097533 graph.cc:151] create OpNode by elementwise_sub
I1018 08:53:57.675272 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.675287 4097533 graph.cc:151] create OpNode by scale
I1018 08:53:57.675297 4097533 graph.cc:151] create OpNode by elementwise_pow
I1018 08:53:57.675309 4097533 graph.cc:151] create OpNode by elementwise_mul
I1018 08:53:57.675321 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.675335 4097533 graph.cc:151] create OpNode by elementwise_mul
I1018 08:53:57.675346 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.675359 4097533 graph.cc:151] create OpNode by elementwise_add
I1018 08:53:57.675369 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.675380 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.675390 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.675400 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.675410 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.675420 4097533 graph.cc:151] create OpNode by elementwise_max
I1018 08:53:57.675431 4097533 graph.cc:151] create OpNode by conv2d
I1018 08:53:57.675444 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.675457 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.675467 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.675478 4097533 graph.cc:151] create OpNode by scale
I1018 08:53:57.675494 4097533 graph.cc:151] create OpNode by elementwise_pow
I1018 08:53:57.675508 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.675521 4097533 graph.cc:151] create OpNode by elementwise_sub
I1018 08:53:57.675532 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.675546 4097533 graph.cc:151] create OpNode by scale
I1018 08:53:57.675557 4097533 graph.cc:151] create OpNode by elementwise_pow
I1018 08:53:57.675568 4097533 graph.cc:151] create OpNode by elementwise_mul
I1018 08:53:57.675578 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.675591 4097533 graph.cc:151] create OpNode by elementwise_mul
I1018 08:53:57.675602 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.675614 4097533 graph.cc:151] create OpNode by elementwise_add
I1018 08:53:57.675626 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.675635 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.675647 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.675657 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.675666 4097533 graph.cc:151] create OpNode by conv2d
I1018 08:53:57.675680 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.675693 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.675704 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.675715 4097533 graph.cc:151] create OpNode by scale
I1018 08:53:57.675727 4097533 graph.cc:151] create OpNode by elementwise_pow
I1018 08:53:57.675738 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.675751 4097533 graph.cc:151] create OpNode by elementwise_sub
I1018 08:53:57.675762 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.675776 4097533 graph.cc:151] create OpNode by scale
I1018 08:53:57.675786 4097533 graph.cc:151] create OpNode by elementwise_pow
I1018 08:53:57.675797 4097533 graph.cc:151] create OpNode by elementwise_mul
I1018 08:53:57.675808 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.675822 4097533 graph.cc:151] create OpNode by elementwise_mul
I1018 08:53:57.675832 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.675845 4097533 graph.cc:151] create OpNode by elementwise_add
I1018 08:53:57.675858 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.675868 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.675879 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.675889 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.675900 4097533 graph.cc:151] create OpNode by elementwise_add
I1018 08:53:57.675910 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.675922 4097533 graph.cc:151] create OpNode by elementwise_max
I1018 08:53:57.675935 4097533 graph.cc:151] create OpNode by conv2d
I1018 08:53:57.675946 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.675957 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.675968 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.675979 4097533 graph.cc:151] create OpNode by scale
I1018 08:53:57.675990 4097533 graph.cc:151] create OpNode by elementwise_pow
I1018 08:53:57.676002 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.676015 4097533 graph.cc:151] create OpNode by elementwise_sub
I1018 08:53:57.676028 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.676039 4097533 graph.cc:151] create OpNode by scale
I1018 08:53:57.676051 4097533 graph.cc:151] create OpNode by elementwise_pow
I1018 08:53:57.676062 4097533 graph.cc:151] create OpNode by elementwise_mul
I1018 08:53:57.676074 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.676086 4097533 graph.cc:151] create OpNode by elementwise_mul
I1018 08:53:57.676097 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.676110 4097533 graph.cc:151] create OpNode by elementwise_add
I1018 08:53:57.676121 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.676131 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.676147 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.676158 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.676169 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.676179 4097533 graph.cc:151] create OpNode by elementwise_max
I1018 08:53:57.676191 4097533 graph.cc:151] create OpNode by conv2d
I1018 08:53:57.676203 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.676214 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.676225 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.676235 4097533 graph.cc:151] create OpNode by scale
I1018 08:53:57.676246 4097533 graph.cc:151] create OpNode by elementwise_pow
I1018 08:53:57.676259 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.676270 4097533 graph.cc:151] create OpNode by elementwise_sub
I1018 08:53:57.676281 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.676293 4097533 graph.cc:151] create OpNode by scale
I1018 08:53:57.676306 4097533 graph.cc:151] create OpNode by elementwise_pow
I1018 08:53:57.676317 4097533 graph.cc:151] create OpNode by elementwise_mul
I1018 08:53:57.676328 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.676342 4097533 graph.cc:151] create OpNode by elementwise_mul
I1018 08:53:57.676352 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.676366 4097533 graph.cc:151] create OpNode by elementwise_add
I1018 08:53:57.676378 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.676388 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.676398 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.676409 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.676417 4097533 graph.cc:151] create OpNode by elementwise_add
I1018 08:53:57.676429 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.676440 4097533 graph.cc:151] create OpNode by elementwise_max
I1018 08:53:57.676451 4097533 graph.cc:151] create OpNode by conv2d
I1018 08:53:57.676463 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.676493 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.676505 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.676517 4097533 graph.cc:151] create OpNode by scale
I1018 08:53:57.676527 4097533 graph.cc:151] create OpNode by elementwise_pow
I1018 08:53:57.676538 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.676553 4097533 graph.cc:151] create OpNode by elementwise_sub
I1018 08:53:57.676564 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.676576 4097533 graph.cc:151] create OpNode by scale
I1018 08:53:57.676589 4097533 graph.cc:151] create OpNode by elementwise_pow
I1018 08:53:57.676599 4097533 graph.cc:151] create OpNode by elementwise_mul
I1018 08:53:57.676609 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.676623 4097533 graph.cc:151] create OpNode by elementwise_mul
I1018 08:53:57.676635 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.676647 4097533 graph.cc:151] create OpNode by elementwise_add
I1018 08:53:57.676658 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.676669 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.676679 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.676690 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.676700 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.676712 4097533 graph.cc:151] create OpNode by elementwise_max
I1018 08:53:57.676723 4097533 graph.cc:151] create OpNode by conv2d
I1018 08:53:57.676733 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.676745 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.676756 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.676767 4097533 graph.cc:151] create OpNode by scale
I1018 08:53:57.676777 4097533 graph.cc:151] create OpNode by elementwise_pow
I1018 08:53:57.676788 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.676801 4097533 graph.cc:151] create OpNode by elementwise_sub
I1018 08:53:57.676816 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.676829 4097533 graph.cc:151] create OpNode by scale
I1018 08:53:57.676841 4097533 graph.cc:151] create OpNode by elementwise_pow
I1018 08:53:57.676851 4097533 graph.cc:151] create OpNode by elementwise_mul
I1018 08:53:57.676862 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.676877 4097533 graph.cc:151] create OpNode by elementwise_mul
I1018 08:53:57.676887 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.676899 4097533 graph.cc:151] create OpNode by elementwise_add
I1018 08:53:57.676911 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.676923 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.676932 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.676942 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.676954 4097533 graph.cc:151] create OpNode by conv2d
I1018 08:53:57.676965 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.676978 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.676988 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.676999 4097533 graph.cc:151] create OpNode by scale
I1018 08:53:57.677011 4097533 graph.cc:151] create OpNode by elementwise_pow
I1018 08:53:57.677021 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.677033 4097533 graph.cc:151] create OpNode by elementwise_sub
I1018 08:53:57.677044 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.677058 4097533 graph.cc:151] create OpNode by scale
I1018 08:53:57.677070 4097533 graph.cc:151] create OpNode by elementwise_pow
I1018 08:53:57.677081 4097533 graph.cc:151] create OpNode by elementwise_mul
I1018 08:53:57.677093 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.677105 4097533 graph.cc:151] create OpNode by elementwise_mul
I1018 08:53:57.677115 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.677127 4097533 graph.cc:151] create OpNode by elementwise_add
I1018 08:53:57.677139 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.677150 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.677160 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.677170 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.677181 4097533 graph.cc:151] create OpNode by elementwise_add
I1018 08:53:57.677191 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.677201 4097533 graph.cc:151] create OpNode by elementwise_max
I1018 08:53:57.677217 4097533 graph.cc:151] create OpNode by conv2d
I1018 08:53:57.677232 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.677244 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.677255 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.677268 4097533 graph.cc:151] create OpNode by scale
I1018 08:53:57.677278 4097533 graph.cc:151] create OpNode by elementwise_pow
I1018 08:53:57.677291 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.677304 4097533 graph.cc:151] create OpNode by elementwise_sub
I1018 08:53:57.677315 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.677328 4097533 graph.cc:151] create OpNode by scale
I1018 08:53:57.677340 4097533 graph.cc:151] create OpNode by elementwise_pow
I1018 08:53:57.677351 4097533 graph.cc:151] create OpNode by elementwise_mul
I1018 08:53:57.677362 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.677376 4097533 graph.cc:151] create OpNode by elementwise_mul
I1018 08:53:57.677386 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.677399 4097533 graph.cc:151] create OpNode by elementwise_add
I1018 08:53:57.677410 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.677421 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.677431 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.677443 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.677453 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.677466 4097533 graph.cc:151] create OpNode by elementwise_max
I1018 08:53:57.677479 4097533 graph.cc:151] create OpNode by conv2d
I1018 08:53:57.677489 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.677502 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.677512 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.677523 4097533 graph.cc:151] create OpNode by scale
I1018 08:53:57.677533 4097533 graph.cc:151] create OpNode by elementwise_pow
I1018 08:53:57.677546 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.677558 4097533 graph.cc:151] create OpNode by elementwise_sub
I1018 08:53:57.677568 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.677582 4097533 graph.cc:151] create OpNode by scale
I1018 08:53:57.677593 4097533 graph.cc:151] create OpNode by elementwise_pow
I1018 08:53:57.677604 4097533 graph.cc:151] create OpNode by elementwise_mul
I1018 08:53:57.677615 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.677628 4097533 graph.cc:151] create OpNode by elementwise_mul
I1018 08:53:57.677639 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.677651 4097533 graph.cc:151] create OpNode by elementwise_add
I1018 08:53:57.677663 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.677673 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.677685 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.677695 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.677706 4097533 graph.cc:151] create OpNode by elementwise_add
I1018 08:53:57.677716 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.677726 4097533 graph.cc:151] create OpNode by elementwise_max
I1018 08:53:57.677737 4097533 graph.cc:151] create OpNode by conv2d
I1018 08:53:57.677749 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.677760 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.677771 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.677783 4097533 graph.cc:151] create OpNode by scale
I1018 08:53:57.677793 4097533 graph.cc:151] create OpNode by elementwise_pow
I1018 08:53:57.677804 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.677817 4097533 graph.cc:151] create OpNode by elementwise_sub
I1018 08:53:57.677829 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.677841 4097533 graph.cc:151] create OpNode by scale
I1018 08:53:57.677852 4097533 graph.cc:151] create OpNode by elementwise_pow
I1018 08:53:57.677863 4097533 graph.cc:151] create OpNode by elementwise_mul
I1018 08:53:57.677874 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.677889 4097533 graph.cc:151] create OpNode by elementwise_mul
I1018 08:53:57.677901 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.677914 4097533 graph.cc:151] create OpNode by elementwise_add
I1018 08:53:57.677925 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.677935 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.677945 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.677955 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.677965 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.677976 4097533 graph.cc:151] create OpNode by elementwise_max
I1018 08:53:57.677989 4097533 graph.cc:151] create OpNode by conv2d
I1018 08:53:57.677999 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.678010 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.678020 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.678031 4097533 graph.cc:151] create OpNode by scale
I1018 08:53:57.678043 4097533 graph.cc:151] create OpNode by elementwise_pow
I1018 08:53:57.678056 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.678069 4097533 graph.cc:151] create OpNode by elementwise_sub
I1018 08:53:57.678079 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.678092 4097533 graph.cc:151] create OpNode by scale
I1018 08:53:57.678104 4097533 graph.cc:151] create OpNode by elementwise_pow
I1018 08:53:57.678119 4097533 graph.cc:151] create OpNode by elementwise_mul
I1018 08:53:57.678131 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.678145 4097533 graph.cc:151] create OpNode by elementwise_mul
I1018 08:53:57.678155 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.678169 4097533 graph.cc:151] create OpNode by elementwise_add
I1018 08:53:57.678182 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.678192 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.678202 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.678212 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.678223 4097533 graph.cc:151] create OpNode by conv2d
I1018 08:53:57.678236 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.678247 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.678257 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.678268 4097533 graph.cc:151] create OpNode by scale
I1018 08:53:57.678280 4097533 graph.cc:151] create OpNode by elementwise_pow
I1018 08:53:57.678292 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.678303 4097533 graph.cc:151] create OpNode by elementwise_sub
I1018 08:53:57.678314 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.678329 4097533 graph.cc:151] create OpNode by scale
I1018 08:53:57.678340 4097533 graph.cc:151] create OpNode by elementwise_pow
I1018 08:53:57.678354 4097533 graph.cc:151] create OpNode by elementwise_mul
I1018 08:53:57.678364 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.678377 4097533 graph.cc:151] create OpNode by elementwise_mul
I1018 08:53:57.678387 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.678400 4097533 graph.cc:151] create OpNode by elementwise_add
I1018 08:53:57.678411 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.678421 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.678431 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.678442 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.678450 4097533 graph.cc:151] create OpNode by elementwise_add
I1018 08:53:57.678460 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.678471 4097533 graph.cc:151] create OpNode by elementwise_max
I1018 08:53:57.678483 4097533 graph.cc:151] create OpNode by conv2d
I1018 08:53:57.678495 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.678506 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.678517 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.678527 4097533 graph.cc:151] create OpNode by scale
I1018 08:53:57.678539 4097533 graph.cc:151] create OpNode by elementwise_pow
I1018 08:53:57.678550 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.678563 4097533 graph.cc:151] create OpNode by elementwise_sub
I1018 08:53:57.678575 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.678588 4097533 graph.cc:151] create OpNode by scale
I1018 08:53:57.678599 4097533 graph.cc:151] create OpNode by elementwise_pow
I1018 08:53:57.678611 4097533 graph.cc:151] create OpNode by elementwise_mul
I1018 08:53:57.678622 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.678635 4097533 graph.cc:151] create OpNode by elementwise_mul
I1018 08:53:57.678645 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.678658 4097533 graph.cc:151] create OpNode by elementwise_add
I1018 08:53:57.678670 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.678680 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.678691 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.678701 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.678712 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.678723 4097533 graph.cc:151] create OpNode by elementwise_max
I1018 08:53:57.678735 4097533 graph.cc:151] create OpNode by conv2d
I1018 08:53:57.678746 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.678758 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.678773 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.678784 4097533 graph.cc:151] create OpNode by scale
I1018 08:53:57.678797 4097533 graph.cc:151] create OpNode by elementwise_pow
I1018 08:53:57.678810 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.678823 4097533 graph.cc:151] create OpNode by elementwise_sub
I1018 08:53:57.678834 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.678849 4097533 graph.cc:151] create OpNode by scale
I1018 08:53:57.678861 4097533 graph.cc:151] create OpNode by elementwise_pow
I1018 08:53:57.678874 4097533 graph.cc:151] create OpNode by elementwise_mul
I1018 08:53:57.678885 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.678900 4097533 graph.cc:151] create OpNode by elementwise_mul
I1018 08:53:57.678911 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.678925 4097533 graph.cc:151] create OpNode by elementwise_add
I1018 08:53:57.678936 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.678948 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.678959 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.678969 4097533 graph.cc:151] create OpNode by assign
I1018 08:53:57.678982 4097533 graph.cc:151] create OpNode by elementwise_add
I1018 08:53:57.678992 4097533 graph.cc:151] create OpNode by fill_constant
I1018 08:53:57.679003 4097533 graph.cc:151] create OpNode by elementwise_max
I1018 08:53:57.679013 4097533 graph.cc:151] create OpNode by pool2d
I1018 08:53:57.679025 4097533 graph.cc:151] create OpNode by reshape2
I1018 08:53:57.679039 4097533 graph.cc:151] create OpNode by matmul_v2
I1018 08:53:57.679050 4097533 graph.cc:151] create OpNode by elementwise_add
I1018 08:53:57.679073 4097533 graph.cc:226] kStaleProgramOpDescs.size: 447
I1018 08:53:57.681555 4097533 parallel_executor.cc:1321] The Program will be executed on CPU using ParallelExecutor, 1 cards are used, so 1 programs are executed in parallel.
I1018 08:53:57.681583 4097533 build_strategy.cc:365] apply all passes
I1018 08:53:57.681593 4097533 pass_builder.cc:29] Append cinn_zero_tensor_trick_pass
I1018 08:53:57.681614 4097533 pass_builder.cc:29] Append build_cinn_pass
I1018 08:53:57.681627 4097533 pass_builder.cc:29] Append fuse_bn_add_act_pass
I1018 08:53:57.681644 4097533 pass_builder.cc:29] Append coalesce_grad_tensor_pass
I1018 08:53:57.681653 4097533 pass_builder.cc:29] Append add_reader_dependency_pass
I1018 08:53:57.681661 4097533 pass_builder.cc:29] Append all_reduce_mode_multi_devices_pass
I1018 08:53:57.681675 4097533 pass_builder.cc:29] Append fuse_all_reduce_op_pass
I1018 08:53:57.681684 4097533 pass_builder.cc:29] Append all_reduce_deps_pass
I1018 08:53:57.681690 4097533 pass_builder.cc:29] Append modify_op_lock_and_record_event_pass
I1018 08:53:57.681694 4097533 pass_builder.cc:29] Append multi_devices_check_pass
I1018 08:53:57.681699 4097533 build_strategy.cc:252] CollectiveContext:endpoints_:trainer_id_:0
I1018 08:53:57.681703 4097533 build_strategy.cc:376] BuildStrategy::Apply pass:cinn_zero_tensor_trick_pass
I1018 08:53:57.681706 4097533 build_strategy.cc:493] Start Apply Pass cinn_zero_tensor_trick_pass
I1018 08:53:57.681710 4097533 build_strategy.cc:496] Apply Pass cinn_zero_tensor_trick_passto SubGraph 0
I1018 08:53:57.682003 4097533 graph_helper.h:104] adj matmul_v20x55b7d0ec38f0 -> elementwise_add0x55b7d0ec5fc0  via linear_0.tmp_00x55b7d0ec5d80
I1018 08:53:57.682008 4097533 graph_helper.h:104] adj pool2d0x55b7d0ebdce0 -> reshape20x55b7d0ec0840  via pool2d_1.tmp_00x55b7d0ec0670
I1018 08:53:57.682010 4097533 graph_helper.h:104] adj elementwise_max0x55b7d0ebab20 -> pool2d0x55b7d0ebdce0  via relu_16.tmp_00x55b7d0ebdb10
I1018 08:53:57.682013 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0e864a0 -> assign0x55b7d0eacdf0  via elementwise_pow_76.tmp_00x55b7d0e894d0
I1018 08:53:57.682015 4097533 graph_helper.h:104] adj reshape20x55b7d0e9ce30 -> elementwise_mul0x55b7d0ea0460  via reshape2_78.tmp_00x55b7d0e9ffc0
I1018 08:53:57.682017 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0e99c90 -> elementwise_mul0x55b7d0ea0460  via tmp_1250x55b7d0e9cc40
I1018 08:53:57.682024 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d0e8ce30 -> elementwise_mul0x55b7d0e99c90  via tmp_1230x55b7d0e8ff60
I1018 08:53:57.682026 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0e96af0 -> elementwise_mul0x55b7d0e99c90  via elementwise_pow_78.tmp_00x55b7d0e99ae0
I1018 08:53:57.682029 4097533 graph_helper.h:104] adj conv2d0x55b7d0e76ae0 -> elementwise_sub0x55b7d0e8ce30  via conv2d_19.tmp_00x55b7d0e799d0
I1018 08:53:57.682031 4097533 graph_helper.h:104] adj reshape20x55b7d0e896c0 -> elementwise_sub0x55b7d0e8ce30  via reshape2_76.tmp_00x55b7d0e8c990
I1018 08:53:57.682034 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0e41940 -> assign0x55b7d0e68290  via elementwise_pow_72.tmp_00x55b7d0e44970
I1018 08:53:57.682037 4097533 graph_helper.h:104] adj scale0x55b7d0e4ec10 -> elementwise_pow0x55b7d0e51f90  via tmp_1180x55b7d0e51dc0
I1018 08:53:57.682039 4097533 graph_helper.h:104] adj fill_constant0x55b7d0e35060 -> elementwise_pow0x55b7d0e51f90  via fill_constant_139.tmp_00x55b7d0e38210
I1018 08:53:57.682042 4097533 graph_helper.h:104] adj reshape20x55b7d0e4b5d0 -> scale0x55b7d0e4ec10  via reshape2_73.tmp_00x55b7d0e4e770
I1018 08:53:57.682045 4097533 graph_helper.h:104] adj fill_constant0x55b7d0e3b690 -> scale0x55b7d0e3e8c0  via fill_constant_143.tmp_00x55b7d0e3e670
I1018 08:53:57.682049 4097533 graph_helper.h:104] adj elementwise_max0x55b7d0e2ede0 -> conv2d0x55b7d0e31fa0  via relu_14.tmp_00x55b7d0e31dd0
I1018 08:53:57.682050 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0ddc790 -> elementwise_add0x55b7d0e29500  via batch_norm_16.tmp_20x55b7d0ddf950
I1018 08:53:57.682052 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0e1b110 -> elementwise_add0x55b7d0e29500  via batch_norm_17.tmp_20x55b7d0e1e2d0
I1018 08:53:57.682055 4097533 graph_helper.h:104] adj scale0x55b7d0e83420 -> elementwise_pow0x55b7d0e864a0  via tmp_1220x55b7d0e862b0
I1018 08:53:57.682058 4097533 graph_helper.h:104] adj fill_constant0x55b7d0e79bc0 -> elementwise_pow0x55b7d0e864a0  via fill_constant_147.tmp_00x55b7d0e7cd70
I1018 08:53:57.682060 4097533 graph_helper.h:104] adj fill_constant0x55b7d0df1260 -> assign0x55b7d0e1e520  via fill_constant_133.tmp_00x55b7d0df4240
I1018 08:53:57.682063 4097533 graph_helper.h:104] adj reshape20x55b7d0e043d0 -> scale0x55b7d0e07a10  via reshape2_69.tmp_00x55b7d0e07570
I1018 08:53:57.682066 4097533 graph_helper.h:104] adj scale0x55b7d0df76c0 -> elementwise_pow0x55b7d0dfa740  via tmp_1100x55b7d0dfa550
I1018 08:53:57.682068 4097533 graph_helper.h:104] adj fill_constant0x55b7d0dede00 -> elementwise_pow0x55b7d0dfa740  via fill_constant_131.tmp_00x55b7d0df1010
I1018 08:53:57.682071 4097533 graph_helper.h:104] adj fill_constant0x55b7d0df4490 -> scale0x55b7d0df76c0  via fill_constant_135.tmp_00x55b7d0df7470
I1018 08:53:57.682075 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0dbbdc0 -> assign0x55b7d0de2710  via elementwise_pow_64.tmp_00x55b7d0dbedf0
I1018 08:53:57.682076 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d0dc2750 -> elementwise_mul0x55b7d0dcf5b0  via tmp_1050x55b7d0dc5880
I1018 08:53:57.682078 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0dcc410 -> elementwise_mul0x55b7d0dcf5b0  via elementwise_pow_66.tmp_00x55b7d0dcf400
I1018 08:53:57.682081 4097533 graph_helper.h:104] adj fill_constant0x55b7d0e7cfc0 -> assign0x55b7d0eaa280  via fill_constant_149.tmp_00x55b7d0e7ffa0
I1018 08:53:57.682085 4097533 graph_helper.h:104] adj scale0x55b7d0db8d40 -> elementwise_pow0x55b7d0dbbdc0  via tmp_1040x55b7d0dbbbd0
I1018 08:53:57.682086 4097533 graph_helper.h:104] adj fill_constant0x55b7d0daf4e0 -> elementwise_pow0x55b7d0dbbdc0  via fill_constant_125.tmp_00x55b7d0db2690
I1018 08:53:57.682089 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0d97c30 -> elementwise_max0x55b7d0da9240  via batch_norm_15.tmp_20x55b7d0d9adf0
I1018 08:53:57.682091 4097533 graph_helper.h:104] adj fill_constant0x55b7d0da6020 -> elementwise_max0x55b7d0da9240  via fill_constant_123.tmp_00x55b7d0da8ff0
I1018 08:53:57.682097 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0d77260 -> assign0x55b7d0d9dbb0  via elementwise_pow_60.tmp_00x55b7d0d7a290
I1018 08:53:57.682101 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0d91220 -> elementwise_add0x55b7d0d97c30  via tmp_1020x55b7d0d943e0
I1018 08:53:57.682102 4097533 graph_helper.h:104] adj reshape20x55b7d0d945b0 -> elementwise_add0x55b7d0d97c30  via reshape2_63.tmp_00x55b7d0d97790
I1018 08:53:57.682104 4097533 graph_helper.h:104] adj scale0x55b7d0d84530 -> elementwise_pow0x55b7d0d878b0  via tmp_1000x55b7d0d876e0
I1018 08:53:57.682107 4097533 graph_helper.h:104] adj fill_constant0x55b7d0d6a980 -> elementwise_pow0x55b7d0d878b0  via fill_constant_117.tmp_00x55b7d0d6db30
I1018 08:53:57.682109 4097533 graph_helper.h:104] adj fill_constant0x55b7d0e38460 -> assign0x55b7d0e65720  via fill_constant_141.tmp_00x55b7d0e3b440
I1018 08:53:57.682112 4097533 graph_helper.h:104] adj scale0x55b7d0d741e0 -> elementwise_pow0x55b7d0d77260  via tmp_980x55b7d0d77070
I1018 08:53:57.682114 4097533 graph_helper.h:104] adj fill_constant0x55b7d0d6a980 -> elementwise_pow0x55b7d0d77260  via fill_constant_117.tmp_00x55b7d0d6db30
I1018 08:53:57.682117 4097533 graph_helper.h:104] adj elementwise_max0x55b7d0d64700 -> conv2d0x55b7d0d678c0  via relu_12.tmp_00x55b7d0d676f0
I1018 08:53:57.682121 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0d5ee40 -> elementwise_max0x55b7d0d64700  via tmp_50x55b7d0d61320
I1018 08:53:57.682122 4097533 graph_helper.h:104] adj fill_constant0x55b7d0d614d0 -> elementwise_max0x55b7d0d64700  via fill_constant_115.tmp_00x55b7d0d644b0
I1018 08:53:57.682125 4097533 graph_helper.h:104] adj reshape20x55b7d0d46a10 -> elementwise_mul0x55b7d0d4a040  via reshape2_58.tmp_00x55b7d0d49ba0
I1018 08:53:57.682127 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0d43870 -> elementwise_mul0x55b7d0d4a040  via tmp_950x55b7d0d46820
I1018 08:53:57.682129 4097533 graph_helper.h:104] adj scale0x55b7d0d3d350 -> elementwise_pow0x55b7d0d406d0  via tmp_940x55b7d0d40500
I1018 08:53:57.682132 4097533 graph_helper.h:104] adj fill_constant0x55b7d0d237a0 -> elementwise_pow0x55b7d0d406d0  via fill_constant_109.tmp_00x55b7d0d26950
I1018 08:53:57.682134 4097533 graph_helper.h:104] adj reshape20x55b7d0d39d10 -> scale0x55b7d0d3d350  via reshape2_57.tmp_00x55b7d0d3ceb0
I1018 08:53:57.682137 4097533 graph_helper.h:104] adj conv2d0x55b7d0d206c0 -> elementwise_sub0x55b7d0d36a10  via conv2d_14.tmp_00x55b7d0d235b0
I1018 08:53:57.682139 4097533 graph_helper.h:104] adj reshape20x55b7d0d332a0 -> elementwise_sub0x55b7d0d36a10  via reshape2_56.tmp_00x55b7d0d36570
I1018 08:53:57.682142 4097533 graph_helper.h:104] adj fill_constant0x55b7d0d29dd0 -> scale0x55b7d0d2d000  via fill_constant_113.tmp_00x55b7d0d2cdb0
I1018 08:53:57.682144 4097533 graph_helper.h:104] adj reshape20x55b7d0e90130 -> scale0x55b7d0e93770  via reshape2_77.tmp_00x55b7d0e932d0
I1018 08:53:57.682147 4097533 graph_helper.h:104] adj elementwise_max0x55b7d0d1d500 -> conv2d0x55b7d0d206c0  via relu_11.tmp_00x55b7d0d20510
I1018 08:53:57.682150 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0d0bef0 -> elementwise_max0x55b7d0d1d500  via batch_norm_13.tmp_20x55b7d0d0f0b0
I1018 08:53:57.682152 4097533 graph_helper.h:104] adj fill_constant0x55b7d0d1a2e0 -> elementwise_max0x55b7d0d1d500  via fill_constant_107.tmp_00x55b7d0d1d2b0
I1018 08:53:57.682155 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d0e482d0 -> elementwise_mul0x55b7d0e55130  via tmp_1170x55b7d0e4b400
I1018 08:53:57.682157 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0e51f90 -> elementwise_mul0x55b7d0e55130  via elementwise_pow_74.tmp_00x55b7d0e54f80
I1018 08:53:57.682160 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0ceb520 -> assign0x55b7d0d11e70  via elementwise_pow_52.tmp_00x55b7d0cee550
I1018 08:53:57.682163 4097533 graph_helper.h:104] adj reshape20x55b7d0d01eb0 -> elementwise_mul0x55b7d0d054e0  via reshape2_54.tmp_00x55b7d0d05040
I1018 08:53:57.682164 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0cfed10 -> elementwise_mul0x55b7d0d054e0  via tmp_890x55b7d0d01cc0
I1018 08:53:57.682169 4097533 graph_helper.h:104] adj reshape20x55b7d0cf51b0 -> scale0x55b7d0cf87f0  via reshape2_53.tmp_00x55b7d0cf8350
I1018 08:53:57.682171 4097533 graph_helper.h:104] adj scale0x55b7d0ce84a0 -> elementwise_pow0x55b7d0ceb520  via tmp_860x55b7d0ceb330
I1018 08:53:57.682174 4097533 graph_helper.h:104] adj fill_constant0x55b7d0cdec40 -> elementwise_pow0x55b7d0ceb520  via fill_constant_101.tmp_00x55b7d0ce1df0
I1018 08:53:57.682176 4097533 graph_helper.h:104] adj fill_constant0x55b7d0ce5270 -> scale0x55b7d0ce84a0  via fill_constant_105.tmp_00x55b7d0ce8250
I1018 08:53:57.682179 4097533 graph_helper.h:104] adj reshape20x55b7d0cbacb0 -> elementwise_mul0x55b7d0cbe2e0  via reshape2_50.tmp_00x55b7d0cbde40
I1018 08:53:57.682183 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0cb7b10 -> elementwise_mul0x55b7d0cbe2e0  via tmp_830x55b7d0cbaac0
I1018 08:53:57.682184 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0e62310 -> elementwise_max0x55b7d0e73920  via batch_norm_18.tmp_20x55b7d0e654d0
I1018 08:53:57.682186 4097533 graph_helper.h:104] adj fill_constant0x55b7d0e70700 -> elementwise_max0x55b7d0e73920  via fill_constant_145.tmp_00x55b7d0e736d0
I1018 08:53:57.682189 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d0caacb0 -> elementwise_mul0x55b7d0cb7b10  via tmp_810x55b7d0cadde0
I1018 08:53:57.682193 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0cb4970 -> elementwise_mul0x55b7d0cb7b10  via elementwise_pow_50.tmp_00x55b7d0cb7960
I1018 08:53:57.682194 4097533 graph_helper.h:104] adj scale0x55b7d0cb15f0 -> elementwise_pow0x55b7d0cb4970  via tmp_820x55b7d0cb47a0
I1018 08:53:57.682196 4097533 graph_helper.h:104] adj fill_constant0x55b7d0c979e0 -> elementwise_pow0x55b7d0cb4970  via fill_constant_93.tmp_00x55b7d0c9abf0
I1018 08:53:57.682199 4097533 graph_helper.h:104] adj reshape20x55b7d0cadfb0 -> scale0x55b7d0cb15f0  via reshape2_49.tmp_00x55b7d0cb1150
I1018 08:53:57.682202 4097533 graph_helper.h:104] adj conv2d0x55b7d0cdbb80 -> elementwise_sub0x55b7d0cf1eb0  via conv2d_13.tmp_00x55b7d0cdea20
I1018 08:53:57.682204 4097533 graph_helper.h:104] adj reshape20x55b7d0cee740 -> elementwise_sub0x55b7d0cf1eb0  via reshape2_52.tmp_00x55b7d0cf1a10
I1018 08:53:57.682207 4097533 graph_helper.h:104] adj scale0x55b7d0ca12a0 -> elementwise_pow0x55b7d0ca4320  via tmp_800x55b7d0ca4130
I1018 08:53:57.682209 4097533 graph_helper.h:104] adj fill_constant0x55b7d0c979e0 -> elementwise_pow0x55b7d0ca4320  via fill_constant_93.tmp_00x55b7d0c9abf0
I1018 08:53:57.682212 4097533 graph_helper.h:104] adj fill_constant0x55b7d0c9e070 -> scale0x55b7d0ca12a0  via fill_constant_97.tmp_00x55b7d0ca1050
I1018 08:53:57.682215 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0d50a50 -> elementwise_add0x55b7d0d5ee40  via batch_norm_14.tmp_20x55b7d0d53c10
I1018 08:53:57.682217 4097533 graph_helper.h:104] adj elementwise_max0x55b7d0cd89c0 -> elementwise_add0x55b7d0d5ee40  via relu_10.tmp_00x55b7d0cdb9b0
I1018 08:53:57.682220 4097533 graph_helper.h:104] adj scale0x55b7d0e93770 -> elementwise_pow0x55b7d0e96af0  via tmp_1240x55b7d0e96920
I1018 08:53:57.682222 4097533 graph_helper.h:104] adj fill_constant0x55b7d0e79bc0 -> elementwise_pow0x55b7d0e96af0  via fill_constant_147.tmp_00x55b7d0e7cd70
I1018 08:53:57.682225 4097533 graph_helper.h:104] adj elementwise_max0x55b7d0c0d100 -> conv2d0x55b7d0c94760  via relu_8.tmp_00x55b7d0c100f0
I1018 08:53:57.682228 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0c659a0 -> assign0x55b7d0c8c2f0  via elementwise_pow_44.tmp_00x55b7d0c689d0
I1018 08:53:57.682230 4097533 graph_helper.h:104] adj reshape20x55b7d0ec0840 -> matmul_v20x55b7d0ec38f0  via flatten_0.tmp_00x55b7d0ec34b0
I1018 08:53:57.682235 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d0c6c330 -> elementwise_mul0x55b7d0c79190  via tmp_750x55b7d0c6f460
I1018 08:53:57.682236 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0c75ff0 -> elementwise_mul0x55b7d0c79190  via elementwise_pow_46.tmp_00x55b7d0c78fe0
I1018 08:53:57.682238 4097533 graph_helper.h:104] adj scale0x55b7d0c72c70 -> elementwise_pow0x55b7d0c75ff0  via tmp_760x55b7d0c75e20
I1018 08:53:57.682243 4097533 graph_helper.h:104] adj fill_constant0x55b7d0c590c0 -> elementwise_pow0x55b7d0c75ff0  via fill_constant_87.tmp_00x55b7d0c5c270
I1018 08:53:57.682246 4097533 graph_helper.h:104] adj fill_constant0x55b7d0d26ba0 -> assign0x55b7d0d53e60  via fill_constant_111.tmp_00x55b7d0d29b80
I1018 08:53:57.682250 4097533 graph_helper.h:104] adj scale0x55b7d0c62920 -> elementwise_pow0x55b7d0c659a0  via tmp_740x55b7d0c657b0
I1018 08:53:57.682251 4097533 graph_helper.h:104] adj fill_constant0x55b7d0c590c0 -> elementwise_pow0x55b7d0c659a0  via fill_constant_87.tmp_00x55b7d0c5c270
I1018 08:53:57.682253 4097533 graph_helper.h:104] adj fill_constant0x55b7d0c5f6f0 -> scale0x55b7d0c62920  via fill_constant_91.tmp_00x55b7d0c626d0
I1018 08:53:57.682256 4097533 graph_helper.h:104] adj conv2d0x55b7d0e31fa0 -> elementwise_sub0x55b7d0e482d0  via conv2d_18.tmp_00x55b7d0e34e40
I1018 08:53:57.682260 4097533 graph_helper.h:104] adj reshape20x55b7d0e44b60 -> elementwise_sub0x55b7d0e482d0  via reshape2_72.tmp_00x55b7d0e47e30
I1018 08:53:57.682261 4097533 graph_helper.h:104] adj elementwise_max0x55b7d0c52e20 -> conv2d0x55b7d0c55fe0  via relu_9.tmp_00x55b7d0c55e30
I1018 08:53:57.682264 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d0d7dbf0 -> elementwise_mul0x55b7d0d8aa50  via tmp_990x55b7d0d80d20
I1018 08:53:57.682266 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0d878b0 -> elementwise_mul0x55b7d0d8aa50  via elementwise_pow_62.tmp_00x55b7d0d8a8a0
I1018 08:53:57.682269 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d0e010d0 -> elementwise_mul0x55b7d0e0df30  via tmp_1110x55b7d0e04200
I1018 08:53:57.682271 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0e0ad90 -> elementwise_mul0x55b7d0e0df30  via elementwise_pow_70.tmp_00x55b7d0e0dd80
I1018 08:53:57.682274 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0c20e40 -> assign0x55b7d0c47790  via elementwise_pow_40.tmp_00x55b7d0c23e70
I1018 08:53:57.682277 4097533 graph_helper.h:104] adj fill_constant0x55b7d0c17c70 -> assign0x55b7d0c44c20  via fill_constant_81.tmp_00x55b7d0c1a970
I1018 08:53:57.682281 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0c3ae00 -> elementwise_add0x55b7d0c41810  via tmp_720x55b7d0c3dfc0
I1018 08:53:57.682282 4097533 graph_helper.h:104] adj reshape20x55b7d0c3e190 -> elementwise_add0x55b7d0c41810  via reshape2_43.tmp_00x55b7d0c41370
I1018 08:53:57.682286 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0ca4320 -> assign0x55b7d0ccac70  via elementwise_pow_48.tmp_00x55b7d0ca7350
I1018 08:53:57.682287 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d0c277d0 -> elementwise_mul0x55b7d0c34630  via tmp_690x55b7d0c2a900
I1018 08:53:57.682291 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0c31490 -> elementwise_mul0x55b7d0c34630  via elementwise_pow_42.tmp_00x55b7d0c34480
I1018 08:53:57.682292 4097533 graph_helper.h:104] adj fill_constant0x55b7d0ce2040 -> assign0x55b7d0d0f300  via fill_constant_103.tmp_00x55b7d0ce5020
I1018 08:53:57.682296 4097533 graph_helper.h:104] adj scale0x55b7d0c1ddc0 -> elementwise_pow0x55b7d0c20e40  via tmp_680x55b7d0c20c50
I1018 08:53:57.682298 4097533 graph_helper.h:104] adj fill_constant0x55b7d0c13380 -> elementwise_pow0x55b7d0c20e40  via fill_constant_79.tmp_00x55b7d0c16530
I1018 08:53:57.682301 4097533 graph_helper.h:104] adj fill_constant0x55b7d0e801f0 -> scale0x55b7d0e83420  via fill_constant_151.tmp_00x55b7d0e831d0
I1018 08:53:57.682303 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0a9bd60 -> elementwise_add0x55b7d0aa2770  via tmp_360x55b7d0a9ef20
I1018 08:53:57.682307 4097533 graph_helper.h:104] adj reshape20x55b7d0a9f0f0 -> elementwise_add0x55b7d0aa2770  via reshape2_19.tmp_00x55b7d0aa22d0
I1018 08:53:57.682308 4097533 graph_helper.h:104] adj fill_constant0x55b7b78735d0 -> assign0x55b7d098d7b0  via fill_constant_3.tmp_00x55b7d0963570
I1018 08:53:57.682312 4097533 graph_helper.h:104] adj reshape20x55b7d0a98730 -> elementwise_mul0x55b7d0a9bd60  via reshape2_18.tmp_00x55b7d0a9b8c0
I1018 08:53:57.682313 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0a95590 -> elementwise_mul0x55b7d0a9bd60  via tmp_350x55b7d0a98540
I1018 08:53:57.682318 4097533 graph_helper.h:104] adj scale0x55b7d0a8f070 -> elementwise_pow0x55b7d0a923f0  via tmp_340x55b7d0a92220
I1018 08:53:57.682322 4097533 graph_helper.h:104] adj fill_constant0x55b7d0a754c0 -> elementwise_pow0x55b7d0a923f0  via fill_constant_33.tmp_00x55b7d0a78670
I1018 08:53:57.682324 4097533 graph_helper.h:104] adj conv2d0x55b7d0a723e0 -> elementwise_sub0x55b7d0a88730  via conv2d_4.tmp_00x55b7d0a752d0
I1018 08:53:57.682327 4097533 graph_helper.h:104] adj reshape20x55b7d0a84fc0 -> elementwise_sub0x55b7d0a88730  via reshape2_16.tmp_00x55b7d0a88290
I1018 08:53:57.682329 4097533 graph_helper.h:104] adj scale0x55b7d0a7ed20 -> elementwise_pow0x55b7d0a81da0  via tmp_320x55b7d0a81bb0
I1018 08:53:57.682332 4097533 graph_helper.h:104] adj fill_constant0x55b7d0a754c0 -> elementwise_pow0x55b7d0a81da0  via fill_constant_33.tmp_00x55b7d0a78670
I1018 08:53:57.682334 4097533 graph_helper.h:104] adj reshape20x55b7d0dc5a50 -> scale0x55b7d0dc9090  via reshape2_65.tmp_00x55b7d0dc8bf0
I1018 08:53:57.682336 4097533 graph_helper.h:104] adj fill_constant0x55b7d0a7baf0 -> scale0x55b7d0a7ed20  via fill_constant_37.tmp_00x55b7d0a7ead0
I1018 08:53:57.682340 4097533 graph_helper.h:104] adj fill_constant0x55b7d0b46a90 -> scale0x55b7d0b49cc0  via fill_constant_59.tmp_00x55b7d0b49a70
I1018 08:53:57.682344 4097533 graph_helper.h:104] adj fill_constant0x55b7d0a36f90 -> scale0x55b7d0a3a1c0  via fill_constant_29.tmp_00x55b7d0a39f70
I1018 08:53:57.682348 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0b0e3c0 -> assign0x55b7d0b34d10  via elementwise_pow_24.tmp_00x55b7d0b113f0
I1018 08:53:57.682350 4097533 graph_helper.h:104] adj reshape20x55b7d0a53bd0 -> elementwise_mul0x55b7d0a57200  via reshape2_14.tmp_00x55b7d0a56d60
I1018 08:53:57.682353 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0a50a30 -> elementwise_mul0x55b7d0a57200  via tmp_290x55b7d0a539e0
I1018 08:53:57.682355 4097533 graph_helper.h:104] adj conv2d0x55b7d09e5eb0 -> elementwise_sub0x55b7d09fc1c0  via conv2d_2.tmp_00x55b7d09e8da0
I1018 08:53:57.682358 4097533 graph_helper.h:104] adj reshape20x55b7d09f8af0 -> elementwise_sub0x55b7d09fc1c0  via reshape2_8.tmp_00x55b7d09fbd20
I1018 08:53:57.682360 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0e5b900 -> elementwise_add0x55b7d0e62310  via tmp_1200x55b7d0e5eac0
I1018 08:53:57.682363 4097533 graph_helper.h:104] adj reshape20x55b7d0e5ec90 -> elementwise_add0x55b7d0e62310  via reshape2_75.tmp_00x55b7d0e61e70
I1018 08:53:57.682366 4097533 graph_helper.h:104] adj scale0x55b7d0a4a510 -> elementwise_pow0x55b7d0a4d890  via tmp_280x55b7d0a4d6c0
I1018 08:53:57.682368 4097533 graph_helper.h:104] adj fill_constant0x55b7d0a30960 -> elementwise_pow0x55b7d0a4d890  via fill_constant_25.tmp_00x55b7d0a33b10
I1018 08:53:57.682371 4097533 graph_helper.h:104] adj fill_constant0x55b7d09a7890 -> assign0x55b7d09d4b20  via fill_constant_11.tmp_00x55b7d09aa870
I1018 08:53:57.682375 4097533 graph_helper.h:104] adj conv2d0x55b7d0a2d8a0 -> elementwise_sub0x55b7d0a43bd0  via conv2d_3.tmp_00x55b7d0a30740
I1018 08:53:57.682377 4097533 graph_helper.h:104] adj reshape20x55b7d0a40460 -> elementwise_sub0x55b7d0a43bd0  via reshape2_12.tmp_00x55b7d0a43730
I1018 08:53:57.682380 4097533 graph_helper.h:104] adj elementwise_max0x55b7d0d64700 -> conv2d0x55b7d0deab80  via relu_12.tmp_00x55b7d0d676f0
I1018 08:53:57.682384 4097533 graph_helper.h:104] adj scale0x55b7d0a3a1c0 -> elementwise_pow0x55b7d0a3d240  via tmp_260x55b7d0a3d050
I1018 08:53:57.682385 4097533 graph_helper.h:104] adj fill_constant0x55b7d0a30960 -> elementwise_pow0x55b7d0a3d240  via fill_constant_25.tmp_00x55b7d0a33b10
I1018 08:53:57.682389 4097533 graph_helper.h:104] adj elementwise_max0x55b7d0a2a6e0 -> conv2d0x55b7d0a2d8a0  via relu_2.tmp_00x55b7d0a2d6d0
I1018 08:53:57.682391 4097533 graph_helper.h:104] adj fill_constant0x55b7d0db5b10 -> scale0x55b7d0db8d40  via fill_constant_129.tmp_00x55b7d0db8af0
I1018 08:53:57.682394 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0a16a60 -> elementwise_add0x55b7d0a24e20  via batch_norm_2.tmp_20x55b7d0a19c20
I1018 08:53:57.682402 4097533 graph_helper.h:104] adj pool2d0x55b7d099eb40 -> elementwise_add0x55b7d0a24e20  via pool2d_0.tmp_00x55b7d09a1410
I1018 08:53:57.682405 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d0d36a10 -> elementwise_mul0x55b7d0d43870  via tmp_930x55b7d0d39b40
I1018 08:53:57.682408 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0d406d0 -> elementwise_mul0x55b7d0d43870  via elementwise_pow_58.tmp_00x55b7d0d436c0
I1018 08:53:57.682410 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d09f5870 -> assign0x55b7d0a1c9e0  via elementwise_pow_8.tmp_00x55b7d09f8880
I1018 08:53:57.682413 4097533 graph_helper.h:104] adj scale0x55b7d09adcf0 -> elementwise_pow0x55b7d09b0d70  via tmp_140x55b7d09b0b80
I1018 08:53:57.682416 4097533 graph_helper.h:104] adj fill_constant0x55b7d09a4440 -> elementwise_pow0x55b7d09b0d70  via fill_constant_9.tmp_00x55b7d09a7640
I1018 08:53:57.682420 4097533 graph_helper.h:104] adj elementwise_max0x55b7d0ab6d00 -> conv2d0x55b7d0ab9ec0  via relu_4.tmp_00x55b7d0ab9cf0
I1018 08:53:57.682422 4097533 graph_helper.h:104] adj reshape20x55b7d0a00bf0 -> scale0x55b7d0a03360  via reshape2_9.tmp_00x55b7d0a02ec0
I1018 08:53:57.682425 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0a57200 -> elementwise_add0x55b7d0a5dc10  via tmp_300x55b7d0a5a3c0
I1018 08:53:57.682427 4097533 graph_helper.h:104] adj reshape20x55b7d0a5a590 -> elementwise_add0x55b7d0a5dc10  via reshape2_15.tmp_00x55b7d0a5d770
I1018 08:53:57.682430 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0bd8a80 -> assign0x55b7d0bff3d0  via elementwise_pow_36.tmp_00x55b7d0bdbab0
I1018 08:53:57.682433 4097533 graph_helper.h:104] adj fill_constant0x55b7d0d6dd80 -> assign0x55b7d0d9b040  via fill_constant_119.tmp_00x55b7d0d70d60
I1018 08:53:57.682436 4097533 graph_helper.h:104] adj reshape20x55b7d0980340 -> elementwise_mul0x55b7d0983990  via reshape2_2.tmp_00x55b7d09834f0
I1018 08:53:57.682438 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d097d250 -> elementwise_mul0x55b7d0983990  via tmp_110x55b7d0980170
I1018 08:53:57.682441 4097533 graph_helper.h:104] adj fill_constant0x55b7d0db28e0 -> assign0x55b7d0ddfba0  via fill_constant_127.tmp_00x55b7d0db58c0
I1018 08:53:57.682444 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0b7bb00 -> elementwise_max0x55b7d0b813e0  via tmp_20x55b7d0b7e000
I1018 08:53:57.682446 4097533 graph_helper.h:104] adj fill_constant0x55b7d0b7e1b0 -> elementwise_max0x55b7d0b813e0  via fill_constant_61.tmp_00x55b7d0b81190
I1018 08:53:57.682449 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0983990 -> elementwise_add0x55b7d098a3a0  via tmp_120x55b7d0986b50
I1018 08:53:57.682451 4097533 graph_helper.h:104] adj reshape20x55b7d0986d20 -> elementwise_add0x55b7d098a3a0  via reshape2_3.tmp_00x55b7d0989f00
I1018 08:53:57.682454 4097533 graph_helper.h:104] adj reshape20x55b7d0973650 -> scale0x55b7d0976c90  via reshape2_1.tmp_00x55b7d09767f0
I1018 08:53:57.682457 4097533 graph_helper.h:104] adj reshape20x55b7d0c377d0 -> elementwise_mul0x55b7d0c3ae00  via reshape2_42.tmp_00x55b7d0c3a960
I1018 08:53:57.682459 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0c34630 -> elementwise_mul0x55b7d0c3ae00  via tmp_710x55b7d0c375e0
I1018 08:53:57.682462 4097533 graph_helper.h:104] adj conv2d0x55b7d0deab80 -> elementwise_sub0x55b7d0e010d0  via conv2d_17.tmp_00x55b7d0dedc30
I1018 08:53:57.682466 4097533 graph_helper.h:104] adj reshape20x55b7d0dfd960 -> elementwise_sub0x55b7d0e010d0  via reshape2_68.tmp_00x55b7d0e00c30
I1018 08:53:57.682468 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0ea0460 -> elementwise_add0x55b7d0ea6e70  via tmp_1260x55b7d0ea3620
I1018 08:53:57.682471 4097533 graph_helper.h:104] adj reshape20x55b7d0ea37f0 -> elementwise_add0x55b7d0ea6e70  via reshape2_79.tmp_00x55b7d0ea69d0
I1018 08:53:57.682473 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0cbe2e0 -> elementwise_add0x55b7d0cc4cf0  via tmp_840x55b7d0cc14a0
I1018 08:53:57.682476 4097533 graph_helper.h:104] adj reshape20x55b7d0cc1670 -> elementwise_add0x55b7d0cc4cf0  via reshape2_51.tmp_00x55b7d0cc4850
I1018 08:53:57.682480 4097533 graph_helper.h:104] adj conv2d0x55b7d0afea00 -> elementwise_sub0x55b7d0b14d50  via conv2d_6.tmp_00x55b7d0b018f0
I1018 08:53:57.682483 4097533 graph_helper.h:104] adj reshape20x55b7d0b115e0 -> elementwise_sub0x55b7d0b14d50  via reshape2_24.tmp_00x55b7d0b148b0
I1018 08:53:57.682485 4097533 graph_helper.h:104] adj scale0x55b7d09669f0 -> elementwise_pow0x55b7d0969a70  via tmp_80x55b7d0969880
I1018 08:53:57.682488 4097533 graph_helper.h:104] adj fill_constant0x55b7c4e22550 -> elementwise_pow0x55b7d0969a70  via fill_constant_1.tmp_00x55b7b77662b0
I1018 08:53:57.682490 4097533 graph_helper.h:104] adj scale0x55b7d0dc9090 -> elementwise_pow0x55b7d0dcc410  via tmp_1060x55b7d0dcc240
I1018 08:53:57.682493 4097533 graph_helper.h:104] adj fill_constant0x55b7d0daf4e0 -> elementwise_pow0x55b7d0dcc410  via fill_constant_125.tmp_00x55b7d0db2690
I1018 08:53:57.682495 4097533 graph_helper.h:104] adj reshape20x55b7d0e110d0 -> elementwise_mul0x55b7d0e14700  via reshape2_70.tmp_00x55b7d0e14260
I1018 08:53:57.682498 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0e0df30 -> elementwise_mul0x55b7d0e14700  via tmp_1130x55b7d0e10ee0
I1018 08:53:57.682502 4097533 graph_helper.h:104] adj scale0x55b7d0b0b340 -> elementwise_pow0x55b7d0b0e3c0  via tmp_440x55b7d0b0e1d0
I1018 08:53:57.682503 4097533 graph_helper.h:104] adj fill_constant0x55b7d0b01ae0 -> elementwise_pow0x55b7d0b0e3c0  via fill_constant_49.tmp_00x55b7d0b04c90
I1018 08:53:57.682507 4097533 graph_helper.h:104] adj scale0x55b7d0ad6b30 -> elementwise_pow0x55b7d0ad9eb0  via tmp_400x55b7d0ad9ce0
I1018 08:53:57.682508 4097533 graph_helper.h:104] adj fill_constant0x55b7d0abcf80 -> elementwise_pow0x55b7d0ad9eb0  via fill_constant_41.tmp_00x55b7d0ac0130
I1018 08:53:57.682511 4097533 graph_helper.h:104] adj fill_constant0x55b7d0b8aa60 -> assign0x55b7d0bb7d00  via fill_constant_65.tmp_00x55b7d0b8da40
I1018 08:53:57.682514 4097533 graph_helper.h:104] adj reshape20x55b7d0a8ba30 -> scale0x55b7d0a8f070  via reshape2_17.tmp_00x55b7d0a8ebd0
I1018 08:53:57.682518 4097533 graph_helper.h:104] adj conv2d0x55b7d0dac400 -> elementwise_sub0x55b7d0dc2750  via conv2d_16.tmp_00x55b7d0daf2f0
I1018 08:53:57.682519 4097533 graph_helper.h:104] adj reshape20x55b7d0dbefe0 -> elementwise_sub0x55b7d0dc2750  via reshape2_64.tmp_00x55b7d0dc22b0
I1018 08:53:57.682523 4097533 graph_helper.h:104] adj pool2d0x55b7d099eb40 -> conv2d0x55b7d09a1600  via pool2d_0.tmp_00x55b7d09a1410
I1018 08:53:57.682526 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0a5dc10 -> elementwise_max0x55b7d0a6f220  via batch_norm_3.tmp_20x55b7d0a60dd0
I1018 08:53:57.682528 4097533 graph_helper.h:104] adj fill_constant0x55b7d0a6c000 -> elementwise_max0x55b7d0a6f220  via fill_constant_31.tmp_00x55b7d0a6efd0
I1018 08:53:57.682531 4097533 graph_helper.h:104] adj scale0x55b7d0a03360 -> elementwise_pow0x55b7d0a066e0  via tmp_220x55b7d0a06510
I1018 08:53:57.682533 4097533 graph_helper.h:104] adj fill_constant0x55b7d09e8f90 -> elementwise_pow0x55b7d0a066e0  via fill_constant_17.tmp_00x55b7d09ec140
I1018 08:53:57.682538 4097533 graph_helper.h:104] adj fill_constant0x55b7d09637c0 -> scale0x55b7d09669f0  via fill_constant_5.tmp_00x55b7d09667a0
I1018 08:53:57.682541 4097533 graph_helper.h:104] adj fill_constant0x55b7d0a33d60 -> assign0x55b7d0a61020  via fill_constant_27.tmp_00x55b7d0a36d40
I1018 08:53:57.682543 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0c41810 -> elementwise_max0x55b7d0c52e20  via batch_norm_10.tmp_20x55b7d0c449d0
I1018 08:53:57.682545 4097533 graph_helper.h:104] adj fill_constant0x55b7d0c4fc00 -> elementwise_max0x55b7d0c52e20  via fill_constant_85.tmp_00x55b7d0c52bd0
I1018 08:53:57.682548 4097533 graph_helper.h:104] adj scale0x55b7d0ba11f0 -> elementwise_pow0x55b7d0ba4570  via tmp_580x55b7d0ba43a0
I1018 08:53:57.682551 4097533 graph_helper.h:104] adj fill_constant0x55b7d0b87660 -> elementwise_pow0x55b7d0ba4570  via fill_constant_63.tmp_00x55b7d0b8a810
I1018 08:53:57.682554 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0cd30e0 -> elementwise_max0x55b7d0cd89c0  via tmp_40x55b7d0cd55e0
I1018 08:53:57.682559 4097533 graph_helper.h:104] adj fill_constant0x55b7d0cd5790 -> elementwise_max0x55b7d0cd89c0  via fill_constant_99.tmp_00x55b7d0cd8770
I1018 08:53:57.682561 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0969a70 -> assign0x55b7d0990320  via elementwise_pow_0.tmp_00x55b7d096ca80
I1018 08:53:57.682564 4097533 graph_helper.h:104] adj elementwise_max0x55b7d099b980 -> pool2d0x55b7d099eb40  via relu_0.tmp_00x55b7d099e990
I1018 08:53:57.682567 4097533 graph_helper.h:104] adj reshape20x55b7d0a46ed0 -> scale0x55b7d0a4a510  via reshape2_13.tmp_00x55b7d0a4a070
I1018 08:53:57.682570 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0ea6e70 -> elementwise_add0x55b7d0eb5260  via batch_norm_19.tmp_20x55b7d0eaa030
I1018 08:53:57.682572 4097533 graph_helper.h:104] adj elementwise_max0x55b7d0e2ede0 -> elementwise_add0x55b7d0eb5260  via relu_14.tmp_00x55b7d0e31dd0
I1018 08:53:57.682575 4097533 graph_helper.h:104] adj elementwise_max0x55b7d0da9240 -> conv2d0x55b7d0dac400  via relu_13.tmp_00x55b7d0dac250
I1018 08:53:57.682579 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0a81da0 -> assign0x55b7d0aa9330  via elementwise_pow_16.tmp_00x55b7d0a84dd0
I1018 08:53:57.682581 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0d30080 -> assign0x55b7d0d569d0  via elementwise_pow_56.tmp_00x55b7d0d330b0
I1018 08:53:57.682585 4097533 graph_helper.h:104] adj conv2d0x55b7d0d678c0 -> elementwise_sub0x55b7d0d7dbf0  via conv2d_15.tmp_00x55b7d0d6a760
I1018 08:53:57.682586 4097533 graph_helper.h:104] adj reshape20x55b7d0d7a480 -> elementwise_sub0x55b7d0d7dbf0  via reshape2_60.tmp_00x55b7d0d7d750
I1018 08:53:57.682590 4097533 graph_helper.h:104] adj fill_constant0x55b7d0bd27d0 -> scale0x55b7d0bd5a00  via fill_constant_75.tmp_00x55b7d0bd57b0
I1018 08:53:57.682593 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0a10050 -> elementwise_add0x55b7d0a16a60  via tmp_240x55b7d0a13210
I1018 08:53:57.682595 4097533 graph_helper.h:104] adj reshape20x55b7d0a133e0 -> elementwise_add0x55b7d0a16a60  via reshape2_11.tmp_00x55b7d0a165c0
I1018 08:53:57.682598 4097533 graph_helper.h:104] adj fill_constant0x55b7d0b43860 -> assign0x55b7d0b70b20  via fill_constant_57.tmp_00x55b7d0b46840
I1018 08:53:57.682601 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0eb5260 -> elementwise_max0x55b7d0ebab20  via tmp_70x55b7d0eb7740
I1018 08:53:57.682603 4097533 graph_helper.h:104] adj fill_constant0x55b7d0eb78f0 -> elementwise_max0x55b7d0ebab20  via fill_constant_153.tmp_00x55b7d0eba8d0
I1018 08:53:57.682606 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d0a88730 -> elementwise_mul0x55b7d0a95590  via tmp_330x55b7d0a8b860
I1018 08:53:57.682608 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0a923f0 -> elementwise_mul0x55b7d0a95590  via elementwise_pow_18.tmp_00x55b7d0a953e0
I1018 08:53:57.682611 4097533 graph_helper.h:104] adj reshape20x55b7d0c7c330 -> elementwise_mul0x55b7d0c7f960  via reshape2_46.tmp_00x55b7d0c7f4c0
I1018 08:53:57.682613 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0c79190 -> elementwise_mul0x55b7d0c7f960  via tmp_770x55b7d0c7c140
I1018 08:53:57.682616 4097533 graph_helper.h:104] adj conv2d0x55b7b85da860 -> elementwise_sub0x55b7d0970350  via conv2d_0.tmp_00x55b7cd159bc0
I1018 08:53:57.682619 4097533 graph_helper.h:104] adj reshape20x55b7d096ccf0 -> elementwise_sub0x55b7d0970350  via reshape2_0.tmp_00x55b7d096feb0
I1018 08:53:57.682621 4097533 graph_helper.h:104] adj scale0x55b7d0d2d000 -> elementwise_pow0x55b7d0d30080  via tmp_920x55b7d0d2fe90
I1018 08:53:57.682624 4097533 graph_helper.h:104] adj fill_constant0x55b7d0d237a0 -> elementwise_pow0x55b7d0d30080  via fill_constant_109.tmp_00x55b7d0d26950
I1018 08:53:57.682627 4097533 graph_helper.h:104] adj fill_constant0x55b7d0c9ae40 -> assign0x55b7d0cc8100  via fill_constant_95.tmp_00x55b7d0c9de20
I1018 08:53:57.682631 4097533 graph_helper.h:104] adj elementwise_max0x55b7d0a6f220 -> conv2d0x55b7d0a723e0  via relu_3.tmp_00x55b7d0a72230
I1018 08:53:57.682633 4097533 graph_helper.h:104] adj fill_constant0x55b7d0a788c0 -> assign0x55b7d0aa7230  via fill_constant_35.tmp_00x55b7d0a7b8a0
I1018 08:53:57.682637 4097533 graph_helper.h:104] adj reshape20x55b7d0b9dbb0 -> scale0x55b7d0ba11f0  via reshape2_33.tmp_00x55b7d0ba0d50
I1018 08:53:57.682641 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d0970350 -> elementwise_mul0x55b7d097d250  via tmp_90x55b7d0973480
I1018 08:53:57.682642 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d097a010 -> elementwise_mul0x55b7d097d250  via elementwise_pow_2.tmp_00x55b7d097d020
I1018 08:53:57.682646 4097533 graph_helper.h:104] adj elementwise_add0x55b7d098a3a0 -> elementwise_max0x55b7d099b980  via batch_norm_0.tmp_20x55b7d098d560
I1018 08:53:57.682648 4097533 graph_helper.h:104] adj fill_constant0x55b7d0998760 -> elementwise_max0x55b7d099b980  via fill_constant_7.tmp_00x55b7d099b730
I1018 08:53:57.682651 4097533 graph_helper.h:104] adj elementwise_max0x55b7d0afb840 -> conv2d0x55b7d0afea00  via relu_5.tmp_00x55b7d0afe850
I1018 08:53:57.682654 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0d4a040 -> elementwise_add0x55b7d0d50a50  via tmp_960x55b7d0d4d200
I1018 08:53:57.682657 4097533 graph_helper.h:104] adj reshape20x55b7d0d4d3d0 -> elementwise_add0x55b7d0d50a50  via reshape2_59.tmp_00x55b7d0d505b0
I1018 08:53:57.682659 4097533 graph_helper.h:104] adj scale0x55b7d0976c90 -> elementwise_pow0x55b7d097a010  via tmp_100x55b7d0979e40
I1018 08:53:57.682662 4097533 graph_helper.h:104] adj fill_constant0x55b7c4e22550 -> elementwise_pow0x55b7d097a010  via fill_constant_1.tmp_00x55b7b77662b0
I1018 08:53:57.682664 4097533 graph_helper.h:104] adj fill_constant0x55b7d09aaac0 -> scale0x55b7d09adcf0  via fill_constant_13.tmp_00x55b7d09adaa0
I1018 08:53:57.682668 4097533 graph_helper.h:104] adj reshape20x55b7d0ae01f0 -> elementwise_mul0x55b7d0ae3820  via reshape2_22.tmp_00x55b7d0ae3380
I1018 08:53:57.682670 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0add050 -> elementwise_mul0x55b7d0ae3820  via tmp_410x55b7d0ae0000
I1018 08:53:57.682673 4097533 graph_helper.h:104] adj reshape20x55b7d0a0ca20 -> elementwise_mul0x55b7d0a10050  via reshape2_10.tmp_00x55b7d0a0fbb0
I1018 08:53:57.682677 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0a09880 -> elementwise_mul0x55b7d0a10050  via tmp_230x55b7d0a0c830
I1018 08:53:57.682679 4097533 graph_helper.h:104] adj scale0x55b7d0b1b690 -> elementwise_pow0x55b7d0b1ea10  via tmp_460x55b7d0b1e840
I1018 08:53:57.682682 4097533 graph_helper.h:104] adj fill_constant0x55b7d0b01ae0 -> elementwise_pow0x55b7d0b1ea10  via fill_constant_49.tmp_00x55b7d0b04c90
I1018 08:53:57.682684 4097533 graph_helper.h:104] adj fill_constant0x55b7d0bcf5a0 -> assign0x55b7d0bfc860  via fill_constant_73.tmp_00x55b7d0bd2580
I1018 08:53:57.682687 4097533 graph_helper.h:104] adj reshape20x55b7d09ba9c0 -> scale0x55b7d09be000  via reshape2_5.tmp_00x55b7d09bdb60
I1018 08:53:57.682690 4097533 graph_helper.h:104] adj scale0x55b7d09be000 -> elementwise_pow0x55b7d09c1380  via tmp_160x55b7d09c11b0
I1018 08:53:57.682693 4097533 graph_helper.h:104] adj fill_constant0x55b7d09a4440 -> elementwise_pow0x55b7d09c1380  via fill_constant_9.tmp_00x55b7d09a7640
I1018 08:53:57.682695 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0e14700 -> elementwise_add0x55b7d0e1b110  via tmp_1140x55b7d0e178c0
I1018 08:53:57.682698 4097533 graph_helper.h:104] adj reshape20x55b7d0e17a90 -> elementwise_add0x55b7d0e1b110  via reshape2_71.tmp_00x55b7d0e1ac70
I1018 08:53:57.682700 4097533 graph_helper.h:104] adj scale0x55b7d0e07a10 -> elementwise_pow0x55b7d0e0ad90  via tmp_1120x55b7d0e0abc0
I1018 08:53:57.682703 4097533 graph_helper.h:104] adj fill_constant0x55b7d0dede00 -> elementwise_pow0x55b7d0e0ad90  via fill_constant_131.tmp_00x55b7d0df1010
I1018 08:53:57.682705 4097533 graph_helper.h:104] adj fill_constant0x55b7d09ef5c0 -> scale0x55b7d09f27f0  via fill_constant_21.tmp_00x55b7d09f25a0
I1018 08:53:57.682708 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0bf2a40 -> elementwise_add0x55b7d0bf9450  via tmp_660x55b7d0bf5c00
I1018 08:53:57.682710 4097533 graph_helper.h:104] adj reshape20x55b7d0bf5dd0 -> elementwise_add0x55b7d0bf9450  via reshape2_39.tmp_00x55b7d0bf8fb0
I1018 08:53:57.682716 4097533 graph_helper.h:104] adj fill_constant0x55b7d0b08110 -> scale0x55b7d0b0b340  via fill_constant_53.tmp_00x55b7d0b0b0f0
I1018 08:53:57.682719 4097533 graph_helper.h:104] adj elementwise_max0x55b7d09e2cf0 -> conv2d0x55b7d09e5eb0  via relu_1.tmp_00x55b7d09e5d00
I1018 08:53:57.682722 4097533 graph_helper.h:104] adj fill_constant0x55b7d09ec390 -> assign0x55b7d0a19e70  via fill_constant_19.tmp_00x55b7d09ef370
I1018 08:53:57.682725 4097533 graph_helper.h:104] adj scale0x55b7d0e3e8c0 -> elementwise_pow0x55b7d0e41940  via tmp_1160x55b7d0e41750
I1018 08:53:57.682727 4097533 graph_helper.h:104] adj fill_constant0x55b7d0e35060 -> elementwise_pow0x55b7d0e41940  via fill_constant_139.tmp_00x55b7d0e38210
I1018 08:53:57.682730 4097533 graph_helper.h:104] adj reshape20x55b7d0c6f630 -> scale0x55b7d0c72c70  via reshape2_45.tmp_00x55b7d0c727d0
I1018 08:53:57.682734 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0ae3820 -> elementwise_add0x55b7d0aea230  via tmp_420x55b7d0ae69e0
I1018 08:53:57.682736 4097533 graph_helper.h:104] adj reshape20x55b7d0ae6bb0 -> elementwise_add0x55b7d0aea230  via reshape2_23.tmp_00x55b7d0ae9d90
I1018 08:53:57.682739 4097533 graph_helper.h:104] adj scale0x55b7d09f27f0 -> elementwise_pow0x55b7d09f5870  via tmp_200x55b7d09f5680
I1018 08:53:57.682741 4097533 graph_helper.h:104] adj fill_constant0x55b7d09e8f90 -> elementwise_pow0x55b7d09f5870  via fill_constant_17.tmp_00x55b7d09ec140
I1018 08:53:57.682744 4097533 graph_helper.h:104] adj elementwise_max0x55b7d0bc5f00 -> conv2d0x55b7d0bc90c0  via relu_7.tmp_00x55b7d0bc8f10
I1018 08:53:57.682747 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0aa2770 -> elementwise_add0x55b7d0ab1440  via batch_norm_4.tmp_20x55b7d0aa5930
I1018 08:53:57.682750 4097533 graph_helper.h:104] adj elementwise_max0x55b7d0a2a6e0 -> elementwise_add0x55b7d0ab1440  via relu_2.tmp_00x55b7d0a2d6d0
I1018 08:53:57.682754 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d09fc1c0 -> elementwise_mul0x55b7d0a09880  via tmp_210x55b7d09ff2f0
I1018 08:53:57.682755 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0a066e0 -> elementwise_mul0x55b7d0a09880  via elementwise_pow_10.tmp_00x55b7d0a096d0
I1018 08:53:57.682758 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0ab1440 -> elementwise_max0x55b7d0ab6d00  via tmp_10x55b7d0ab3920
I1018 08:53:57.682760 4097533 graph_helper.h:104] adj fill_constant0x55b7d0ab3ad0 -> elementwise_max0x55b7d0ab6d00  via fill_constant_39.tmp_00x55b7d0ab6ab0
I1018 08:53:57.682763 4097533 graph_helper.h:104] adj scale0x55b7d0c2e110 -> elementwise_pow0x55b7d0c31490  via tmp_700x55b7d0c312c0
I1018 08:53:57.682766 4097533 graph_helper.h:104] adj fill_constant0x55b7d0c13380 -> elementwise_pow0x55b7d0c31490  via fill_constant_79.tmp_00x55b7d0c16530
I1018 08:53:57.682770 4097533 graph_helper.h:104] adj elementwise_max0x55b7d0cd89c0 -> conv2d0x55b7d0cdbb80  via relu_10.tmp_00x55b7d0cdb9b0
I1018 08:53:57.682771 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0c86370 -> elementwise_add0x55b7d0cd30e0  via batch_norm_11.tmp_20x55b7d0c89530
I1018 08:53:57.682775 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0cc4cf0 -> elementwise_add0x55b7d0cd30e0  via batch_norm_12.tmp_20x55b7d0cc7eb0
I1018 08:53:57.682776 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0b28380 -> elementwise_add0x55b7d0b2ed90  via tmp_480x55b7d0b2b540
I1018 08:53:57.682780 4097533 graph_helper.h:104] adj reshape20x55b7d0b2b710 -> elementwise_add0x55b7d0b2ed90  via reshape2_27.tmp_00x55b7d0b2e8f0
I1018 08:53:57.682782 4097533 graph_helper.h:104] adj reshape20x55b7d0baa8b0 -> elementwise_mul0x55b7d0badee0  via reshape2_34.tmp_00x55b7d0bada40
I1018 08:53:57.682785 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0ba7710 -> elementwise_mul0x55b7d0badee0  via tmp_590x55b7d0baa6c0
I1018 08:53:57.682787 4097533 graph_helper.h:104] adj fill_constant0x55b7d0ac35b0 -> scale0x55b7d0ac67e0  via fill_constant_45.tmp_00x55b7d0ac6590
I1018 08:53:57.682790 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0b66d00 -> elementwise_add0x55b7d0b6d710  via tmp_540x55b7d0b69ec0
I1018 08:53:57.682796 4097533 graph_helper.h:104] adj reshape20x55b7d0b6a090 -> elementwise_add0x55b7d0b6d710  via reshape2_31.tmp_00x55b7d0b6d270
I1018 08:53:57.682797 4097533 graph_helper.h:104] adj scale0x55b7d0ac67e0 -> elementwise_pow0x55b7d0ac9860  via tmp_380x55b7d0ac9670
I1018 08:53:57.682801 4097533 graph_helper.h:104] adj fill_constant0x55b7d0abcf80 -> elementwise_pow0x55b7d0ac9860  via fill_constant_41.tmp_00x55b7d0ac0130
I1018 08:53:57.682803 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d0cf1eb0 -> elementwise_mul0x55b7d0cfed10  via tmp_870x55b7d0cf4fe0
I1018 08:53:57.682806 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0cfbb70 -> elementwise_mul0x55b7d0cfed10  via elementwise_pow_54.tmp_00x55b7d0cfeb60
I1018 08:53:57.682808 4097533 graph_helper.h:104] adj scale0x55b7d0be5d50 -> elementwise_pow0x55b7d0be90d0  via tmp_640x55b7d0be8f00
I1018 08:53:57.682811 4097533 graph_helper.h:104] adj fill_constant0x55b7d0bcc1a0 -> elementwise_pow0x55b7d0be90d0  via fill_constant_71.tmp_00x55b7d0bcf350
I1018 08:53:57.682813 4097533 graph_helper.h:104] adj conv2d0x55b7d0ab9ec0 -> elementwise_sub0x55b7d0ad0210  via conv2d_5.tmp_00x55b7d0abcd60
I1018 08:53:57.682816 4097533 graph_helper.h:104] adj reshape20x55b7d0acca80 -> elementwise_sub0x55b7d0ad0210  via reshape2_20.tmp_00x55b7d0acfd70
I1018 08:53:57.682818 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0c7f960 -> elementwise_add0x55b7d0c86370  via tmp_780x55b7d0c82b20
I1018 08:53:57.682821 4097533 graph_helper.h:104] adj reshape20x55b7d0c82cf0 -> elementwise_add0x55b7d0c86370  via reshape2_47.tmp_00x55b7d0c85ed0
I1018 08:53:57.682823 4097533 graph_helper.h:104] adj reshape20x55b7d0ad34f0 -> scale0x55b7d0ad6b30  via reshape2_21.tmp_00x55b7d0ad6690
I1018 08:53:57.682826 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0a3d240 -> assign0x55b7d0a63b90  via elementwise_pow_12.tmp_00x55b7d0a40270
I1018 08:53:57.682829 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d0ad0210 -> elementwise_mul0x55b7d0add050  via tmp_390x55b7d0ad3320
I1018 08:53:57.682832 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0ad9eb0 -> elementwise_mul0x55b7d0add050  via elementwise_pow_22.tmp_00x55b7d0adcea0
I1018 08:53:57.682834 4097533 graph_helper.h:104] adj elementwise_add0x55b7d09d1710 -> elementwise_max0x55b7d09e2cf0  via batch_norm_1.tmp_20x55b7d09d48d0
I1018 08:53:57.682837 4097533 graph_helper.h:104] adj fill_constant0x55b7d09dfad0 -> elementwise_max0x55b7d09e2cf0  via fill_constant_15.tmp_00x55b7d09e2aa0
I1018 08:53:57.682839 4097533 graph_helper.h:104] adj fill_constant0x55b7d0ac0380 -> assign0x55b7d0aed640  via fill_constant_43.tmp_00x55b7d0ac3360
I1018 08:53:57.682842 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0ac9860 -> assign0x55b7d0af01b0  via elementwise_pow_20.tmp_00x55b7d0acc890
I1018 08:53:57.682845 4097533 graph_helper.h:104] adj reshape20x55b7d0b24d50 -> elementwise_mul0x55b7d0b28380  via reshape2_26.tmp_00x55b7d0b27ee0
I1018 08:53:57.682847 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0b21bb0 -> elementwise_mul0x55b7d0b28380  via tmp_470x55b7d0b24b60
I1018 08:53:57.682850 4097533 graph_helper.h:104] adj conv2d0x55b7d0c102c0 -> elementwise_sub0x55b7d0c277d0  via conv2d_10.tmp_00x55b7d0c13160
I1018 08:53:57.682853 4097533 graph_helper.h:104] adj reshape20x55b7d0c24060 -> elementwise_sub0x55b7d0c277d0  via reshape2_40.tmp_00x55b7d0c27330
I1018 08:53:57.682857 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0aea230 -> elementwise_max0x55b7d0afb840  via batch_norm_5.tmp_20x55b7d0aed3f0
I1018 08:53:57.682858 4097533 graph_helper.h:104] adj fill_constant0x55b7d0af8620 -> elementwise_max0x55b7d0afb840  via fill_constant_47.tmp_00x55b7d0afb5f0
I1018 08:53:57.682861 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0b93f20 -> assign0x55b7d0bba870  via elementwise_pow_32.tmp_00x55b7d0b96f50
I1018 08:53:57.682864 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d09b76c0 -> elementwise_mul0x55b7d09c45c0  via tmp_150x55b7d09ba7f0
I1018 08:53:57.682868 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d09c1380 -> elementwise_mul0x55b7d09c45c0  via elementwise_pow_6.tmp_00x55b7d09c4390
I1018 08:53:57.682873 4097533 graph_helper.h:104] adj reshape20x55b7d0bef410 -> elementwise_mul0x55b7d0bf2a40  via reshape2_38.tmp_00x55b7d0bf25a0
I1018 08:53:57.682874 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0bec270 -> elementwise_mul0x55b7d0bf2a40  via tmp_650x55b7d0bef220
I1018 08:53:57.682878 4097533 graph_helper.h:104] adj reshape20x55b7d0dd2750 -> elementwise_mul0x55b7d0dd5d80  via reshape2_66.tmp_00x55b7d0dd58e0
I1018 08:53:57.682880 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0dcf5b0 -> elementwise_mul0x55b7d0dd5d80  via tmp_1070x55b7d0dd2560
I1018 08:53:57.682883 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d0b9a8b0 -> elementwise_mul0x55b7d0ba7710  via tmp_570x55b7d0b9d9e0
I1018 08:53:57.682885 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0ba4570 -> elementwise_mul0x55b7d0ba7710  via elementwise_pow_34.tmp_00x55b7d0ba7560
I1018 08:53:57.682888 4097533 graph_helper.h:104] adj reshape20x55b7d0b18050 -> scale0x55b7d0b1b690  via reshape2_25.tmp_00x55b7d0b1b1f0
I1018 08:53:57.682891 4097533 graph_helper.h:104] adj elementwise_max0x55b7d0e73920 -> conv2d0x55b7d0e76ae0  via relu_15.tmp_00x55b7d0e76930
I1018 08:53:57.682894 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0badee0 -> elementwise_add0x55b7d0bb48f0  via tmp_600x55b7d0bb10a0
I1018 08:53:57.682896 4097533 graph_helper.h:104] adj reshape20x55b7d0bb1270 -> elementwise_add0x55b7d0bb48f0  via reshape2_35.tmp_00x55b7d0bb4450
I1018 08:53:57.682899 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d0b14d50 -> elementwise_mul0x55b7d0b21bb0  via tmp_450x55b7d0b17e80
I1018 08:53:57.682901 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0b1ea10 -> elementwise_mul0x55b7d0b21bb0  via elementwise_pow_26.tmp_00x55b7d0b21a00
I1018 08:53:57.682904 4097533 graph_helper.h:104] adj fill_constant0x55b7d0b04ee0 -> assign0x55b7d0b321a0  via fill_constant_51.tmp_00x55b7d0b07ec0
I1018 08:53:57.682907 4097533 graph_helper.h:104] adj conv2d0x55b7d0c55fe0 -> elementwise_sub0x55b7d0c6c330  via conv2d_11.tmp_00x55b7d0c58ed0
I1018 08:53:57.682909 4097533 graph_helper.h:104] adj reshape20x55b7d0c68bc0 -> elementwise_sub0x55b7d0c6c330  via reshape2_44.tmp_00x55b7d0c6be90
I1018 08:53:57.682912 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0e29500 -> elementwise_max0x55b7d0e2ede0  via tmp_60x55b7d0e2ba00
I1018 08:53:57.682914 4097533 graph_helper.h:104] adj fill_constant0x55b7d0e2bbb0 -> elementwise_max0x55b7d0e2ede0  via fill_constant_137.tmp_00x55b7d0e2eb90
I1018 08:53:57.682917 4097533 graph_helper.h:104] adj elementwise_max0x55b7d0ab6d00 -> conv2d0x55b7d0b3d180  via relu_4.tmp_00x55b7d0ab9cf0
I1018 08:53:57.682921 4097533 graph_helper.h:104] adj reshape20x55b7d0d80ef0 -> scale0x55b7d0d84530  via reshape2_61.tmp_00x55b7d0d84090
I1018 08:53:57.682924 4097533 graph_helper.h:104] adj scale0x55b7d0b49cc0 -> elementwise_pow0x55b7d0b4cd40  via tmp_500x55b7d0b4cb50
I1018 08:53:57.682926 4097533 graph_helper.h:104] adj fill_constant0x55b7d0b40400 -> elementwise_pow0x55b7d0b4cd40  via fill_constant_55.tmp_00x55b7d0b43610
I1018 08:53:57.682929 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d09cad00 -> elementwise_add0x55b7d09d1710  via tmp_180x55b7d09cdec0
I1018 08:53:57.682931 4097533 graph_helper.h:104] adj reshape20x55b7d09ce090 -> elementwise_add0x55b7d09d1710  via reshape2_7.tmp_00x55b7d09d1270
I1018 08:53:57.682934 4097533 graph_helper.h:104] adj conv2d0x55b7d0b3d180 -> elementwise_sub0x55b7d0b536d0  via conv2d_7.tmp_00x55b7d0b40230
I1018 08:53:57.682936 4097533 graph_helper.h:104] adj reshape20x55b7d0b4ff60 -> elementwise_sub0x55b7d0b536d0  via reshape2_28.tmp_00x55b7d0b53230
I1018 08:53:57.682940 4097533 graph_helper.h:104] adj fill_constant0x55b7d0d70fb0 -> scale0x55b7d0d741e0  via fill_constant_121.tmp_00x55b7d0d73f90
I1018 08:53:57.682942 4097533 graph_helper.h:104] adj scale0x55b7d0bd5a00 -> elementwise_pow0x55b7d0bd8a80  via tmp_620x55b7d0bd8890
I1018 08:53:57.682945 4097533 graph_helper.h:104] adj fill_constant0x55b7d0bcc1a0 -> elementwise_pow0x55b7d0bd8a80  via fill_constant_71.tmp_00x55b7d0bcf350
I1018 08:53:57.682950 4097533 graph_helper.h:104] adj reshape20x55b7d0b569d0 -> scale0x55b7d0b5a010  via reshape2_29.tmp_00x55b7d0b59b70
I1018 08:53:57.682952 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d0a43bd0 -> elementwise_mul0x55b7d0a50a30  via tmp_270x55b7d0a46d00
I1018 08:53:57.682955 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0a4d890 -> elementwise_mul0x55b7d0a50a30  via elementwise_pow_14.tmp_00x55b7d0a50880
I1018 08:53:57.682957 4097533 graph_helper.h:104] adj scale0x55b7d0b5a010 -> elementwise_pow0x55b7d0b5d390  via tmp_520x55b7d0b5d1c0
I1018 08:53:57.682960 4097533 graph_helper.h:104] adj fill_constant0x55b7d0b40400 -> elementwise_pow0x55b7d0b5d390  via fill_constant_55.tmp_00x55b7d0b43610
I1018 08:53:57.682962 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d0b536d0 -> elementwise_mul0x55b7d0b60530  via tmp_510x55b7d0b56800
I1018 08:53:57.682965 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0b5d390 -> elementwise_mul0x55b7d0b60530  via elementwise_pow_30.tmp_00x55b7d0b60380
I1018 08:53:57.682968 4097533 graph_helper.h:104] adj fill_constant0x55b7d0b8dc90 -> scale0x55b7d0b90ee0  via fill_constant_67.tmp_00x55b7d0b90c70
I1018 08:53:57.682971 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0a24e20 -> elementwise_max0x55b7d0a2a6e0  via tmp_00x55b7d0a27300
I1018 08:53:57.682973 4097533 graph_helper.h:104] adj fill_constant0x55b7d0a274b0 -> elementwise_max0x55b7d0a2a6e0  via fill_constant_23.tmp_00x55b7d0a2a490
I1018 08:53:57.682976 4097533 graph_helper.h:104] adj reshape20x55b7d0b636d0 -> elementwise_mul0x55b7d0b66d00  via reshape2_30.tmp_00x55b7d0b66860
I1018 08:53:57.682978 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0b60530 -> elementwise_mul0x55b7d0b66d00  via tmp_530x55b7d0b634e0
I1018 08:53:57.682981 4097533 graph_helper.h:104] adj conv2d0x55b7d0c94760 -> elementwise_sub0x55b7d0caacb0  via conv2d_12.tmp_00x55b7d0c97810
I1018 08:53:57.682983 4097533 graph_helper.h:104] adj reshape20x55b7d0ca7540 -> elementwise_sub0x55b7d0caacb0  via reshape2_48.tmp_00x55b7d0caa810
I1018 08:53:57.682986 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0c07840 -> elementwise_max0x55b7d0c0d100  via tmp_30x55b7d0c09d20
I1018 08:53:57.682988 4097533 graph_helper.h:104] adj fill_constant0x55b7d0c09ed0 -> elementwise_max0x55b7d0c0d100  via fill_constant_77.tmp_00x55b7d0c0ceb0
I1018 08:53:57.682991 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0b2ed90 -> elementwise_add0x55b7d0b7bb00  via batch_norm_6.tmp_20x55b7d0b31f50
I1018 08:53:57.682994 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0b6d710 -> elementwise_add0x55b7d0b7bb00  via batch_norm_7.tmp_20x55b7d0b708d0
I1018 08:53:57.682997 4097533 graph_helper.h:104] adj elementwise_max0x55b7d0b813e0 -> conv2d0x55b7d0b845a0  via relu_6.tmp_00x55b7d0b843d0
I1018 08:53:57.682999 4097533 graph_helper.h:104] adj scale0x55b7d0cf87f0 -> elementwise_pow0x55b7d0cfbb70  via tmp_880x55b7d0cfb9a0
I1018 08:53:57.683002 4097533 graph_helper.h:104] adj fill_constant0x55b7d0cdec40 -> elementwise_pow0x55b7d0cfbb70  via fill_constant_101.tmp_00x55b7d0ce1df0
I1018 08:53:57.683004 4097533 graph_helper.h:104] adj conv2d0x55b7d09a1600 -> elementwise_sub0x55b7d09b76c0  via conv2d_1.tmp_00x55b7d09a4220
I1018 08:53:57.683007 4097533 graph_helper.h:104] adj reshape20x55b7d09b3ff0 -> elementwise_sub0x55b7d09b76c0  via reshape2_4.tmp_00x55b7d09b7220
I1018 08:53:57.683009 4097533 graph_helper.h:104] adj scale0x55b7d0b90ee0 -> elementwise_pow0x55b7d0b93f20  via tmp_560x55b7d0b93d30
I1018 08:53:57.683012 4097533 graph_helper.h:104] adj fill_constant0x55b7d0b87660 -> elementwise_pow0x55b7d0b93f20  via fill_constant_63.tmp_00x55b7d0b8a810
I1018 08:53:57.683017 4097533 graph_helper.h:104] adj reshape20x55b7d0e582d0 -> elementwise_mul0x55b7d0e5b900  via reshape2_74.tmp_00x55b7d0e5b460
I1018 08:53:57.683018 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0e55130 -> elementwise_mul0x55b7d0e5b900  via tmp_1190x55b7d0e580e0
I1018 08:53:57.683022 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0b4cd40 -> assign0x55b7d0b73690  via elementwise_pow_28.tmp_00x55b7d0b4fd70
I1018 08:53:57.683027 4097533 graph_helper.h:104] adj conv2d0x55b7d0b845a0 -> elementwise_sub0x55b7d0b9a8b0  via conv2d_8.tmp_00x55b7d0b87440
I1018 08:53:57.683028 4097533 graph_helper.h:104] adj reshape20x55b7d0b97140 -> elementwise_sub0x55b7d0b9a8b0  via reshape2_32.tmp_00x55b7d0b9a410
I1018 08:53:57.683032 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0dfa740 -> assign0x55b7d0e21090  via elementwise_pow_68.tmp_00x55b7d0dfd770
I1018 08:53:57.683035 4097533 graph_helper.h:104] adj reshape20x55b7d09c76b0 -> elementwise_mul0x55b7d09cad00  via reshape2_6.tmp_00x55b7d09ca860
I1018 08:53:57.683038 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d09c45c0 -> elementwise_mul0x55b7d09cad00  via tmp_170x55b7d09c74e0
I1018 08:53:57.683041 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d09b0d70 -> assign0x55b7d09d7690  via elementwise_pow_4.tmp_00x55b7d09b3d80
I1018 08:53:57.683044 4097533 graph_helper.h:104] adj conv2d0x55b7d0bc90c0 -> elementwise_sub0x55b7d0bdf410  via conv2d_9.tmp_00x55b7d0bcbfb0
I1018 08:53:57.683046 4097533 graph_helper.h:104] adj reshape20x55b7d0bdbca0 -> elementwise_sub0x55b7d0bdf410  via reshape2_36.tmp_00x55b7d0bdef70
I1018 08:53:57.683049 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0dd5d80 -> elementwise_add0x55b7d0ddc790  via tmp_1080x55b7d0dd8f40
I1018 08:53:57.683051 4097533 graph_helper.h:104] adj reshape20x55b7d0dd9110 -> elementwise_add0x55b7d0ddc790  via reshape2_67.tmp_00x55b7d0ddc2f0
I1018 08:53:57.683055 4097533 graph_helper.h:104] adj reshape20x55b7d0d8dbf0 -> elementwise_mul0x55b7d0d91220  via reshape2_62.tmp_00x55b7d0d90d80
I1018 08:53:57.683058 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0d8aa50 -> elementwise_mul0x55b7d0d91220  via tmp_1010x55b7d0d8da00
I1018 08:53:57.683060 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0bb48f0 -> elementwise_max0x55b7d0bc5f00  via batch_norm_8.tmp_20x55b7d0bb7ab0
I1018 08:53:57.683063 4097533 graph_helper.h:104] adj fill_constant0x55b7d0bc2ce0 -> elementwise_max0x55b7d0bc5f00  via fill_constant_69.tmp_00x55b7d0bc5cb0
I1018 08:53:57.683066 4097533 graph_helper.h:104] adj fill_constant0x55b7d0c1abc0 -> scale0x55b7d0c1ddc0  via fill_constant_83.tmp_00x55b7d0c1db70
I1018 08:53:57.683069 4097533 graph_helper.h:104] adj reshape20x55b7d0be2710 -> scale0x55b7d0be5d50  via reshape2_37.tmp_00x55b7d0be58b0
I1018 08:53:57.683072 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d0bdf410 -> elementwise_mul0x55b7d0bec270  via tmp_630x55b7d0be2540
I1018 08:53:57.683074 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0be90d0 -> elementwise_mul0x55b7d0bec270  via elementwise_pow_38.tmp_00x55b7d0bec0c0
I1018 08:53:57.683077 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0d054e0 -> elementwise_add0x55b7d0d0bef0  via tmp_900x55b7d0d086a0
I1018 08:53:57.683080 4097533 graph_helper.h:104] adj reshape20x55b7d0d08870 -> elementwise_add0x55b7d0d0bef0  via reshape2_55.tmp_00x55b7d0d0ba50
I1018 08:53:57.683082 4097533 graph_helper.h:104] adj reshape20x55b7d0c2aad0 -> scale0x55b7d0c2e110  via reshape2_41.tmp_00x55b7d0c2dc70
I1018 08:53:57.683085 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0bf9450 -> elementwise_add0x55b7d0c07840  via batch_norm_9.tmp_20x55b7d0bfc610
I1018 08:53:57.683089 4097533 graph_helper.h:104] adj elementwise_max0x55b7d0b813e0 -> elementwise_add0x55b7d0c07840  via relu_6.tmp_00x55b7d0b843d0
I1018 08:53:57.683091 4097533 graph_helper.h:104] adj fill_constant0x55b7d0c5c4c0 -> assign0x55b7d0c89780  via fill_constant_89.tmp_00x55b7d0c5f4a0
I1018 08:53:57.683094 4097533 graph_helper.h:104] adj elementwise_max0x55b7d0c0d100 -> conv2d0x55b7d0c102c0  via relu_8.tmp_00x55b7d0c100f0
I1018 08:53:57.683528 4097533 build_strategy.cc:502] Finish Apply Pass cinn_zero_tensor_trick_pass
I1018 08:53:57.683532 4097533 build_strategy.cc:376] BuildStrategy::Apply pass:build_cinn_pass
I1018 08:53:57.683535 4097533 build_strategy.cc:493] Start Apply Pass build_cinn_pass
I1018 08:53:57.683540 4097533 build_strategy.cc:496] Apply Pass build_cinn_passto SubGraph 0
I1018 08:53:57.683565 4097533 build_cinn_pass.cc:699] The allowed Cinn Ops: []
I1018 08:53:57.683568 4097533 build_cinn_pass.cc:700] The denied Cinn Ops: []
I1018 08:53:57.684933 4097533 build_cinn_pass.cc:702] --- [build_cinn_pass] detected 77 cinn supported subgraphs
I1018 08:53:57.684939 4097533 build_cinn_pass.cc:719] All skip gc var names are: [linear_0.tmp_1, _jst.0.x.0, ]
I1018 08:53:57.685000 4097533 build_cinn_pass.cc:165] All deny var names are []
I1018 08:53:57.685015 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_1.tmp_0, is_only_used_internal: 1
I1018 08:53:57.685019 4097533 build_cinn_pass.cc:557] var_node->outputs[1]: fill_constant_1.tmp_0, is_only_used_internal: 1
I1018 08:53:57.685020 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_1.tmp_0
I1018 08:53:57.685024 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_2.tmp_1
I1018 08:53:57.685025 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: elementwise_pow_2.tmp_0, is_only_used_internal: 1
I1018 08:53:57.685029 4097533 build_cinn_pass.cc:562] insert internal var: elementwise_pow_2.tmp_0
I1018 08:53:57.685030 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: elementwise_pow_0.tmp_0, is_only_used_internal: 1
I1018 08:53:57.685032 4097533 build_cinn_pass.cc:562] insert internal var: elementwise_pow_0.tmp_0
I1018 08:53:57.685034 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_3.tmp_1
I1018 08:53:57.685036 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_0.tmp_0, is_only_used_internal: 1
I1018 08:53:57.685038 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_0.tmp_0
I1018 08:53:57.685041 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_1.tmp_1
I1018 08:53:57.685043 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_10, is_only_used_internal: 1
I1018 08:53:57.685045 4097533 build_cinn_pass.cc:562] insert internal var: tmp_10
I1018 08:53:57.685047 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_11, is_only_used_internal: 1
I1018 08:53:57.685050 4097533 build_cinn_pass.cc:562] insert internal var: tmp_11
I1018 08:53:57.685052 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_2.tmp_0, is_only_used_internal: 1
I1018 08:53:57.685055 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_2.tmp_0
I1018 08:53:57.685056 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_12, is_only_used_internal: 1
I1018 08:53:57.685058 4097533 build_cinn_pass.cc:562] insert internal var: tmp_12
I1018 08:53:57.685060 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_0.tmp_1
I1018 08:53:57.685063 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_7.tmp_0, is_only_used_internal: 1
I1018 08:53:57.685065 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_7.tmp_0
I1018 08:53:57.685067 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_1.tmp_0, is_only_used_internal: 1
I1018 08:53:57.685070 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_1.tmp_0
I1018 08:53:57.685071 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: relu_0.tmp_0, is_only_used_internal: 1
I1018 08:53:57.685074 4097533 build_cinn_pass.cc:562] insert internal var: relu_0.tmp_0
I1018 08:53:57.685076 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_8, is_only_used_internal: 1
I1018 08:53:57.685078 4097533 build_cinn_pass.cc:562] insert internal var: tmp_8
I1018 08:53:57.685081 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_9, is_only_used_internal: 1
I1018 08:53:57.685082 4097533 build_cinn_pass.cc:562] insert internal var: tmp_9
I1018 08:53:57.685084 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_3.tmp_0, is_only_used_internal: 1
I1018 08:53:57.685086 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_3.tmp_0
I1018 08:53:57.685089 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: batch_norm_0.tmp_2, is_only_used_internal: 1
I1018 08:53:57.685091 4097533 build_cinn_pass.cc:562] insert internal var: batch_norm_0.tmp_2
I1018 08:53:57.685096 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_5.tmp_0, is_only_used_internal: 1
I1018 08:53:57.685098 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_5.tmp_0
I1018 08:53:57.685102 4097533 build_cinn_pass.cc:742] Cluster Ops: (reshape2, reshape2, scale, elementwise_mul, elementwise_mul, fill_constant, pool2d, elementwise_add, elementwise_max, fill_constant, scale, reshape2, elementwise_pow, elementwise_sub, elementwise_pow, assign, reshape2, fill_constant, )
I1018 08:53:57.685106 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_0.w_0, conv2d_0.tmp_0, batch_norm2d_0.b_0, batch_norm2d_0.w_1, batch_norm2d_0.w_2, )
I1018 08:53:57.685108 4097533 build_cinn_pass.cc:744] Cluster output vars: (assign_1.tmp_0, pool2d_0.tmp_0, )
I1018 08:53:57.685110 4097533 build_cinn_pass.cc:745] Cluster internal vars: (fill_constant_5.tmp_0, batch_norm_0.tmp_2, reshape2_3.tmp_0, tmp_9, tmp_8, relu_0.tmp_0, reshape2_1.tmp_0, fill_constant_1.tmp_0, reshape2_3.tmp_1, reshape2_2.tmp_1, elementwise_pow_2.tmp_0, elementwise_pow_0.tmp_0, reshape2_0.tmp_0, reshape2_1.tmp_1, tmp_10, tmp_11, tmp_12, reshape2_2.tmp_0, fill_constant_7.tmp_0, reshape2_0.tmp_1, )
I1018 08:53:57.685122 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.685348 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: conv2d_0.tmp_0
I1018 08:53:57.685353 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_0.w_0
I1018 08:53:57.685357 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_0.b_0
I1018 08:53:57.685360 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_0.w_2
I1018 08:53:57.685364 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_0.w_1
I1018 08:53:57.685367 4097533 build_cinn_pass.cc:274] Add Output Var Node: assign_1.tmp_0
I1018 08:53:57.685371 4097533 build_cinn_pass.cc:274] Add Output Var Node: pool2d_0.tmp_0
I1018 08:53:57.685397 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_51[label="elementwise_pow_2.tmp_0
[1,64,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_50[label="reshape2_2.tmp_0
[1,64,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_48[label="pool2d" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_47[label="reshape2_3.tmp_1
[0,64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_46[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_45[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_44[label="fill_constant_1.tmp_0
[1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_43[label="relu_0.tmp_0
[1,64,112,112]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_42[label="batch_norm_0.tmp_2
[1,64,112,112]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_39[label="reshape2_0.tmp_1
[0,64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_37[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_35[label="tmp_12
[1,64,112,112]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_32[label="elementwise_mul" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_31[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_30[label="tmp_10
[1,64,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_3[label="pool2d_0.tmp_0
[1,64,56,56]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_21[label="reshape2_1.tmp_0
[1,64,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_38[label="fill_constant_5.tmp_0
[64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_8[label="batch_norm2d_0.b_0
[64]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_49[label="reshape2_2.tmp_1
[0,64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_7[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_6[label="batch_norm2d_0.w_1
[64]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_33[label="batch_norm2d_0.w_0
[64]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_26[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_11[label="tmp_11
[1,64,112,112]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_16[label="elementwise_sub" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_12[label="reshape2_0.tmp_0
[1,64,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_5[label="batch_norm2d_0.w_2
[64]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_4[label="assign_1.tmp_0
[64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_24[label="elementwise_add" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_27[label="elementwise_max" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_9[label="conv2d_0.tmp_0
[1,64,112,112]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_36[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_17[label="reshape2_3.tmp_0
[1,64,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_10[label="fill_constant_7.tmp_0
[1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_19[label="scale" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_41[label="tmp_9
[1,64,112,112]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_34[label="elementwise_pow" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_13[label="elementwise_pow_0.tmp_0
[64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_1[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_23[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_14[label="reshape2_1.tmp_1
[0,64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_2[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_15[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_18[label="elementwise_pow" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_20[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_40[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_22[label="tmp_8
[64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_25[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_29[label="elementwise_mul" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_28[label="scale" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_5
   node_2->node_9
   node_3->node_0
   node_4->node_45
   node_5->node_31
   node_6->node_40
   node_7->node_6
   node_8->node_46
   node_9->node_16
   node_10->node_27
   node_11->node_29
   node_12->node_16
   node_13->node_36
   node_15->node_50
   node_15->node_49
   node_16->node_41
   node_17->node_24
   node_18->node_13
   node_19->node_30
   node_20->node_8
   node_21->node_19
   node_22->node_18
   node_23->node_33
   node_24->node_42
   node_25->node_44
   node_26->node_10
   node_27->node_43
   node_28->node_22
   node_29->node_35
   node_30->node_34
   node_31->node_21
   node_31->node_14
   node_32->node_11
   node_33->node_15
   node_34->node_51
   node_35->node_24
   node_36->node_4
   node_37->node_38
   node_38->node_28
   node_40->node_12
   node_40->node_39
   node_41->node_32
   node_42->node_27
   node_43->node_48
   node_44->node_18
   node_44->node_34
   node_46->node_17
   node_46->node_47
   node_48->node_3
   node_50->node_29
   node_51->node_32
} // end G
I1018 08:53:57.685689 4097533 var_desc.cc:415] Flush  pool2d_0.tmp_0 1
I1018 08:53:57.685693 4097533 var_desc.cc:415] Flush  assign_1.tmp_0 1
I1018 08:53:57.685696 4097533 var_desc.cc:415] Flush  batch_norm2d_0.w_2 1
I1018 08:53:57.685699 4097533 var_desc.cc:415] Flush  batch_norm2d_0.w_1 1
I1018 08:53:57.685701 4097533 var_desc.cc:415] Flush  batch_norm2d_0.b_0 1
I1018 08:53:57.685704 4097533 var_desc.cc:415] Flush  conv2d_0.tmp_0 1
I1018 08:53:57.685707 4097533 var_desc.cc:415] Flush  fill_constant_7.tmp_0 1
I1018 08:53:57.685709 4097533 var_desc.cc:415] Flush  tmp_11 1
I1018 08:53:57.685712 4097533 var_desc.cc:415] Flush  reshape2_0.tmp_0 1
I1018 08:53:57.685715 4097533 var_desc.cc:415] Flush  elementwise_pow_0.tmp_0 1
I1018 08:53:57.685717 4097533 var_desc.cc:415] Flush  reshape2_1.tmp_1 1
I1018 08:53:57.685720 4097533 var_desc.cc:415] Flush  reshape2_3.tmp_0 1
I1018 08:53:57.685724 4097533 var_desc.cc:415] Flush  reshape2_1.tmp_0 1
I1018 08:53:57.685725 4097533 var_desc.cc:415] Flush  tmp_8 1
I1018 08:53:57.685729 4097533 var_desc.cc:415] Flush  tmp_10 1
I1018 08:53:57.685731 4097533 var_desc.cc:415] Flush  batch_norm2d_0.w_0 1
I1018 08:53:57.685734 4097533 var_desc.cc:415] Flush  tmp_12 1
I1018 08:53:57.685736 4097533 var_desc.cc:415] Flush  fill_constant_5.tmp_0 1
I1018 08:53:57.685739 4097533 var_desc.cc:415] Flush  reshape2_0.tmp_1 1
I1018 08:53:57.685741 4097533 var_desc.cc:415] Flush  tmp_9 1
I1018 08:53:57.685745 4097533 var_desc.cc:415] Flush  batch_norm_0.tmp_2 1
I1018 08:53:57.685746 4097533 var_desc.cc:415] Flush  relu_0.tmp_0 1
I1018 08:53:57.685750 4097533 var_desc.cc:415] Flush  fill_constant_1.tmp_0 1
I1018 08:53:57.685751 4097533 var_desc.cc:415] Flush  reshape2_3.tmp_1 1
I1018 08:53:57.685755 4097533 var_desc.cc:415] Flush  reshape2_2.tmp_1 1
I1018 08:53:57.685757 4097533 var_desc.cc:415] Flush  reshape2_2.tmp_0 1
I1018 08:53:57.685760 4097533 var_desc.cc:415] Flush  elementwise_pow_2.tmp_0 1
I1018 08:53:57.685778 4097533 graph_helper.h:104] adj pool2d0x55b7d095cb80 -> fetch0x55b7d0ef6f90  via pool2d_0.tmp_00x55b7d0ef6080
I1018 08:53:57.685788 4097533 graph_helper.h:104] adj feed0x55b7d0ef64a0 -> reshape20x55b7d0eedd60  via batch_norm2d_0.w_00x55b7d0ef54e0
I1018 08:53:57.685797 4097533 graph_helper.h:104] adj reshape20x55b7d0edf4a0 -> elementwise_sub0x55b7d0ee7510  via reshape2_0.tmp_00x55b7d0ef46e0
I1018 08:53:57.685803 4097533 graph_helper.h:104] adj feed0x55b7d0ef6270 -> elementwise_sub0x55b7d0ee7510  via conv2d_0.tmp_00x55b7d0ef56d0
I1018 08:53:57.685808 4097533 graph_helper.h:104] adj scale0x55b7b87a3dd0 -> elementwise_pow0x55b7d0ee5080  via tmp_80x55b7d0ef2bd0
I1018 08:53:57.685813 4097533 graph_helper.h:104] adj fill_constant0x55b7d0ef0110 -> elementwise_pow0x55b7d0ee5080  via fill_constant_1.tmp_00x55b7d0ef3fb0
I1018 08:53:57.685819 4097533 graph_helper.h:104] adj reshape20x55b7d0943960 -> scale0x55b7d0941260  via reshape2_1.tmp_00x55b7d0ef3e40
I1018 08:53:57.685827 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d093d210 -> elementwise_add0x55b7d0945ba0  via tmp_120x55b7d0ef4d20
I1018 08:53:57.685834 4097533 graph_helper.h:104] adj reshape20x55b7d0ee2c90 -> elementwise_add0x55b7d0945ba0  via reshape2_3.tmp_00x55b7d0ef28f0
I1018 08:53:57.685840 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0945ba0 -> elementwise_max0x55b7d0947b00  via batch_norm_0.tmp_20x55b7d0ef2780
I1018 08:53:57.685845 4097533 graph_helper.h:104] adj fill_constant0x55b7d094a1b0 -> elementwise_max0x55b7d0947b00  via fill_constant_7.tmp_00x55b7d0ef5100
I1018 08:53:57.685851 4097533 graph_helper.h:104] adj fill_constant0x55b7b87a17e0 -> scale0x55b7b87a3dd0  via fill_constant_5.tmp_00x55b7d0ef26c0
I1018 08:53:57.685856 4097533 graph_helper.h:104] adj reshape20x55b7d0eedd60 -> elementwise_mul0x55b7d093d210  via reshape2_2.tmp_00x55b7d0ef4f10
I1018 08:53:57.685863 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d094d0e0 -> elementwise_mul0x55b7d093d210  via tmp_110x55b7d0ef4b30
I1018 08:53:57.685868 4097533 graph_helper.h:104] adj feed0x55b7d0ef6900 -> reshape20x55b7d0943960  via batch_norm2d_0.w_20x55b7d0ef5ca0
I1018 08:53:57.685874 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d0ee7510 -> elementwise_mul0x55b7d094d0e0  via tmp_90x55b7d0ef2a60
I1018 08:53:57.685879 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0ee98d0 -> elementwise_mul0x55b7d094d0e0  via elementwise_pow_2.tmp_00x55b7d0ef4400
I1018 08:53:57.685885 4097533 graph_helper.h:104] adj scale0x55b7d0941260 -> elementwise_pow0x55b7d0ee98d0  via tmp_100x55b7d0ef49c0
I1018 08:53:57.685890 4097533 graph_helper.h:104] adj fill_constant0x55b7d0ef0110 -> elementwise_pow0x55b7d0ee98d0  via fill_constant_1.tmp_00x55b7d0ef3fb0
I1018 08:53:57.685896 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0ee5080 -> assign0x55b7d0eebd50  via elementwise_pow_0.tmp_00x55b7d0ef4570
I1018 08:53:57.685901 4097533 graph_helper.h:104] adj feed0x55b7d0ef6b30 -> reshape20x55b7d0edf4a0  via batch_norm2d_0.w_10x55b7d0ef5ab0
I1018 08:53:57.685909 4097533 graph_helper.h:104] adj assign0x55b7d0eebd50 -> fetch0x55b7d0ef6d60  via assign_1.tmp_00x55b7d0ef5e90
I1018 08:53:57.685912 4097533 graph_helper.h:104] adj feed0x55b7d0ef66d0 -> reshape20x55b7d0ee2c90  via batch_norm2d_0.b_00x55b7d0ef58c0
I1018 08:53:57.685920 4097533 graph_helper.h:104] adj elementwise_max0x55b7d0947b00 -> pool2d0x55b7d095cb80  via relu_0.tmp_00x55b7d0ef2d40
I1018 08:53:57.686065 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.686069 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.686074 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.686076 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.686085 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.686089 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.686098 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.686101 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.686108 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.686111 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.686136 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.686137 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.686141 4097533 graph_helper.cc:698] convert op node to desc elementwise_sub
I1018 08:53:57.686144 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.686151 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.686153 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.686161 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.686164 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.686208 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.686210 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.686216 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.686219 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.686223 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.686225 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.686233 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.686239 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.686246 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.686249 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.686256 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.686259 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.686261 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.686264 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.686272 4097533 graph_helper.cc:698] convert op node to desc elementwise_add
I1018 08:53:57.686275 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.686367 4097533 graph_helper.cc:698] convert op node to desc elementwise_max
I1018 08:53:57.686370 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.686378 4097533 graph_helper.cc:698] convert op node to desc pool2d
I1018 08:53:57.686379 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.686388 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.686389 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.686396 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.686399 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.686406 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.686409 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.686416 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.686419 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.686425 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.686429 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.686431 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.686434 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.687273 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.687853 4097533 block_desc.cc:205] vars in desc 27
I1018 08:53:57.687860 4097533 block_desc.cc:209] Flush pool2d_0.tmp_0
I1018 08:53:57.687862 4097533 var_desc.cc:415] Flush  pool2d_0.tmp_0 1
I1018 08:53:57.687865 4097533 block_desc.cc:209] Flush assign_1.tmp_0
I1018 08:53:57.687867 4097533 var_desc.cc:415] Flush  assign_1.tmp_0 1
I1018 08:53:57.687870 4097533 block_desc.cc:209] Flush batch_norm2d_0.w_2
I1018 08:53:57.687872 4097533 var_desc.cc:415] Flush  batch_norm2d_0.w_2 1
I1018 08:53:57.687875 4097533 block_desc.cc:209] Flush batch_norm2d_0.w_1
I1018 08:53:57.687877 4097533 var_desc.cc:415] Flush  batch_norm2d_0.w_1 1
I1018 08:53:57.687880 4097533 block_desc.cc:209] Flush batch_norm2d_0.b_0
I1018 08:53:57.687882 4097533 var_desc.cc:415] Flush  batch_norm2d_0.b_0 1
I1018 08:53:57.687884 4097533 block_desc.cc:209] Flush conv2d_0.tmp_0
I1018 08:53:57.687886 4097533 var_desc.cc:415] Flush  conv2d_0.tmp_0 1
I1018 08:53:57.687889 4097533 block_desc.cc:209] Flush fill_constant_7.tmp_0
I1018 08:53:57.687891 4097533 var_desc.cc:415] Flush  fill_constant_7.tmp_0 1
I1018 08:53:57.687893 4097533 block_desc.cc:209] Flush tmp_11
I1018 08:53:57.687896 4097533 var_desc.cc:415] Flush  tmp_11 1
I1018 08:53:57.687898 4097533 block_desc.cc:209] Flush reshape2_0.tmp_0
I1018 08:53:57.687901 4097533 var_desc.cc:415] Flush  reshape2_0.tmp_0 1
I1018 08:53:57.687903 4097533 block_desc.cc:209] Flush elementwise_pow_0.tmp_0
I1018 08:53:57.687906 4097533 var_desc.cc:415] Flush  elementwise_pow_0.tmp_0 1
I1018 08:53:57.687907 4097533 block_desc.cc:209] Flush reshape2_1.tmp_1
I1018 08:53:57.687911 4097533 var_desc.cc:415] Flush  reshape2_1.tmp_1 1
I1018 08:53:57.687912 4097533 block_desc.cc:209] Flush reshape2_3.tmp_0
I1018 08:53:57.687914 4097533 var_desc.cc:415] Flush  reshape2_3.tmp_0 1
I1018 08:53:57.687917 4097533 block_desc.cc:209] Flush reshape2_1.tmp_0
I1018 08:53:57.687919 4097533 var_desc.cc:415] Flush  reshape2_1.tmp_0 1
I1018 08:53:57.687922 4097533 block_desc.cc:209] Flush tmp_8
I1018 08:53:57.687923 4097533 var_desc.cc:415] Flush  tmp_8 1
I1018 08:53:57.687927 4097533 block_desc.cc:209] Flush tmp_10
I1018 08:53:57.687928 4097533 var_desc.cc:415] Flush  tmp_10 1
I1018 08:53:57.687930 4097533 block_desc.cc:209] Flush batch_norm2d_0.w_0
I1018 08:53:57.687937 4097533 var_desc.cc:415] Flush  batch_norm2d_0.w_0 1
I1018 08:53:57.687939 4097533 block_desc.cc:209] Flush tmp_12
I1018 08:53:57.687942 4097533 var_desc.cc:415] Flush  tmp_12 1
I1018 08:53:57.687943 4097533 block_desc.cc:209] Flush fill_constant_5.tmp_0
I1018 08:53:57.687945 4097533 var_desc.cc:415] Flush  fill_constant_5.tmp_0 1
I1018 08:53:57.687948 4097533 block_desc.cc:209] Flush reshape2_0.tmp_1
I1018 08:53:57.687950 4097533 var_desc.cc:415] Flush  reshape2_0.tmp_1 1
I1018 08:53:57.687953 4097533 block_desc.cc:209] Flush tmp_9
I1018 08:53:57.687955 4097533 var_desc.cc:415] Flush  tmp_9 1
I1018 08:53:57.687958 4097533 block_desc.cc:209] Flush batch_norm_0.tmp_2
I1018 08:53:57.687960 4097533 var_desc.cc:415] Flush  batch_norm_0.tmp_2 1
I1018 08:53:57.687963 4097533 block_desc.cc:209] Flush relu_0.tmp_0
I1018 08:53:57.687964 4097533 var_desc.cc:415] Flush  relu_0.tmp_0 1
I1018 08:53:57.687968 4097533 block_desc.cc:209] Flush fill_constant_1.tmp_0
I1018 08:53:57.687969 4097533 var_desc.cc:415] Flush  fill_constant_1.tmp_0 1
I1018 08:53:57.687971 4097533 block_desc.cc:209] Flush reshape2_3.tmp_1
I1018 08:53:57.687973 4097533 var_desc.cc:415] Flush  reshape2_3.tmp_1 1
I1018 08:53:57.687976 4097533 block_desc.cc:209] Flush reshape2_2.tmp_1
I1018 08:53:57.687978 4097533 var_desc.cc:415] Flush  reshape2_2.tmp_1 1
I1018 08:53:57.687981 4097533 block_desc.cc:209] Flush reshape2_2.tmp_0
I1018 08:53:57.687983 4097533 var_desc.cc:415] Flush  reshape2_2.tmp_0 1
I1018 08:53:57.687985 4097533 block_desc.cc:209] Flush elementwise_pow_2.tmp_0
I1018 08:53:57.687987 4097533 var_desc.cc:415] Flush  elementwise_pow_2.tmp_0 1
I1018 08:53:57.685681 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "pool2d_0.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 56
          dims: 56
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "assign_1.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "batch_norm2d_0.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "batch_norm2d_0.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "batch_norm2d_0.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "conv2d_0.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 112
          dims: 112
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "fill_constant_7.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_11"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 112
          dims: 112
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "reshape2_0.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "elementwise_pow_0.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_1.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_3.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_1.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_8"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  vars {
    name: "tmp_10"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  vars {
    name: "batch_norm2d_0.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "tmp_12"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 112
          dims: 112
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "fill_constant_5.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_0.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_9"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 112
          dims: 112
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "batch_norm_0.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 112
          dims: 112
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "relu_0.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 112
          dims: 112
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "fill_constant_1.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_3.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_2.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_2.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "elementwise_pow_2.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_0.w_1"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_0.w_1"
    }
    outputs {
      parameter: "Out"
      arguments: "reshape2_0.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_0.tmp_1"
    }
    type: "reshape2"
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 123, in composite_batchnorm"
      strings: "    x_hat = (x - reshape(run_mean, stats_shape)) * pow("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/manipulation.py\", line 4518, in reshape"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 1
      ints: 64
      ints: 1
      ints: 1
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_0.w_2"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_0.w_2"
    }
    outputs {
      parameter: "Out"
      arguments: "reshape2_1.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_1.tmp_1"
    }
    type: "reshape2"
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 124, in composite_batchnorm"
      strings: "    (reshape(run_var, stats_shape) + epsilon), half"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/manipulation.py\", line 4518, in reshape"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 1
      ints: 64
      ints: 1
      ints: 1
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "BiasTensor"
    }
    inputs {
      parameter: "ScaleTensor"
    }
    inputs {
      parameter: "X"
      arguments: "reshape2_1.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_10"
    }
    type: "scale"
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "bias"
      type: FLOAT
      f: 1e-05
    }
    attrs {
      name: "bias_after_scale"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 124, in composite_batchnorm"
      strings: "    (reshape(run_var, stats_shape) + epsilon), half"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 557, in __impl__"
      strings: "    return scalar_method(self, other_var)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 530, in _scalar_add_"
      strings: "    return _scalar_op_(var, 1.0, value)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 451, in _scalar_op_"
      strings: "    block.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
    
I1018 08:53:57.689914 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.690137 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: elementwise_pow_6.tmp_0, is_only_used_internal: 1
I1018 08:53:57.690141 4097533 build_cinn_pass.cc:562] insert internal var: elementwise_pow_6.tmp_0
I1018 08:53:57.690145 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_7.tmp_1
I1018 08:53:57.690146 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: elementwise_pow_4.tmp_0, is_only_used_internal: 1
I1018 08:53:57.690148 4097533 build_cinn_pass.cc:562] insert internal var: elementwise_pow_4.tmp_0
I1018 08:53:57.690151 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_6.tmp_1
I1018 08:53:57.690153 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_9.tmp_0, is_only_used_internal: 1
I1018 08:53:57.690155 4097533 build_cinn_pass.cc:557] var_node->outputs[1]: fill_constant_9.tmp_0, is_only_used_internal: 1
I1018 08:53:57.690157 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_9.tmp_0
I1018 08:53:57.690160 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_16, is_only_used_internal: 1
I1018 08:53:57.690165 4097533 build_cinn_pass.cc:562] insert internal var: tmp_16
I1018 08:53:57.690167 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_5.tmp_1
I1018 08:53:57.690169 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_4.tmp_0, is_only_used_internal: 1
I1018 08:53:57.690171 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_4.tmp_0
I1018 08:53:57.690174 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_17, is_only_used_internal: 1
I1018 08:53:57.690176 4097533 build_cinn_pass.cc:562] insert internal var: tmp_17
I1018 08:53:57.690178 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_4.tmp_1
I1018 08:53:57.690181 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_15.tmp_0, is_only_used_internal: 1
I1018 08:53:57.690183 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_15.tmp_0
I1018 08:53:57.690186 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_7.tmp_0, is_only_used_internal: 1
I1018 08:53:57.690187 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_7.tmp_0
I1018 08:53:57.690189 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: batch_norm_1.tmp_2, is_only_used_internal: 1
I1018 08:53:57.690191 4097533 build_cinn_pass.cc:562] insert internal var: batch_norm_1.tmp_2
I1018 08:53:57.690194 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_15, is_only_used_internal: 1
I1018 08:53:57.690196 4097533 build_cinn_pass.cc:562] insert internal var: tmp_15
I1018 08:53:57.690198 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_5.tmp_0, is_only_used_internal: 1
I1018 08:53:57.690200 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_5.tmp_0
I1018 08:53:57.690202 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_13.tmp_0, is_only_used_internal: 1
I1018 08:53:57.690205 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_13.tmp_0
I1018 08:53:57.690207 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_6.tmp_0, is_only_used_internal: 1
I1018 08:53:57.690209 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_6.tmp_0
I1018 08:53:57.690212 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_18, is_only_used_internal: 1
I1018 08:53:57.690213 4097533 build_cinn_pass.cc:562] insert internal var: tmp_18
I1018 08:53:57.690215 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_14, is_only_used_internal: 1
I1018 08:53:57.690218 4097533 build_cinn_pass.cc:562] insert internal var: tmp_14
I1018 08:53:57.690222 4097533 build_cinn_pass.cc:742] Cluster Ops: (scale, reshape2, elementwise_mul, elementwise_add, fill_constant, elementwise_max, reshape2, elementwise_sub, elementwise_mul, fill_constant, scale, reshape2, elementwise_pow, assign, elementwise_pow, reshape2, fill_constant, )
I1018 08:53:57.690224 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_1.w_0, batch_norm2d_1.b_0, batch_norm2d_1.w_1, batch_norm2d_1.w_2, conv2d_1.tmp_0, )
I1018 08:53:57.690227 4097533 build_cinn_pass.cc:744] Cluster output vars: (assign_5.tmp_0, relu_1.tmp_0, )
I1018 08:53:57.690229 4097533 build_cinn_pass.cc:745] Cluster internal vars: (tmp_14, tmp_18, reshape2_6.tmp_0, fill_constant_13.tmp_0, reshape2_5.tmp_0, elementwise_pow_6.tmp_0, reshape2_7.tmp_1, elementwise_pow_4.tmp_0, reshape2_6.tmp_1, reshape2_4.tmp_0, fill_constant_9.tmp_0, tmp_16, reshape2_5.tmp_1, tmp_17, fill_constant_15.tmp_0, reshape2_4.tmp_1, tmp_15, reshape2_7.tmp_0, batch_norm_1.tmp_2, )
I1018 08:53:57.690239 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.690435 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: conv2d_1.tmp_0
I1018 08:53:57.690440 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_1.b_0
I1018 08:53:57.690443 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_1.w_0
I1018 08:53:57.690446 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_1.w_2
I1018 08:53:57.690452 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_1.w_1
I1018 08:53:57.690456 4097533 build_cinn_pass.cc:274] Add Output Var Node: assign_5.tmp_0
I1018 08:53:57.690460 4097533 build_cinn_pass.cc:274] Add Output Var Node: relu_1.tmp_0
I1018 08:53:57.690482 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_48[label="reshape2_4.tmp_1
[0,64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_47[label="reshape2_4.tmp_0
[1,64,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_46[label="elementwise_sub" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_45[label="batch_norm2d_1.w_2
[64]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_44[label="reshape2_7.tmp_1
[0,64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_43[label="elementwise_pow_4.tmp_0
[64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_42[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_39[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_37[label="batch_norm_1.tmp_2
[1,64,56,56]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_35[label="reshape2_5.tmp_0
[1,64,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_32[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_31[label="assign_5.tmp_0
[64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_30[label="tmp_16
[1,64,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_3[label="batch_norm2d_1.w_0
[64]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_21[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_38[label="reshape2_6.tmp_0
[1,64,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_8[label="reshape2_5.tmp_1
[0,64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_49[label="fill_constant_9.tmp_0
[1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_7[label="batch_norm2d_1.b_0
[64]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_6[label="tmp_17
[1,64,56,56]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_33[label="tmp_14
[64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_26[label="elementwise_add" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_11[label="conv2d_1.tmp_0
[1,64,56,56]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_16[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_12[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_5[label="batch_norm2d_1.w_1
[64]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_4[label="tmp_15
[1,64,56,56]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_24[label="scale" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_27[label="elementwise_pow" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_9[label="scale" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_36[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_17[label="elementwise_max" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_10[label="elementwise_pow" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_19[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_41[label="elementwise_pow_6.tmp_0
[1,64,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_34[label="tmp_18
[1,64,56,56]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_13[label="elementwise_mul" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_23[label="reshape2_6.tmp_1
[0,64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_14[label="relu_1.tmp_0
[1,64,56,56]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_2[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_15[label="fill_constant_15.tmp_0
[1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_18[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_20[label="fill_constant_13.tmp_0
[64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_40[label="reshape2_7.tmp_0
[1,64,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_22[label="elementwise_mul" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_25[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_29[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_28[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_5
   node_2->node_11
   node_3->node_16
   node_4->node_22
   node_5->node_21
   node_6->node_13
   node_7->node_28
   node_9->node_33
   node_10->node_43
   node_11->node_46
   node_12->node_31
   node_13->node_34
   node_14->node_0
   node_15->node_17
   node_16->node_38
   node_16->node_23
   node_17->node_14
   node_18->node_20
   node_19->node_15
   node_20->node_9
   node_21->node_47
   node_21->node_48
   node_22->node_6
   node_24->node_30
   node_25->node_7
   node_26->node_37
   node_27->node_41
   node_28->node_40
   node_28->node_44
   node_29->node_35
   node_29->node_8
   node_30->node_27
   node_31->node_39
   node_32->node_49
   node_33->node_10
   node_34->node_26
   node_35->node_24
   node_36->node_45
   node_37->node_17
   node_38->node_13
   node_40->node_26
   node_41->node_22
   node_42->node_3
   node_43->node_12
   node_45->node_29
   node_46->node_4
   node_47->node_46
   node_49->node_10
   node_49->node_27
} // end G
I1018 08:53:57.690745 4097533 var_desc.cc:415] Flush  batch_norm2d_1.w_0 1
I1018 08:53:57.690749 4097533 var_desc.cc:415] Flush  tmp_15 1
I1018 08:53:57.690752 4097533 var_desc.cc:415] Flush  batch_norm2d_1.w_1 1
I1018 08:53:57.690754 4097533 var_desc.cc:415] Flush  tmp_17 1
I1018 08:53:57.690757 4097533 var_desc.cc:415] Flush  batch_norm2d_1.b_0 1
I1018 08:53:57.690760 4097533 var_desc.cc:415] Flush  reshape2_5.tmp_1 1
I1018 08:53:57.690762 4097533 var_desc.cc:415] Flush  conv2d_1.tmp_0 1
I1018 08:53:57.690765 4097533 var_desc.cc:415] Flush  relu_1.tmp_0 1
I1018 08:53:57.690768 4097533 var_desc.cc:415] Flush  fill_constant_15.tmp_0 1
I1018 08:53:57.690771 4097533 var_desc.cc:415] Flush  fill_constant_13.tmp_0 1
I1018 08:53:57.690773 4097533 var_desc.cc:415] Flush  reshape2_6.tmp_1 1
I1018 08:53:57.690776 4097533 var_desc.cc:415] Flush  tmp_16 1
I1018 08:53:57.690779 4097533 var_desc.cc:415] Flush  assign_5.tmp_0 1
I1018 08:53:57.690781 4097533 var_desc.cc:415] Flush  tmp_14 1
I1018 08:53:57.690784 4097533 var_desc.cc:415] Flush  tmp_18 1
I1018 08:53:57.690788 4097533 var_desc.cc:415] Flush  reshape2_5.tmp_0 1
I1018 08:53:57.690791 4097533 var_desc.cc:415] Flush  batch_norm_1.tmp_2 1
I1018 08:53:57.690794 4097533 var_desc.cc:415] Flush  reshape2_6.tmp_0 1
I1018 08:53:57.690797 4097533 var_desc.cc:415] Flush  reshape2_7.tmp_0 1
I1018 08:53:57.690800 4097533 var_desc.cc:415] Flush  elementwise_pow_6.tmp_0 1
I1018 08:53:57.690802 4097533 var_desc.cc:415] Flush  elementwise_pow_4.tmp_0 1
I1018 08:53:57.690805 4097533 var_desc.cc:415] Flush  reshape2_7.tmp_1 1
I1018 08:53:57.690807 4097533 var_desc.cc:415] Flush  batch_norm2d_1.w_2 1
I1018 08:53:57.690810 4097533 var_desc.cc:415] Flush  reshape2_4.tmp_0 1
I1018 08:53:57.690814 4097533 var_desc.cc:415] Flush  reshape2_4.tmp_1 1
I1018 08:53:57.690815 4097533 var_desc.cc:415] Flush  fill_constant_9.tmp_0 1
I1018 08:53:57.690832 4097533 graph_helper.h:104] adj elementwise_max0x55b7c4e22550 -> fetch0x55b7d0f9afe0  via relu_1.tmp_00x55b7d0efa5a0
I1018 08:53:57.690841 4097533 graph_helper.h:104] adj fill_constant0x55b7d0f52e00 -> scale0x55b7d099fcc0  via fill_constant_13.tmp_00x55b7d0fae100
I1018 08:53:57.690848 4097533 graph_helper.h:104] adj scale0x55b7d099fcc0 -> elementwise_pow0x55b7d0efcf40  via tmp_140x55b7d0faddc0
I1018 08:53:57.690855 4097533 graph_helper.h:104] adj fill_constant0x55b7d0f03140 -> elementwise_pow0x55b7d0efcf40  via fill_constant_9.tmp_00x55b7d0f51dd0
I1018 08:53:57.690860 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0efcf40 -> assign0x55b7d0f13b30  via elementwise_pow_4.tmp_00x55b7d0f43840
I1018 08:53:57.690866 4097533 graph_helper.h:104] adj reshape20x55b7d098a8d0 -> elementwise_mul0x55b7d09782b0  via reshape2_6.tmp_00x55b7d0fadfc0
I1018 08:53:57.690871 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d097a010 -> elementwise_mul0x55b7d09782b0  via tmp_170x55b7d0f526b0
I1018 08:53:57.690876 4097533 graph_helper.h:104] adj feed0x55b7d0f93f30 -> reshape20x55b7d098a8d0  via batch_norm2d_1.w_00x55b7d0f7ef50
I1018 08:53:57.690883 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0990320 -> elementwise_max0x55b7c4e22550  via batch_norm_1.tmp_20x55b7d0f7ebd0
I1018 08:53:57.690889 4097533 graph_helper.h:104] adj fill_constant0x55b7d0980340 -> elementwise_max0x55b7c4e22550  via fill_constant_15.tmp_00x55b7d0f529f0
I1018 08:53:57.690896 4097533 graph_helper.h:104] adj feed0x55b7d0f94650 -> reshape20x55b7d0970350  via batch_norm2d_1.w_10x55b7d0f3d370
I1018 08:53:57.690901 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d0f51050 -> elementwise_mul0x55b7d097a010  via tmp_150x55b7d0f7e510
I1018 08:53:57.690907 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d09637c0 -> elementwise_mul0x55b7d097a010  via elementwise_pow_6.tmp_00x55b7d0fae3a0
I1018 08:53:57.690912 4097533 graph_helper.h:104] adj reshape20x55b7d0efc110 -> scale0x55b7d0969a70  via reshape2_5.tmp_00x55b7d0fae260
I1018 08:53:57.690918 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d09782b0 -> elementwise_add0x55b7d0990320  via tmp_180x55b7d0fadec0
I1018 08:53:57.690923 4097533 graph_helper.h:104] adj reshape20x55b7d0f84540 -> elementwise_add0x55b7d0990320  via reshape2_7.tmp_00x55b7d0f7e850
I1018 08:53:57.690929 4097533 graph_helper.h:104] adj scale0x55b7d0969a70 -> elementwise_pow0x55b7d09637c0  via tmp_160x55b7d0f52080
I1018 08:53:57.690934 4097533 graph_helper.h:104] adj fill_constant0x55b7d0f03140 -> elementwise_pow0x55b7d09637c0  via fill_constant_9.tmp_00x55b7d0f51dd0
I1018 08:53:57.690940 4097533 graph_helper.h:104] adj feed0x55b7d0f93ba0 -> reshape20x55b7d0f84540  via batch_norm2d_1.b_00x55b7d0f3d000
I1018 08:53:57.690946 4097533 graph_helper.h:104] adj feed0x55b7d0f942c0 -> reshape20x55b7d0efc110  via batch_norm2d_1.w_20x55b7d0f3d6e0
I1018 08:53:57.690953 4097533 graph_helper.h:104] adj assign0x55b7d0f13b30 -> fetch0x55b7d0efb270  via assign_5.tmp_00x55b7d0f3dd90
I1018 08:53:57.690958 4097533 graph_helper.h:104] adj reshape20x55b7d0970350 -> elementwise_sub0x55b7d0f51050  via reshape2_4.tmp_00x55b7d0f43e20
I1018 08:53:57.690964 4097533 graph_helper.h:104] adj feed0x55b7d0efb1b0 -> elementwise_sub0x55b7d0f51050  via conv2d_1.tmp_00x55b7d0f3da50
I1018 08:53:57.691102 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.691104 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.691109 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.691112 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.691121 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.691124 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.691139 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.691141 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.691145 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.691148 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.691170 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.691172 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.691175 4097533 graph_helper.cc:698] convert op node to desc elementwise_sub
I1018 08:53:57.691179 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.691186 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.691188 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.691196 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.691200 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.691239 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.691242 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.691249 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.691251 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.691255 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.691258 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.691267 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.691269 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.691277 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.691278 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.691282 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.691284 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.691291 4097533 graph_helper.cc:698] convert op node to desc elementwise_add
I1018 08:53:57.691294 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.691301 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.691304 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.691380 4097533 graph_helper.cc:698] convert op node to desc elementwise_max
I1018 08:53:57.691382 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.691390 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.691392 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.691399 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.691402 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.691409 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.691412 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.691419 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.691421 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.691428 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.691431 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.691434 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.691437 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.692080 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.692536 4097533 block_desc.cc:205] vars in desc 26
I1018 08:53:57.692541 4097533 block_desc.cc:209] Flush batch_norm2d_1.w_0
I1018 08:53:57.692544 4097533 var_desc.cc:415] Flush  batch_norm2d_1.w_0 1
I1018 08:53:57.692548 4097533 block_desc.cc:209] Flush tmp_15
I1018 08:53:57.692550 4097533 var_desc.cc:415] Flush  tmp_15 1
I1018 08:53:57.692552 4097533 block_desc.cc:209] Flush batch_norm2d_1.w_1
I1018 08:53:57.692555 4097533 var_desc.cc:415] Flush  batch_norm2d_1.w_1 1
I1018 08:53:57.692557 4097533 block_desc.cc:209] Flush tmp_17
I1018 08:53:57.692559 4097533 var_desc.cc:415] Flush  tmp_17 1
I1018 08:53:57.692565 4097533 block_desc.cc:209] Flush batch_norm2d_1.b_0
I1018 08:53:57.692569 4097533 var_desc.cc:415] Flush  batch_norm2d_1.b_0 1
I1018 08:53:57.692570 4097533 block_desc.cc:209] Flush reshape2_5.tmp_1
I1018 08:53:57.692572 4097533 var_desc.cc:415] Flush  reshape2_5.tmp_1 1
I1018 08:53:57.692575 4097533 block_desc.cc:209] Flush conv2d_1.tmp_0
I1018 08:53:57.692577 4097533 var_desc.cc:415] Flush  conv2d_1.tmp_0 1
I1018 08:53:57.692580 4097533 block_desc.cc:209] Flush relu_1.tmp_0
I1018 08:53:57.692582 4097533 var_desc.cc:415] Flush  relu_1.tmp_0 1
I1018 08:53:57.692584 4097533 block_desc.cc:209] Flush fill_constant_15.tmp_0
I1018 08:53:57.692586 4097533 var_desc.cc:415] Flush  fill_constant_15.tmp_0 1
I1018 08:53:57.692589 4097533 block_desc.cc:209] Flush fill_constant_13.tmp_0
I1018 08:53:57.692591 4097533 var_desc.cc:415] Flush  fill_constant_13.tmp_0 1
I1018 08:53:57.692593 4097533 block_desc.cc:209] Flush reshape2_6.tmp_1
I1018 08:53:57.692595 4097533 var_desc.cc:415] Flush  reshape2_6.tmp_1 1
I1018 08:53:57.692598 4097533 block_desc.cc:209] Flush tmp_16
I1018 08:53:57.692600 4097533 var_desc.cc:415] Flush  tmp_16 1
I1018 08:53:57.692603 4097533 block_desc.cc:209] Flush assign_5.tmp_0
I1018 08:53:57.692605 4097533 var_desc.cc:415] Flush  assign_5.tmp_0 1
I1018 08:53:57.692607 4097533 block_desc.cc:209] Flush tmp_14
I1018 08:53:57.692610 4097533 var_desc.cc:415] Flush  tmp_14 1
I1018 08:53:57.692612 4097533 block_desc.cc:209] Flush tmp_18
I1018 08:53:57.692615 4097533 var_desc.cc:415] Flush  tmp_18 1
I1018 08:53:57.692616 4097533 block_desc.cc:209] Flush reshape2_5.tmp_0
I1018 08:53:57.692618 4097533 var_desc.cc:415] Flush  reshape2_5.tmp_0 1
I1018 08:53:57.692621 4097533 block_desc.cc:209] Flush batch_norm_1.tmp_2
I1018 08:53:57.692623 4097533 var_desc.cc:415] Flush  batch_norm_1.tmp_2 1
I1018 08:53:57.692625 4097533 block_desc.cc:209] Flush reshape2_6.tmp_0
I1018 08:53:57.692627 4097533 var_desc.cc:415] Flush  reshape2_6.tmp_0 1
I1018 08:53:57.692631 4097533 block_desc.cc:209] Flush reshape2_7.tmp_0
I1018 08:53:57.692632 4097533 var_desc.cc:415] Flush  reshape2_7.tmp_0 1
I1018 08:53:57.692636 4097533 block_desc.cc:209] Flush elementwise_pow_6.tmp_0
I1018 08:53:57.692637 4097533 var_desc.cc:415] Flush  elementwise_pow_6.tmp_0 1
I1018 08:53:57.692639 4097533 block_desc.cc:209] Flush elementwise_pow_4.tmp_0
I1018 08:53:57.692641 4097533 var_desc.cc:415] Flush  elementwise_pow_4.tmp_0 1
I1018 08:53:57.692644 4097533 block_desc.cc:209] Flush reshape2_7.tmp_1
I1018 08:53:57.692646 4097533 var_desc.cc:415] Flush  reshape2_7.tmp_1 1
I1018 08:53:57.692648 4097533 block_desc.cc:209] Flush batch_norm2d_1.w_2
I1018 08:53:57.692651 4097533 var_desc.cc:415] Flush  batch_norm2d_1.w_2 1
I1018 08:53:57.692653 4097533 block_desc.cc:209] Flush reshape2_4.tmp_0
I1018 08:53:57.692656 4097533 var_desc.cc:415] Flush  reshape2_4.tmp_0 1
I1018 08:53:57.692657 4097533 block_desc.cc:209] Flush reshape2_4.tmp_1
I1018 08:53:57.692659 4097533 var_desc.cc:415] Flush  reshape2_4.tmp_1 1
I1018 08:53:57.692662 4097533 block_desc.cc:209] Flush fill_constant_9.tmp_0
I1018 08:53:57.692664 4097533 var_desc.cc:415] Flush  fill_constant_9.tmp_0 1
I1018 08:53:57.690737 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_1.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "tmp_15"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 56
          dims: 56
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_1.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "tmp_17"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 56
          dims: 56
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_1.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "reshape2_5.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "conv2d_1.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 56
          dims: 56
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "relu_1.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 56
          dims: 56
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "fill_constant_15.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_13.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_6.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "tmp_16"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  vars {
    name: "assign_5.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_14"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  vars {
    name: "tmp_18"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 56
          dims: 56
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "reshape2_5.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "batch_norm_1.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 56
          dims: 56
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_6.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_7.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "elementwise_pow_6.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "elementwise_pow_4.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_7.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_1.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "reshape2_4.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_4.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_9.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_1.w_2"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_1.w_2"
    }
    outputs {
      parameter: "Out"
      arguments: "reshape2_5.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_5.tmp_1"
    }
    type: "reshape2"
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 124, in composite_batchnorm"
      strings: "    (reshape(run_var, stats_shape) + epsilon), half"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/manipulation.py\", line 4518, in reshape"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 1
      ints: 64
      ints: 1
      ints: 1
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "BiasTensor"
    }
    inputs {
      parameter: "ScaleTensor"
    }
    inputs {
      parameter: "X"
      arguments: "reshape2_5.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_16"
    }
    type: "scale"
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "bias"
      type: FLOAT
      f: 1e-05
    }
    attrs {
      name: "bias_after_scale"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 124, in composite_batchnorm"
      strings: "    (reshape(run_var, stats_shape) + epsilon), half"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 557, in __impl__"
      strings: "    return scalar_method(self, other_var)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 530, in _scalar_add_"
      strings: "    return _scalar_op_(var, 1.0, value)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 451, in _scalar_op_"
      strings: "    block.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "scale"
      type: FLOAT
      f: 1
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_1.w_1"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_1.w_1"
    }
    outputs {
      parameter: "Out"
      arguments: "reshape2_4.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_4.tmp_1"
    }
    type: "reshape2"
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 123, in composite_batchnorm"
      strings: "    x_hat = (x - reshape(run_mean, stats_shape)) * pow("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/manipulation.py\", line 4518, in reshape"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 1
      ints: 64
      ints: 1
      ints: 1
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "co
I1018 08:53:57.693897 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.694085 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_20, is_only_used_internal: 1
I1018 08:53:57.694088 4097533 build_cinn_pass.cc:562] insert internal var: tmp_20
I1018 08:53:57.694092 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: elementwise_pow_8.tmp_0, is_only_used_internal: 1
I1018 08:53:57.694093 4097533 build_cinn_pass.cc:562] insert internal var: elementwise_pow_8.tmp_0
I1018 08:53:57.694096 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_9.tmp_1
I1018 08:53:57.694098 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_21, is_only_used_internal: 1
I1018 08:53:57.694100 4097533 build_cinn_pass.cc:562] insert internal var: tmp_21
I1018 08:53:57.694103 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_9.tmp_0, is_only_used_internal: 1
I1018 08:53:57.694105 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_9.tmp_0
I1018 08:53:57.694108 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_21.tmp_0, is_only_used_internal: 1
I1018 08:53:57.694109 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_21.tmp_0
I1018 08:53:57.694111 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: elementwise_pow_10.tmp_0, is_only_used_internal: 1
I1018 08:53:57.694114 4097533 build_cinn_pass.cc:562] insert internal var: elementwise_pow_10.tmp_0
I1018 08:53:57.694116 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_17.tmp_0, is_only_used_internal: 1
I1018 08:53:57.694118 4097533 build_cinn_pass.cc:557] var_node->outputs[1]: fill_constant_17.tmp_0, is_only_used_internal: 1
I1018 08:53:57.694120 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_17.tmp_0
I1018 08:53:57.694123 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_10.tmp_1
I1018 08:53:57.694125 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_10.tmp_0, is_only_used_internal: 1
I1018 08:53:57.694128 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_10.tmp_0
I1018 08:53:57.694129 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_24, is_only_used_internal: 1
I1018 08:53:57.694131 4097533 build_cinn_pass.cc:562] insert internal var: tmp_24
I1018 08:53:57.694133 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_0, is_only_used_internal: 1
I1018 08:53:57.694135 4097533 build_cinn_pass.cc:562] insert internal var: tmp_0
I1018 08:53:57.694137 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_8.tmp_0, is_only_used_internal: 1
I1018 08:53:57.694139 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_8.tmp_0
I1018 08:53:57.694142 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_23.tmp_0, is_only_used_internal: 1
I1018 08:53:57.694144 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_23.tmp_0
I1018 08:53:57.694146 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_22, is_only_used_internal: 1
I1018 08:53:57.694149 4097533 build_cinn_pass.cc:562] insert internal var: tmp_22
I1018 08:53:57.694150 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_23, is_only_used_internal: 1
I1018 08:53:57.694154 4097533 build_cinn_pass.cc:562] insert internal var: tmp_23
I1018 08:53:57.694155 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_11.tmp_1
I1018 08:53:57.694157 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_11.tmp_0, is_only_used_internal: 1
I1018 08:53:57.694159 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_11.tmp_0
I1018 08:53:57.694162 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: batch_norm_2.tmp_2, is_only_used_internal: 1
I1018 08:53:57.694166 4097533 build_cinn_pass.cc:562] insert internal var: batch_norm_2.tmp_2
I1018 08:53:57.694169 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_8.tmp_1
I1018 08:53:57.694172 4097533 build_cinn_pass.cc:742] Cluster Ops: (elementwise_pow, reshape2, elementwise_mul, scale, reshape2, elementwise_add, elementwise_add, reshape2, elementwise_mul, fill_constant, elementwise_max, fill_constant, reshape2, scale, elementwise_pow, elementwise_sub, assign, fill_constant, )
I1018 08:53:57.694175 4097533 build_cinn_pass.cc:743] Cluster input vars: (conv2d_2.tmp_0, batch_norm2d_2.w_2, batch_norm2d_2.w_0, batch_norm2d_2.b_0, pool2d_0.tmp_0, batch_norm2d_2.w_1, )
I1018 08:53:57.694178 4097533 build_cinn_pass.cc:744] Cluster output vars: (assign_9.tmp_0, relu_2.tmp_0, )
I1018 08:53:57.694180 4097533 build_cinn_pass.cc:745] Cluster internal vars: (reshape2_8.tmp_1, batch_norm_2.tmp_2, reshape2_11.tmp_0, reshape2_11.tmp_1, tmp_23, tmp_22, fill_constant_23.tmp_0, tmp_20, reshape2_9.tmp_1, elementwise_pow_8.tmp_0, tmp_21, reshape2_9.tmp_0, fill_constant_21.tmp_0, elementwise_pow_10.tmp_0, reshape2_10.tmp_1, fill_constant_17.tmp_0, tmp_0, reshape2_8.tmp_0, tmp_24, reshape2_10.tmp_0, )
I1018 08:53:57.694192 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.694409 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: conv2d_2.tmp_0
I1018 08:53:57.694413 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: pool2d_0.tmp_0
I1018 08:53:57.694417 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_2.w_2
I1018 08:53:57.694422 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_2.w_1
I1018 08:53:57.694424 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_2.b_0
I1018 08:53:57.694427 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_2.w_0
I1018 08:53:57.694432 4097533 build_cinn_pass.cc:274] Add Output Var Node: assign_9.tmp_0
I1018 08:53:57.694435 4097533 build_cinn_pass.cc:274] Add Output Var Node: relu_2.tmp_0
I1018 08:53:57.694458 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_52[label="reshape2_10.tmp_1
[0,64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_51[label="batch_norm2d_2.w_2
[64]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_50[label="reshape2_11.tmp_1
[0,64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_48[label="tmp_22
[1,64,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_47[label="reshape2_11.tmp_0
[1,64,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_46[label="assign_9.tmp_0
[64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_45[label="reshape2_8.tmp_1
[0,64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_44[label="batch_norm2d_2.b_0
[64]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_43[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_42[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_39[label="elementwise_sub" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_37[label="fill_constant_23.tmp_0
[1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_53[label="tmp_21
[1,64,56,56]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_35[label="scale" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_32[label="elementwise_add" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_31[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_30[label="elementwise_pow_8.tmp_0
[64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_3[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_21[label="reshape2_10.tmp_0
[1,64,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_38[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_8[label="tmp_24
[1,64,56,56]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_49[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_7[label="conv2d_2.tmp_0
[1,64,56,56]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_6[label="batch_norm2d_2.w_1
[64]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_33[label="tmp_23
[1,64,56,56]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_26[label="tmp_20
[64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_11[label="fill_constant_17.tmp_0
[1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_16[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_12[label="pool2d_0.tmp_0
[1,64,56,56]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_5[label="relu_2.tmp_0
[1,64,56,56]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_4[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_24[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_27[label="elementwise_add" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_9[label="tmp_0
[1,64,56,56]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_36[label="elementwise_pow" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_17[label="elementwise_mul" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_10[label="batch_norm2d_2.w_0
[64]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_19[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_41[label="reshape2_9.tmp_1
[0,64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_34[label="elementwise_pow_10.tmp_0
[1,64,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_13[label="fill_constant_21.tmp_0
[64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_1[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_23[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_14[label="reshape2_8.tmp_0
[1,64,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_2[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_15[label="reshape2_9.tmp_0
[1,64,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_18[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_20[label="batch_norm_2.tmp_2
[1,64,56,56]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_40[label="elementwise_pow" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_22[label="elementwise_mul" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_25[label="elementwise_max" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_29[label="scale" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_28[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_2->node_10
   node_3->node_12
   node_4->node_7
   node_5->node_0
   node_6->node_18
   node_7->node_39
   node_8->node_27
   node_9->node_25
   node_10->node_16
   node_11->node_36
   node_11->node_40
   node_12->node_32
   node_13->node_35
   node_14->node_39
   node_15->node_29
   node_16->node_21
   node_16->node_52
   node_17->node_33
   node_18->node_14
   node_18->node_45
   node_19->node_51
   node_20->node_32
   node_21->node_22
   node_22->node_8
   node_23->node_11
   node_24->node_15
   node_24->node_41
   node_25->node_5
   node_26->node_40
   node_27->node_20
   node_28->node_6
   node_29->node_48
   node_30->node_43
   node_31->node_13
   node_32->node_9
   node_33->node_22
   node_34->node_17
   node_35->node_26
   node_36->node_34
   node_37->node_25
   node_38->node_44
   node_39->node_53
   node_40->node_30
   node_42->node_37
   node_43->node_46
   node_44->node_49
   node_46->node_1
   node_47->node_27
   node_48->node_36
   node_49->node_47
   node_49->node_50
   node_51->node_24
   node_53->node_17
} // end G
I1018 08:53:57.694741 4097533 var_desc.cc:415] Flush  relu_2.tmp_0 1
I1018 08:53:57.694744 4097533 var_desc.cc:415] Flush  batch_norm2d_2.w_1 1
I1018 08:53:57.694747 4097533 var_desc.cc:415] Flush  conv2d_2.tmp_0 1
I1018 08:53:57.694751 4097533 var_desc.cc:415] Flush  tmp_24 1
I1018 08:53:57.694753 4097533 var_desc.cc:415] Flush  tmp_0 1
I1018 08:53:57.694756 4097533 var_desc.cc:415] Flush  batch_norm2d_2.w_0 1
I1018 08:53:57.694758 4097533 var_desc.cc:415] Flush  fill_constant_17.tmp_0 1
I1018 08:53:57.694761 4097533 var_desc.cc:415] Flush  pool2d_0.tmp_0 1
I1018 08:53:57.694764 4097533 var_desc.cc:415] Flush  fill_constant_21.tmp_0 1
I1018 08:53:57.694767 4097533 var_desc.cc:415] Flush  reshape2_8.tmp_0 1
I1018 08:53:57.694769 4097533 var_desc.cc:415] Flush  reshape2_9.tmp_0 1
I1018 08:53:57.694772 4097533 var_desc.cc:415] Flush  batch_norm_2.tmp_2 1
I1018 08:53:57.694775 4097533 var_desc.cc:415] Flush  reshape2_10.tmp_0 1
I1018 08:53:57.694778 4097533 var_desc.cc:415] Flush  tmp_20 1
I1018 08:53:57.694780 4097533 var_desc.cc:415] Flush  elementwise_pow_8.tmp_0 1
I1018 08:53:57.694783 4097533 var_desc.cc:415] Flush  tmp_23 1
I1018 08:53:57.694787 4097533 var_desc.cc:415] Flush  elementwise_pow_10.tmp_0 1
I1018 08:53:57.694789 4097533 var_desc.cc:415] Flush  fill_constant_23.tmp_0 1
I1018 08:53:57.694792 4097533 var_desc.cc:415] Flush  reshape2_9.tmp_1 1
I1018 08:53:57.694795 4097533 var_desc.cc:415] Flush  batch_norm2d_2.b_0 1
I1018 08:53:57.694797 4097533 var_desc.cc:415] Flush  reshape2_8.tmp_1 1
I1018 08:53:57.694800 4097533 var_desc.cc:415] Flush  assign_9.tmp_0 1
I1018 08:53:57.694802 4097533 var_desc.cc:415] Flush  reshape2_11.tmp_0 1
I1018 08:53:57.694805 4097533 var_desc.cc:415] Flush  tmp_22 1
I1018 08:53:57.694808 4097533 var_desc.cc:415] Flush  reshape2_11.tmp_1 1
I1018 08:53:57.694810 4097533 var_desc.cc:415] Flush  batch_norm2d_2.w_2 1
I1018 08:53:57.694813 4097533 var_desc.cc:415] Flush  reshape2_10.tmp_1 1
I1018 08:53:57.694815 4097533 var_desc.cc:415] Flush  tmp_21 1
I1018 08:53:57.694834 4097533 graph_helper.h:104] adj elementwise_max0x55b7d09d9aa0 -> fetch0x55b7d0f058d0  via relu_2.tmp_00x55b7d09d08d0
I1018 08:53:57.694841 4097533 graph_helper.h:104] adj assign0x55b7d096de60 -> fetch0x55b7d0f05520  via assign_9.tmp_00x55b7d09d05a0
I1018 08:53:57.694846 4097533 graph_helper.h:104] adj feed0x55b7d0f051d0 -> reshape20x55b7d09c76b0  via batch_norm2d_2.w_00x55b7d09814f0
I1018 08:53:57.694855 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d0f1c560 -> elementwise_mul0x55b7d09caed0  via tmp_210x55b7d097e540
I1018 08:53:57.694861 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d09adcf0 -> elementwise_mul0x55b7d09caed0  via elementwise_pow_10.tmp_00x55b7d09c7da0
I1018 08:53:57.694869 4097533 graph_helper.h:104] adj feed0x55b7d09e2790 -> reshape20x55b7d09bf330  via batch_norm2d_2.w_10x55b7d09d0230
I1018 08:53:57.694876 4097533 graph_helper.h:104] adj reshape20x55b7d09c76b0 -> elementwise_mul0x55b7d09b0d70  via reshape2_10.tmp_00x55b7d0980ac0
I1018 08:53:57.694881 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d09caed0 -> elementwise_mul0x55b7d09b0d70  via tmp_230x55b7d0988d70
I1018 08:53:57.694887 4097533 graph_helper.h:104] adj feed0x55b7d09e2400 -> reshape20x55b7d0f762d0  via batch_norm2d_2.w_20x55b7d0981180
I1018 08:53:57.694893 4097533 graph_helper.h:104] adj elementwise_add0x55b7d09a4440 -> elementwise_max0x55b7d09d9aa0  via tmp_00x55b7d09c8820
I1018 08:53:57.694900 4097533 graph_helper.h:104] adj fill_constant0x55b7d0f17510 -> elementwise_max0x55b7d09d9aa0  via fill_constant_23.tmp_00x55b7d0988fe0
I1018 08:53:57.694904 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d09b0d70 -> elementwise_add0x55b7d09ce090  via tmp_240x55b7d0980780
I1018 08:53:57.694909 4097533 graph_helper.h:104] adj reshape20x55b7d09c1380 -> elementwise_add0x55b7d09ce090  via reshape2_11.tmp_00x55b7d0988ad0
I1018 08:53:57.694916 4097533 graph_helper.h:104] adj reshape20x55b7d0f762d0 -> scale0x55b7d09d7690  via reshape2_9.tmp_00x55b7d09c7770
I1018 08:53:57.694922 4097533 graph_helper.h:104] adj elementwise_add0x55b7d09ce090 -> elementwise_add0x55b7d09a4440  via batch_norm_2.tmp_20x55b7d0988990
I1018 08:53:57.694927 4097533 graph_helper.h:104] adj feed0x55b7d09e2070 -> elementwise_add0x55b7d09a4440  via pool2d_0.tmp_00x55b7d09cfef0
I1018 08:53:57.694933 4097533 graph_helper.h:104] adj fill_constant0x55b7d0f57170 -> scale0x55b7d099c100  via fill_constant_21.tmp_00x55b7d09c7a30
I1018 08:53:57.694938 4097533 graph_helper.h:104] adj scale0x55b7d09d7690 -> elementwise_pow0x55b7d09adcf0  via tmp_220x55b7d0988e70
I1018 08:53:57.694944 4097533 graph_helper.h:104] adj fill_constant0x55b7d09ad1a0 -> elementwise_pow0x55b7d09adcf0  via fill_constant_17.tmp_00x55b7d09c84b0
I1018 08:53:57.694950 4097533 graph_helper.h:104] adj reshape20x55b7d09bf330 -> elementwise_sub0x55b7d0f1c560  via reshape2_8.tmp_00x55b7d0980400
I1018 08:53:57.694955 4097533 graph_helper.h:104] adj feed0x55b7d09e1d00 -> elementwise_sub0x55b7d0f1c560  via conv2d_2.tmp_00x55b7d0980e40
I1018 08:53:57.694962 4097533 graph_helper.h:104] adj scale0x55b7d099c100 -> elementwise_pow0x55b7d0f775f0  via tmp_200x55b7d097dcb0
I1018 08:53:57.694967 4097533 graph_helper.h:104] adj fill_constant0x55b7d09ad1a0 -> elementwise_pow0x55b7d0f775f0  via fill_constant_17.tmp_00x55b7d09c84b0
I1018 08:53:57.694972 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0f775f0 -> assign0x55b7d096de60  via elementwise_pow_8.tmp_00x55b7d097e250
I1018 08:53:57.694978 4097533 graph_helper.h:104] adj feed0x55b7d09e2b20 -> reshape20x55b7d09c1380  via batch_norm2d_2.b_00x55b7d09cfbc0
I1018 08:53:57.695122 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.695125 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.695130 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.695134 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.695142 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.695144 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.695159 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.695163 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.695169 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.695173 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.695199 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.695201 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.695205 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.695207 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.695215 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.695217 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.695221 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.695226 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.695266 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.695268 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.695271 4097533 graph_helper.cc:698] convert op node to desc elementwise_sub
I1018 08:53:57.695274 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.695281 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.695284 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.695293 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.695297 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.695302 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.695305 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.695308 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.695312 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.695318 4097533 graph_helper.cc:698] convert op node to desc elementwise_add
I1018 08:53:57.695322 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.695328 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.695330 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.695399 4097533 graph_helper.cc:698] convert op node to desc elementwise_add
I1018 08:53:57.695402 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.695408 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.695411 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.695420 4097533 graph_helper.cc:698] convert op node to desc elementwise_max
I1018 08:53:57.695421 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.695428 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.695431 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.695438 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.695441 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.695447 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.695451 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.695457 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.695459 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.695466 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.695469 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.695472 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.695475 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.696130 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.696604 4097533 block_desc.cc:205] vars in desc 28
I1018 08:53:57.696609 4097533 block_desc.cc:209] Flush relu_2.tmp_0
I1018 08:53:57.696611 4097533 var_desc.cc:415] Flush  relu_2.tmp_0 1
I1018 08:53:57.696615 4097533 block_desc.cc:209] Flush batch_norm2d_2.w_1
I1018 08:53:57.696617 4097533 var_desc.cc:415] Flush  batch_norm2d_2.w_1 1
I1018 08:53:57.696620 4097533 block_desc.cc:209] Flush conv2d_2.tmp_0
I1018 08:53:57.696622 4097533 var_desc.cc:415] Flush  conv2d_2.tmp_0 1
I1018 08:53:57.696625 4097533 block_desc.cc:209] Flush tmp_24
I1018 08:53:57.696627 4097533 var_desc.cc:415] Flush  tmp_24 1
I1018 08:53:57.696630 4097533 block_desc.cc:209] Flush tmp_0
I1018 08:53:57.696632 4097533 var_desc.cc:415] Flush  tmp_0 1
I1018 08:53:57.696635 4097533 block_desc.cc:209] Flush batch_norm2d_2.w_0
I1018 08:53:57.696636 4097533 var_desc.cc:415] Flush  batch_norm2d_2.w_0 1
I1018 08:53:57.696640 4097533 block_desc.cc:209] Flush fill_constant_17.tmp_0
I1018 08:53:57.696641 4097533 var_desc.cc:415] Flush  fill_constant_17.tmp_0 1
I1018 08:53:57.696645 4097533 block_desc.cc:209] Flush pool2d_0.tmp_0
I1018 08:53:57.696646 4097533 var_desc.cc:415] Flush  pool2d_0.tmp_0 1
I1018 08:53:57.696648 4097533 block_desc.cc:209] Flush fill_constant_21.tmp_0
I1018 08:53:57.696650 4097533 var_desc.cc:415] Flush  fill_constant_21.tmp_0 1
I1018 08:53:57.696653 4097533 block_desc.cc:209] Flush reshape2_8.tmp_0
I1018 08:53:57.696655 4097533 var_desc.cc:415] Flush  reshape2_8.tmp_0 1
I1018 08:53:57.696662 4097533 block_desc.cc:209] Flush reshape2_9.tmp_0
I1018 08:53:57.696664 4097533 var_desc.cc:415] Flush  reshape2_9.tmp_0 1
I1018 08:53:57.696666 4097533 block_desc.cc:209] Flush batch_norm_2.tmp_2
I1018 08:53:57.696668 4097533 var_desc.cc:415] Flush  batch_norm_2.tmp_2 1
I1018 08:53:57.696671 4097533 block_desc.cc:209] Flush reshape2_10.tmp_0
I1018 08:53:57.696673 4097533 var_desc.cc:415] Flush  reshape2_10.tmp_0 1
I1018 08:53:57.696676 4097533 block_desc.cc:209] Flush tmp_20
I1018 08:53:57.696678 4097533 var_desc.cc:415] Flush  tmp_20 1
I1018 08:53:57.696681 4097533 block_desc.cc:209] Flush elementwise_pow_8.tmp_0
I1018 08:53:57.696682 4097533 var_desc.cc:415] Flush  elementwise_pow_8.tmp_0 1
I1018 08:53:57.696686 4097533 block_desc.cc:209] Flush tmp_23
I1018 08:53:57.696687 4097533 var_desc.cc:415] Flush  tmp_23 1
I1018 08:53:57.696689 4097533 block_desc.cc:209] Flush elementwise_pow_10.tmp_0
I1018 08:53:57.696692 4097533 var_desc.cc:415] Flush  elementwise_pow_10.tmp_0 1
I1018 08:53:57.696694 4097533 block_desc.cc:209] Flush fill_constant_23.tmp_0
I1018 08:53:57.696696 4097533 var_desc.cc:415] Flush  fill_constant_23.tmp_0 1
I1018 08:53:57.696698 4097533 block_desc.cc:209] Flush reshape2_9.tmp_1
I1018 08:53:57.696700 4097533 var_desc.cc:415] Flush  reshape2_9.tmp_1 1
I1018 08:53:57.696703 4097533 block_desc.cc:209] Flush batch_norm2d_2.b_0
I1018 08:53:57.696705 4097533 var_desc.cc:415] Flush  batch_norm2d_2.b_0 1
I1018 08:53:57.696708 4097533 block_desc.cc:209] Flush reshape2_8.tmp_1
I1018 08:53:57.696710 4097533 var_desc.cc:415] Flush  reshape2_8.tmp_1 1
I1018 08:53:57.696712 4097533 block_desc.cc:209] Flush assign_9.tmp_0
I1018 08:53:57.696714 4097533 var_desc.cc:415] Flush  assign_9.tmp_0 1
I1018 08:53:57.696717 4097533 block_desc.cc:209] Flush reshape2_11.tmp_0
I1018 08:53:57.696719 4097533 var_desc.cc:415] Flush  reshape2_11.tmp_0 1
I1018 08:53:57.696722 4097533 block_desc.cc:209] Flush tmp_22
I1018 08:53:57.696723 4097533 var_desc.cc:415] Flush  tmp_22 1
I1018 08:53:57.696727 4097533 block_desc.cc:209] Flush reshape2_11.tmp_1
I1018 08:53:57.696728 4097533 var_desc.cc:415] Flush  reshape2_11.tmp_1 1
I1018 08:53:57.696730 4097533 block_desc.cc:209] Flush batch_norm2d_2.w_2
I1018 08:53:57.696733 4097533 var_desc.cc:415] Flush  batch_norm2d_2.w_2 1
I1018 08:53:57.696735 4097533 block_desc.cc:209] Flush reshape2_10.tmp_1
I1018 08:53:57.696738 4097533 var_desc.cc:415] Flush  reshape2_10.tmp_1 1
I1018 08:53:57.696739 4097533 block_desc.cc:209] Flush tmp_21
I1018 08:53:57.696743 4097533 var_desc.cc:415] Flush  tmp_21 1
I1018 08:53:57.694733 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "relu_2.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 56
          dims: 56
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_2.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "conv2d_2.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 56
          dims: 56
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "tmp_24"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 56
          dims: 56
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 56
          dims: 56
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_2.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "fill_constant_17.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "pool2d_0.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 56
          dims: 56
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "fill_constant_21.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_8.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_9.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "batch_norm_2.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 56
          dims: 56
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_10.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "tmp_20"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  vars {
    name: "elementwise_pow_8.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_23"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 56
          dims: 56
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "elementwise_pow_10.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_23.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_9.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "batch_norm2d_2.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "reshape2_8.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "assign_9.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_11.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "tmp_22"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  vars {
    name: "reshape2_11.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_2.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "reshape2_10.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "tmp_21"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 56
          dims: 56
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_2.w_2"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_2.w_2"
    }
    outputs {
      parameter: "Out"
      arguments: "reshape2_9.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_9.tmp_1"
    }
    type: "reshape2"
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 124, in composite_batchnorm"
      strings: "    (reshape(run_var, stats_shape) + epsilon), half"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/manipulation.py\", line 4518, in reshape"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 1
      ints: 64
      ints: 1
      ints: 1
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "BiasTensor"
    }
    inputs {
      parameter: "ScaleTensor"
    }
    inputs {
      parameter: "X"
      arguments: "reshape2_9.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_22"
    }
    type: "scale"
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "bias"
      type: FLOAT
      f: 1e-05
    }
    attrs {
      name: "bias_after_scale"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 124, in composite_batchnorm"
      strings: "    (reshape(run_var, stats_shape) + epsilon), half"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 557, in __impl__"
      strings: "    return scalar_method(self, other_var)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 530, in _scalar_add_"
      strings: "    return _scalar_op_(var, 1.0, value)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 451, in _scalar_op_"
      strings: "    block.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "scale"
      type: FLOAT
      f: 1
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "ShapeTensorList"
    }
    inputs {
      parameter: "ValueTensor"
    }
    outputs {
      parameter: "Out"
      arguments: "fill_constant_17.tmp_0"
    }
    type: "fill_constant"
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "dtype"
      type: INT
      i: 5
    }
    attrs {
      name: "force_cpu"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 100, in composite_batchnorm"
      strings: "    half = full([1], -0.5, x.dtype)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 1302, in full"
      strings: "    return fill_constant(shape=shape, dtype=dtype, value=fill_value, name=name)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 977, in fill_constant"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs
I1018 08:53:57.698009 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.698189 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_25.tmp_0, is_only_used_internal: 1
I1018 08:53:57.698196 4097533 build_cinn_pass.cc:557] var_node->outputs[1]: fill_constant_25.tmp_0, is_only_used_internal: 1
I1018 08:53:57.698199 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_25.tmp_0
I1018 08:53:57.698200 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: elementwise_pow_12.tmp_0, is_only_used_internal: 1
I1018 08:53:57.698202 4097533 build_cinn_pass.cc:562] insert internal var: elementwise_pow_12.tmp_0
I1018 08:53:57.698205 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_26, is_only_used_internal: 1
I1018 08:53:57.698207 4097533 build_cinn_pass.cc:562] insert internal var: tmp_26
I1018 08:53:57.698210 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_27, is_only_used_internal: 1
I1018 08:53:57.698211 4097533 build_cinn_pass.cc:562] insert internal var: tmp_27
I1018 08:53:57.698213 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_13.tmp_0, is_only_used_internal: 1
I1018 08:53:57.698216 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_13.tmp_0
I1018 08:53:57.698218 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_28, is_only_used_internal: 1
I1018 08:53:57.698220 4097533 build_cinn_pass.cc:562] insert internal var: tmp_28
I1018 08:53:57.698222 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_29, is_only_used_internal: 1
I1018 08:53:57.698225 4097533 build_cinn_pass.cc:562] insert internal var: tmp_29
I1018 08:53:57.698227 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_13.tmp_1
I1018 08:53:57.698230 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_31.tmp_0, is_only_used_internal: 1
I1018 08:53:57.698231 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_31.tmp_0
I1018 08:53:57.698233 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_12.tmp_0, is_only_used_internal: 1
I1018 08:53:57.698235 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_12.tmp_0
I1018 08:53:57.698238 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_29.tmp_0, is_only_used_internal: 1
I1018 08:53:57.698240 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_29.tmp_0
I1018 08:53:57.698242 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_12.tmp_1
I1018 08:53:57.698244 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: elementwise_pow_14.tmp_0, is_only_used_internal: 1
I1018 08:53:57.698246 4097533 build_cinn_pass.cc:562] insert internal var: elementwise_pow_14.tmp_0
I1018 08:53:57.698249 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_14.tmp_1
I1018 08:53:57.698251 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_14.tmp_0, is_only_used_internal: 1
I1018 08:53:57.698253 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_14.tmp_0
I1018 08:53:57.698256 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_30, is_only_used_internal: 1
I1018 08:53:57.698258 4097533 build_cinn_pass.cc:562] insert internal var: tmp_30
I1018 08:53:57.698261 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_15.tmp_1
I1018 08:53:57.698262 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_15.tmp_0, is_only_used_internal: 1
I1018 08:53:57.698264 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_15.tmp_0
I1018 08:53:57.698266 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: batch_norm_3.tmp_2, is_only_used_internal: 1
I1018 08:53:57.698268 4097533 build_cinn_pass.cc:562] insert internal var: batch_norm_3.tmp_2
I1018 08:53:57.698272 4097533 build_cinn_pass.cc:742] Cluster Ops: (scale, reshape2, elementwise_pow, elementwise_mul, reshape2, elementwise_mul, fill_constant, reshape2, elementwise_max, elementwise_add, fill_constant, reshape2, elementwise_sub, scale, elementwise_pow, assign, fill_constant, )
I1018 08:53:57.698275 4097533 build_cinn_pass.cc:743] Cluster input vars: (conv2d_3.tmp_0, batch_norm2d_3.w_2, batch_norm2d_3.w_1, batch_norm2d_3.w_0, batch_norm2d_3.b_0, )
I1018 08:53:57.698278 4097533 build_cinn_pass.cc:744] Cluster output vars: (assign_13.tmp_0, relu_3.tmp_0, )
I1018 08:53:57.698282 4097533 build_cinn_pass.cc:745] Cluster internal vars: (batch_norm_3.tmp_2, reshape2_15.tmp_0, reshape2_15.tmp_1, tmp_30, reshape2_14.tmp_0, reshape2_14.tmp_1, fill_constant_25.tmp_0, elementwise_pow_12.tmp_0, tmp_26, tmp_28, tmp_29, reshape2_13.tmp_1, fill_constant_31.tmp_0, tmp_27, reshape2_13.tmp_0, reshape2_12.tmp_0, fill_constant_29.tmp_0, reshape2_12.tmp_1, elementwise_pow_14.tmp_0, )
I1018 08:53:57.698292 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.698498 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: conv2d_3.tmp_0
I1018 08:53:57.698503 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_3.w_2
I1018 08:53:57.698506 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_3.b_0
I1018 08:53:57.698510 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_3.w_0
I1018 08:53:57.698513 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_3.w_1
I1018 08:53:57.698518 4097533 build_cinn_pass.cc:274] Add Output Var Node: assign_13.tmp_0
I1018 08:53:57.698520 4097533 build_cinn_pass.cc:274] Add Output Var Node: relu_3.tmp_0
I1018 08:53:57.698542 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_48[label="conv2d_3.tmp_0
[1,64,56,56]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_47[label="elementwise_pow_12.tmp_0
[64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_46[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_45[label="batch_norm2d_3.w_2
[64]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_44[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_43[label="fill_constant_25.tmp_0
[1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_42[label="reshape2_14.tmp_1
[0,64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_39[label="batch_norm_3.tmp_2
[1,64,56,56]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_37[label="reshape2_15.tmp_0
[1,64,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_35[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_32[label="batch_norm2d_3.w_0
[64]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_31[label="scale" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_30[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_21[label="elementwise_pow" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_38[label="elementwise_max" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_8[label="elementwise_sub" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_49[label="reshape2_13.tmp_1
[0,64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_7[label="fill_constant_31.tmp_0
[1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_6[label="reshape2_12.tmp_1
[0,64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_33[label="elementwise_mul" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_26[label="tmp_29
[1,64,56,56]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_11[label="batch_norm2d_3.b_0
[64]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_16[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_12[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_5[label="elementwise_pow_14.tmp_0
[1,64,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_4[label="assign_13.tmp_0
[64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_24[label="reshape2_13.tmp_0
[1,64,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_27[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_9[label="relu_3.tmp_0
[1,64,56,56]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_36[label="reshape2_12.tmp_0
[1,64,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_17[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_10[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_19[label="fill_constant_29.tmp_0
[64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_41[label="reshape2_15.tmp_1
[0,64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_34[label="elementwise_pow" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_13[label="elementwise_add" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_23[label="tmp_26
[64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_14[label="batch_norm2d_3.w_1
[64]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_2[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_15[label="tmp_30
[1,64,56,56]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_18[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_20[label="reshape2_14.tmp_0
[1,64,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_40[label="tmp_28
[1,64,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_22[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_25[label="elementwise_mul" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_29[label="scale" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_28[label="tmp_27
[1,64,56,56]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_1->node_32
   node_2->node_45
   node_3->node_48
   node_4->node_0
   node_5->node_25
   node_7->node_38
   node_8->node_28
   node_9->node_44
   node_10->node_24
   node_10->node_49
   node_11->node_46
   node_12->node_19
   node_13->node_39
   node_14->node_22
   node_15->node_13
   node_16->node_7
   node_17->node_20
   node_17->node_42
   node_18->node_14
   node_19->node_31
   node_20->node_33
   node_21->node_5
   node_22->node_36
   node_22->node_6
   node_23->node_34
   node_24->node_29
   node_25->node_26
   node_26->node_33
   node_27->node_4
   node_28->node_25
   node_29->node_40
   node_30->node_11
   node_31->node_23
   node_32->node_17
   node_33->node_15
   node_34->node_47
   node_35->node_43
   node_36->node_8
   node_37->node_13
   node_38->node_9
   node_39->node_38
   node_40->node_21
   node_43->node_21
   node_43->node_34
   node_45->node_10
   node_46->node_37
   node_46->node_41
   node_47->node_27
   node_48->node_8
} // end G
I1018 08:53:57.698802 4097533 var_desc.cc:415] Flush  assign_13.tmp_0 1
I1018 08:53:57.698808 4097533 var_desc.cc:415] Flush  elementwise_pow_14.tmp_0 1
I1018 08:53:57.698812 4097533 var_desc.cc:415] Flush  reshape2_12.tmp_1 1
I1018 08:53:57.698813 4097533 var_desc.cc:415] Flush  fill_constant_31.tmp_0 1
I1018 08:53:57.698817 4097533 var_desc.cc:415] Flush  relu_3.tmp_0 1
I1018 08:53:57.698819 4097533 var_desc.cc:415] Flush  batch_norm2d_3.b_0 1
I1018 08:53:57.698822 4097533 var_desc.cc:415] Flush  batch_norm2d_3.w_1 1
I1018 08:53:57.698824 4097533 var_desc.cc:415] Flush  tmp_30 1
I1018 08:53:57.698827 4097533 var_desc.cc:415] Flush  fill_constant_29.tmp_0 1
I1018 08:53:57.698830 4097533 var_desc.cc:415] Flush  reshape2_14.tmp_0 1
I1018 08:53:57.698832 4097533 var_desc.cc:415] Flush  tmp_26 1
I1018 08:53:57.698835 4097533 var_desc.cc:415] Flush  reshape2_13.tmp_0 1
I1018 08:53:57.698837 4097533 var_desc.cc:415] Flush  tmp_29 1
I1018 08:53:57.698840 4097533 var_desc.cc:415] Flush  tmp_27 1
I1018 08:53:57.698843 4097533 var_desc.cc:415] Flush  batch_norm2d_3.w_0 1
I1018 08:53:57.698845 4097533 var_desc.cc:415] Flush  reshape2_12.tmp_0 1
I1018 08:53:57.698848 4097533 var_desc.cc:415] Flush  reshape2_15.tmp_0 1
I1018 08:53:57.698851 4097533 var_desc.cc:415] Flush  batch_norm_3.tmp_2 1
I1018 08:53:57.698854 4097533 var_desc.cc:415] Flush  tmp_28 1
I1018 08:53:57.698856 4097533 var_desc.cc:415] Flush  reshape2_15.tmp_1 1
I1018 08:53:57.698859 4097533 var_desc.cc:415] Flush  reshape2_14.tmp_1 1
I1018 08:53:57.698861 4097533 var_desc.cc:415] Flush  fill_constant_25.tmp_0 1
I1018 08:53:57.698864 4097533 var_desc.cc:415] Flush  batch_norm2d_3.w_2 1
I1018 08:53:57.698866 4097533 var_desc.cc:415] Flush  elementwise_pow_12.tmp_0 1
I1018 08:53:57.698869 4097533 var_desc.cc:415] Flush  conv2d_3.tmp_0 1
I1018 08:53:57.698871 4097533 var_desc.cc:415] Flush  reshape2_13.tmp_1 1
I1018 08:53:57.698889 4097533 graph_helper.h:104] adj assign0x55b7d0f9f0a0 -> fetch0x55b7d0f485d0  via assign_13.tmp_00x55b7d09f3710
I1018 08:53:57.698896 4097533 graph_helper.h:104] adj reshape20x55b7d09fc1c0 -> elementwise_sub0x55b7d0fb0610  via reshape2_12.tmp_00x55b7d09f17d0
I1018 08:53:57.698904 4097533 graph_helper.h:104] adj feed0x55b7d0a2b1f0 -> elementwise_sub0x55b7d0fb0610  via conv2d_3.tmp_00x55b7d09f25d0
I1018 08:53:57.698908 4097533 graph_helper.h:104] adj feed0x55b7d0a2b5a0 -> reshape20x55b7d0fa8950  via batch_norm2d_3.w_20x55b7d09f2950
I1018 08:53:57.698916 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d09d2490 -> elementwise_add0x55b7d0f4b140  via tmp_300x55b7d0a271d0
I1018 08:53:57.698922 4097533 graph_helper.h:104] adj reshape20x55b7d0a048f0 -> elementwise_add0x55b7d0f4b140  via reshape2_15.tmp_00x55b7d0a26f50
I1018 08:53:57.698927 4097533 graph_helper.h:104] adj feed0x55b7d0f47ef0 -> reshape20x55b7d0f9d110  via batch_norm2d_3.w_00x55b7d09f3030
I1018 08:53:57.698935 4097533 graph_helper.h:104] adj scale0x55b7d09f5870 -> elementwise_pow0x55b7d0a1c9e0  via tmp_280x55b7d09b98e0
I1018 08:53:57.698940 4097533 graph_helper.h:104] adj fill_constant0x55b7d0f89060 -> elementwise_pow0x55b7d0a1c9e0  via fill_constant_25.tmp_00x55b7d0a275e0
I1018 08:53:57.698945 4097533 graph_helper.h:104] adj feed0x55b7d0f48280 -> reshape20x55b7d09fc1c0  via batch_norm2d_3.w_10x55b7d09f2cc0
I1018 08:53:57.698951 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d0fb0610 -> elementwise_mul0x55b7d09e8f90  via tmp_270x55b7d09ba4c0
I1018 08:53:57.698957 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0a1c9e0 -> elementwise_mul0x55b7d09e8f90  via elementwise_pow_14.tmp_00x55b7d09f2230
I1018 08:53:57.698962 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d09c9fd0 -> assign0x55b7d0f9f0a0  via elementwise_pow_12.tmp_00x55b7d09b9320
I1018 08:53:57.698967 4097533 graph_helper.h:104] adj reshape20x55b7d0fa8950 -> scale0x55b7d09f5870  via reshape2_13.tmp_00x55b7d09f1450
I1018 08:53:57.698974 4097533 graph_helper.h:104] adj fill_constant0x55b7d097e9c0 -> scale0x55b7d0f6f960  via fill_constant_29.tmp_00x55b7d09f1b50
I1018 08:53:57.698979 4097533 graph_helper.h:104] adj reshape20x55b7d0f9d110 -> elementwise_mul0x55b7d09d2490  via reshape2_14.tmp_00x55b7d0a272d0
I1018 08:53:57.698987 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d09e8f90 -> elementwise_mul0x55b7d09d2490  via tmp_290x55b7d09b9ba0
I1018 08:53:57.698992 4097533 graph_helper.h:104] adj scale0x55b7d0f6f960 -> elementwise_pow0x55b7d09c9fd0  via tmp_260x55b7d09b9630
I1018 08:53:57.698998 4097533 graph_helper.h:104] adj fill_constant0x55b7d0f89060 -> elementwise_pow0x55b7d09c9fd0  via fill_constant_25.tmp_00x55b7d0a275e0
I1018 08:53:57.699003 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0f4b140 -> elementwise_max0x55b7d0a03340  via batch_norm_3.tmp_20x55b7d09fb990
I1018 08:53:57.699009 4097533 graph_helper.h:104] adj fill_constant0x55b7d0f9e7e0 -> elementwise_max0x55b7d0a03340  via fill_constant_31.tmp_00x55b7d09ba150
I1018 08:53:57.699014 4097533 graph_helper.h:104] adj elementwise_max0x55b7d0a03340 -> fetch0x55b7d0f48920  via relu_3.tmp_00x55b7d09f3a40
I1018 08:53:57.699019 4097533 graph_helper.h:104] adj feed0x55b7d0a2b930 -> reshape20x55b7d0a048f0  via batch_norm2d_3.b_00x55b7d09f33a0
I1018 08:53:57.699154 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.699156 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.699162 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.699165 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.699173 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.699177 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.699190 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.699193 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.699196 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.699199 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.699220 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.699223 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.699230 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.699234 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.699241 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.699244 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.699246 4097533 graph_helper.cc:698] convert op node to desc elementwise_sub
I1018 08:53:57.699249 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.699288 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.699290 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.699297 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.699301 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.699303 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.699306 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.699313 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.699316 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.699323 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.699326 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.699333 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.699335 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.699338 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.699342 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.699348 4097533 graph_helper.cc:698] convert op node to desc elementwise_add
I1018 08:53:57.699352 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.699424 4097533 graph_helper.cc:698] convert op node to desc elementwise_max
I1018 08:53:57.699426 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.699433 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.699435 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.699443 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.699446 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.699453 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.699456 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.699462 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.699467 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.699474 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.699477 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.699481 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.699482 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.700116 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.700568 4097533 block_desc.cc:205] vars in desc 26
I1018 08:53:57.700573 4097533 block_desc.cc:209] Flush assign_13.tmp_0
I1018 08:53:57.700575 4097533 var_desc.cc:415] Flush  assign_13.tmp_0 1
I1018 08:53:57.700578 4097533 block_desc.cc:209] Flush elementwise_pow_14.tmp_0
I1018 08:53:57.700582 4097533 var_desc.cc:415] Flush  elementwise_pow_14.tmp_0 1
I1018 08:53:57.700584 4097533 block_desc.cc:209] Flush reshape2_12.tmp_1
I1018 08:53:57.700587 4097533 var_desc.cc:415] Flush  reshape2_12.tmp_1 1
I1018 08:53:57.700589 4097533 block_desc.cc:209] Flush fill_constant_31.tmp_0
I1018 08:53:57.700591 4097533 var_desc.cc:415] Flush  fill_constant_31.tmp_0 1
I1018 08:53:57.700593 4097533 block_desc.cc:209] Flush relu_3.tmp_0
I1018 08:53:57.700596 4097533 var_desc.cc:415] Flush  relu_3.tmp_0 1
I1018 08:53:57.700598 4097533 block_desc.cc:209] Flush batch_norm2d_3.b_0
I1018 08:53:57.700600 4097533 var_desc.cc:415] Flush  batch_norm2d_3.b_0 1
I1018 08:53:57.700603 4097533 block_desc.cc:209] Flush batch_norm2d_3.w_1
I1018 08:53:57.700605 4097533 var_desc.cc:415] Flush  batch_norm2d_3.w_1 1
I1018 08:53:57.700608 4097533 block_desc.cc:209] Flush tmp_30
I1018 08:53:57.700609 4097533 var_desc.cc:415] Flush  tmp_30 1
I1018 08:53:57.700613 4097533 block_desc.cc:209] Flush fill_constant_29.tmp_0
I1018 08:53:57.700614 4097533 var_desc.cc:415] Flush  fill_constant_29.tmp_0 1
I1018 08:53:57.700618 4097533 block_desc.cc:209] Flush reshape2_14.tmp_0
I1018 08:53:57.700619 4097533 var_desc.cc:415] Flush  reshape2_14.tmp_0 1
I1018 08:53:57.700621 4097533 block_desc.cc:209] Flush tmp_26
I1018 08:53:57.700624 4097533 var_desc.cc:415] Flush  tmp_26 1
I1018 08:53:57.700626 4097533 block_desc.cc:209] Flush reshape2_13.tmp_0
I1018 08:53:57.700628 4097533 var_desc.cc:415] Flush  reshape2_13.tmp_0 1
I1018 08:53:57.700631 4097533 block_desc.cc:209] Flush tmp_29
I1018 08:53:57.700634 4097533 var_desc.cc:415] Flush  tmp_29 1
I1018 08:53:57.700635 4097533 block_desc.cc:209] Flush tmp_27
I1018 08:53:57.700639 4097533 var_desc.cc:415] Flush  tmp_27 1
I1018 08:53:57.700640 4097533 block_desc.cc:209] Flush batch_norm2d_3.w_0
I1018 08:53:57.700642 4097533 var_desc.cc:415] Flush  batch_norm2d_3.w_0 1
I1018 08:53:57.700645 4097533 block_desc.cc:209] Flush reshape2_12.tmp_0
I1018 08:53:57.700647 4097533 var_desc.cc:415] Flush  reshape2_12.tmp_0 1
I1018 08:53:57.700649 4097533 block_desc.cc:209] Flush reshape2_15.tmp_0
I1018 08:53:57.700651 4097533 var_desc.cc:415] Flush  reshape2_15.tmp_0 1
I1018 08:53:57.700654 4097533 block_desc.cc:209] Flush batch_norm_3.tmp_2
I1018 08:53:57.700656 4097533 var_desc.cc:415] Flush  batch_norm_3.tmp_2 1
I1018 08:53:57.700659 4097533 block_desc.cc:209] Flush tmp_28
I1018 08:53:57.700661 4097533 var_desc.cc:415] Flush  tmp_28 1
I1018 08:53:57.700663 4097533 block_desc.cc:209] Flush reshape2_15.tmp_1
I1018 08:53:57.700665 4097533 var_desc.cc:415] Flush  reshape2_15.tmp_1 1
I1018 08:53:57.700668 4097533 block_desc.cc:209] Flush reshape2_14.tmp_1
I1018 08:53:57.700670 4097533 var_desc.cc:415] Flush  reshape2_14.tmp_1 1
I1018 08:53:57.700672 4097533 block_desc.cc:209] Flush fill_constant_25.tmp_0
I1018 08:53:57.700675 4097533 var_desc.cc:415] Flush  fill_constant_25.tmp_0 1
I1018 08:53:57.700677 4097533 block_desc.cc:209] Flush batch_norm2d_3.w_2
I1018 08:53:57.700680 4097533 var_desc.cc:415] Flush  batch_norm2d_3.w_2 1
I1018 08:53:57.700682 4097533 block_desc.cc:209] Flush elementwise_pow_12.tmp_0
I1018 08:53:57.700685 4097533 var_desc.cc:415] Flush  elementwise_pow_12.tmp_0 1
I1018 08:53:57.700686 4097533 block_desc.cc:209] Flush conv2d_3.tmp_0
I1018 08:53:57.700688 4097533 var_desc.cc:415] Flush  conv2d_3.tmp_0 1
I1018 08:53:57.700695 4097533 block_desc.cc:209] Flush reshape2_13.tmp_1
I1018 08:53:57.700697 4097533 var_desc.cc:415] Flush  reshape2_13.tmp_1 1
I1018 08:53:57.698796 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "assign_13.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "elementwise_pow_14.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_12.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_31.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "relu_3.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 56
          dims: 56
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_3.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_3.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "tmp_30"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 56
          dims: 56
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "fill_constant_29.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_14.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "tmp_26"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  vars {
    name: "reshape2_13.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_29"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 56
          dims: 56
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "tmp_27"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 56
          dims: 56
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_3.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "reshape2_12.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_15.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "batch_norm_3.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 56
          dims: 56
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "tmp_28"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  vars {
    name: "reshape2_15.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_14.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "fill_constant_25.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "batch_norm2d_3.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "elementwise_pow_12.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "conv2d_3.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 56
          dims: 56
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_13.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_3.w_2"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_3.w_2"
    }
    outputs {
      parameter: "Out"
      arguments: "reshape2_13.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_13.tmp_1"
    }
    type: "reshape2"
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 124, in composite_batchnorm"
      strings: "    (reshape(run_var, stats_shape) + epsilon), half"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/manipulation.py\", line 4518, in reshape"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 1
      ints: 64
      ints: 1
      ints: 1
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "BiasTensor"
    }
    inputs {
      parameter: "ScaleTensor"
    }
    inputs {
      parameter: "X"
      arguments: "reshape2_13.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_28"
    }
    type: "scale"
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "bias"
      type: FLOAT
      f: 1e-05
    }
    attrs {
      name: "bias_after_scale"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 124, in composite_batchnorm"
      strings: "    (reshape(run_var, stats_shape) + epsilon), half"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 557, in __impl__"
      strings: "    return scalar_method(self, other_var)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 530, in _scalar_add_"
      strings: "    return _scalar_op_(var, 1.0, value)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 451, in _scalar_op_"
      strings: "    block.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "scale"
      type: FLOAT
      f: 1
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_3.w_1"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_3.w_1"
    }
    outputs {
      parameter: "Out"
      arguments: "reshape2_12.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_12.tmp_1"
    }
    type: "reshape2"
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 123, in composite_batchnorm"
      strings: "    x_hat = (x - reshape(run_mean, stats_shape)) * pow("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/manipulation.py\", line 4518, in reshape"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 1
      ints: 64
      ints: 1
      ints: 1
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "ShapeTens
I1018 08:53:57.701872 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.702047 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_1, is_only_used_internal: 1
I1018 08:53:57.702050 4097533 build_cinn_pass.cc:562] insert internal var: tmp_1
I1018 08:53:57.702054 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_17.tmp_1
I1018 08:53:57.702055 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_17.tmp_0, is_only_used_internal: 1
I1018 08:53:57.702057 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_17.tmp_0
I1018 08:53:57.702060 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_32, is_only_used_internal: 1
I1018 08:53:57.702062 4097533 build_cinn_pass.cc:562] insert internal var: tmp_32
I1018 08:53:57.702064 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_33.tmp_0, is_only_used_internal: 1
I1018 08:53:57.702066 4097533 build_cinn_pass.cc:557] var_node->outputs[1]: fill_constant_33.tmp_0, is_only_used_internal: 1
I1018 08:53:57.702068 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_33.tmp_0
I1018 08:53:57.702071 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: elementwise_pow_16.tmp_0, is_only_used_internal: 1
I1018 08:53:57.702073 4097533 build_cinn_pass.cc:562] insert internal var: elementwise_pow_16.tmp_0
I1018 08:53:57.702075 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: elementwise_pow_18.tmp_0, is_only_used_internal: 1
I1018 08:53:57.702077 4097533 build_cinn_pass.cc:562] insert internal var: elementwise_pow_18.tmp_0
I1018 08:53:57.702080 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_16.tmp_0, is_only_used_internal: 1
I1018 08:53:57.702081 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_16.tmp_0
I1018 08:53:57.702083 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_39.tmp_0, is_only_used_internal: 1
I1018 08:53:57.702085 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_39.tmp_0
I1018 08:53:57.702088 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_18.tmp_1
I1018 08:53:57.702090 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_35, is_only_used_internal: 1
I1018 08:53:57.702092 4097533 build_cinn_pass.cc:562] insert internal var: tmp_35
I1018 08:53:57.702095 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_34, is_only_used_internal: 1
I1018 08:53:57.702096 4097533 build_cinn_pass.cc:562] insert internal var: tmp_34
I1018 08:53:57.702100 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_18.tmp_0, is_only_used_internal: 1
I1018 08:53:57.702104 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_18.tmp_0
I1018 08:53:57.702106 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_36, is_only_used_internal: 1
I1018 08:53:57.702108 4097533 build_cinn_pass.cc:562] insert internal var: tmp_36
I1018 08:53:57.702111 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_37.tmp_0, is_only_used_internal: 1
I1018 08:53:57.702112 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_37.tmp_0
I1018 08:53:57.702116 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_16.tmp_1
I1018 08:53:57.702117 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_19.tmp_1
I1018 08:53:57.702119 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_19.tmp_0, is_only_used_internal: 1
I1018 08:53:57.702121 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_19.tmp_0
I1018 08:53:57.702124 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: batch_norm_4.tmp_2, is_only_used_internal: 1
I1018 08:53:57.702126 4097533 build_cinn_pass.cc:562] insert internal var: batch_norm_4.tmp_2
I1018 08:53:57.702128 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_33, is_only_used_internal: 1
I1018 08:53:57.702131 4097533 build_cinn_pass.cc:562] insert internal var: tmp_33
I1018 08:53:57.702133 4097533 build_cinn_pass.cc:742] Cluster Ops: (reshape2, reshape2, elementwise_mul, elementwise_mul, scale, reshape2, elementwise_add, fill_constant, elementwise_max, elementwise_sub, elementwise_pow, fill_constant, elementwise_pow, scale, assign, reshape2, elementwise_add, fill_constant, )
I1018 08:53:57.702137 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_4.w_2, relu_2.tmp_0, conv2d_4.tmp_0, batch_norm2d_4.w_0, batch_norm2d_4.w_1, batch_norm2d_4.b_0, )
I1018 08:53:57.702139 4097533 build_cinn_pass.cc:744] Cluster output vars: (assign_17.tmp_0, relu_4.tmp_0, )
I1018 08:53:57.702142 4097533 build_cinn_pass.cc:745] Cluster internal vars: (tmp_33, batch_norm_4.tmp_2, reshape2_19.tmp_0, reshape2_19.tmp_1, reshape2_16.tmp_1, fill_constant_37.tmp_0, tmp_1, fill_constant_39.tmp_0, reshape2_17.tmp_1, tmp_32, tmp_35, tmp_34, reshape2_17.tmp_0, fill_constant_33.tmp_0, elementwise_pow_16.tmp_0, elementwise_pow_18.tmp_0, reshape2_18.tmp_1, reshape2_16.tmp_0, tmp_36, reshape2_18.tmp_0, )
I1018 08:53:57.702150 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.702364 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: relu_2.tmp_0
I1018 08:53:57.702368 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: conv2d_4.tmp_0
I1018 08:53:57.702373 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_4.w_2
I1018 08:53:57.702376 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_4.b_0
I1018 08:53:57.702379 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_4.w_0
I1018 08:53:57.702382 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_4.w_1
I1018 08:53:57.702386 4097533 build_cinn_pass.cc:274] Add Output Var Node: assign_17.tmp_0
I1018 08:53:57.702390 4097533 build_cinn_pass.cc:274] Add Output Var Node: relu_4.tmp_0
I1018 08:53:57.702411 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_52[label="tmp_33
[1,64,56,56]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_51[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_50[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_48[label="reshape2_17.tmp_1
[0,64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_47[label="fill_constant_37.tmp_0
[64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_46[label="reshape2_19.tmp_1
[0,64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_45[label="reshape2_19.tmp_0
[1,64,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_44[label="reshape2_16.tmp_0
[1,64,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_43[label="batch_norm_4.tmp_2
[1,64,56,56]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_42[label="reshape2_17.tmp_0
[1,64,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_39[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_37[label="elementwise_add" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_53[label="fill_constant_39.tmp_0
[1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_35[label="tmp_32
[64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_32[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_31[label="conv2d_4.tmp_0
[1,64,56,56]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_30[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_21[label="relu_2.tmp_0
[1,64,56,56]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_38[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_8[label="batch_norm2d_4.w_0
[64]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_49[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_7[label="batch_norm2d_4.w_1
[64]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_6[label="batch_norm2d_4.b_0
[64]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_33[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_26[label="elementwise_pow" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_11[label="tmp_36
[1,64,56,56]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_16[label="tmp_34
[1,64,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_12[label="reshape2_18.tmp_1
[0,64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_5[label="assign_17.tmp_0
[64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_4[label="relu_4.tmp_0
[1,64,56,56]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_24[label="tmp_1
[1,64,56,56]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_27[label="scale" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_9[label="batch_norm2d_4.w_2
[64]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_36[label="elementwise_pow_16.tmp_0
[64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_17[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_10[label="reshape2_18.tmp_0
[1,64,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_19[label="tmp_35
[1,64,56,56]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_0[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_41[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_34[label="scale" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_13[label="elementwise_pow_18.tmp_0
[1,64,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_1[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_23[label="elementwise_mul" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_14[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_2[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_15[label="fill_constant_33.tmp_0
[1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_18[label="elementwise_mul" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_20[label="elementwise_max" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_40[label="elementwise_add" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_22[label="reshape2_16.tmp_1
[0,64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_25[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_29[label="elementwise_sub" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_28[label="elementwise_pow" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0->node_7
   node_1->node_8
   node_2->node_31
   node_3->node_21
   node_4->node_49
   node_5->node_50
   node_6->node_51
   node_7->node_33
   node_8->node_30
   node_9->node_39
   node_10->node_23
   node_11->node_37
   node_13->node_18
   node_14->node_6
   node_15->node_28
   node_15->node_26
   node_16->node_28
   node_17->node_9
   node_18->node_19
   node_19->node_23
   node_20->node_4
   node_21->node_40
   node_23->node_11
   node_24->node_20
   node_25->node_47
   node_26->node_36
   node_27->node_35
   node_28->node_13
   node_29->node_52
   node_30->node_10
   node_30->node_12
   node_31->node_29
   node_32->node_53
   node_33->node_44
   node_33->node_22
   node_34->node_16
   node_35->node_26
   node_36->node_38
   node_37->node_43
   node_38->node_5
   node_39->node_42
   node_39->node_48
   node_40->node_24
   node_41->node_15
   node_42->node_34
   node_43->node_40
   node_44->node_29
   node_45->node_37
   node_47->node_27
   node_51->node_45
   node_51->node_46
   node_52->node_18
   node_53->node_20
} // end G
I1018 08:53:57.702690 4097533 var_desc.cc:415] Flush  relu_4.tmp_0 1
I1018 08:53:57.702693 4097533 var_desc.cc:415] Flush  assign_17.tmp_0 1
I1018 08:53:57.702697 4097533 var_desc.cc:415] Flush  batch_norm2d_4.b_0 1
I1018 08:53:57.702699 4097533 var_desc.cc:415] Flush  batch_norm2d_4.w_1 1
I1018 08:53:57.702702 4097533 var_desc.cc:415] Flush  batch_norm2d_4.w_0 1
I1018 08:53:57.702704 4097533 var_desc.cc:415] Flush  batch_norm2d_4.w_2 1
I1018 08:53:57.702708 4097533 var_desc.cc:415] Flush  reshape2_18.tmp_0 1
I1018 08:53:57.702709 4097533 var_desc.cc:415] Flush  tmp_36 1
I1018 08:53:57.702713 4097533 var_desc.cc:415] Flush  reshape2_18.tmp_1 1
I1018 08:53:57.702715 4097533 var_desc.cc:415] Flush  elementwise_pow_18.tmp_0 1
I1018 08:53:57.702718 4097533 var_desc.cc:415] Flush  fill_constant_33.tmp_0 1
I1018 08:53:57.702721 4097533 var_desc.cc:415] Flush  tmp_34 1
I1018 08:53:57.702723 4097533 var_desc.cc:415] Flush  tmp_35 1
I1018 08:53:57.702726 4097533 var_desc.cc:415] Flush  relu_2.tmp_0 1
I1018 08:53:57.702728 4097533 var_desc.cc:415] Flush  reshape2_16.tmp_1 1
I1018 08:53:57.702731 4097533 var_desc.cc:415] Flush  tmp_1 1
I1018 08:53:57.702733 4097533 var_desc.cc:415] Flush  conv2d_4.tmp_0 1
I1018 08:53:57.702737 4097533 var_desc.cc:415] Flush  tmp_32 1
I1018 08:53:57.702739 4097533 var_desc.cc:415] Flush  elementwise_pow_16.tmp_0 1
I1018 08:53:57.702742 4097533 var_desc.cc:415] Flush  reshape2_17.tmp_0 1
I1018 08:53:57.702744 4097533 var_desc.cc:415] Flush  batch_norm_4.tmp_2 1
I1018 08:53:57.702747 4097533 var_desc.cc:415] Flush  reshape2_16.tmp_0 1
I1018 08:53:57.702750 4097533 var_desc.cc:415] Flush  reshape2_19.tmp_0 1
I1018 08:53:57.702754 4097533 var_desc.cc:415] Flush  reshape2_19.tmp_1 1
I1018 08:53:57.702757 4097533 var_desc.cc:415] Flush  fill_constant_37.tmp_0 1
I1018 08:53:57.702759 4097533 var_desc.cc:415] Flush  reshape2_17.tmp_1 1
I1018 08:53:57.702762 4097533 var_desc.cc:415] Flush  tmp_33 1
I1018 08:53:57.702765 4097533 var_desc.cc:415] Flush  fill_constant_39.tmp_0 1
I1018 08:53:57.702785 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d0a002c0 -> elementwise_mul0x55b7d0a43bd0  via tmp_330x55b7d0972e20
I1018 08:53:57.702792 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d09b0860 -> elementwise_mul0x55b7d0a43bd0  via elementwise_pow_18.tmp_00x55b7d0a48160
I1018 08:53:57.702798 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0f39320 -> elementwise_max0x55b7d0a42020  via tmp_10x55b7d0a6f1e0
I1018 08:53:57.702803 4097533 graph_helper.h:104] adj fill_constant0x55b7d0a4b7d0 -> elementwise_max0x55b7d0a42020  via fill_constant_39.tmp_00x55b7d0a6f3f0
I1018 08:53:57.702809 4097533 graph_helper.h:104] adj reshape20x55b7d0a46ed0 -> elementwise_mul0x55b7d0a3a1c0  via reshape2_18.tmp_00x55b7d0a5cac0
I1018 08:53:57.702814 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0a43bd0 -> elementwise_mul0x55b7d0a3a1c0  via tmp_350x55b7d0a47160
I1018 08:53:57.702821 4097533 graph_helper.h:104] adj scale0x55b7d0a133a0 -> elementwise_pow0x55b7d0a14d70  via tmp_320x55b7d0a6f9d0
I1018 08:53:57.702826 4097533 graph_helper.h:104] adj fill_constant0x55b7d0a3ac50 -> elementwise_pow0x55b7d0a14d70  via fill_constant_33.tmp_00x55b7d0a47a60
I1018 08:53:57.702831 4097533 graph_helper.h:104] adj fill_constant0x55b7d09f9320 -> scale0x55b7d0a133a0  via fill_constant_37.tmp_00x55b7d0973440
I1018 08:53:57.702837 4097533 graph_helper.h:104] adj scale0x55b7d0a3d240 -> elementwise_pow0x55b7d09b0860  via tmp_340x55b7d0a47420
I1018 08:53:57.702843 4097533 graph_helper.h:104] adj fill_constant0x55b7d0a3ac50 -> elementwise_pow0x55b7d09b0860  via fill_constant_33.tmp_00x55b7d0a47a60
I1018 08:53:57.702848 4097533 graph_helper.h:104] adj reshape20x55b7d0a36f90 -> elementwise_sub0x55b7d0a002c0  via reshape2_16.tmp_00x55b7d0a5c400
I1018 08:53:57.702854 4097533 graph_helper.h:104] adj feed0x55b7d0f90bf0 -> elementwise_sub0x55b7d0a002c0  via conv2d_4.tmp_00x55b7d0a6dd40
I1018 08:53:57.702859 4097533 graph_helper.h:104] adj feed0x55b7d0f913d0 -> reshape20x55b7d0a46ed0  via batch_norm2d_4.w_00x55b7d0a6e050
I1018 08:53:57.702867 4097533 graph_helper.h:104] adj feed0x55b7d0a54130 -> reshape20x55b7d0a36f90  via batch_norm2d_4.w_10x55b7d0a6e3c0
I1018 08:53:57.702873 4097533 graph_helper.h:104] adj reshape20x55b7d0f56440 -> scale0x55b7d0a3d240  via reshape2_17.tmp_00x55b7d0a476e0
I1018 08:53:57.702879 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0a3a1c0 -> elementwise_add0x55b7d0a30960  via tmp_360x55b7d0a5c780
I1018 08:53:57.702884 4097533 graph_helper.h:104] adj reshape20x55b7d0a63b90 -> elementwise_add0x55b7d0a30960  via reshape2_19.tmp_00x55b7d0973040
I1018 08:53:57.702890 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0a14d70 -> assign0x55b7d0a40f30  via elementwise_pow_16.tmp_00x55b7d0a47dd0
I1018 08:53:57.702895 4097533 graph_helper.h:104] adj feed0x55b7d0f90f80 -> reshape20x55b7d0f56440  via batch_norm2d_4.w_20x55b7d0a5ce40
I1018 08:53:57.702901 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0a30960 -> elementwise_add0x55b7d0f39320  via batch_norm_4.tmp_20x55b7d0972f00
I1018 08:53:57.702908 4097533 graph_helper.h:104] adj feed0x55b7d0f90880 -> elementwise_add0x55b7d0f39320  via relu_2.tmp_00x55b7d0a5d1b0
I1018 08:53:57.702912 4097533 graph_helper.h:104] adj elementwise_max0x55b7d0a42020 -> fetch0x55b7d0a54830  via relu_4.tmp_00x55b7d0a6edd0
I1018 08:53:57.702917 4097533 graph_helper.h:104] adj assign0x55b7d0a40f30 -> fetch0x55b7d0a54480  via assign_17.tmp_00x55b7d0a6eaa0
I1018 08:53:57.702921 4097533 graph_helper.h:104] adj feed0x55b7d0f91310 -> reshape20x55b7d0a63b90  via batch_norm2d_4.b_00x55b7d0a6e730
I1018 08:53:57.703068 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.703069 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.703078 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.703080 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.703088 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.703091 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.703102 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.703105 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.703111 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.703114 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.703132 4097533 graph_helper.cc:698] convert op node to desc elementwise_sub
I1018 08:53:57.703135 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.703142 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.703145 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.703147 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.703150 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.703157 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.703161 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.703194 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.703197 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.703204 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.703207 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.703215 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.703218 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.703224 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.703227 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.703234 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.703238 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.703240 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.703243 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.703250 4097533 graph_helper.cc:698] convert op node to desc elementwise_add
I1018 08:53:57.703253 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.703259 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.703262 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.703334 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.703336 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.703339 4097533 graph_helper.cc:698] convert op node to desc elementwise_add
I1018 08:53:57.703342 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.703348 4097533 graph_helper.cc:698] convert op node to desc elementwise_max
I1018 08:53:57.703351 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.703358 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.703361 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.703368 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.703370 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.703377 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.703380 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.703387 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.703390 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.703397 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.703399 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.703402 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.703405 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.704066 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.704540 4097533 block_desc.cc:205] vars in desc 28
I1018 08:53:57.704545 4097533 block_desc.cc:209] Flush relu_4.tmp_0
I1018 08:53:57.704547 4097533 var_desc.cc:415] Flush  relu_4.tmp_0 1
I1018 08:53:57.704550 4097533 block_desc.cc:209] Flush assign_17.tmp_0
I1018 08:53:57.704553 4097533 var_desc.cc:415] Flush  assign_17.tmp_0 1
I1018 08:53:57.704555 4097533 block_desc.cc:209] Flush batch_norm2d_4.b_0
I1018 08:53:57.704557 4097533 var_desc.cc:415] Flush  batch_norm2d_4.b_0 1
I1018 08:53:57.704564 4097533 block_desc.cc:209] Flush batch_norm2d_4.w_1
I1018 08:53:57.704566 4097533 var_desc.cc:415] Flush  batch_norm2d_4.w_1 1
I1018 08:53:57.704569 4097533 block_desc.cc:209] Flush batch_norm2d_4.w_0
I1018 08:53:57.704571 4097533 var_desc.cc:415] Flush  batch_norm2d_4.w_0 1
I1018 08:53:57.704573 4097533 block_desc.cc:209] Flush batch_norm2d_4.w_2
I1018 08:53:57.704576 4097533 var_desc.cc:415] Flush  batch_norm2d_4.w_2 1
I1018 08:53:57.704578 4097533 block_desc.cc:209] Flush reshape2_18.tmp_0
I1018 08:53:57.704581 4097533 var_desc.cc:415] Flush  reshape2_18.tmp_0 1
I1018 08:53:57.704582 4097533 block_desc.cc:209] Flush tmp_36
I1018 08:53:57.704586 4097533 var_desc.cc:415] Flush  tmp_36 1
I1018 08:53:57.704587 4097533 block_desc.cc:209] Flush reshape2_18.tmp_1
I1018 08:53:57.704589 4097533 var_desc.cc:415] Flush  reshape2_18.tmp_1 1
I1018 08:53:57.704592 4097533 block_desc.cc:209] Flush elementwise_pow_18.tmp_0
I1018 08:53:57.704594 4097533 var_desc.cc:415] Flush  elementwise_pow_18.tmp_0 1
I1018 08:53:57.704596 4097533 block_desc.cc:209] Flush fill_constant_33.tmp_0
I1018 08:53:57.704599 4097533 var_desc.cc:415] Flush  fill_constant_33.tmp_0 1
I1018 08:53:57.704602 4097533 block_desc.cc:209] Flush tmp_34
I1018 08:53:57.704603 4097533 var_desc.cc:415] Flush  tmp_34 1
I1018 08:53:57.704607 4097533 block_desc.cc:209] Flush tmp_35
I1018 08:53:57.704608 4097533 var_desc.cc:415] Flush  tmp_35 1
I1018 08:53:57.704610 4097533 block_desc.cc:209] Flush relu_2.tmp_0
I1018 08:53:57.704613 4097533 var_desc.cc:415] Flush  relu_2.tmp_0 1
I1018 08:53:57.704615 4097533 block_desc.cc:209] Flush reshape2_16.tmp_1
I1018 08:53:57.704617 4097533 var_desc.cc:415] Flush  reshape2_16.tmp_1 1
I1018 08:53:57.704620 4097533 block_desc.cc:209] Flush tmp_1
I1018 08:53:57.704622 4097533 var_desc.cc:415] Flush  tmp_1 1
I1018 08:53:57.704624 4097533 block_desc.cc:209] Flush conv2d_4.tmp_0
I1018 08:53:57.704627 4097533 var_desc.cc:415] Flush  conv2d_4.tmp_0 1
I1018 08:53:57.704629 4097533 block_desc.cc:209] Flush tmp_32
I1018 08:53:57.704631 4097533 var_desc.cc:415] Flush  tmp_32 1
I1018 08:53:57.704634 4097533 block_desc.cc:209] Flush elementwise_pow_16.tmp_0
I1018 08:53:57.704636 4097533 var_desc.cc:415] Flush  elementwise_pow_16.tmp_0 1
I1018 08:53:57.704638 4097533 block_desc.cc:209] Flush reshape2_17.tmp_0
I1018 08:53:57.704641 4097533 var_desc.cc:415] Flush  reshape2_17.tmp_0 1
I1018 08:53:57.704643 4097533 block_desc.cc:209] Flush batch_norm_4.tmp_2
I1018 08:53:57.704645 4097533 var_desc.cc:415] Flush  batch_norm_4.tmp_2 1
I1018 08:53:57.704648 4097533 block_desc.cc:209] Flush reshape2_16.tmp_0
I1018 08:53:57.704650 4097533 var_desc.cc:415] Flush  reshape2_16.tmp_0 1
I1018 08:53:57.704653 4097533 block_desc.cc:209] Flush reshape2_19.tmp_0
I1018 08:53:57.704654 4097533 var_desc.cc:415] Flush  reshape2_19.tmp_0 1
I1018 08:53:57.704658 4097533 block_desc.cc:209] Flush reshape2_19.tmp_1
I1018 08:53:57.704659 4097533 var_desc.cc:415] Flush  reshape2_19.tmp_1 1
I1018 08:53:57.704661 4097533 block_desc.cc:209] Flush fill_constant_37.tmp_0
I1018 08:53:57.704663 4097533 var_desc.cc:415] Flush  fill_constant_37.tmp_0 1
I1018 08:53:57.704666 4097533 block_desc.cc:209] Flush reshape2_17.tmp_1
I1018 08:53:57.704668 4097533 var_desc.cc:415] Flush  reshape2_17.tmp_1 1
I1018 08:53:57.704670 4097533 block_desc.cc:209] Flush tmp_33
I1018 08:53:57.704674 4097533 var_desc.cc:415] Flush  tmp_33 1
I1018 08:53:57.704675 4097533 block_desc.cc:209] Flush fill_constant_39.tmp_0
I1018 08:53:57.704677 4097533 var_desc.cc:415] Flush  fill_constant_39.tmp_0 1
I1018 08:53:57.702684 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "relu_4.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 56
          dims: 56
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "assign_17.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "batch_norm2d_4.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_4.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "batch_norm2d_4.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_4.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "reshape2_18.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "tmp_36"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 56
          dims: 56
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "reshape2_18.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "elementwise_pow_18.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_33.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_34"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  vars {
    name: "tmp_35"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 56
          dims: 56
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "relu_2.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 56
          dims: 56
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_16.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 56
          dims: 56
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "conv2d_4.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 56
          dims: 56
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "tmp_32"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  vars {
    name: "elementwise_pow_16.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_17.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "batch_norm_4.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 56
          dims: 56
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_16.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_19.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_19.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "fill_constant_37.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_17.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_33"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 56
          dims: 56
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "fill_constant_39.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_4.w_1"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_4.w_1"
    }
    outputs {
      parameter: "Out"
      arguments: "reshape2_16.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_16.tmp_1"
    }
    type: "reshape2"
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 123, in composite_batchnorm"
      strings: "    x_hat = (x - reshape(run_mean, stats_shape)) * pow("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/manipulation.py\", line 4518, in reshape"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 1
      ints: 64
      ints: 1
      ints: 1
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_4.w_0"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_4.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "reshape2_18.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_18.tmp_1"
    }
    type: "reshape2"
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 129, in composite_batchnorm"
      strings: "    y = reshape(scale, stats_shape) * x_hat + reshape(bias, stats_shape)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/manipulation.py\", line 4518, in reshape"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 1
      ints: 64
      ints: 1
      ints: 1
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "conv2d_4.tmp_0"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "conv2d_4.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "reshape2_16.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_33"
    }
    type: "elementwise_sub"
    attrs {
      name: "Scale_out"
      type: FLOAT
      f: 1
    }
    attrs {
      name: "Scale_x"
      type: FLOAT
      f: 1
    }
    attrs {
      name: "Scale_y"
      type: FLOAT
      f: 1
    }
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "axis"
      type: INT
      i: -1
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 123, in composite_batchnorm"
      strings: "    x_hat = (x - reshape(run_mean, stats_shape)) * pow("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 657, in __impl__"
      strings: "    current_block(self).append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packa
I1018 08:53:57.705937 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.706104 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_41.tmp_0, is_only_used_internal: 1
I1018 08:53:57.706108 4097533 build_cinn_pass.cc:557] var_node->outputs[1]: fill_constant_41.tmp_0, is_only_used_internal: 1
I1018 08:53:57.706110 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_41.tmp_0
I1018 08:53:57.706113 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: elementwise_pow_20.tmp_0, is_only_used_internal: 1
I1018 08:53:57.706115 4097533 build_cinn_pass.cc:562] insert internal var: elementwise_pow_20.tmp_0
I1018 08:53:57.706118 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_38, is_only_used_internal: 1
I1018 08:53:57.706120 4097533 build_cinn_pass.cc:562] insert internal var: tmp_38
I1018 08:53:57.706122 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_45.tmp_0, is_only_used_internal: 1
I1018 08:53:57.706125 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_45.tmp_0
I1018 08:53:57.706126 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_40, is_only_used_internal: 1
I1018 08:53:57.706128 4097533 build_cinn_pass.cc:562] insert internal var: tmp_40
I1018 08:53:57.706131 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_41, is_only_used_internal: 1
I1018 08:53:57.706133 4097533 build_cinn_pass.cc:562] insert internal var: tmp_41
I1018 08:53:57.706135 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_21.tmp_0, is_only_used_internal: 1
I1018 08:53:57.706137 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_21.tmp_0
I1018 08:53:57.706139 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_20.tmp_0, is_only_used_internal: 1
I1018 08:53:57.706141 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_20.tmp_0
I1018 08:53:57.706144 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_23.tmp_1
I1018 08:53:57.706146 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_21.tmp_1
I1018 08:53:57.706148 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_47.tmp_0, is_only_used_internal: 1
I1018 08:53:57.706151 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_47.tmp_0
I1018 08:53:57.706153 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_20.tmp_1
I1018 08:53:57.706156 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_39, is_only_used_internal: 1
I1018 08:53:57.706157 4097533 build_cinn_pass.cc:562] insert internal var: tmp_39
I1018 08:53:57.706159 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: elementwise_pow_22.tmp_0, is_only_used_internal: 1
I1018 08:53:57.706161 4097533 build_cinn_pass.cc:562] insert internal var: elementwise_pow_22.tmp_0
I1018 08:53:57.706164 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_22.tmp_1
I1018 08:53:57.706166 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_22.tmp_0, is_only_used_internal: 1
I1018 08:53:57.706168 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_22.tmp_0
I1018 08:53:57.706171 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_42, is_only_used_internal: 1
I1018 08:53:57.706172 4097533 build_cinn_pass.cc:562] insert internal var: tmp_42
I1018 08:53:57.706180 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: batch_norm_5.tmp_2, is_only_used_internal: 1
I1018 08:53:57.706182 4097533 build_cinn_pass.cc:562] insert internal var: batch_norm_5.tmp_2
I1018 08:53:57.706184 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_23.tmp_0, is_only_used_internal: 1
I1018 08:53:57.706187 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_23.tmp_0
I1018 08:53:57.706189 4097533 build_cinn_pass.cc:742] Cluster Ops: (scale, reshape2, elementwise_sub, elementwise_pow, elementwise_mul, reshape2, elementwise_mul, fill_constant, reshape2, elementwise_max, elementwise_add, fill_constant, reshape2, scale, elementwise_pow, assign, fill_constant, )
I1018 08:53:57.706192 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_6.w_2, conv2d_5.tmp_0, batch_norm2d_6.w_0, batch_norm2d_6.w_1, batch_norm2d_6.b_0, )
I1018 08:53:57.706195 4097533 build_cinn_pass.cc:744] Cluster output vars: (assign_21.tmp_0, relu_5.tmp_0, )
I1018 08:53:57.706197 4097533 build_cinn_pass.cc:745] Cluster internal vars: (reshape2_23.tmp_0, batch_norm_5.tmp_2, tmp_42, reshape2_22.tmp_0, reshape2_22.tmp_1, elementwise_pow_22.tmp_0, fill_constant_41.tmp_0, elementwise_pow_20.tmp_0, tmp_38, tmp_40, tmp_41, fill_constant_45.tmp_0, tmp_39, reshape2_21.tmp_0, reshape2_20.tmp_0, reshape2_23.tmp_1, reshape2_21.tmp_1, fill_constant_47.tmp_0, reshape2_20.tmp_1, )
I1018 08:53:57.706207 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.706413 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: conv2d_5.tmp_0
I1018 08:53:57.706418 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_6.w_2
I1018 08:53:57.706421 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_6.b_0
I1018 08:53:57.706424 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_6.w_0
I1018 08:53:57.706427 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_6.w_1
I1018 08:53:57.706431 4097533 build_cinn_pass.cc:274] Add Output Var Node: assign_21.tmp_0
I1018 08:53:57.706435 4097533 build_cinn_pass.cc:274] Add Output Var Node: relu_5.tmp_0
I1018 08:53:57.706455 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_48[label="tmp_38
[128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_47[label="elementwise_pow_20.tmp_0
[128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_46[label="elementwise_pow_22.tmp_0
[1,128,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_45[label="tmp_41
[1,128,28,28]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_44[label="reshape2_22.tmp_0
[1,128,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_43[label="batch_norm_5.tmp_2
[1,128,28,28]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_42[label="reshape2_23.tmp_0
[1,128,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_39[label="tmp_40
[1,128,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_37[label="reshape2_22.tmp_1
[0,128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_35[label="tmp_42
[1,128,28,28]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_32[label="batch_norm2d_6.w_2
[128]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_31[label="scale" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_30[label="batch_norm2d_6.b_0
[128]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_3[label="relu_5.tmp_0
[1,128,28,28]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_21[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_38[label="scale" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_8[label="conv2d_5.tmp_0
[1,128,28,28]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_49[label="fill_constant_45.tmp_0
[128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_7[label="batch_norm2d_6.w_0
[128]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_6[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_33[label="elementwise_pow" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_26[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_11[label="reshape2_21.tmp_1
[0,128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_16[label="elementwise_add" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_12[label="reshape2_23.tmp_1
[0,128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_5[label="batch_norm2d_6.w_1
[128]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_4[label="assign_21.tmp_0
[128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_24[label="elementwise_mul" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_27[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_9[label="reshape2_20.tmp_1
[0,128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_36[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_17[label="fill_constant_41.tmp_0
[1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_10[label="fill_constant_47.tmp_0
[1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_19[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_41[label="elementwise_sub" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_34[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_13[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_23[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_14[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_2[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_15[label="reshape2_21.tmp_0
[1,128,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_18[label="reshape2_20.tmp_0
[1,128,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_20[label="elementwise_max" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_40[label="tmp_39
[1,128,28,28]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_22[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_25[label="elementwise_mul" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_29[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_28[label="elementwise_pow" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0->node_5
   node_1->node_32
   node_2->node_8
   node_3->node_34
   node_4->node_27
   node_5->node_29
   node_6->node_7
   node_7->node_26
   node_8->node_41
   node_10->node_20
   node_13->node_15
   node_13->node_11
   node_14->node_49
   node_15->node_31
   node_16->node_43
   node_17->node_28
   node_17->node_33
   node_18->node_41
   node_19->node_42
   node_19->node_12
   node_20->node_3
   node_21->node_10
   node_22->node_30
   node_23->node_4
   node_24->node_35
   node_25->node_45
   node_26->node_44
   node_26->node_37
   node_28->node_46
   node_29->node_18
   node_29->node_9
   node_30->node_19
   node_31->node_39
   node_32->node_13
   node_33->node_47
   node_35->node_16
   node_36->node_17
   node_38->node_48
   node_39->node_28
   node_40->node_25
   node_41->node_40
   node_42->node_16
   node_43->node_20
   node_44->node_24
   node_45->node_24
   node_46->node_25
   node_47->node_23
   node_48->node_33
   node_49->node_38
} // end G
I1018 08:53:57.706718 4097533 var_desc.cc:415] Flush  relu_5.tmp_0 1
I1018 08:53:57.706722 4097533 var_desc.cc:415] Flush  assign_21.tmp_0 1
I1018 08:53:57.706724 4097533 var_desc.cc:415] Flush  batch_norm2d_6.w_1 1
I1018 08:53:57.706727 4097533 var_desc.cc:415] Flush  batch_norm2d_6.w_0 1
I1018 08:53:57.706730 4097533 var_desc.cc:415] Flush  conv2d_5.tmp_0 1
I1018 08:53:57.706732 4097533 var_desc.cc:415] Flush  reshape2_20.tmp_1 1
I1018 08:53:57.706735 4097533 var_desc.cc:415] Flush  fill_constant_47.tmp_0 1
I1018 08:53:57.706737 4097533 var_desc.cc:415] Flush  reshape2_21.tmp_1 1
I1018 08:53:57.706740 4097533 var_desc.cc:415] Flush  reshape2_23.tmp_1 1
I1018 08:53:57.706743 4097533 var_desc.cc:415] Flush  reshape2_21.tmp_0 1
I1018 08:53:57.706746 4097533 var_desc.cc:415] Flush  fill_constant_41.tmp_0 1
I1018 08:53:57.706748 4097533 var_desc.cc:415] Flush  reshape2_20.tmp_0 1
I1018 08:53:57.706751 4097533 var_desc.cc:415] Flush  batch_norm2d_6.b_0 1
I1018 08:53:57.706753 4097533 var_desc.cc:415] Flush  batch_norm2d_6.w_2 1
I1018 08:53:57.706756 4097533 var_desc.cc:415] Flush  tmp_42 1
I1018 08:53:57.706759 4097533 var_desc.cc:415] Flush  reshape2_22.tmp_1 1
I1018 08:53:57.706761 4097533 var_desc.cc:415] Flush  tmp_40 1
I1018 08:53:57.706764 4097533 var_desc.cc:415] Flush  tmp_39 1
I1018 08:53:57.706768 4097533 var_desc.cc:415] Flush  reshape2_23.tmp_0 1
I1018 08:53:57.706769 4097533 var_desc.cc:415] Flush  batch_norm_5.tmp_2 1
I1018 08:53:57.706772 4097533 var_desc.cc:415] Flush  reshape2_22.tmp_0 1
I1018 08:53:57.706774 4097533 var_desc.cc:415] Flush  tmp_41 1
I1018 08:53:57.706777 4097533 var_desc.cc:415] Flush  elementwise_pow_22.tmp_0 1
I1018 08:53:57.706780 4097533 var_desc.cc:415] Flush  elementwise_pow_20.tmp_0 1
I1018 08:53:57.706784 4097533 var_desc.cc:415] Flush  tmp_38 1
I1018 08:53:57.706785 4097533 var_desc.cc:415] Flush  fill_constant_45.tmp_0 1
I1018 08:53:57.706804 4097533 graph_helper.h:104] adj feed0x55b7d0fba8e0 -> reshape20x55b7d0f23640  via batch_norm2d_6.w_20x55b7d0a188a0
I1018 08:53:57.706815 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0a0c6f0 -> elementwise_add0x55b7d0f2fe80  via tmp_420x55b7d0a7dba0
I1018 08:53:57.706821 4097533 graph_helper.h:104] adj reshape20x55b7d0a86c20 -> elementwise_add0x55b7d0f2fe80  via reshape2_23.tmp_00x55b7d0a7d940
I1018 08:53:57.706826 4097533 graph_helper.h:104] adj feed0x55b7d0fbac70 -> reshape20x55b7d0a86c20  via batch_norm2d_6.b_00x55b7d0f367a0
I1018 08:53:57.706833 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0f2fe80 -> elementwise_max0x55b7d0a198b0  via batch_norm_5.tmp_20x55b7d0a7da60
I1018 08:53:57.706838 4097533 graph_helper.h:104] adj fill_constant0x55b7d0a559e0 -> elementwise_max0x55b7d0a198b0  via fill_constant_47.tmp_00x55b7d0a181c0
I1018 08:53:57.706844 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d09b5e60 -> assign0x55b7d0a7fc10  via elementwise_pow_20.tmp_00x55b7d0ab7530
I1018 08:53:57.706849 4097533 graph_helper.h:104] adj reshape20x55b7d0fbcbe0 -> elementwise_mul0x55b7d0a0c6f0  via reshape2_22.tmp_00x55b7d0a7dca0
I1018 08:53:57.706856 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0a754c0 -> elementwise_mul0x55b7d0a0c6f0  via tmp_410x55b7d0ab7d00
I1018 08:53:57.706862 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d0a8ba30 -> elementwise_mul0x55b7d0a754c0  via tmp_390x55b7d0a8bde0
I1018 08:53:57.706868 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0ab1440 -> elementwise_mul0x55b7d0a754c0  via elementwise_pow_22.tmp_00x55b7d0a7df70
I1018 08:53:57.706873 4097533 graph_helper.h:104] adj feed0x55b7d0fbb000 -> reshape20x55b7d0fbcbe0  via batch_norm2d_6.w_00x55b7d0a18f50
I1018 08:53:57.706880 4097533 graph_helper.h:104] adj assign0x55b7d0a7fc10 -> fetch0x55b7d0fbb6e0  via assign_21.tmp_00x55b7d0f36b10
I1018 08:53:57.706884 4097533 graph_helper.h:104] adj scale0x55b7d0a7ed20 -> elementwise_pow0x55b7d0ab1440  via tmp_400x55b7d0ab7a40
I1018 08:53:57.706890 4097533 graph_helper.h:104] adj fill_constant0x55b7d0ab4210 -> elementwise_pow0x55b7d0ab1440  via fill_constant_41.tmp_00x55b7d0ab7330
I1018 08:53:57.706895 4097533 graph_helper.h:104] adj feed0x55b7d0fbb390 -> reshape20x55b7d0aa9330  via batch_norm2d_6.w_10x55b7d0a192c0
I1018 08:53:57.706902 4097533 graph_helper.h:104] adj reshape20x55b7d0f23640 -> scale0x55b7d0a7ed20  via reshape2_21.tmp_00x55b7d0a8c120
I1018 08:53:57.706909 4097533 graph_helper.h:104] adj scale0x55b7d0a92c40 -> elementwise_pow0x55b7d09b5e60  via tmp_380x55b7d0ab7790
I1018 08:53:57.706914 4097533 graph_helper.h:104] adj fill_constant0x55b7d0ab4210 -> elementwise_pow0x55b7d09b5e60  via fill_constant_41.tmp_00x55b7d0ab7330
I1018 08:53:57.706920 4097533 graph_helper.h:104] adj elementwise_max0x55b7d0a198b0 -> fetch0x55b7d0aa0c20  via relu_5.tmp_00x55b7d0f36e40
I1018 08:53:57.706924 4097533 graph_helper.h:104] adj fill_constant0x55b7d0aaad50 -> scale0x55b7d0a92c40  via fill_constant_45.tmp_00x55b7d0a8baf0
I1018 08:53:57.706930 4097533 graph_helper.h:104] adj reshape20x55b7d0aa9330 -> elementwise_sub0x55b7d0a8ba30  via reshape2_20.tmp_00x55b7d0a8c4a0
I1018 08:53:57.706936 4097533 graph_helper.h:104] adj feed0x55b7d0f37a50 -> elementwise_sub0x55b7d0a8ba30  via conv2d_5.tmp_00x55b7d0a18c10
I1018 08:53:57.707070 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.707073 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.707078 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.707082 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.707089 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.707093 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.707106 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.707109 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.707113 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.707115 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.707136 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.707139 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.707141 4097533 graph_helper.cc:698] convert op node to desc elementwise_sub
I1018 08:53:57.707144 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.707151 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.707154 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.707161 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.707165 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.707203 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.707206 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.707213 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.707216 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.707219 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.707221 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.707229 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.707232 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.707239 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.707242 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.707252 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.707255 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.707258 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.707262 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.707268 4097533 graph_helper.cc:698] convert op node to desc elementwise_add
I1018 08:53:57.707270 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.707343 4097533 graph_helper.cc:698] convert op node to desc elementwise_max
I1018 08:53:57.707346 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.707353 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.707356 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.707365 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.707366 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.707374 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.707376 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.707383 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.707386 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.707392 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.707396 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.707398 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.707401 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.708036 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.708492 4097533 block_desc.cc:205] vars in desc 26
I1018 08:53:57.708496 4097533 block_desc.cc:209] Flush relu_5.tmp_0
I1018 08:53:57.708499 4097533 var_desc.cc:415] Flush  relu_5.tmp_0 1
I1018 08:53:57.708503 4097533 block_desc.cc:209] Flush assign_21.tmp_0
I1018 08:53:57.708504 4097533 var_desc.cc:415] Flush  assign_21.tmp_0 1
I1018 08:53:57.708508 4097533 block_desc.cc:209] Flush batch_norm2d_6.w_1
I1018 08:53:57.708509 4097533 var_desc.cc:415] Flush  batch_norm2d_6.w_1 1
I1018 08:53:57.708513 4097533 block_desc.cc:209] Flush batch_norm2d_6.w_0
I1018 08:53:57.708514 4097533 var_desc.cc:415] Flush  batch_norm2d_6.w_0 1
I1018 08:53:57.708518 4097533 block_desc.cc:209] Flush conv2d_5.tmp_0
I1018 08:53:57.708519 4097533 var_desc.cc:415] Flush  conv2d_5.tmp_0 1
I1018 08:53:57.708521 4097533 block_desc.cc:209] Flush reshape2_20.tmp_1
I1018 08:53:57.708524 4097533 var_desc.cc:415] Flush  reshape2_20.tmp_1 1
I1018 08:53:57.708526 4097533 block_desc.cc:209] Flush fill_constant_47.tmp_0
I1018 08:53:57.708528 4097533 var_desc.cc:415] Flush  fill_constant_47.tmp_0 1
I1018 08:53:57.708531 4097533 block_desc.cc:209] Flush reshape2_21.tmp_1
I1018 08:53:57.708534 4097533 var_desc.cc:415] Flush  reshape2_21.tmp_1 1
I1018 08:53:57.708535 4097533 block_desc.cc:209] Flush reshape2_23.tmp_1
I1018 08:53:57.708539 4097533 var_desc.cc:415] Flush  reshape2_23.tmp_1 1
I1018 08:53:57.708540 4097533 block_desc.cc:209] Flush reshape2_21.tmp_0
I1018 08:53:57.708542 4097533 var_desc.cc:415] Flush  reshape2_21.tmp_0 1
I1018 08:53:57.708545 4097533 block_desc.cc:209] Flush fill_constant_41.tmp_0
I1018 08:53:57.708547 4097533 var_desc.cc:415] Flush  fill_constant_41.tmp_0 1
I1018 08:53:57.708549 4097533 block_desc.cc:209] Flush reshape2_20.tmp_0
I1018 08:53:57.708551 4097533 var_desc.cc:415] Flush  reshape2_20.tmp_0 1
I1018 08:53:57.708554 4097533 block_desc.cc:209] Flush batch_norm2d_6.b_0
I1018 08:53:57.708556 4097533 var_desc.cc:415] Flush  batch_norm2d_6.b_0 1
I1018 08:53:57.708559 4097533 block_desc.cc:209] Flush batch_norm2d_6.w_2
I1018 08:53:57.708561 4097533 var_desc.cc:415] Flush  batch_norm2d_6.w_2 1
I1018 08:53:57.708563 4097533 block_desc.cc:209] Flush tmp_42
I1018 08:53:57.708565 4097533 var_desc.cc:415] Flush  tmp_42 1
I1018 08:53:57.708568 4097533 block_desc.cc:209] Flush reshape2_22.tmp_1
I1018 08:53:57.708570 4097533 var_desc.cc:415] Flush  reshape2_22.tmp_1 1
I1018 08:53:57.708573 4097533 block_desc.cc:209] Flush tmp_40
I1018 08:53:57.708575 4097533 var_desc.cc:415] Flush  tmp_40 1
I1018 08:53:57.708577 4097533 block_desc.cc:209] Flush tmp_39
I1018 08:53:57.708580 4097533 var_desc.cc:415] Flush  tmp_39 1
I1018 08:53:57.708585 4097533 block_desc.cc:209] Flush reshape2_23.tmp_0
I1018 08:53:57.708588 4097533 var_desc.cc:415] Flush  reshape2_23.tmp_0 1
I1018 08:53:57.708590 4097533 block_desc.cc:209] Flush batch_norm_5.tmp_2
I1018 08:53:57.708592 4097533 var_desc.cc:415] Flush  batch_norm_5.tmp_2 1
I1018 08:53:57.708595 4097533 block_desc.cc:209] Flush reshape2_22.tmp_0
I1018 08:53:57.708597 4097533 var_desc.cc:415] Flush  reshape2_22.tmp_0 1
I1018 08:53:57.708599 4097533 block_desc.cc:209] Flush tmp_41
I1018 08:53:57.708601 4097533 var_desc.cc:415] Flush  tmp_41 1
I1018 08:53:57.708604 4097533 block_desc.cc:209] Flush elementwise_pow_22.tmp_0
I1018 08:53:57.708606 4097533 var_desc.cc:415] Flush  elementwise_pow_22.tmp_0 1
I1018 08:53:57.708608 4097533 block_desc.cc:209] Flush elementwise_pow_20.tmp_0
I1018 08:53:57.708611 4097533 var_desc.cc:415] Flush  elementwise_pow_20.tmp_0 1
I1018 08:53:57.708613 4097533 block_desc.cc:209] Flush tmp_38
I1018 08:53:57.708616 4097533 var_desc.cc:415] Flush  tmp_38 1
I1018 08:53:57.708618 4097533 block_desc.cc:209] Flush fill_constant_45.tmp_0
I1018 08:53:57.708621 4097533 var_desc.cc:415] Flush  fill_constant_45.tmp_0 1
I1018 08:53:57.706712 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "relu_5.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 28
          dims: 28
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "assign_21.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "batch_norm2d_6.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "batch_norm2d_6.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "conv2d_5.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 28
          dims: 28
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_20.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_47.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_21.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_23.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_21.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_41.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_20.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "batch_norm2d_6.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_6.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "tmp_42"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 28
          dims: 28
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "reshape2_22.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "tmp_40"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  vars {
    name: "tmp_39"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 28
          dims: 28
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "reshape2_23.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "batch_norm_5.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 28
          dims: 28
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_22.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "tmp_41"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 28
          dims: 28
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "elementwise_pow_22.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "elementwise_pow_20.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_38"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  vars {
    name: "fill_constant_45.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_6.w_2"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_6.w_2"
    }
    outputs {
      parameter: "Out"
      arguments: "reshape2_21.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_21.tmp_1"
    }
    type: "reshape2"
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 124, in composite_batchnorm"
      strings: "    (reshape(run_var, stats_shape) + epsilon), half"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/manipulation.py\", line 4518, in reshape"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 1
      ints: 128
      ints: 1
      ints: 1
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "BiasTensor"
    }
    inputs {
      parameter: "ScaleTensor"
    }
    inputs {
      parameter: "X"
      arguments: "reshape2_21.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_40"
    }
    type: "scale"
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "bias"
      type: FLOAT
      f: 1e-05
    }
    attrs {
      name: "bias_after_scale"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 124, in composite_batchnorm"
      strings: "    (reshape(run_var, stats_shape) + epsilon), half"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 557, in __impl__"
      strings: "    return scalar_method(self, other_var)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 530, in _scalar_add_"
      strings: "    return _scalar_op_(var, 1.0, value)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 451, in _scalar_op_"
      strings: "    block.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "scale"
      type: FLOAT
      f: 1
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_6.w_1"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_6.w_1"
    }
    outputs {
      parameter: "Out"
      arguments: "reshape2_20.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_20.tmp_1"
    }
    type: "reshape2"
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 123, in composite_batchnorm"
      strings: "    x_hat = (x - reshape(run_mean, stats_shape)) * pow("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/manipulation.py\", line 4518, in reshape"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 1
      ints: 128
      ints: 1
      ints: 1
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    outputs {
I1018 08:53:57.709801 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.709969 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_61.tmp_0, is_only_used_internal: 1
I1018 08:53:57.709972 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_61.tmp_0
I1018 08:53:57.709975 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_53.tmp_0, is_only_used_internal: 1
I1018 08:53:57.709977 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_53.tmp_0
I1018 08:53:57.709980 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_51, is_only_used_internal: 1
I1018 08:53:57.709982 4097533 build_cinn_pass.cc:562] insert internal var: tmp_51
I1018 08:53:57.709985 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_2, is_only_used_internal: 1
I1018 08:53:57.709986 4097533 build_cinn_pass.cc:562] insert internal var: tmp_2
I1018 08:53:57.709988 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_44, is_only_used_internal: 1
I1018 08:53:57.709990 4097533 build_cinn_pass.cc:562] insert internal var: tmp_44
I1018 08:53:57.709993 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_31.tmp_1
I1018 08:53:57.709995 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_31.tmp_0, is_only_used_internal: 1
I1018 08:53:57.709997 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_31.tmp_0
I1018 08:53:57.710000 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_54, is_only_used_internal: 1
I1018 08:53:57.710001 4097533 build_cinn_pass.cc:562] insert internal var: tmp_54
I1018 08:53:57.710006 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_30.tmp_0, is_only_used_internal: 1
I1018 08:53:57.710008 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_30.tmp_0
I1018 08:53:57.710011 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: elementwise_pow_30.tmp_0, is_only_used_internal: 1
I1018 08:53:57.710013 4097533 build_cinn_pass.cc:562] insert internal var: elementwise_pow_30.tmp_0
I1018 08:53:57.710016 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_46, is_only_used_internal: 1
I1018 08:53:57.710017 4097533 build_cinn_pass.cc:562] insert internal var: tmp_46
I1018 08:53:57.710019 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_55.tmp_0, is_only_used_internal: 1
I1018 08:53:57.710021 4097533 build_cinn_pass.cc:557] var_node->outputs[1]: fill_constant_55.tmp_0, is_only_used_internal: 1
I1018 08:53:57.710024 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_55.tmp_0
I1018 08:53:57.710026 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: batch_norm_7.tmp_2, is_only_used_internal: 1
I1018 08:53:57.710028 4097533 build_cinn_pass.cc:562] insert internal var: batch_norm_7.tmp_2
I1018 08:53:57.710031 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_28.tmp_1
I1018 08:53:57.710033 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_59.tmp_0, is_only_used_internal: 1
I1018 08:53:57.710036 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_59.tmp_0
I1018 08:53:57.710037 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_45, is_only_used_internal: 1
I1018 08:53:57.710039 4097533 build_cinn_pass.cc:562] insert internal var: tmp_45
I1018 08:53:57.710041 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_28.tmp_0, is_only_used_internal: 1
I1018 08:53:57.710043 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_28.tmp_0
I1018 08:53:57.710047 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_25.tmp_1
I1018 08:53:57.710048 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_25.tmp_0, is_only_used_internal: 1
I1018 08:53:57.710050 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_25.tmp_0
I1018 08:53:57.710052 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: elementwise_pow_24.tmp_0, is_only_used_internal: 1
I1018 08:53:57.710054 4097533 build_cinn_pass.cc:562] insert internal var: elementwise_pow_24.tmp_0
I1018 08:53:57.710057 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_50, is_only_used_internal: 1
I1018 08:53:57.710059 4097533 build_cinn_pass.cc:562] insert internal var: tmp_50
I1018 08:53:57.710062 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_24.tmp_0, is_only_used_internal: 1
I1018 08:53:57.710063 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_24.tmp_0
I1018 08:53:57.710065 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_30.tmp_1
I1018 08:53:57.710067 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_26.tmp_0, is_only_used_internal: 1
I1018 08:53:57.710069 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_26.tmp_0
I1018 08:53:57.710072 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_52, is_only_used_internal: 1
I1018 08:53:57.710074 4097533 build_cinn_pass.cc:562] insert internal var: tmp_52
I1018 08:53:57.710076 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_48, is_only_used_internal: 1
I1018 08:53:57.710078 4097533 build_cinn_pass.cc:562] insert internal var: tmp_48
I1018 08:53:57.710080 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_53, is_only_used_internal: 1
I1018 08:53:57.710083 4097533 build_cinn_pass.cc:562] insert internal var: tmp_53
I1018 08:53:57.710084 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_26.tmp_1
I1018 08:53:57.710086 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_47, is_only_used_internal: 1
I1018 08:53:57.710089 4097533 build_cinn_pass.cc:562] insert internal var: tmp_47
I1018 08:53:57.710091 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_29.tmp_0, is_only_used_internal: 1
I1018 08:53:57.710094 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_29.tmp_0
I1018 08:53:57.710100 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: elementwise_pow_26.tmp_0, is_only_used_internal: 1
I1018 08:53:57.710103 4097533 build_cinn_pass.cc:562] insert internal var: elementwise_pow_26.tmp_0
I1018 08:53:57.710105 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_29.tmp_1
I1018 08:53:57.710107 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: elementwise_pow_28.tmp_0, is_only_used_internal: 1
I1018 08:53:57.710109 4097533 build_cinn_pass.cc:562] insert internal var: elementwise_pow_28.tmp_0
I1018 08:53:57.710111 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_27.tmp_0, is_only_used_internal: 1
I1018 08:53:57.710114 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_27.tmp_0
I1018 08:53:57.710116 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_49.tmp_0, is_only_used_internal: 1
I1018 08:53:57.710119 4097533 build_cinn_pass.cc:557] var_node->outputs[1]: fill_constant_49.tmp_0, is_only_used_internal: 1
I1018 08:53:57.710120 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_49.tmp_0
I1018 08:53:57.710122 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_24.tmp_1
I1018 08:53:57.710124 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: batch_norm_6.tmp_2, is_only_used_internal: 1
I1018 08:53:57.710126 4097533 build_cinn_pass.cc:562] insert internal var: batch_norm_6.tmp_2
I1018 08:53:57.710129 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_27.tmp_1
I1018 08:53:57.710134 4097533 build_cinn_pass.cc:742] Cluster Ops: (fill_constant, scale, reshape2, scale, reshape2, elementwise_pow, elementwise_mul, elementwise_mul, scale, reshape2, reshape2, reshape2, elementwise_pow, reshape2, elementwise_mul, reshape2, elementwise_mul, elementwise_pow, reshape2, elementwise_sub, elementwise_add, elementwise_add, elementwise_sub, elementwise_add, elementwise_max, fill_constant, assign, fill_constant, scale, elementwise_pow, assign, fill_constant, fill_constant, )
I1018 08:53:57.710137 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_5.b_0, batch_norm2d_5.w_1, batch_norm2d_7.w_1, conv2d_7.tmp_0, batch_norm2d_7.w_2, batch_norm2d_5.w_0, batch_norm2d_7.w_0, batch_norm2d_5.w_2, batch_norm2d_7.b_0, conv2d_6.tmp_0, )
I1018 08:53:57.710139 4097533 build_cinn_pass.cc:744] Cluster output vars: (assign_29.tmp_0, relu_6.tmp_0, assign_25.tmp_0, )
I1018 08:53:57.710142 4097533 build_cinn_pass.cc:745] Cluster internal vars: (reshape2_27.tmp_1, batch_norm_6.tmp_2, reshape2_24.tmp_1, fill_constant_49.tmp_0, reshape2_27.tmp_0, elementwise_pow_28.tmp_0, reshape2_29.tmp_1, elementwise_pow_26.tmp_0, reshape2_29.tmp_0, tmp_54, reshape2_30.tmp_0, reshape2_31.tmp_0, tmp_46, elementwise_pow_30.tmp_0, reshape2_31.tmp_1, tmp_44, tmp_51, fill_constant_53.tmp_0, fill_constant_61.tmp_0, tmp_2, fill_constant_55.tmp_0, batch_norm_7.tmp_2, reshape2_28.tmp_1, fill_constant_59.tmp_0, tmp_45, reshape2_28.tmp_0, reshape2_25.tmp_1, reshape2_25.tmp_0, elementwise_pow_24.tmp_0, tmp_50, reshape2_24.tmp_0, reshape2_30.tmp_1, reshape2_26.tmp_0, tmp_48, tmp_52, tmp_53, reshape2_26.tmp_1, tmp_47, )
I1018 08:53:57.710151 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.710506 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: conv2d_7.tmp_0
I1018 08:53:57.710511 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: conv2d_6.tmp_0
I1018 08:53:57.710515 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_7.b_0
I1018 08:53:57.710518 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_5.w_2
I1018 08:53:57.710522 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_7.w_0
I1018 08:53:57.710525 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_5.w_0
I1018 08:53:57.710528 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_7.w_1
I1018 08:53:57.710536 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_5.b_0
I1018 08:53:57.710538 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_5.w_1
I1018 08:53:57.710542 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_7.w_2
I1018 08:53:57.710546 4097533 build_cinn_pass.cc:274] Add Output Var Node: assign_25.tmp_0
I1018 08:53:57.710549 4097533 build_cinn_pass.cc:274] Add Output Var Node: assign_29.tmp_0
I1018 08:53:57.710552 4097533 build_cinn_pass.cc:274] Add Output Var Node: relu_6.tmp_0
I1018 08:53:57.710582 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_96[label="reshape2_28.tmp_0
[1,128,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_95[label="tmp_45
[1,128,28,28]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_94[label="reshape2_31.tmp_1
[0,128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_92[label="fill_constant_59.tmp_0
[128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_93[label="elementwise_pow" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_91[label="tmp_51
[1,128,28,28]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_83[label="elementwise_pow_28.tmp_0
[128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_89[label="reshape2_31.tmp_0
[1,128,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_80[label="fill_constant_49.tmp_0
[1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_79[label="reshape2_26.tmp_1
[0,128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_78[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_77[label="reshape2_24.tmp_1
[0,128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_84[label="tmp_44
[128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_76[label="elementwise_pow" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_75[label="reshape2_25.tmp_1
[0,128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_73[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_72[label="fill_constant_61.tmp_0
[1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_71[label="elementwise_mul" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_70[label="reshape2_27.tmp_1
[0,128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_66[label="conv2d_7.tmp_0
[1,128,28,28]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_63[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_62[label="elementwise_pow" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_60[label="scale" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_59[label="elementwise_mul" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_82[label="reshape2_26.tmp_0
[1,128,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_29[label="elementwise_add" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_85[label="reshape2_29.tmp_1
[0,128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_25[label="elementwise_max" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_22[label="reshape2_24.tmp_0
[1,128,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_40[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_20[label="tmp_48
[1,128,28,28]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_18[label="tmp_53
[1,128,28,28]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_13[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_34[label="tmp_54
[1,128,28,28]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_41[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_87[label="reshape2_29.tmp_0
[1,128,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_1[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_19[label="tmp_52
[1,128,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_10[label="batch_norm2d_7.b_0
[128]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_36[label="elementwise_add" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_9[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_4[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_5[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_16[label="batch_norm2d_5.b_0
[128]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_90[label="batch_norm_6.tmp_2
[1,128,28,28]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_69[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_64[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_58[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_11[label="batch_norm2d_5.w_2
[128]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_68[label="fill_constant_55.tmp_0
[1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_28[label="elementwise_sub" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_26[label="fill_constant_53.tmp_0
[128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_23[label="tmp_50
[128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_33[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_6[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_7[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_81[label="reshape2_27.tmp_0
[1,128,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_49[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_8[label="relu_6.tmp_0
[1,128,28,28]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_38[label="reshape2_25.tmp_0
[1,128,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_21[label="reshape2_30.tmp_1
[0,128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_3[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_88[label="batch_norm_7.tmp_2
[1,128,28,28]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_30[label="conv2d_6.tmp_0
[1,128,28,28]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_2[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_31[label="elementwise_mul" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_17[label="tmp_47
[1,128,28,28]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_32[label="batch_norm2d_7.w_1
[128]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_86[label="batch_norm2d_7.w_0
[128]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_35[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_53[label="assign_29.tmp_0
[128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_24[label="elementwise_pow_24.tmp_0
[128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_37[label="assign_25.tmp_0
[128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_39[label="elementwise_pow_26.tmp_0
[1,128,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_42[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_43[label="elementwise_mul" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_15[label="batch_norm2d_5.w_1
[128]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_44[label="scale" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_65[label="elementwise_pow_30.tmp_0
[1,128,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_45[label="elementwise_sub" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_46[label="tmp_46
[1,128,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_74[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_27[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_47[label="scale" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_48[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_50[label="reshape2_28.tmp_1
[0,128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_67[label="reshape2_30.tmp_0
[1,128,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_12[label="batch_norm2d_5.w_0
[128]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_51[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_52[label="elementwise_add" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_61[label="tmp_2
[1,128,28,28]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_14[label="batch_norm2d_7.w_2
[128]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_54[label="scale" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_55[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_56[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_57[label="elementwise_pow" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3->node_16
   node_4->node_12
   node_5->node_11
   node_6->node_30
   node_7->node_66
   node_8->node_0
   node_9->node_14
   node_10->node_42
   node_11->node_33
   node_12->node_58
   node_13->node_86
   node_14->node_64
   node_15->node_48
   node_16->node_55
   node_17->node_31
   node_18->node_71
   node_19->node_57
   node_20->node_29
   node_22->node_45
   node_23->node_93
   node_24->node_69
   node_25->node_8
   node_26->node_60
   node_27->node_92
   node_28->node_91
   node_29->node_90
   node_30->node_45
   node_31->node_20
   node_32->node_56
   node_33->node_87
   node_33->node_85
   node_34->node_52
   node_35->node_72
   node_36->node_61
   node_37->node_2
   node_38->node_54
   node_39->node_59
   node_40->node_82
   node_40->node_79
   node_41->node_53
   node_42->node_81
   node_42->node_70
   node_43->node_18
   node_44->node_23
   node_45->node_95
   node_46->node_76
   node_47->node_19
   node_48->node_96
   node_48->node_50
   node_49->node_26
   node_51->node_32
   node_52->node_88
   node_53->node_1
   node_54->node_46
   node_55->node_89
   node_55->node_94
   node_56->node_22
   node_56->node_77
   node_57->node_65
   node_58->node_67
   node_58->node_21
   node_59->node_17
   node_60->node_84
   node_61->node_25
   node_62->node_24
   node_63->node_10
   node_64->node_38
   node_64->node_75
   node_65->node_43
   node_66->node_28
   node_67->node_71
   node_68->node_57
   node_68->node_93
   node_69->node_37
   node_71->node_34
   node_72->node_25
   node_73->node_80
   node_74->node_68
   node_76->node_39
   node_78->node_15
   node_80->node_76
   node_80->node_62
   node_81->node_29
   node_82->node_31
   node_83->node_41
   node_84->node_62
   node_86->node_40
   node_87->node_47
   node_88->node_36
   node_89->node_52
   node_90->node_36
   node_91->node_43
   node_92->node_44
   node_93->node_83
   node_95->node_59
   node_96->node_28
} // end G
I1018 08:53:57.711066 4097533 var_desc.cc:415] Flush  relu_6.tmp_0 1
I1018 08:53:57.711068 4097533 var_desc.cc:415] Flush  batch_norm2d_7.b_0 1
I1018 08:53:57.711071 4097533 var_desc.cc:415] Flush  batch_norm2d_5.w_2 1
I1018 08:53:57.711074 4097533 var_desc.cc:415] Flush  batch_norm2d_5.w_0 1
I1018 08:53:57.711077 4097533 var_desc.cc:415] Flush  batch_norm2d_7.w_2 1
I1018 08:53:57.711081 4097533 var_desc.cc:415] Flush  batch_norm2d_5.w_1 1
I1018 08:53:57.711082 4097533 var_desc.cc:415] Flush  batch_norm2d_5.b_0 1
I1018 08:53:57.711086 4097533 var_desc.cc:415] Flush  tmp_47 1
I1018 08:53:57.711088 4097533 var_desc.cc:415] Flush  tmp_53 1
I1018 08:53:57.711091 4097533 var_desc.cc:415] Flush  tmp_52 1
I1018 08:53:57.711093 4097533 var_desc.cc:415] Flush  tmp_48 1
I1018 08:53:57.711097 4097533 var_desc.cc:415] Flush  reshape2_30.tmp_1 1
I1018 08:53:57.711099 4097533 var_desc.cc:415] Flush  reshape2_24.tmp_0 1
I1018 08:53:57.711102 4097533 var_desc.cc:415] Flush  tmp_50 1
I1018 08:53:57.711104 4097533 var_desc.cc:415] Flush  elementwise_pow_24.tmp_0 1
I1018 08:53:57.711107 4097533 var_desc.cc:415] Flush  fill_constant_53.tmp_0 1
I1018 08:53:57.711109 4097533 var_desc.cc:415] Flush  conv2d_6.tmp_0 1
I1018 08:53:57.711112 4097533 var_desc.cc:415] Flush  batch_norm2d_7.w_1 1
I1018 08:53:57.711115 4097533 var_desc.cc:415] Flush  tmp_54 1
I1018 08:53:57.711118 4097533 var_desc.cc:415] Flush  assign_25.tmp_0 1
I1018 08:53:57.711120 4097533 var_desc.cc:415] Flush  reshape2_25.tmp_0 1
I1018 08:53:57.711123 4097533 var_desc.cc:415] Flush  elementwise_pow_26.tmp_0 1
I1018 08:53:57.711126 4097533 var_desc.cc:415] Flush  tmp_46 1
I1018 08:53:57.711128 4097533 var_desc.cc:415] Flush  reshape2_28.tmp_1 1
I1018 08:53:57.711131 4097533 var_desc.cc:415] Flush  assign_29.tmp_0 1
I1018 08:53:57.711134 4097533 var_desc.cc:415] Flush  tmp_2 1
I1018 08:53:57.711136 4097533 var_desc.cc:415] Flush  elementwise_pow_30.tmp_0 1
I1018 08:53:57.711139 4097533 var_desc.cc:415] Flush  conv2d_7.tmp_0 1
I1018 08:53:57.711141 4097533 var_desc.cc:415] Flush  reshape2_30.tmp_0 1
I1018 08:53:57.711144 4097533 var_desc.cc:415] Flush  fill_constant_55.tmp_0 1
I1018 08:53:57.711148 4097533 var_desc.cc:415] Flush  reshape2_27.tmp_1 1
I1018 08:53:57.711149 4097533 var_desc.cc:415] Flush  fill_constant_61.tmp_0 1
I1018 08:53:57.711153 4097533 var_desc.cc:415] Flush  reshape2_25.tmp_1 1
I1018 08:53:57.711158 4097533 var_desc.cc:415] Flush  reshape2_24.tmp_1 1
I1018 08:53:57.711161 4097533 var_desc.cc:415] Flush  reshape2_26.tmp_1 1
I1018 08:53:57.711163 4097533 var_desc.cc:415] Flush  fill_constant_49.tmp_0 1
I1018 08:53:57.711166 4097533 var_desc.cc:415] Flush  reshape2_27.tmp_0 1
I1018 08:53:57.711169 4097533 var_desc.cc:415] Flush  reshape2_26.tmp_0 1
I1018 08:53:57.711171 4097533 var_desc.cc:415] Flush  elementwise_pow_28.tmp_0 1
I1018 08:53:57.711174 4097533 var_desc.cc:415] Flush  tmp_44 1
I1018 08:53:57.711176 4097533 var_desc.cc:415] Flush  reshape2_29.tmp_1 1
I1018 08:53:57.711179 4097533 var_desc.cc:415] Flush  batch_norm2d_7.w_0 1
I1018 08:53:57.711181 4097533 var_desc.cc:415] Flush  reshape2_29.tmp_0 1
I1018 08:53:57.711184 4097533 var_desc.cc:415] Flush  batch_norm_7.tmp_2 1
I1018 08:53:57.711186 4097533 var_desc.cc:415] Flush  reshape2_31.tmp_0 1
I1018 08:53:57.711189 4097533 var_desc.cc:415] Flush  batch_norm_6.tmp_2 1
I1018 08:53:57.711191 4097533 var_desc.cc:415] Flush  tmp_51 1
I1018 08:53:57.711194 4097533 var_desc.cc:415] Flush  fill_constant_59.tmp_0 1
I1018 08:53:57.711196 4097533 var_desc.cc:415] Flush  reshape2_31.tmp_1 1
I1018 08:53:57.711198 4097533 var_desc.cc:415] Flush  tmp_45 1
I1018 08:53:57.711201 4097533 var_desc.cc:415] Flush  reshape2_28.tmp_0 1
I1018 08:53:57.711227 4097533 graph_helper.h:104] adj elementwise_max0x55b7d0f39210 -> fetch0x55b7d0a33170  via relu_6.tmp_00x55b7d0a77070
I1018 08:53:57.711234 4097533 graph_helper.h:104] adj assign0x55b7d0fc0370 -> fetch0x55b7d0a32da0  via assign_29.tmp_00x55b7d0a76d40
I1018 08:53:57.711238 4097533 graph_helper.h:104] adj assign0x55b7d0ff01b0 -> fetch0x55b7d0a329f0  via assign_25.tmp_00x55b7d0a773b0
I1018 08:53:57.711244 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0afa360 -> elementwise_max0x55b7d0f39210  via tmp_20x55b7d09eb2d0
I1018 08:53:57.711251 4097533 graph_helper.h:104] adj fill_constant0x55b7d0f7f440 -> elementwise_max0x55b7d0f39210  via fill_constant_61.tmp_00x55b7d09eaf60
I1018 08:53:57.711256 4097533 graph_helper.h:104] adj reshape20x55b7d0ac9860 -> elementwise_sub0x55b7d09ff1b0  via reshape2_28.tmp_00x55b7d0a9f7a0
I1018 08:53:57.711262 4097533 graph_helper.h:104] adj feed0x55b7d09cc700 -> elementwise_sub0x55b7d09ff1b0  via conv2d_7.tmp_00x55b7d0f498f0
I1018 08:53:57.711267 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0a995a0 -> elementwise_add0x55b7d0adf8f0  via tmp_480x55b7d0aec110
I1018 08:53:57.711273 4097533 graph_helper.h:104] adj reshape20x55b7d0acf2d0 -> elementwise_add0x55b7d0adf8f0  via reshape2_27.tmp_00x55b7d09c6150
I1018 08:53:57.711278 4097533 graph_helper.h:104] adj reshape20x55b7d0a5b2a0 -> elementwise_mul0x55b7d0a995a0  via reshape2_26.tmp_00x55b7d0aebd90
I1018 08:53:57.711284 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0f08330 -> elementwise_mul0x55b7d0a995a0  via tmp_470x55b7d0aece40
I1018 08:53:57.711289 4097533 graph_helper.h:104] adj feed0x55b7d09cd190 -> reshape20x55b7d0adac70  via batch_norm2d_5.w_20x55b7d0f4a680
I1018 08:53:57.711297 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0adf8f0 -> elementwise_add0x55b7d0afa360  via batch_norm_6.tmp_20x55b7d09c5d50
I1018 08:53:57.711302 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0fedeb0 -> elementwise_add0x55b7d0afa360  via batch_norm_7.tmp_20x55b7d09eb980
I1018 08:53:57.711308 4097533 graph_helper.h:104] adj feed0x55b7d09cd520 -> reshape20x55b7d0a5b2a0  via batch_norm2d_7.w_00x55b7d0f4a310
I1018 08:53:57.711314 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0ac6f60 -> assign0x55b7d0fc0370  via elementwise_pow_28.tmp_00x55b7d09c6290
I1018 08:53:57.711320 4097533 graph_helper.h:104] adj feed0x55b7d09cce00 -> reshape20x55b7d0acf2d0  via batch_norm2d_7.b_00x55b7d0f4a9f0
I1018 08:53:57.711326 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d09ff1b0 -> elementwise_mul0x55b7d0abcf80  via tmp_510x55b7d09ea8b0
I1018 08:53:57.711333 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0af01b0 -> elementwise_mul0x55b7d0abcf80  via elementwise_pow_30.tmp_00x55b7d0ae7f90
I1018 08:53:57.711340 4097533 graph_helper.h:104] adj fill_constant0x55b7d0aea230 -> scale0x55b7d0ac35b0  via fill_constant_59.tmp_00x55b7d0a9f0f0
I1018 08:53:57.711346 4097533 graph_helper.h:104] adj reshape20x55b7d0f31ac0 -> elementwise_sub0x55b7d0afc9b0  via reshape2_24.tmp_00x55b7d0aa08d0
I1018 08:53:57.711352 4097533 graph_helper.h:104] adj feed0x55b7d09cca70 -> elementwise_sub0x55b7d0afc9b0  via conv2d_6.tmp_00x55b7d0a76a00
I1018 08:53:57.711357 4097533 graph_helper.h:104] adj reshape20x55b7d0adac70 -> scale0x55b7d0ac67e0  via reshape2_29.tmp_00x55b7d0ae7090
I1018 08:53:57.711364 4097533 graph_helper.h:104] adj feed0x55b7d0a32250 -> reshape20x55b7d0ac9860  via batch_norm2d_5.w_10x55b7d0f49210
I1018 08:53:57.711371 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0ac9b50 -> elementwise_add0x55b7d0fedeb0  via tmp_540x55b7d0ae7390
I1018 08:53:57.711376 4097533 graph_helper.h:104] adj reshape20x55b7d0ac5170 -> elementwise_add0x55b7d0fedeb0  via reshape2_31.tmp_00x55b7d0ae7950
I1018 08:53:57.711382 4097533 graph_helper.h:104] adj reshape20x55b7d0ad34f0 -> scale0x55b7d0f812b0  via reshape2_25.tmp_00x55b7d0a9fe90
I1018 08:53:57.711388 4097533 graph_helper.h:104] adj feed0x55b7d0a31e60 -> reshape20x55b7d0ac5170  via batch_norm2d_5.b_00x55b7d0aed180
I1018 08:53:57.711395 4097533 graph_helper.h:104] adj feed0x55b7d09cdc40 -> reshape20x55b7d0f31ac0  via batch_norm2d_7.w_10x55b7d0f49580
I1018 08:53:57.711401 4097533 graph_helper.h:104] adj scale0x55b7d0ac67e0 -> elementwise_pow0x55b7d0af01b0  via tmp_520x55b7d0aec450
I1018 08:53:57.711407 4097533 graph_helper.h:104] adj fill_constant0x55b7d0a99e10 -> elementwise_pow0x55b7d0af01b0  via fill_constant_55.tmp_00x55b7d09eb610
I1018 08:53:57.711412 4097533 graph_helper.h:104] adj feed0x55b7d09cd8b0 -> reshape20x55b7d0a8de10  via batch_norm2d_5.w_00x55b7d0f49fa0
I1018 08:53:57.711419 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d0afc9b0 -> elementwise_mul0x55b7d0f08330  via tmp_450x55b7d0a9f460
I1018 08:53:57.711424 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0a0f550 -> elementwise_mul0x55b7d0f08330  via elementwise_pow_26.tmp_00x55b7d0ae6da0
I1018 08:53:57.711431 4097533 graph_helper.h:104] adj fill_constant0x55b7d0faa3c0 -> scale0x55b7d0ad4ab0  via fill_constant_53.tmp_00x55b7d09eabf0
I1018 08:53:57.711436 4097533 graph_helper.h:104] adj scale0x55b7d0ad4ab0 -> elementwise_pow0x55b7d09a5b90  via tmp_440x55b7d09ea580
I1018 08:53:57.711441 4097533 graph_helper.h:104] adj fill_constant0x55b7d0982140 -> elementwise_pow0x55b7d09a5b90  via fill_constant_49.tmp_00x55b7d09c5ff0
I1018 08:53:57.711447 4097533 graph_helper.h:104] adj feed0x55b7d0a32640 -> reshape20x55b7d0ad34f0  via batch_norm2d_7.w_20x55b7d0f49c30
I1018 08:53:57.711454 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d09a5b90 -> assign0x55b7d0ff01b0  via elementwise_pow_24.tmp_00x55b7d0aa0210
I1018 08:53:57.711459 4097533 graph_helper.h:104] adj reshape20x55b7d0a8de10 -> elementwise_mul0x55b7d0ac9b50  via reshape2_30.tmp_00x55b7d0ae7650
I1018 08:53:57.711464 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0abcf80 -> elementwise_mul0x55b7d0ac9b50  via tmp_530x55b7d0aec790
I1018 08:53:57.711470 4097533 graph_helper.h:104] adj scale0x55b7d0f812b0 -> elementwise_pow0x55b7d0a0f550  via tmp_460x55b7d0ae7c50
I1018 08:53:57.711475 4097533 graph_helper.h:104] adj fill_constant0x55b7d0982140 -> elementwise_pow0x55b7d0a0f550  via fill_constant_49.tmp_00x55b7d09c5ff0
I1018 08:53:57.711483 4097533 graph_helper.h:104] adj scale0x55b7d0ac35b0 -> elementwise_pow0x55b7d0ac6f60  via tmp_500x55b7d0aa05a0
I1018 08:53:57.711488 4097533 graph_helper.h:104] adj fill_constant0x55b7d0a99e10 -> elementwise_pow0x55b7d0ac6f60  via fill_constant_55.tmp_00x55b7d09eb610
I1018 08:53:57.711740 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.711742 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.711752 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.711755 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.711769 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.711774 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.711791 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.711794 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.711802 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.711804 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.711828 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.711830 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.711838 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.711840 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.711848 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.711850 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.711853 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.711856 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.711895 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.711898 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.711905 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.711908 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.711916 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.711918 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.711922 4097533 graph_helper.cc:698] convert op node to desc elementwise_sub
I1018 08:53:57.711925 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.711932 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.711935 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.711942 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.711944 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.711948 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.711951 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.711958 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.711961 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.712033 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.712035 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.712044 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.712045 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.712049 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.712051 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.712059 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.712060 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.712064 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.712067 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.712074 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.712076 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.712083 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.712086 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.712093 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.712095 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.712100 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.712101 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.712108 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.712110 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.712113 4097533 graph_helper.cc:698] convert op node to desc elementwise_sub
I1018 08:53:57.712116 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.712123 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.712126 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.712132 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.712136 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.712142 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.712145 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.712152 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.712154 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.712162 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.712164 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.712298 4097533 graph_helper.cc:698] convert op node to desc elementwise_add
I1018 08:53:57.712301 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.712307 4097533 graph_helper.cc:698] convert op node to desc elementwise_add
I1018 08:53:57.712311 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.712317 4097533 graph_helper.cc:698] convert op node to desc elementwise_add
I1018 08:53:57.712320 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.712327 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.712329 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.712337 4097533 graph_helper.cc:698] convert op node to desc elementwise_max
I1018 08:53:57.712339 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.712345 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.712348 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.712355 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.712357 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.712364 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.712366 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.712373 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.712376 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.712383 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.712385 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.712391 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.712394 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.712397 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.712400 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.712404 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.712406 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.713605 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.714638 4097533 block_desc.cc:205] vars in desc 51
I1018 08:53:57.714645 4097533 block_desc.cc:209] Flush relu_6.tmp_0
I1018 08:53:57.714648 4097533 var_desc.cc:415] Flush  relu_6.tmp_0 1
I1018 08:53:57.714651 4097533 block_desc.cc:209] Flush batch_norm2d_7.b_0
I1018 08:53:57.714653 4097533 var_desc.cc:415] Flush  batch_norm2d_7.b_0 1
I1018 08:53:57.714656 4097533 block_desc.cc:209] Flush batch_norm2d_5.w_2
I1018 08:53:57.714658 4097533 var_desc.cc:415] Flush  batch_norm2d_5.w_2 1
I1018 08:53:57.714661 4097533 block_desc.cc:209] Flush batch_norm2d_5.w_0
I1018 08:53:57.714663 4097533 var_desc.cc:415] Flush  batch_norm2d_5.w_0 1
I1018 08:53:57.714665 4097533 block_desc.cc:209] Flush batch_norm2d_7.w_2
I1018 08:53:57.714668 4097533 var_desc.cc:415] Flush  batch_norm2d_7.w_2 1
I1018 08:53:57.714670 4097533 block_desc.cc:209] Flush batch_norm2d_5.w_1
I1018 08:53:57.714672 4097533 var_desc.cc:415] Flush  batch_norm2d_5.w_1 1
I1018 08:53:57.714674 4097533 block_desc.cc:209] Flush batch_norm2d_5.b_0
I1018 08:53:57.714677 4097533 var_desc.cc:415] Flush  batch_norm2d_5.b_0 1
I1018 08:53:57.714679 4097533 block_desc.cc:209] Flush tmp_47
I1018 08:53:57.714681 4097533 var_desc.cc:415] Flush  tmp_47 1
I1018 08:53:57.714684 4097533 block_desc.cc:209] Flush tmp_53
I1018 08:53:57.714686 4097533 var_desc.cc:415] Flush  tmp_53 1
I1018 08:53:57.714689 4097533 block_desc.cc:209] Flush tmp_52
I1018 08:53:57.714691 4097533 var_desc.cc:415] Flush  tmp_52 1
I1018 08:53:57.714694 4097533 block_desc.cc:209] Flush tmp_48
I1018 08:53:57.714695 4097533 var_desc.cc:415] Flush  tmp_48 1
I1018 08:53:57.714699 4097533 block_desc.cc:209] Flush reshape2_30.tmp_1
I1018 08:53:57.714700 4097533 var_desc.cc:415] Flush  reshape2_30.tmp_1 1
I1018 08:53:57.714702 4097533 block_desc.cc:209] Flush reshape2_24.tmp_0
I1018 08:53:57.714705 4097533 var_desc.cc:415] Flush  reshape2_24.tmp_0 1
I1018 08:53:57.714707 4097533 block_desc.cc:209] Flush tmp_50
I1018 08:53:57.714709 4097533 var_desc.cc:415] Flush  tmp_50 1
I1018 08:53:57.714715 4097533 block_desc.cc:209] Flush elementwise_pow_24.tmp_0
I1018 08:53:57.714718 4097533 var_desc.cc:415] Flush  elementwise_pow_24.tmp_0 1
I1018 08:53:57.714720 4097533 block_desc.cc:209] Flush fill_constant_53.tmp_0
I1018 08:53:57.714722 4097533 var_desc.cc:415] Flush  fill_constant_53.tmp_0 1
I1018 08:53:57.714725 4097533 block_desc.cc:209] Flush conv2d_6.tmp_0
I1018 08:53:57.714727 4097533 var_desc.cc:415] Flush  conv2d_6.tmp_0 1
I1018 08:53:57.714730 4097533 block_desc.cc:209] Flush batch_norm2d_7.w_1
I1018 08:53:57.714732 4097533 var_desc.cc:415] Flush  batch_norm2d_7.w_1 1
I1018 08:53:57.714735 4097533 block_desc.cc:209] Flush tmp_54
I1018 08:53:57.714736 4097533 var_desc.cc:415] Flush  tmp_54 1
I1018 08:53:57.714740 4097533 block_desc.cc:209] Flush assign_25.tmp_0
I1018 08:53:57.714741 4097533 var_desc.cc:415] Flush  assign_25.tmp_0 1
I1018 08:53:57.714744 4097533 block_desc.cc:209] Flush reshape2_25.tmp_0
I1018 08:53:57.714746 4097533 var_desc.cc:415] Flush  reshape2_25.tmp_0 1
I1018 08:53:57.714748 4097533 block_desc.cc:209] Flush elementwise_pow_26.tmp_0
I1018 08:53:57.714751 4097533 var_desc.cc:415] Flush  elementwise_pow_26.tmp_0 1
I1018 08:53:57.714753 4097533 block_desc.cc:209] Flush tmp_46
I1018 08:53:57.714756 4097533 var_desc.cc:415] Flush  tmp_46 1
I1018 08:53:57.714758 4097533 block_desc.cc:209] Flush reshape2_28.tmp_1
I1018 08:53:57.714761 4097533 var_desc.cc:415] Flush  reshape2_28.tmp_1 1
I1018 08:53:57.714762 4097533 block_desc.cc:209] Flush assign_29.tmp_0
I1018 08:53:57.714766 4097533 var_desc.cc:415] Flush  assign_29.tmp_0 1
I1018 08:53:57.714767 4097533 block_desc.cc:209] Flush tmp_2
I1018 08:53:57.714769 4097533 var_desc.cc:415] Flush  tmp_2 1
I1018 08:53:57.714772 4097533 block_desc.cc:209] Flush elementwise_pow_30.tmp_0
I1018 08:53:57.714774 4097533 var_desc.cc:415] Flush  elementwise_pow_30.tmp_0 1
I1018 08:53:57.714776 4097533 block_desc.cc:209] Flush conv2d_7.tmp_0
I1018 08:53:57.714779 4097533 var_desc.cc:415] Flush  conv2d_7.tmp_0 1
I1018 08:53:57.714781 4097533 block_desc.cc:209] Flush reshape2_30.tmp_0
I1018 08:53:57.714783 4097533 var_desc.cc:415] Flush  reshape2_30.tmp_0 1
I1018 08:53:57.714787 4097533 block_desc.cc:209] Flush fill_constant_55.tmp_0
I1018 08:53:57.714788 4097533 var_desc.cc:415] Flush  fill_constant_55.tmp_0 1
I1018 08:53:57.714792 4097533 block_desc.cc:209] Flush reshape2_27.tmp_1
I1018 08:53:57.714793 4097533 var_desc.cc:415] Flush  reshape2_27.tmp_1 1
I1018 08:53:57.714795 4097533 block_desc.cc:209] Flush fill_constant_61.tmp_0
I1018 08:53:57.714797 4097533 var_desc.cc:415] Flush  fill_constant_61.tmp_0 1
I1018 08:53:57.714800 4097533 block_desc.cc:209] Flush reshape2_25.tmp_1
I1018 08:53:57.714802 4097533 var_desc.cc:415] Flush  reshape2_25.tmp_1 1
I1018 08:53:57.714804 4097533 block_desc.cc:209] Flush reshape2_24.tmp_1
I1018 08:53:57.714807 4097533 var_desc.cc:415] Flush  reshape2_24.tmp_1 1
I1018 08:53:57.714809 4097533 block_desc.cc:209] Flush reshape2_26.tmp_1
I1018 08:53:57.714812 4097533 var_desc.cc:415] Flush  reshape2_26.tmp_1 1
I1018 08:53:57.714813 4097533 block_desc.cc:209] Flush fill_constant_49.tmp_0
I1018 08:53:57.714816 4097533 var_desc.cc:415] Flush  fill_constant_49.tmp_0 1
I1018 08:53:57.714818 4097533 block_desc.cc:209] Flush reshape2_27.tmp_0
I1018 08:53:57.714820 4097533 var_desc.cc:415] Flush  reshape2_27.tmp_0 1
I1018 08:53:57.714823 4097533 block_desc.cc:209] Flush reshape2_26.tmp_0
I1018 08:53:57.714825 4097533 var_desc.cc:415] Flush  reshape2_26.tmp_0 1
I1018 08:53:57.714828 4097533 block_desc.cc:209] Flush elementwise_pow_28.tmp_0
I1018 08:53:57.714829 4097533 var_desc.cc:415] Flush  elementwise_pow_28.tmp_0 1
I1018 08:53:57.714833 4097533 block_desc.cc:209] Flush tmp_44
I1018 08:53:57.714834 4097533 var_desc.cc:415] Flush  tmp_44 1
I1018 08:53:57.714836 4097533 block_desc.cc:209] Flush reshape2_29.tmp_1
I1018 08:53:57.714839 4097533 var_desc.cc:415] Flush  reshape2_29.tmp_1 1
I1018 08:53:57.714841 4097533 block_desc.cc:209] Flush batch_norm2d_7.w_0
I1018 08:53:57.714843 4097533 var_desc.cc:415] Flush  batch_norm2d_7.w_0 1
I1018 08:53:57.714848 4097533 block_desc.cc:209] Flush reshape2_29.tmp_0
I1018 08:53:57.714850 4097533 var_desc.cc:415] Flush  reshape2_29.tmp_0 1
I1018 08:53:57.714852 4097533 block_desc.cc:209] Flush batch_norm_7.tmp_2
I1018 08:53:57.714854 4097533 var_desc.cc:415] Flush  batch_norm_7.tmp_2 1
I1018 08:53:57.714857 4097533 block_desc.cc:209] Flush reshape2_31.tmp_0
I1018 08:53:57.714859 4097533 var_desc.cc:415] Flush  reshape2_31.tmp_0 1
I1018 08:53:57.714862 4097533 block_desc.cc:209] Flush batch_norm_6.tmp_2
I1018 08:53:57.714864 4097533 var_desc.cc:415] Flush  batch_norm_6.tmp_2 1
I1018 08:53:57.714866 4097533 block_desc.cc:209] Flush tmp_51
I1018 08:53:57.714869 4097533 var_desc.cc:415] Flush  tmp_51 1
I1018 08:53:57.714871 4097533 block_desc.cc:209] Flush fill_constant_59.tmp_0
I1018 08:53:57.714874 4097533 var_desc.cc:415] Flush  fill_constant_59.tmp_0 1
I1018 08:53:57.714875 4097533 block_desc.cc:209] Flush reshape2_31.tmp_1
I1018 08:53:57.714879 4097533 var_desc.cc:415] Flush  reshape2_31.tmp_1 1
I1018 08:53:57.714880 4097533 block_desc.cc:209] Flush tmp_45
I1018 08:53:57.714882 4097533 var_desc.cc:415] Flush  tmp_45 1
I1018 08:53:57.714885 4097533 block_desc.cc:209] Flush reshape2_28.tmp_0
I1018 08:53:57.714887 4097533 var_desc.cc:415] Flush  reshape2_28.tmp_0 1
I1018 08:53:57.711056 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "relu_6.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 28
          dims: 28
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_7.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_5.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "batch_norm2d_5.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_7.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "batch_norm2d_5.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "batch_norm2d_5.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "tmp_47"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 28
          dims: 28
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "tmp_53"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 28
          dims: 28
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "tmp_52"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  vars {
    name: "tmp_48"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 28
          dims: 28
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "reshape2_30.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_24.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_50"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  vars {
    name: "elementwise_pow_24.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_53.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "conv2d_6.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 28
          dims: 28
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_7.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "tmp_54"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 28
          dims: 28
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "assign_25.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_25.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "elementwise_pow_26.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_46"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  vars {
    name: "reshape2_28.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "assign_29.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 28
          dims: 28
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "elementwise_pow_30.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "conv2d_7.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 28
          dims: 28
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_30.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "fill_constant_55.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_27.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "fill_constant_61.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_25.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_24.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_26.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "fill_constant_49.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_27.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_26.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "elementwise_pow_28.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_44"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  vars {
    name: "reshape2_29.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "batch_norm2d_7.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "reshape2_29.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "batch_norm_7.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 28
          dims: 28
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_31.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "batch_norm_6.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 28
          dims: 28
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "tmp_51"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 28
          dims: 28
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "fill_constant_59.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_31.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "tmp_45"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 28
          dims: 28
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "reshape2_28.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  ops {
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "ShapeTensorList"
    }
    inputs {
      parameter: "ValueTensor"
    }
    outputs {
      parameter: "Out"
      arguments: "fill_constant_59.tmp_0"
    }
    type: "fill_constant"
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "dtype"
      type: INT
      i: 5
    }
    attrs {
      name: "force_cpu"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 118, in composite_batchnorm"
      strings: "    batch_var = zeros(run_var.shape, run_var.dtype)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 1117, in zeros"
      strings: "    return fill_constant(value=0.0, shape=shape, dtype=dtype, name=name)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 977, in fill_constant"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "place_type"
      type: INT
      i: -1
    }
    attrs {
      name: "shape"
      type: LONGS
      longs: 128
    }
    attrs {
      name: "str_value"
      type: STRING
      s: "0.0"
    }
    attrs {
      name: "value"
      type: FLOAT
      f: 0
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "BiasTensor"
    }
    inputs {
      parameter: "ScaleTensor"
    }
    inputs {
      parameter: "X"
      arguments: "fill_constant_59.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_50"
    }
    type: "scale"
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "bias"
      type: FLOAT
      f: 1e-05
    }
    attrs {
      name: "bias_after_scale"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 119, in composite_batchnorm"
      strings: "    inv_std = pow((batch_var + epsilon), half)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 557, in __impl__"
      strings: "    return scalar_method(self, other_var)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 530, in _scalar_add_"
      strings: "    return _scalar_op_(var, 1.0, value)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 451, in _scalar_op_"
      strings: "    block.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "scale"
      type: FLOAT
      f: 1
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_7.w_2"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_7.w_2"
    }
    outputs {
      parameter: "Out"
      arguments: "reshape2_25.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_25.tmp_1"
    }
    type: "reshape2"
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=Tru
I1018 08:53:57.717522 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.717741 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_63.tmp_0, is_only_used_internal: 1
I1018 08:53:57.717743 4097533 build_cinn_pass.cc:557] var_node->outputs[1]: fill_constant_63.tmp_0, is_only_used_internal: 1
I1018 08:53:57.717746 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_63.tmp_0
I1018 08:53:57.717749 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: elementwise_pow_32.tmp_0, is_only_used_internal: 1
I1018 08:53:57.717751 4097533 build_cinn_pass.cc:562] insert internal var: elementwise_pow_32.tmp_0
I1018 08:53:57.717753 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_56, is_only_used_internal: 1
I1018 08:53:57.717756 4097533 build_cinn_pass.cc:562] insert internal var: tmp_56
I1018 08:53:57.717758 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_59, is_only_used_internal: 1
I1018 08:53:57.717761 4097533 build_cinn_pass.cc:562] insert internal var: tmp_59
I1018 08:53:57.717763 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_35.tmp_1
I1018 08:53:57.717765 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_58, is_only_used_internal: 1
I1018 08:53:57.717767 4097533 build_cinn_pass.cc:562] insert internal var: tmp_58
I1018 08:53:57.717770 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_67.tmp_0, is_only_used_internal: 1
I1018 08:53:57.717772 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_67.tmp_0
I1018 08:53:57.717778 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_33.tmp_1
I1018 08:53:57.717780 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_69.tmp_0, is_only_used_internal: 1
I1018 08:53:57.717782 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_69.tmp_0
I1018 08:53:57.717785 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_32.tmp_0, is_only_used_internal: 1
I1018 08:53:57.717787 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_32.tmp_0
I1018 08:53:57.717789 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_32.tmp_1
I1018 08:53:57.717792 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_33.tmp_0, is_only_used_internal: 1
I1018 08:53:57.717793 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_33.tmp_0
I1018 08:53:57.717795 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_57, is_only_used_internal: 1
I1018 08:53:57.717797 4097533 build_cinn_pass.cc:562] insert internal var: tmp_57
I1018 08:53:57.717800 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: elementwise_pow_34.tmp_0, is_only_used_internal: 1
I1018 08:53:57.717803 4097533 build_cinn_pass.cc:562] insert internal var: elementwise_pow_34.tmp_0
I1018 08:53:57.717804 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_34.tmp_1
I1018 08:53:57.717808 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_34.tmp_0, is_only_used_internal: 1
I1018 08:53:57.717809 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_34.tmp_0
I1018 08:53:57.717811 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_60, is_only_used_internal: 1
I1018 08:53:57.717813 4097533 build_cinn_pass.cc:562] insert internal var: tmp_60
I1018 08:53:57.717815 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: batch_norm_8.tmp_2, is_only_used_internal: 1
I1018 08:53:57.717818 4097533 build_cinn_pass.cc:562] insert internal var: batch_norm_8.tmp_2
I1018 08:53:57.717819 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_35.tmp_0, is_only_used_internal: 1
I1018 08:53:57.717823 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_35.tmp_0
I1018 08:53:57.717825 4097533 build_cinn_pass.cc:742] Cluster Ops: (scale, reshape2, reshape2, elementwise_sub, elementwise_pow, reshape2, elementwise_mul, elementwise_add, fill_constant, reshape2, elementwise_max, fill_constant, elementwise_mul, scale, elementwise_pow, assign, fill_constant, )
I1018 08:53:57.717828 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_8.w_2, batch_norm2d_8.w_1, batch_norm2d_8.w_0, conv2d_8.tmp_0, batch_norm2d_8.b_0, )
I1018 08:53:57.717831 4097533 build_cinn_pass.cc:744] Cluster output vars: (assign_33.tmp_0, relu_7.tmp_0, )
I1018 08:53:57.717833 4097533 build_cinn_pass.cc:745] Cluster internal vars: (reshape2_35.tmp_0, batch_norm_8.tmp_2, tmp_60, reshape2_34.tmp_0, reshape2_34.tmp_1, elementwise_pow_34.tmp_0, fill_constant_63.tmp_0, reshape2_32.tmp_1, elementwise_pow_32.tmp_0, tmp_56, tmp_59, tmp_58, fill_constant_67.tmp_0, reshape2_33.tmp_1, fill_constant_69.tmp_0, reshape2_35.tmp_1, reshape2_32.tmp_0, reshape2_33.tmp_0, tmp_57, )
I1018 08:53:57.717842 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.718075 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: conv2d_8.tmp_0
I1018 08:53:57.718080 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_8.b_0
I1018 08:53:57.718084 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_8.w_0
I1018 08:53:57.718087 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_8.w_2
I1018 08:53:57.718091 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_8.w_1
I1018 08:53:57.718094 4097533 build_cinn_pass.cc:274] Add Output Var Node: assign_33.tmp_0
I1018 08:53:57.718098 4097533 build_cinn_pass.cc:274] Add Output Var Node: relu_7.tmp_0
I1018 08:53:57.718119 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_48[label="tmp_60
[1,128,28,28]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_47[label="tmp_59
[1,128,28,28]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_46[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_45[label="elementwise_pow" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_44[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_43[label="elementwise_pow_32.tmp_0
[128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_42[label="reshape2_32.tmp_1
[0,128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_39[label="elementwise_pow_34.tmp_0
[1,128,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_37[label="reshape2_34.tmp_1
[0,128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_35[label="reshape2_34.tmp_0
[1,128,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_32[label="elementwise_mul" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_31[label="reshape2_35.tmp_0
[1,128,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_30[label="batch_norm2d_8.w_1
[128]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_3[label="relu_7.tmp_0
[1,128,28,28]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_21[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_38[label="reshape2_35.tmp_1
[0,128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_8[label="reshape2_33.tmp_0
[1,128,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_49[label="tmp_58
[1,128,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_7[label="batch_norm2d_8.w_2
[128]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_6[label="conv2d_8.tmp_0
[1,128,28,28]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_33[label="elementwise_pow" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_26[label="batch_norm_8.tmp_2
[1,128,28,28]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_11[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_16[label="elementwise_add" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_12[label="tmp_56
[128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_5[label="batch_norm2d_8.b_0
[128]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_4[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_24[label="elementwise_sub" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_27[label="scale" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_9[label="fill_constant_69.tmp_0
[1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_36[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_17[label="elementwise_mul" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_10[label="fill_constant_67.tmp_0
[128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_19[label="reshape2_33.tmp_1
[0,128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_41[label="batch_norm2d_8.w_0
[128]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_34[label="tmp_57
[1,128,28,28]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_13[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_23[label="elementwise_max" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_14[label="reshape2_32.tmp_0
[1,128,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_2[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_15[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_18[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_20[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_40[label="fill_constant_63.tmp_0
[1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_22[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_25[label="scale" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_29[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_28[label="assign_33.tmp_0
[128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_2->node_6
   node_3->node_0
   node_4->node_30
   node_5->node_44
   node_6->node_24
   node_7->node_22
   node_8->node_25
   node_9->node_23
   node_10->node_27
   node_11->node_10
   node_12->node_33
   node_13->node_7
   node_14->node_24
   node_15->node_9
   node_16->node_26
   node_17->node_48
   node_18->node_5
   node_20->node_35
   node_20->node_37
   node_21->node_28
   node_22->node_8
   node_22->node_19
   node_23->node_3
   node_24->node_34
   node_25->node_49
   node_26->node_23
   node_27->node_12
   node_28->node_1
   node_29->node_40
   node_30->node_46
   node_31->node_16
   node_32->node_47
   node_33->node_43
   node_34->node_32
   node_35->node_17
   node_36->node_41
   node_39->node_32
   node_40->node_45
   node_40->node_33
   node_41->node_20
   node_43->node_21
   node_44->node_31
   node_44->node_38
   node_45->node_39
   node_46->node_14
   node_46->node_42
   node_47->node_17
   node_48->node_16
   node_49->node_45
} // end G
I1018 08:53:57.718380 4097533 var_desc.cc:415] Flush  relu_7.tmp_0 1
I1018 08:53:57.718384 4097533 var_desc.cc:415] Flush  batch_norm2d_8.b_0 1
I1018 08:53:57.718387 4097533 var_desc.cc:415] Flush  conv2d_8.tmp_0 1
I1018 08:53:57.718389 4097533 var_desc.cc:415] Flush  batch_norm2d_8.w_2 1
I1018 08:53:57.718392 4097533 var_desc.cc:415] Flush  reshape2_33.tmp_0 1
I1018 08:53:57.718395 4097533 var_desc.cc:415] Flush  fill_constant_69.tmp_0 1
I1018 08:53:57.718398 4097533 var_desc.cc:415] Flush  fill_constant_67.tmp_0 1
I1018 08:53:57.718400 4097533 var_desc.cc:415] Flush  tmp_56 1
I1018 08:53:57.718403 4097533 var_desc.cc:415] Flush  reshape2_32.tmp_0 1
I1018 08:53:57.718406 4097533 var_desc.cc:415] Flush  reshape2_33.tmp_1 1
I1018 08:53:57.718410 4097533 var_desc.cc:415] Flush  batch_norm_8.tmp_2 1
I1018 08:53:57.718412 4097533 var_desc.cc:415] Flush  assign_33.tmp_0 1
I1018 08:53:57.718415 4097533 var_desc.cc:415] Flush  batch_norm2d_8.w_1 1
I1018 08:53:57.718417 4097533 var_desc.cc:415] Flush  reshape2_35.tmp_0 1
I1018 08:53:57.718420 4097533 var_desc.cc:415] Flush  tmp_57 1
I1018 08:53:57.718422 4097533 var_desc.cc:415] Flush  reshape2_34.tmp_0 1
I1018 08:53:57.718425 4097533 var_desc.cc:415] Flush  reshape2_34.tmp_1 1
I1018 08:53:57.718428 4097533 var_desc.cc:415] Flush  reshape2_35.tmp_1 1
I1018 08:53:57.718432 4097533 var_desc.cc:415] Flush  elementwise_pow_34.tmp_0 1
I1018 08:53:57.718436 4097533 var_desc.cc:415] Flush  fill_constant_63.tmp_0 1
I1018 08:53:57.718438 4097533 var_desc.cc:415] Flush  batch_norm2d_8.w_0 1
I1018 08:53:57.718441 4097533 var_desc.cc:415] Flush  reshape2_32.tmp_1 1
I1018 08:53:57.718443 4097533 var_desc.cc:415] Flush  elementwise_pow_32.tmp_0 1
I1018 08:53:57.718446 4097533 var_desc.cc:415] Flush  tmp_59 1
I1018 08:53:57.718448 4097533 var_desc.cc:415] Flush  tmp_60 1
I1018 08:53:57.718451 4097533 var_desc.cc:415] Flush  tmp_58 1
I1018 08:53:57.718468 4097533 graph_helper.h:104] adj elementwise_max0x55b7d0f32430 -> fetch0x55b7d0b144d0  via relu_7.tmp_00x55b7d0b0b630
I1018 08:53:57.718475 4097533 graph_helper.h:104] adj assign0x55b7d0b50e90 -> fetch0x55b7d0b14120  via assign_33.tmp_00x55b7d0f43110
I1018 08:53:57.718480 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0b40400 -> elementwise_add0x55b7d0b01d30  via tmp_600x55b7d0b4cf10
I1018 08:53:57.718487 4097533 graph_helper.h:104] adj reshape20x55b7d0b7d2f0 -> elementwise_add0x55b7d0b01d30  via reshape2_35.tmp_00x55b7d0b4cd90
I1018 08:53:57.718493 4097533 graph_helper.h:104] adj reshape20x55b7d0b01ae0 -> elementwise_mul0x55b7d0b40400  via reshape2_34.tmp_00x55b7d0b4cfd0
I1018 08:53:57.718499 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0b6d890 -> elementwise_mul0x55b7d0b40400  via tmp_590x55b7d0b5d9c0
I1018 08:53:57.718504 4097533 graph_helper.h:104] adj feed0x55b7d0b136b0 -> reshape20x55b7d0b01ae0  via batch_norm2d_8.w_00x55b7d0f426f0
I1018 08:53:57.718513 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d1045b00 -> assign0x55b7d0b50e90  via elementwise_pow_32.tmp_00x55b7d0b5d4c0
I1018 08:53:57.718518 4097533 graph_helper.h:104] adj feed0x55b7d0b13a40 -> reshape20x55b7d0b0b340  via batch_norm2d_8.w_20x55b7d0f42050
I1018 08:53:57.718523 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0b01d30 -> elementwise_max0x55b7d0f32430  via batch_norm_8.tmp_20x55b7d0b4ce50
I1018 08:53:57.718529 4097533 graph_helper.h:104] adj fill_constant0x55b7d0b29890 -> elementwise_max0x55b7d0f32430  via fill_constant_69.tmp_00x55b7d0a8a9f0
I1018 08:53:57.718534 4097533 graph_helper.h:104] adj reshape20x55b7d0b7e1b0 -> elementwise_sub0x55b7d0b0e3c0  via reshape2_32.tmp_00x55b7d0a8af90
I1018 08:53:57.718540 4097533 graph_helper.h:104] adj feed0x55b7d0b0c240 -> elementwise_sub0x55b7d0b0e3c0  via conv2d_8.tmp_00x55b7d0f42a60
I1018 08:53:57.718545 4097533 graph_helper.h:104] adj reshape20x55b7d0b0b340 -> scale0x55b7d0b73690  via reshape2_33.tmp_00x55b7d0a8b2d0
I1018 08:53:57.718551 4097533 graph_helper.h:104] adj fill_constant0x55b7d0b1b860 -> scale0x55b7d0b7d8f0  via fill_constant_67.tmp_00x55b7d0b5dec0
I1018 08:53:57.718559 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d0b0e3c0 -> elementwise_mul0x55b7d0b6d890  via tmp_570x55b7d0a8b650
I1018 08:53:57.718564 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0b34d10 -> elementwise_mul0x55b7d0b6d890  via elementwise_pow_34.tmp_00x55b7d0b4d150
I1018 08:53:57.718569 4097533 graph_helper.h:104] adj scale0x55b7d0b7d8f0 -> elementwise_pow0x55b7d1045b00  via tmp_560x55b7d0b5d770
I1018 08:53:57.718575 4097533 graph_helper.h:104] adj fill_constant0x55b7d0b0e510 -> elementwise_pow0x55b7d1045b00  via fill_constant_63.tmp_00x55b7d0b4d310
I1018 08:53:57.718580 4097533 graph_helper.h:104] adj feed0x55b7d0b13320 -> reshape20x55b7d0b7d2f0  via batch_norm2d_8.b_00x55b7d0f42da0
I1018 08:53:57.718587 4097533 graph_helper.h:104] adj scale0x55b7d0b73690 -> elementwise_pow0x55b7d0b34d10  via tmp_580x55b7d0b5dc40
I1018 08:53:57.718592 4097533 graph_helper.h:104] adj fill_constant0x55b7d0b0e510 -> elementwise_pow0x55b7d0b34d10  via fill_constant_63.tmp_00x55b7d0b4d310
I1018 08:53:57.718598 4097533 graph_helper.h:104] adj feed0x55b7d0b13dd0 -> reshape20x55b7d0b7e1b0  via batch_norm2d_8.w_10x55b7d0f42380
I1018 08:53:57.718734 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.718736 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.718741 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.718746 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.718755 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.718758 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.718773 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.718775 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.718780 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.718781 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.718803 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.718806 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.718809 4097533 graph_helper.cc:698] convert op node to desc elementwise_sub
I1018 08:53:57.718812 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.718819 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.718822 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.718829 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.718832 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.718871 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.718873 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.718878 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.718879 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.718887 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.718890 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.718897 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.718900 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.718907 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.718909 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.718912 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.718915 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.718922 4097533 graph_helper.cc:698] convert op node to desc elementwise_add
I1018 08:53:57.718925 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.718932 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.718935 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.719007 4097533 graph_helper.cc:698] convert op node to desc elementwise_max
I1018 08:53:57.719009 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.719017 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.719019 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.719026 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.719029 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.719035 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.719038 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.719045 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.719048 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.719054 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.719058 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.719060 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.719063 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.719704 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.720160 4097533 block_desc.cc:205] vars in desc 26
I1018 08:53:57.720165 4097533 block_desc.cc:209] Flush relu_7.tmp_0
I1018 08:53:57.720167 4097533 var_desc.cc:415] Flush  relu_7.tmp_0 1
I1018 08:53:57.720170 4097533 block_desc.cc:209] Flush batch_norm2d_8.b_0
I1018 08:53:57.720172 4097533 var_desc.cc:415] Flush  batch_norm2d_8.b_0 1
I1018 08:53:57.720175 4097533 block_desc.cc:209] Flush conv2d_8.tmp_0
I1018 08:53:57.720177 4097533 var_desc.cc:415] Flush  conv2d_8.tmp_0 1
I1018 08:53:57.720180 4097533 block_desc.cc:209] Flush batch_norm2d_8.w_2
I1018 08:53:57.720182 4097533 var_desc.cc:415] Flush  batch_norm2d_8.w_2 1
I1018 08:53:57.720185 4097533 block_desc.cc:209] Flush reshape2_33.tmp_0
I1018 08:53:57.720187 4097533 var_desc.cc:415] Flush  reshape2_33.tmp_0 1
I1018 08:53:57.720189 4097533 block_desc.cc:209] Flush fill_constant_69.tmp_0
I1018 08:53:57.720196 4097533 var_desc.cc:415] Flush  fill_constant_69.tmp_0 1
I1018 08:53:57.720198 4097533 block_desc.cc:209] Flush fill_constant_67.tmp_0
I1018 08:53:57.720201 4097533 var_desc.cc:415] Flush  fill_constant_67.tmp_0 1
I1018 08:53:57.720202 4097533 block_desc.cc:209] Flush tmp_56
I1018 08:53:57.720206 4097533 var_desc.cc:415] Flush  tmp_56 1
I1018 08:53:57.720207 4097533 block_desc.cc:209] Flush reshape2_32.tmp_0
I1018 08:53:57.720209 4097533 var_desc.cc:415] Flush  reshape2_32.tmp_0 1
I1018 08:53:57.720212 4097533 block_desc.cc:209] Flush reshape2_33.tmp_1
I1018 08:53:57.720214 4097533 var_desc.cc:415] Flush  reshape2_33.tmp_1 1
I1018 08:53:57.720216 4097533 block_desc.cc:209] Flush batch_norm_8.tmp_2
I1018 08:53:57.720218 4097533 var_desc.cc:415] Flush  batch_norm_8.tmp_2 1
I1018 08:53:57.720221 4097533 block_desc.cc:209] Flush assign_33.tmp_0
I1018 08:53:57.720223 4097533 var_desc.cc:415] Flush  assign_33.tmp_0 1
I1018 08:53:57.720225 4097533 block_desc.cc:209] Flush batch_norm2d_8.w_1
I1018 08:53:57.720228 4097533 var_desc.cc:415] Flush  batch_norm2d_8.w_1 1
I1018 08:53:57.720230 4097533 block_desc.cc:209] Flush reshape2_35.tmp_0
I1018 08:53:57.720232 4097533 var_desc.cc:415] Flush  reshape2_35.tmp_0 1
I1018 08:53:57.720235 4097533 block_desc.cc:209] Flush tmp_57
I1018 08:53:57.720237 4097533 var_desc.cc:415] Flush  tmp_57 1
I1018 08:53:57.720240 4097533 block_desc.cc:209] Flush reshape2_34.tmp_0
I1018 08:53:57.720242 4097533 var_desc.cc:415] Flush  reshape2_34.tmp_0 1
I1018 08:53:57.720244 4097533 block_desc.cc:209] Flush reshape2_34.tmp_1
I1018 08:53:57.720247 4097533 var_desc.cc:415] Flush  reshape2_34.tmp_1 1
I1018 08:53:57.720249 4097533 block_desc.cc:209] Flush reshape2_35.tmp_1
I1018 08:53:57.720252 4097533 var_desc.cc:415] Flush  reshape2_35.tmp_1 1
I1018 08:53:57.720253 4097533 block_desc.cc:209] Flush elementwise_pow_34.tmp_0
I1018 08:53:57.720255 4097533 var_desc.cc:415] Flush  elementwise_pow_34.tmp_0 1
I1018 08:53:57.720258 4097533 block_desc.cc:209] Flush fill_constant_63.tmp_0
I1018 08:53:57.720260 4097533 var_desc.cc:415] Flush  fill_constant_63.tmp_0 1
I1018 08:53:57.720263 4097533 block_desc.cc:209] Flush batch_norm2d_8.w_0
I1018 08:53:57.720265 4097533 var_desc.cc:415] Flush  batch_norm2d_8.w_0 1
I1018 08:53:57.720268 4097533 block_desc.cc:209] Flush reshape2_32.tmp_1
I1018 08:53:57.720269 4097533 var_desc.cc:415] Flush  reshape2_32.tmp_1 1
I1018 08:53:57.720273 4097533 block_desc.cc:209] Flush elementwise_pow_32.tmp_0
I1018 08:53:57.720274 4097533 var_desc.cc:415] Flush  elementwise_pow_32.tmp_0 1
I1018 08:53:57.720276 4097533 block_desc.cc:209] Flush tmp_59
I1018 08:53:57.720278 4097533 var_desc.cc:415] Flush  tmp_59 1
I1018 08:53:57.720281 4097533 block_desc.cc:209] Flush tmp_60
I1018 08:53:57.720283 4097533 var_desc.cc:415] Flush  tmp_60 1
I1018 08:53:57.720285 4097533 block_desc.cc:209] Flush tmp_58
I1018 08:53:57.720288 4097533 var_desc.cc:415] Flush  tmp_58 1
I1018 08:53:57.718375 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "relu_7.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 28
          dims: 28
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_8.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "conv2d_8.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 28
          dims: 28
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_8.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "reshape2_33.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_69.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_67.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_56"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  vars {
    name: "reshape2_32.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_33.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "batch_norm_8.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 28
          dims: 28
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "assign_33.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "batch_norm2d_8.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "reshape2_35.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "tmp_57"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 28
          dims: 28
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "reshape2_34.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_34.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_35.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "elementwise_pow_34.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_63.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "batch_norm2d_8.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "reshape2_32.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "elementwise_pow_32.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_59"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 28
          dims: 28
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "tmp_60"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 28
          dims: 28
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "tmp_58"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_8.w_2"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_8.w_2"
    }
    outputs {
      parameter: "Out"
      arguments: "reshape2_33.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_33.tmp_1"
    }
    type: "reshape2"
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 124, in composite_batchnorm"
      strings: "    (reshape(run_var, stats_shape) + epsilon), half"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/manipulation.py\", line 4518, in reshape"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 1
      ints: 128
      ints: 1
      ints: 1
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "BiasTensor"
    }
    inputs {
      parameter: "ScaleTensor"
    }
    inputs {
      parameter: "X"
      arguments: "reshape2_33.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_58"
    }
    type: "scale"
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "bias"
      type: FLOAT
      f: 1e-05
    }
    attrs {
      name: "bias_after_scale"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 124, in composite_batchnorm"
      strings: "    (reshape(run_var, stats_shape) + epsilon), half"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 557, in __impl__"
      strings: "    return scalar_method(self, other_var)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 530, in _scalar_add_"
      strings: "    return _scalar_op_(var, 1.0, value)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 451, in _scalar_op_"
      strings: "    block.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "scale"
      type: FLOAT
      f: 1
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_8.w_1"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_8.w_1"
    }
    outputs {
      parameter: "Out"
      arguments: "reshape2_32.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_32.tmp_1"
    }
    type: "reshape2"
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 123, in composite_batchnorm"
      strings: "    x_hat = (x - reshape(run_mean, stats_shape)) * pow("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/manipulation.py\", line 4518, in reshape"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 1
      ints: 128
      ints: 1
      ints: 1
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    outputs {
I1018 08:53:57.721467 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.721622 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_37.tmp_1
I1018 08:53:57.721626 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_37.tmp_0, is_only_used_internal: 1
I1018 08:53:57.721628 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_37.tmp_0
I1018 08:53:57.721632 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_71.tmp_0, is_only_used_internal: 1
I1018 08:53:57.721633 4097533 build_cinn_pass.cc:557] var_node->outputs[1]: fill_constant_71.tmp_0, is_only_used_internal: 1
I1018 08:53:57.721635 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_71.tmp_0
I1018 08:53:57.721637 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: elementwise_pow_36.tmp_0, is_only_used_internal: 1
I1018 08:53:57.721639 4097533 build_cinn_pass.cc:562] insert internal var: elementwise_pow_36.tmp_0
I1018 08:53:57.721642 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_62, is_only_used_internal: 1
I1018 08:53:57.721644 4097533 build_cinn_pass.cc:562] insert internal var: tmp_62
I1018 08:53:57.721647 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: elementwise_pow_38.tmp_0, is_only_used_internal: 1
I1018 08:53:57.721648 4097533 build_cinn_pass.cc:562] insert internal var: elementwise_pow_38.tmp_0
I1018 08:53:57.721650 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_36.tmp_0, is_only_used_internal: 1
I1018 08:53:57.721652 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_36.tmp_0
I1018 08:53:57.721654 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_75.tmp_0, is_only_used_internal: 1
I1018 08:53:57.721657 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_75.tmp_0
I1018 08:53:57.721659 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_36.tmp_1
I1018 08:53:57.721661 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_38.tmp_1
I1018 08:53:57.721664 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_65, is_only_used_internal: 1
I1018 08:53:57.721666 4097533 build_cinn_pass.cc:562] insert internal var: tmp_65
I1018 08:53:57.721668 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_64, is_only_used_internal: 1
I1018 08:53:57.721670 4097533 build_cinn_pass.cc:562] insert internal var: tmp_64
I1018 08:53:57.721673 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_39.tmp_1
I1018 08:53:57.721675 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_77.tmp_0, is_only_used_internal: 1
I1018 08:53:57.721678 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_77.tmp_0
I1018 08:53:57.721679 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_39.tmp_0, is_only_used_internal: 1
I1018 08:53:57.721681 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_39.tmp_0
I1018 08:53:57.721683 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: batch_norm_9.tmp_2, is_only_used_internal: 1
I1018 08:53:57.721686 4097533 build_cinn_pass.cc:562] insert internal var: batch_norm_9.tmp_2
I1018 08:53:57.721688 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_63, is_only_used_internal: 1
I1018 08:53:57.721690 4097533 build_cinn_pass.cc:562] insert internal var: tmp_63
I1018 08:53:57.721693 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_3, is_only_used_internal: 1
I1018 08:53:57.721694 4097533 build_cinn_pass.cc:562] insert internal var: tmp_3
I1018 08:53:57.721696 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_38.tmp_0, is_only_used_internal: 1
I1018 08:53:57.721698 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_38.tmp_0
I1018 08:53:57.721701 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_66, is_only_used_internal: 1
I1018 08:53:57.721704 4097533 build_cinn_pass.cc:562] insert internal var: tmp_66
I1018 08:53:57.721709 4097533 build_cinn_pass.cc:742] Cluster Ops: (reshape2, reshape2, elementwise_mul, scale, reshape2, elementwise_add, elementwise_add, elementwise_mul, elementwise_max, elementwise_sub, elementwise_pow, fill_constant, scale, elementwise_pow, reshape2, fill_constant, assign, fill_constant, )
I1018 08:53:57.721712 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_9.w_2, conv2d_9.tmp_0, batch_norm2d_9.w_0, batch_norm2d_9.b_0, relu_6.tmp_0, batch_norm2d_9.w_1, )
I1018 08:53:57.721715 4097533 build_cinn_pass.cc:744] Cluster output vars: (assign_37.tmp_0, relu_8.tmp_0, )
I1018 08:53:57.721717 4097533 build_cinn_pass.cc:745] Cluster internal vars: (tmp_66, reshape2_38.tmp_0, tmp_3, tmp_63, batch_norm_9.tmp_2, reshape2_39.tmp_0, fill_constant_77.tmp_0, reshape2_37.tmp_1, tmp_62, tmp_65, tmp_64, reshape2_37.tmp_0, fill_constant_71.tmp_0, elementwise_pow_36.tmp_0, elementwise_pow_38.tmp_0, reshape2_38.tmp_1, reshape2_36.tmp_0, fill_constant_75.tmp_0, reshape2_36.tmp_1, reshape2_39.tmp_1, )
I1018 08:53:57.721726 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.721942 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: conv2d_9.tmp_0
I1018 08:53:57.721946 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: relu_6.tmp_0
I1018 08:53:57.721951 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_9.w_2
I1018 08:53:57.721953 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_9.b_0
I1018 08:53:57.721957 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_9.w_0
I1018 08:53:57.721961 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_9.w_1
I1018 08:53:57.721964 4097533 build_cinn_pass.cc:274] Add Output Var Node: assign_37.tmp_0
I1018 08:53:57.721967 4097533 build_cinn_pass.cc:274] Add Output Var Node: relu_8.tmp_0
I1018 08:53:57.721988 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_52[label="tmp_62
[128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_51[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_50[label="elementwise_max" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_48[label="fill_constant_77.tmp_0
[1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_47[label="reshape2_38.tmp_1
[0,128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_46[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_45[label="reshape2_39.tmp_0
[1,128,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_44[label="batch_norm_9.tmp_2
[1,128,28,28]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_43[label="batch_norm2d_9.b_0
[128]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_42[label="tmp_3
[1,128,28,28]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_39[label="elementwise_mul" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_37[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_53[label="tmp_64
[1,128,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_35[label="tmp_65
[1,128,28,28]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_32[label="scale" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_31[label="elementwise_pow" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_30[label="reshape2_36.tmp_1
[0,128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_3[label="relu_8.tmp_0
[1,128,28,28]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_21[label="elementwise_mul" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_38[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_8[label="batch_norm2d_9.w_2
[128]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_49[label="elementwise_pow_38.tmp_0
[1,128,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_7[label="conv2d_9.tmp_0
[1,128,28,28]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_6[label="batch_norm2d_9.w_0
[128]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_33[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_26[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_11[label="assign_37.tmp_0
[128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_16[label="batch_norm2d_9.w_1
[128]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_12[label="reshape2_36.tmp_0
[1,128,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_5[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_4[label="relu_6.tmp_0
[1,128,28,28]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_24[label="elementwise_add" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_27[label="scale" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_9[label="reshape2_39.tmp_1
[0,128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_36[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_17[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_10[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_19[label="elementwise_sub" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_41[label="tmp_66
[1,128,28,28]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_34[label="reshape2_37.tmp_1
[0,128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_13[label="elementwise_pow_36.tmp_0
[128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_1[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_23[label="reshape2_38.tmp_0
[1,128,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_14[label="fill_constant_71.tmp_0
[1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_2[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_15[label="reshape2_37.tmp_0
[1,128,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_18[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_20[label="fill_constant_75.tmp_0
[128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_40[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_22[label="elementwise_add" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_25[label="tmp_63
[1,128,28,28]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_29[label="elementwise_pow" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_28[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0->node_6
   node_1->node_43
   node_2->node_4
   node_3->node_18
   node_4->node_24
   node_5->node_16
   node_6->node_26
   node_7->node_19
   node_8->node_37
   node_10->node_8
   node_11->node_46
   node_12->node_19
   node_13->node_51
   node_14->node_31
   node_14->node_29
   node_15->node_32
   node_16->node_33
   node_17->node_20
   node_19->node_25
   node_20->node_27
   node_21->node_35
   node_22->node_44
   node_23->node_39
   node_24->node_42
   node_25->node_21
   node_26->node_23
   node_26->node_47
   node_27->node_52
   node_28->node_45
   node_28->node_9
   node_29->node_13
   node_31->node_49
   node_32->node_53
   node_33->node_12
   node_33->node_30
   node_35->node_39
   node_36->node_7
   node_37->node_15
   node_37->node_34
   node_38->node_48
   node_39->node_41
   node_40->node_14
   node_41->node_22
   node_42->node_50
   node_43->node_28
   node_44->node_24
   node_45->node_22
   node_48->node_50
   node_49->node_21
   node_50->node_3
   node_51->node_11
   node_52->node_29
   node_53->node_31
} // end G
I1018 08:53:57.722270 4097533 var_desc.cc:415] Flush  relu_8.tmp_0 1
I1018 08:53:57.722272 4097533 var_desc.cc:415] Flush  relu_6.tmp_0 1
I1018 08:53:57.722275 4097533 var_desc.cc:415] Flush  batch_norm2d_9.w_0 1
I1018 08:53:57.722278 4097533 var_desc.cc:415] Flush  conv2d_9.tmp_0 1
I1018 08:53:57.722280 4097533 var_desc.cc:415] Flush  batch_norm2d_9.w_2 1
I1018 08:53:57.722283 4097533 var_desc.cc:415] Flush  reshape2_39.tmp_1 1
I1018 08:53:57.722286 4097533 var_desc.cc:415] Flush  assign_37.tmp_0 1
I1018 08:53:57.722288 4097533 var_desc.cc:415] Flush  reshape2_36.tmp_0 1
I1018 08:53:57.722291 4097533 var_desc.cc:415] Flush  elementwise_pow_36.tmp_0 1
I1018 08:53:57.722294 4097533 var_desc.cc:415] Flush  fill_constant_71.tmp_0 1
I1018 08:53:57.722297 4097533 var_desc.cc:415] Flush  reshape2_37.tmp_0 1
I1018 08:53:57.722301 4097533 var_desc.cc:415] Flush  batch_norm2d_9.w_1 1
I1018 08:53:57.722302 4097533 var_desc.cc:415] Flush  fill_constant_75.tmp_0 1
I1018 08:53:57.722306 4097533 var_desc.cc:415] Flush  reshape2_38.tmp_0 1
I1018 08:53:57.722307 4097533 var_desc.cc:415] Flush  tmp_63 1
I1018 08:53:57.722311 4097533 var_desc.cc:415] Flush  reshape2_36.tmp_1 1
I1018 08:53:57.722313 4097533 var_desc.cc:415] Flush  reshape2_37.tmp_1 1
I1018 08:53:57.722316 4097533 var_desc.cc:415] Flush  tmp_65 1
I1018 08:53:57.722319 4097533 var_desc.cc:415] Flush  tmp_66 1
I1018 08:53:57.722321 4097533 var_desc.cc:415] Flush  tmp_3 1
I1018 08:53:57.722324 4097533 var_desc.cc:415] Flush  batch_norm2d_9.b_0 1
I1018 08:53:57.722326 4097533 var_desc.cc:415] Flush  batch_norm_9.tmp_2 1
I1018 08:53:57.722329 4097533 var_desc.cc:415] Flush  reshape2_39.tmp_0 1
I1018 08:53:57.722332 4097533 var_desc.cc:415] Flush  reshape2_38.tmp_1 1
I1018 08:53:57.722334 4097533 var_desc.cc:415] Flush  fill_constant_77.tmp_0 1
I1018 08:53:57.722337 4097533 var_desc.cc:415] Flush  elementwise_pow_38.tmp_0 1
I1018 08:53:57.722339 4097533 var_desc.cc:415] Flush  tmp_62 1
I1018 08:53:57.722342 4097533 var_desc.cc:415] Flush  tmp_64 1
I1018 08:53:57.722361 4097533 graph_helper.h:104] adj elementwise_max0x55b7d0b9a9e0 -> fetch0x55b7d0bb3b60  via relu_8.tmp_00x55b7d0b2e360
I1018 08:53:57.722368 4097533 graph_helper.h:104] adj reshape20x55b7d0bc5f00 -> elementwise_sub0x55b7d108cc90  via reshape2_36.tmp_00x55b7d0b607c0
I1018 08:53:57.722374 4097533 graph_helper.h:104] adj feed0x55b7d0bc5390 -> elementwise_sub0x55b7d108cc90  via conv2d_9.tmp_00x55b7d0b618c0
I1018 08:53:57.722379 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d108cc90 -> elementwise_mul0x55b7d0ba7710  via tmp_630x55b7d0b496f0
I1018 08:53:57.722385 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0fd71b0 -> elementwise_mul0x55b7d0ba7710  via elementwise_pow_38.tmp_00x55b7d0b907a0
I1018 08:53:57.722390 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0b9ba70 -> elementwise_add0x55b7d0bba870  via tmp_660x55b7d0b493d0
I1018 08:53:57.722398 4097533 graph_helper.h:104] adj reshape20x55b7d0b93f20 -> elementwise_add0x55b7d0bba870  via reshape2_39.tmp_00x55b7d0b49930
I1018 08:53:57.722404 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0bba870 -> elementwise_add0x55b7d0b87660  via batch_norm_9.tmp_20x55b7d0b497f0
I1018 08:53:57.722409 4097533 graph_helper.h:104] adj feed0x55b7d0bc5700 -> elementwise_add0x55b7d0b87660  via relu_6.tmp_00x55b7d0b2d980
I1018 08:53:57.722415 4097533 graph_helper.h:104] adj feed0x55b7d0bb3130 -> reshape20x55b7d0b8dc90  via batch_norm2d_9.w_00x55b7d0b2d2a0
I1018 08:53:57.722422 4097533 graph_helper.h:104] adj fill_constant0x55b7d0bc35f0 -> scale0x55b7d1015e70  via fill_constant_75.tmp_00x55b7d0b60b00
I1018 08:53:57.722429 4097533 graph_helper.h:104] adj feed0x55b7d0bb2da0 -> reshape20x55b7d0b93f20  via batch_norm2d_9.b_00x55b7d0b2d610
I1018 08:53:57.722435 4097533 graph_helper.h:104] adj scale0x55b7d1015e70 -> elementwise_pow0x55b7d0b98180  via tmp_620x55b7d0b49f40
I1018 08:53:57.722440 4097533 graph_helper.h:104] adj fill_constant0x55b7d0bc6850 -> elementwise_pow0x55b7d0b98180  via fill_constant_71.tmp_00x55b7d0b900a0
I1018 08:53:57.722446 4097533 graph_helper.h:104] adj scale0x55b7d0b90ee0 -> elementwise_pow0x55b7d0fd71b0  via tmp_640x55b7d0b8fae0
I1018 08:53:57.722451 4097533 graph_helper.h:104] adj fill_constant0x55b7d0bc6850 -> elementwise_pow0x55b7d0fd71b0  via fill_constant_71.tmp_00x55b7d0b900a0
I1018 08:53:57.722456 4097533 graph_helper.h:104] adj reshape20x55b7d0ba4f50 -> scale0x55b7d0b90ee0  via reshape2_37.tmp_00x55b7d0b8fda0
I1018 08:53:57.722462 4097533 graph_helper.h:104] adj feed0x55b7d0bb34c0 -> reshape20x55b7d0bc5f00  via batch_norm2d_9.w_10x55b7d0b2dcc0
I1018 08:53:57.722469 4097533 graph_helper.h:104] adj feed0x55b7d0bc5a90 -> reshape20x55b7d0ba4f50  via batch_norm2d_9.w_20x55b7d0b61550
I1018 08:53:57.722476 4097533 graph_helper.h:104] adj reshape20x55b7d0b8dc90 -> elementwise_mul0x55b7d0b9ba70  via reshape2_38.tmp_00x55b7d0b494b0
I1018 08:53:57.722481 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0ba7710 -> elementwise_mul0x55b7d0b9ba70  via tmp_650x55b7d0b4a1f0
I1018 08:53:57.722486 4097533 graph_helper.h:104] adj assign0x55b7d0b0f6a0 -> fetch0x55b7d0bb3810  via assign_37.tmp_00x55b7d0b2e030
I1018 08:53:57.722491 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0b87660 -> elementwise_max0x55b7d0b9a9e0  via tmp_30x55b7d0b495f0
I1018 08:53:57.722496 4097533 graph_helper.h:104] adj fill_constant0x55b7d0b31790 -> elementwise_max0x55b7d0b9a9e0  via fill_constant_77.tmp_00x55b7d0b49b80
I1018 08:53:57.722502 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0b98180 -> assign0x55b7d0b0f6a0  via elementwise_pow_36.tmp_00x55b7d0b90410
I1018 08:53:57.722644 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.722647 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.722652 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.722656 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.722664 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.722666 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.722678 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.722680 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.722687 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.722690 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.722707 4097533 graph_helper.cc:698] convert op node to desc elementwise_sub
I1018 08:53:57.722710 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.722718 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.722719 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.722723 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.722726 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.722733 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.722736 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.722770 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.722774 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.722782 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.722785 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.722793 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.722796 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.722803 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.722805 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.722810 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.722811 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.722819 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.722822 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.722828 4097533 graph_helper.cc:698] convert op node to desc elementwise_add
I1018 08:53:57.722831 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.722838 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.722841 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.722909 4097533 graph_helper.cc:698] convert op node to desc elementwise_add
I1018 08:53:57.722913 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.722918 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.722921 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.722929 4097533 graph_helper.cc:698] convert op node to desc elementwise_max
I1018 08:53:57.722932 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.722939 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.722942 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.722949 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.722951 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.722958 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.722961 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.722968 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.722971 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.722977 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.722980 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.722983 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.722986 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.723651 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.724129 4097533 block_desc.cc:205] vars in desc 28
I1018 08:53:57.724134 4097533 block_desc.cc:209] Flush relu_8.tmp_0
I1018 08:53:57.724138 4097533 var_desc.cc:415] Flush  relu_8.tmp_0 1
I1018 08:53:57.724139 4097533 block_desc.cc:209] Flush relu_6.tmp_0
I1018 08:53:57.724143 4097533 var_desc.cc:415] Flush  relu_6.tmp_0 1
I1018 08:53:57.724144 4097533 block_desc.cc:209] Flush batch_norm2d_9.w_0
I1018 08:53:57.724148 4097533 var_desc.cc:415] Flush  batch_norm2d_9.w_0 1
I1018 08:53:57.724150 4097533 block_desc.cc:209] Flush conv2d_9.tmp_0
I1018 08:53:57.724152 4097533 var_desc.cc:415] Flush  conv2d_9.tmp_0 1
I1018 08:53:57.724154 4097533 block_desc.cc:209] Flush batch_norm2d_9.w_2
I1018 08:53:57.724156 4097533 var_desc.cc:415] Flush  batch_norm2d_9.w_2 1
I1018 08:53:57.724159 4097533 block_desc.cc:209] Flush reshape2_39.tmp_1
I1018 08:53:57.724161 4097533 var_desc.cc:415] Flush  reshape2_39.tmp_1 1
I1018 08:53:57.724164 4097533 block_desc.cc:209] Flush assign_37.tmp_0
I1018 08:53:57.724166 4097533 var_desc.cc:415] Flush  assign_37.tmp_0 1
I1018 08:53:57.724169 4097533 block_desc.cc:209] Flush reshape2_36.tmp_0
I1018 08:53:57.724171 4097533 var_desc.cc:415] Flush  reshape2_36.tmp_0 1
I1018 08:53:57.724174 4097533 block_desc.cc:209] Flush elementwise_pow_36.tmp_0
I1018 08:53:57.724175 4097533 var_desc.cc:415] Flush  elementwise_pow_36.tmp_0 1
I1018 08:53:57.724179 4097533 block_desc.cc:209] Flush fill_constant_71.tmp_0
I1018 08:53:57.724180 4097533 var_desc.cc:415] Flush  fill_constant_71.tmp_0 1
I1018 08:53:57.724182 4097533 block_desc.cc:209] Flush reshape2_37.tmp_0
I1018 08:53:57.724185 4097533 var_desc.cc:415] Flush  reshape2_37.tmp_0 1
I1018 08:53:57.724191 4097533 block_desc.cc:209] Flush batch_norm2d_9.w_1
I1018 08:53:57.724193 4097533 var_desc.cc:415] Flush  batch_norm2d_9.w_1 1
I1018 08:53:57.724195 4097533 block_desc.cc:209] Flush fill_constant_75.tmp_0
I1018 08:53:57.724197 4097533 var_desc.cc:415] Flush  fill_constant_75.tmp_0 1
I1018 08:53:57.724200 4097533 block_desc.cc:209] Flush reshape2_38.tmp_0
I1018 08:53:57.724202 4097533 var_desc.cc:415] Flush  reshape2_38.tmp_0 1
I1018 08:53:57.724205 4097533 block_desc.cc:209] Flush tmp_63
I1018 08:53:57.724207 4097533 var_desc.cc:415] Flush  tmp_63 1
I1018 08:53:57.724210 4097533 block_desc.cc:209] Flush reshape2_36.tmp_1
I1018 08:53:57.724212 4097533 var_desc.cc:415] Flush  reshape2_36.tmp_1 1
I1018 08:53:57.724215 4097533 block_desc.cc:209] Flush reshape2_37.tmp_1
I1018 08:53:57.724216 4097533 var_desc.cc:415] Flush  reshape2_37.tmp_1 1
I1018 08:53:57.724220 4097533 block_desc.cc:209] Flush tmp_65
I1018 08:53:57.724221 4097533 var_desc.cc:415] Flush  tmp_65 1
I1018 08:53:57.724223 4097533 block_desc.cc:209] Flush tmp_66
I1018 08:53:57.724225 4097533 var_desc.cc:415] Flush  tmp_66 1
I1018 08:53:57.724228 4097533 block_desc.cc:209] Flush tmp_3
I1018 08:53:57.724231 4097533 var_desc.cc:415] Flush  tmp_3 1
I1018 08:53:57.724233 4097533 block_desc.cc:209] Flush batch_norm2d_9.b_0
I1018 08:53:57.724236 4097533 var_desc.cc:415] Flush  batch_norm2d_9.b_0 1
I1018 08:53:57.724237 4097533 block_desc.cc:209] Flush batch_norm_9.tmp_2
I1018 08:53:57.724239 4097533 var_desc.cc:415] Flush  batch_norm_9.tmp_2 1
I1018 08:53:57.724242 4097533 block_desc.cc:209] Flush reshape2_39.tmp_0
I1018 08:53:57.724244 4097533 var_desc.cc:415] Flush  reshape2_39.tmp_0 1
I1018 08:53:57.724247 4097533 block_desc.cc:209] Flush reshape2_38.tmp_1
I1018 08:53:57.724249 4097533 var_desc.cc:415] Flush  reshape2_38.tmp_1 1
I1018 08:53:57.724251 4097533 block_desc.cc:209] Flush fill_constant_77.tmp_0
I1018 08:53:57.724253 4097533 var_desc.cc:415] Flush  fill_constant_77.tmp_0 1
I1018 08:53:57.724256 4097533 block_desc.cc:209] Flush elementwise_pow_38.tmp_0
I1018 08:53:57.724258 4097533 var_desc.cc:415] Flush  elementwise_pow_38.tmp_0 1
I1018 08:53:57.724260 4097533 block_desc.cc:209] Flush tmp_62
I1018 08:53:57.724263 4097533 var_desc.cc:415] Flush  tmp_62 1
I1018 08:53:57.724265 4097533 block_desc.cc:209] Flush tmp_64
I1018 08:53:57.724267 4097533 var_desc.cc:415] Flush  tmp_64 1
I1018 08:53:57.722262 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "relu_8.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 28
          dims: 28
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "relu_6.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 28
          dims: 28
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_9.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "conv2d_9.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 28
          dims: 28
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_9.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "reshape2_39.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "assign_37.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_36.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "elementwise_pow_36.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_71.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_37.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "batch_norm2d_9.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "fill_constant_75.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_38.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "tmp_63"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 28
          dims: 28
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "reshape2_36.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_37.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_65"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 28
          dims: 28
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "tmp_66"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 28
          dims: 28
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "tmp_3"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 28
          dims: 28
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_9.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "batch_norm_9.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 28
          dims: 28
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_39.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_38.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "fill_constant_77.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "elementwise_pow_38.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_62"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  vars {
    name: "tmp_64"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 128
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_9.w_1"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_9.w_1"
    }
    outputs {
      parameter: "Out"
      arguments: "reshape2_36.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_36.tmp_1"
    }
    type: "reshape2"
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 123, in composite_batchnorm"
      strings: "    x_hat = (x - reshape(run_mean, stats_shape)) * pow("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/manipulation.py\", line 4518, in reshape"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 1
      ints: 128
      ints: 1
      ints: 1
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_9.w_0"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_9.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "reshape2_38.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_38.tmp_1"
    }
    type: "reshape2"
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 129, in composite_batchnorm"
      strings: "    y = reshape(scale, stats_shape) * x_hat + reshape(bias, stats_shape)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/manipulation.py\", line 4518, in reshape"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 1
      ints: 128
      ints: 1
      ints: 1
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "conv2d_9.tmp_0"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "conv2d_9.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "reshape2_36.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_63"
    }
    type: "elementwise_sub"
    attrs {
      name: "Scale_out"
      type: FLOAT
      f: 1
    }
    attrs {
      name: "Scale_x"
      type: FLOAT
      f: 1
    }
    attrs {
      name: "Scale_y"
      type: FLOAT
      f: 1
    }
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "axis"
      type: INT
      i: -1
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 123, in composite_batchnorm"
      strings: "    x_hat = (x - reshape(run_mean, stats_shape)) * pow("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 657, in __impl__"
      strings: "    current_block(self).append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.lo
I1018 08:53:57.725517 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.725675 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: elementwise_pow_40.tmp_0, is_only_used_internal: 1
I1018 08:53:57.725678 4097533 build_cinn_pass.cc:562] insert internal var: elementwise_pow_40.tmp_0
I1018 08:53:57.725682 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_68, is_only_used_internal: 1
I1018 08:53:57.725687 4097533 build_cinn_pass.cc:562] insert internal var: tmp_68
I1018 08:53:57.725688 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_69, is_only_used_internal: 1
I1018 08:53:57.725690 4097533 build_cinn_pass.cc:562] insert internal var: tmp_69
I1018 08:53:57.725692 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_41.tmp_0, is_only_used_internal: 1
I1018 08:53:57.725694 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_41.tmp_0
I1018 08:53:57.725697 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_79.tmp_0, is_only_used_internal: 1
I1018 08:53:57.725699 4097533 build_cinn_pass.cc:557] var_node->outputs[1]: fill_constant_79.tmp_0, is_only_used_internal: 1
I1018 08:53:57.725701 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_79.tmp_0
I1018 08:53:57.725703 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_70, is_only_used_internal: 1
I1018 08:53:57.725705 4097533 build_cinn_pass.cc:562] insert internal var: tmp_70
I1018 08:53:57.725708 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_71, is_only_used_internal: 1
I1018 08:53:57.725709 4097533 build_cinn_pass.cc:562] insert internal var: tmp_71
I1018 08:53:57.725713 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_41.tmp_1
I1018 08:53:57.725714 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_85.tmp_0, is_only_used_internal: 1
I1018 08:53:57.725716 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_85.tmp_0
I1018 08:53:57.725718 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_40.tmp_0, is_only_used_internal: 1
I1018 08:53:57.725720 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_40.tmp_0
I1018 08:53:57.725723 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: elementwise_pow_42.tmp_0, is_only_used_internal: 1
I1018 08:53:57.725725 4097533 build_cinn_pass.cc:562] insert internal var: elementwise_pow_42.tmp_0
I1018 08:53:57.725728 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_42.tmp_1
I1018 08:53:57.725729 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: batch_norm_10.tmp_2, is_only_used_internal: 1
I1018 08:53:57.725731 4097533 build_cinn_pass.cc:562] insert internal var: batch_norm_10.tmp_2
I1018 08:53:57.725733 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_43.tmp_0, is_only_used_internal: 1
I1018 08:53:57.725735 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_43.tmp_0
I1018 08:53:57.725739 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_43.tmp_1
I1018 08:53:57.725740 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_42.tmp_0, is_only_used_internal: 1
I1018 08:53:57.725742 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_42.tmp_0
I1018 08:53:57.725745 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_72, is_only_used_internal: 1
I1018 08:53:57.725747 4097533 build_cinn_pass.cc:562] insert internal var: tmp_72
I1018 08:53:57.725749 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_40.tmp_1
I1018 08:53:57.725751 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_83.tmp_0, is_only_used_internal: 1
I1018 08:53:57.725754 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_83.tmp_0
I1018 08:53:57.725756 4097533 build_cinn_pass.cc:742] Cluster Ops: (scale, reshape2, elementwise_pow, elementwise_mul, fill_constant, reshape2, elementwise_max, reshape2, elementwise_mul, fill_constant, reshape2, elementwise_sub, scale, elementwise_pow, assign, elementwise_add, fill_constant, )
I1018 08:53:57.725760 4097533 build_cinn_pass.cc:743] Cluster input vars: (conv2d_10.tmp_0, batch_norm2d_11.w_1, batch_norm2d_11.w_2, batch_norm2d_11.b_0, batch_norm2d_11.w_0, )
I1018 08:53:57.725762 4097533 build_cinn_pass.cc:744] Cluster output vars: (assign_41.tmp_0, relu_9.tmp_0, )
I1018 08:53:57.725764 4097533 build_cinn_pass.cc:745] Cluster internal vars: (fill_constant_83.tmp_0, reshape2_40.tmp_1, tmp_72, reshape2_42.tmp_0, reshape2_43.tmp_1, elementwise_pow_40.tmp_0, tmp_68, tmp_71, reshape2_41.tmp_1, fill_constant_85.tmp_0, tmp_69, reshape2_43.tmp_0, batch_norm_10.tmp_2, reshape2_41.tmp_0, tmp_70, fill_constant_79.tmp_0, reshape2_40.tmp_0, elementwise_pow_42.tmp_0, reshape2_42.tmp_1, )
I1018 08:53:57.725775 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.725986 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: conv2d_10.tmp_0
I1018 08:53:57.725989 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_11.w_2
I1018 08:53:57.725993 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_11.w_0
I1018 08:53:57.725996 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_11.b_0
I1018 08:53:57.725999 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_11.w_1
I1018 08:53:57.726003 4097533 build_cinn_pass.cc:274] Add Output Var Node: assign_41.tmp_0
I1018 08:53:57.726007 4097533 build_cinn_pass.cc:274] Add Output Var Node: relu_9.tmp_0
I1018 08:53:57.726028 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_48[label="reshape2_41.tmp_1
[0,256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_47[label="tmp_71
[1,256,14,14]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_46[label="elementwise_mul" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_45[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_44[label="elementwise_pow_40.tmp_0
[256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_43[label="tmp_72
[1,256,14,14]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_42[label="batch_norm_10.tmp_2
[1,256,14,14]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_39[label="reshape2_40.tmp_1
[0,256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_37[label="reshape2_40.tmp_0
[1,256,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_35[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_32[label="elementwise_add" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_31[label="elementwise_pow" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_30[label="elementwise_sub" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_21[label="reshape2_43.tmp_0
[1,256,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_38[label="reshape2_42.tmp_0
[1,256,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_8[label="batch_norm2d_11.b_0
[256]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_49[label="fill_constant_85.tmp_0
[1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_7[label="batch_norm2d_11.w_0
[256]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_6[label="assign_41.tmp_0
[256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_33[label="elementwise_mul" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_26[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_11[label="fill_constant_79.tmp_0
[1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_16[label="reshape2_42.tmp_1
[0,256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_12[label="reshape2_41.tmp_0
[1,256,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_5[label="relu_9.tmp_0
[1,256,14,14]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_4[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_24[label="elementwise_pow" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_27[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_9[label="batch_norm2d_11.w_2
[256]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_36[label="fill_constant_83.tmp_0
[256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_17[label="tmp_70
[1,256,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_10[label="elementwise_pow_42.tmp_0
[1,256,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_19[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_41[label="reshape2_43.tmp_1
[0,256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_34[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_13[label="tmp_68
[256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_1[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_23[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_14[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_2[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_15[label="scale" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_18[label="scale" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_20[label="tmp_69
[1,256,14,14]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_40[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_22[label="elementwise_max" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_25[label="batch_norm2d_11.w_1
[256]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_29[label="conv2d_10.tmp_0
[1,256,14,14]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_28[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_25
   node_2->node_7
   node_3->node_9
   node_4->node_29
   node_5->node_0
   node_6->node_45
   node_7->node_14
   node_8->node_19
   node_9->node_40
   node_10->node_46
   node_11->node_31
   node_11->node_24
   node_12->node_18
   node_13->node_24
   node_14->node_38
   node_14->node_16
   node_15->node_13
   node_17->node_31
   node_18->node_17
   node_19->node_21
   node_19->node_41
   node_20->node_46
   node_21->node_32
   node_22->node_5
   node_23->node_36
   node_24->node_44
   node_25->node_26
   node_26->node_37
   node_26->node_39
   node_27->node_49
   node_28->node_6
   node_29->node_30
   node_30->node_20
   node_31->node_10
   node_32->node_42
   node_33->node_43
   node_34->node_11
   node_35->node_8
   node_36->node_15
   node_37->node_30
   node_38->node_33
   node_40->node_12
   node_40->node_48
   node_42->node_22
   node_43->node_32
   node_44->node_28
   node_46->node_47
   node_47->node_33
   node_49->node_22
} // end G
I1018 08:53:57.726287 4097533 var_desc.cc:415] Flush  relu_9.tmp_0 1
I1018 08:53:57.726290 4097533 var_desc.cc:415] Flush  assign_41.tmp_0 1
I1018 08:53:57.726294 4097533 var_desc.cc:415] Flush  batch_norm2d_11.w_0 1
I1018 08:53:57.726297 4097533 var_desc.cc:415] Flush  batch_norm2d_11.b_0 1
I1018 08:53:57.726300 4097533 var_desc.cc:415] Flush  batch_norm2d_11.w_2 1
I1018 08:53:57.726303 4097533 var_desc.cc:415] Flush  elementwise_pow_42.tmp_0 1
I1018 08:53:57.726306 4097533 var_desc.cc:415] Flush  fill_constant_79.tmp_0 1
I1018 08:53:57.726308 4097533 var_desc.cc:415] Flush  reshape2_41.tmp_0 1
I1018 08:53:57.726311 4097533 var_desc.cc:415] Flush  tmp_68 1
I1018 08:53:57.726315 4097533 var_desc.cc:415] Flush  reshape2_42.tmp_1 1
I1018 08:53:57.726317 4097533 var_desc.cc:415] Flush  tmp_70 1
I1018 08:53:57.726320 4097533 var_desc.cc:415] Flush  tmp_69 1
I1018 08:53:57.726322 4097533 var_desc.cc:415] Flush  reshape2_43.tmp_0 1
I1018 08:53:57.726325 4097533 var_desc.cc:415] Flush  batch_norm2d_11.w_1 1
I1018 08:53:57.726327 4097533 var_desc.cc:415] Flush  conv2d_10.tmp_0 1
I1018 08:53:57.726330 4097533 var_desc.cc:415] Flush  fill_constant_83.tmp_0 1
I1018 08:53:57.726332 4097533 var_desc.cc:415] Flush  reshape2_40.tmp_0 1
I1018 08:53:57.726336 4097533 var_desc.cc:415] Flush  reshape2_42.tmp_0 1
I1018 08:53:57.726338 4097533 var_desc.cc:415] Flush  reshape2_40.tmp_1 1
I1018 08:53:57.726341 4097533 var_desc.cc:415] Flush  reshape2_43.tmp_1 1
I1018 08:53:57.726343 4097533 var_desc.cc:415] Flush  batch_norm_10.tmp_2 1
I1018 08:53:57.726346 4097533 var_desc.cc:415] Flush  tmp_72 1
I1018 08:53:57.726348 4097533 var_desc.cc:415] Flush  elementwise_pow_40.tmp_0 1
I1018 08:53:57.726351 4097533 var_desc.cc:415] Flush  tmp_71 1
I1018 08:53:57.726354 4097533 var_desc.cc:415] Flush  reshape2_41.tmp_1 1
I1018 08:53:57.726356 4097533 var_desc.cc:415] Flush  fill_constant_85.tmp_0 1
I1018 08:53:57.726373 4097533 graph_helper.h:104] adj elementwise_max0x55b7d0b9e770 -> fetch0x55b7d0bacf40  via relu_9.tmp_00x55b7d0c0d450
I1018 08:53:57.726380 4097533 graph_helper.h:104] adj feed0x55b7d0bac180 -> reshape20x55b7d0be7010  via batch_norm2d_11.w_00x55b7d0bd6a10
I1018 08:53:57.726389 4097533 graph_helper.h:104] adj fill_constant0x55b7d0bd4110 -> scale0x55b7d0a09540  via fill_constant_83.tmp_00x55b7d0a3f7c0
I1018 08:53:57.726397 4097533 graph_helper.h:104] adj reshape20x55b7d0a4e8a0 -> scale0x55b7d0c09ed0  via reshape2_41.tmp_00x55b7d0bd4780
I1018 08:53:57.726402 4097533 graph_helper.h:104] adj feed0x55b7d0bac510 -> reshape20x55b7d0b9e5c0  via batch_norm2d_11.b_00x55b7d0bd66a0
I1018 08:53:57.726409 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0be9240 -> elementwise_max0x55b7d0b9e770  via batch_norm_10.tmp_20x55b7d0bdeb90
I1018 08:53:57.726415 4097533 graph_helper.h:104] adj fill_constant0x55b7d0bab770 -> elementwise_max0x55b7d0b9e770  via fill_constant_85.tmp_00x55b7d0bde2e0
I1018 08:53:57.726420 4097533 graph_helper.h:104] adj scale0x55b7d0a09540 -> elementwise_pow0x55b7d0b56360  via tmp_680x55b7d0a3ff60
I1018 08:53:57.726426 4097533 graph_helper.h:104] adj fill_constant0x55b7d0f5dfc0 -> elementwise_pow0x55b7d0b56360  via fill_constant_79.tmp_00x55b7d0bd4e40
I1018 08:53:57.726431 4097533 graph_helper.h:104] adj feed0x55b7d0bac8a0 -> reshape20x55b7d0bff3d0  via batch_norm2d_11.w_10x55b7d0bd5fc0
I1018 08:53:57.726438 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0b56360 -> assign0x55b7d0f5f270  via elementwise_pow_40.tmp_00x55b7d0a3fe00
I1018 08:53:57.726444 4097533 graph_helper.h:104] adj reshape20x55b7d0bff3d0 -> elementwise_sub0x55b7d0bd3040  via reshape2_40.tmp_00x55b7d0bd51b0
I1018 08:53:57.726449 4097533 graph_helper.h:104] adj feed0x55b7d0c0e060 -> elementwise_sub0x55b7d0bd3040  via conv2d_10.tmp_00x55b7d0bd5c80
I1018 08:53:57.726454 4097533 graph_helper.h:104] adj scale0x55b7d0c09ed0 -> elementwise_pow0x55b7d0bcc1a0  via tmp_700x55b7d0bd4b00
I1018 08:53:57.726460 4097533 graph_helper.h:104] adj fill_constant0x55b7d0f5dfc0 -> elementwise_pow0x55b7d0bcc1a0  via fill_constant_79.tmp_00x55b7d0bd4e40
I1018 08:53:57.726465 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0bf9590 -> elementwise_add0x55b7d0be9240  via tmp_720x55b7d0a3fa60
I1018 08:53:57.726471 4097533 graph_helper.h:104] adj reshape20x55b7d0b9e5c0 -> elementwise_add0x55b7d0be9240  via reshape2_43.tmp_00x55b7d0bde890
I1018 08:53:57.726480 4097533 graph_helper.h:104] adj reshape20x55b7d0be7010 -> elementwise_mul0x55b7d0bf9590  via reshape2_42.tmp_00x55b7d0a3fb60
I1018 08:53:57.726485 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0bab980 -> elementwise_mul0x55b7d0bf9590  via tmp_710x55b7d0bddd60
I1018 08:53:57.726490 4097533 graph_helper.h:104] adj feed0x55b7d0babe00 -> reshape20x55b7d0a4e8a0  via batch_norm2d_11.w_20x55b7d0bd6330
I1018 08:53:57.726497 4097533 graph_helper.h:104] adj assign0x55b7d0f5f270 -> fetch0x55b7d0bacbf0  via assign_41.tmp_00x55b7d0c0d120
I1018 08:53:57.726501 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d0bd3040 -> elementwise_mul0x55b7d0bab980  via tmp_690x55b7d0bde5d0
I1018 08:53:57.726507 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0bcc1a0 -> elementwise_mul0x55b7d0bab980  via elementwise_pow_42.tmp_00x55b7d0bd5530
I1018 08:53:57.726642 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.726645 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.726650 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.726653 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.726661 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.726665 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.726678 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.726680 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.726684 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.726687 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.726707 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.726711 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.726717 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.726720 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.726727 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.726729 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.726733 4097533 graph_helper.cc:698] convert op node to desc elementwise_sub
I1018 08:53:57.726737 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.726774 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.726778 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.726784 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.726787 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.726795 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.726797 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.726801 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.726804 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.726810 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.726812 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.726816 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.726819 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.726826 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.726830 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.726835 4097533 graph_helper.cc:698] convert op node to desc elementwise_add
I1018 08:53:57.726838 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.726910 4097533 graph_helper.cc:698] convert op node to desc elementwise_max
I1018 08:53:57.726914 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.726922 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.726923 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.726931 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.726934 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.726941 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.726944 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.726950 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.726953 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.726959 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.726964 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.726969 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.726970 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.727614 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.728071 4097533 block_desc.cc:205] vars in desc 26
I1018 08:53:57.728076 4097533 block_desc.cc:209] Flush relu_9.tmp_0
I1018 08:53:57.728078 4097533 var_desc.cc:415] Flush  relu_9.tmp_0 1
I1018 08:53:57.728081 4097533 block_desc.cc:209] Flush assign_41.tmp_0
I1018 08:53:57.728084 4097533 var_desc.cc:415] Flush  assign_41.tmp_0 1
I1018 08:53:57.728086 4097533 block_desc.cc:209] Flush batch_norm2d_11.w_0
I1018 08:53:57.728088 4097533 var_desc.cc:415] Flush  batch_norm2d_11.w_0 1
I1018 08:53:57.728091 4097533 block_desc.cc:209] Flush batch_norm2d_11.b_0
I1018 08:53:57.728093 4097533 var_desc.cc:415] Flush  batch_norm2d_11.b_0 1
I1018 08:53:57.728096 4097533 block_desc.cc:209] Flush batch_norm2d_11.w_2
I1018 08:53:57.728098 4097533 var_desc.cc:415] Flush  batch_norm2d_11.w_2 1
I1018 08:53:57.728101 4097533 block_desc.cc:209] Flush elementwise_pow_42.tmp_0
I1018 08:53:57.728103 4097533 var_desc.cc:415] Flush  elementwise_pow_42.tmp_0 1
I1018 08:53:57.728106 4097533 block_desc.cc:209] Flush fill_constant_79.tmp_0
I1018 08:53:57.728107 4097533 var_desc.cc:415] Flush  fill_constant_79.tmp_0 1
I1018 08:53:57.728111 4097533 block_desc.cc:209] Flush reshape2_41.tmp_0
I1018 08:53:57.728112 4097533 var_desc.cc:415] Flush  reshape2_41.tmp_0 1
I1018 08:53:57.728116 4097533 block_desc.cc:209] Flush tmp_68
I1018 08:53:57.728117 4097533 var_desc.cc:415] Flush  tmp_68 1
I1018 08:53:57.728119 4097533 block_desc.cc:209] Flush reshape2_42.tmp_1
I1018 08:53:57.728122 4097533 var_desc.cc:415] Flush  reshape2_42.tmp_1 1
I1018 08:53:57.728124 4097533 block_desc.cc:209] Flush tmp_70
I1018 08:53:57.728127 4097533 var_desc.cc:415] Flush  tmp_70 1
I1018 08:53:57.728129 4097533 block_desc.cc:209] Flush tmp_69
I1018 08:53:57.728132 4097533 var_desc.cc:415] Flush  tmp_69 1
I1018 08:53:57.728134 4097533 block_desc.cc:209] Flush reshape2_43.tmp_0
I1018 08:53:57.728137 4097533 var_desc.cc:415] Flush  reshape2_43.tmp_0 1
I1018 08:53:57.728138 4097533 block_desc.cc:209] Flush batch_norm2d_11.w_1
I1018 08:53:57.728140 4097533 var_desc.cc:415] Flush  batch_norm2d_11.w_1 1
I1018 08:53:57.728143 4097533 block_desc.cc:209] Flush conv2d_10.tmp_0
I1018 08:53:57.728145 4097533 var_desc.cc:415] Flush  conv2d_10.tmp_0 1
I1018 08:53:57.728148 4097533 block_desc.cc:209] Flush fill_constant_83.tmp_0
I1018 08:53:57.728150 4097533 var_desc.cc:415] Flush  fill_constant_83.tmp_0 1
I1018 08:53:57.728152 4097533 block_desc.cc:209] Flush reshape2_40.tmp_0
I1018 08:53:57.728154 4097533 var_desc.cc:415] Flush  reshape2_40.tmp_0 1
I1018 08:53:57.728157 4097533 block_desc.cc:209] Flush reshape2_42.tmp_0
I1018 08:53:57.728159 4097533 var_desc.cc:415] Flush  reshape2_42.tmp_0 1
I1018 08:53:57.728163 4097533 block_desc.cc:209] Flush reshape2_40.tmp_1
I1018 08:53:57.728164 4097533 var_desc.cc:415] Flush  reshape2_40.tmp_1 1
I1018 08:53:57.728166 4097533 block_desc.cc:209] Flush reshape2_43.tmp_1
I1018 08:53:57.728168 4097533 var_desc.cc:415] Flush  reshape2_43.tmp_1 1
I1018 08:53:57.728171 4097533 block_desc.cc:209] Flush batch_norm_10.tmp_2
I1018 08:53:57.728173 4097533 var_desc.cc:415] Flush  batch_norm_10.tmp_2 1
I1018 08:53:57.728175 4097533 block_desc.cc:209] Flush tmp_72
I1018 08:53:57.728178 4097533 var_desc.cc:415] Flush  tmp_72 1
I1018 08:53:57.728180 4097533 block_desc.cc:209] Flush elementwise_pow_40.tmp_0
I1018 08:53:57.728183 4097533 var_desc.cc:415] Flush  elementwise_pow_40.tmp_0 1
I1018 08:53:57.728185 4097533 block_desc.cc:209] Flush tmp_71
I1018 08:53:57.728188 4097533 var_desc.cc:415] Flush  tmp_71 1
I1018 08:53:57.728189 4097533 block_desc.cc:209] Flush reshape2_41.tmp_1
I1018 08:53:57.728191 4097533 var_desc.cc:415] Flush  reshape2_41.tmp_1 1
I1018 08:53:57.728194 4097533 block_desc.cc:209] Flush fill_constant_85.tmp_0
I1018 08:53:57.728196 4097533 var_desc.cc:415] Flush  fill_constant_85.tmp_0 1
I1018 08:53:57.726281 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "relu_9.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 14
          dims: 14
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "assign_41.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "batch_norm2d_11.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_11.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_11.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "elementwise_pow_42.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_79.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_41.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_68"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  vars {
    name: "reshape2_42.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "tmp_70"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  vars {
    name: "tmp_69"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 14
          dims: 14
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "reshape2_43.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_11.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "conv2d_10.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 14
          dims: 14
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "fill_constant_83.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_40.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_42.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_40.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_43.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "batch_norm_10.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 14
          dims: 14
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "tmp_72"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 14
          dims: 14
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "elementwise_pow_40.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_71"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 14
          dims: 14
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "reshape2_41.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_85.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_11.w_2"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_11.w_2"
    }
    outputs {
      parameter: "Out"
      arguments: "reshape2_41.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_41.tmp_1"
    }
    type: "reshape2"
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 124, in composite_batchnorm"
      strings: "    (reshape(run_var, stats_shape) + epsilon), half"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/manipulation.py\", line 4518, in reshape"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 1
      ints: 256
      ints: 1
      ints: 1
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "BiasTensor"
    }
    inputs {
      parameter: "ScaleTensor"
    }
    inputs {
      parameter: "X"
      arguments: "reshape2_41.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_70"
    }
    type: "scale"
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "bias"
      type: FLOAT
      f: 1e-05
    }
    attrs {
      name: "bias_after_scale"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 124, in composite_batchnorm"
      strings: "    (reshape(run_var, stats_shape) + epsilon), half"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 557, in __impl__"
      strings: "    return scalar_method(self, other_var)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 530, in _scalar_add_"
      strings: "    return _scalar_op_(var, 1.0, value)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 451, in _scalar_op_"
      strings: "    block.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "scale"
      type: FLOAT
      f: 1
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_11.w_1"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_11.w_1"
    }
    outputs {
      parameter: "Out"
      arguments: "reshape2_40.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_40.tmp_1"
    }
    type: "reshape2"
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 123, in composite_batchnorm"
      strings: "    x_hat = (x - reshape(run_mean, stats_shape)) * pow("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/manipulation.py\", line 4518, in reshape"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 1
      ints: 256
      ints: 1
      ints: 1
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    
I1018 08:53:57.729380 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.729542 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_99.tmp_0, is_only_used_internal: 1
I1018 08:53:57.729544 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_99.tmp_0
I1018 08:53:57.729547 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_91.tmp_0, is_only_used_internal: 1
I1018 08:53:57.729550 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_91.tmp_0
I1018 08:53:57.729552 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_81, is_only_used_internal: 1
I1018 08:53:57.729554 4097533 build_cinn_pass.cc:562] insert internal var: tmp_81
I1018 08:53:57.729557 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_4, is_only_used_internal: 1
I1018 08:53:57.729558 4097533 build_cinn_pass.cc:562] insert internal var: tmp_4
I1018 08:53:57.729561 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_74, is_only_used_internal: 1
I1018 08:53:57.729563 4097533 build_cinn_pass.cc:562] insert internal var: tmp_74
I1018 08:53:57.729565 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_51.tmp_1
I1018 08:53:57.729568 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_51.tmp_0, is_only_used_internal: 1
I1018 08:53:57.729569 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_51.tmp_0
I1018 08:53:57.729573 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_84, is_only_used_internal: 1
I1018 08:53:57.729574 4097533 build_cinn_pass.cc:562] insert internal var: tmp_84
I1018 08:53:57.729576 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_50.tmp_0, is_only_used_internal: 1
I1018 08:53:57.729578 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_50.tmp_0
I1018 08:53:57.729580 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: elementwise_pow_50.tmp_0, is_only_used_internal: 1
I1018 08:53:57.729583 4097533 build_cinn_pass.cc:562] insert internal var: elementwise_pow_50.tmp_0
I1018 08:53:57.729584 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_76, is_only_used_internal: 1
I1018 08:53:57.729588 4097533 build_cinn_pass.cc:562] insert internal var: tmp_76
I1018 08:53:57.729589 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_93.tmp_0, is_only_used_internal: 1
I1018 08:53:57.729591 4097533 build_cinn_pass.cc:557] var_node->outputs[1]: fill_constant_93.tmp_0, is_only_used_internal: 1
I1018 08:53:57.729593 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_93.tmp_0
I1018 08:53:57.729595 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: batch_norm_12.tmp_2, is_only_used_internal: 1
I1018 08:53:57.729597 4097533 build_cinn_pass.cc:562] insert internal var: batch_norm_12.tmp_2
I1018 08:53:57.729604 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_48.tmp_1
I1018 08:53:57.729605 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_97.tmp_0, is_only_used_internal: 1
I1018 08:53:57.729607 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_97.tmp_0
I1018 08:53:57.729610 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_75, is_only_used_internal: 1
I1018 08:53:57.729612 4097533 build_cinn_pass.cc:562] insert internal var: tmp_75
I1018 08:53:57.729614 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_48.tmp_0, is_only_used_internal: 1
I1018 08:53:57.729616 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_48.tmp_0
I1018 08:53:57.729619 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_45.tmp_1
I1018 08:53:57.729621 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_45.tmp_0, is_only_used_internal: 1
I1018 08:53:57.729624 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_45.tmp_0
I1018 08:53:57.729625 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: elementwise_pow_44.tmp_0, is_only_used_internal: 1
I1018 08:53:57.729627 4097533 build_cinn_pass.cc:562] insert internal var: elementwise_pow_44.tmp_0
I1018 08:53:57.729629 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_80, is_only_used_internal: 1
I1018 08:53:57.729631 4097533 build_cinn_pass.cc:562] insert internal var: tmp_80
I1018 08:53:57.729635 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_44.tmp_0, is_only_used_internal: 1
I1018 08:53:57.729636 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_44.tmp_0
I1018 08:53:57.729638 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_50.tmp_1
I1018 08:53:57.729640 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_46.tmp_0, is_only_used_internal: 1
I1018 08:53:57.729642 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_46.tmp_0
I1018 08:53:57.729645 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_82, is_only_used_internal: 1
I1018 08:53:57.729646 4097533 build_cinn_pass.cc:562] insert internal var: tmp_82
I1018 08:53:57.729650 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_78, is_only_used_internal: 1
I1018 08:53:57.729651 4097533 build_cinn_pass.cc:562] insert internal var: tmp_78
I1018 08:53:57.729653 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_83, is_only_used_internal: 1
I1018 08:53:57.729655 4097533 build_cinn_pass.cc:562] insert internal var: tmp_83
I1018 08:53:57.729657 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_46.tmp_1
I1018 08:53:57.729660 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_77, is_only_used_internal: 1
I1018 08:53:57.729661 4097533 build_cinn_pass.cc:562] insert internal var: tmp_77
I1018 08:53:57.729664 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_49.tmp_0, is_only_used_internal: 1
I1018 08:53:57.729666 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_49.tmp_0
I1018 08:53:57.729671 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: elementwise_pow_46.tmp_0, is_only_used_internal: 1
I1018 08:53:57.729673 4097533 build_cinn_pass.cc:562] insert internal var: elementwise_pow_46.tmp_0
I1018 08:53:57.729676 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_49.tmp_1
I1018 08:53:57.729678 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: elementwise_pow_48.tmp_0, is_only_used_internal: 1
I1018 08:53:57.729681 4097533 build_cinn_pass.cc:562] insert internal var: elementwise_pow_48.tmp_0
I1018 08:53:57.729682 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_47.tmp_0, is_only_used_internal: 1
I1018 08:53:57.729684 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_47.tmp_0
I1018 08:53:57.729686 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_87.tmp_0, is_only_used_internal: 1
I1018 08:53:57.729688 4097533 build_cinn_pass.cc:557] var_node->outputs[1]: fill_constant_87.tmp_0, is_only_used_internal: 1
I1018 08:53:57.729692 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_87.tmp_0
I1018 08:53:57.729696 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_44.tmp_1
I1018 08:53:57.729697 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: batch_norm_11.tmp_2, is_only_used_internal: 1
I1018 08:53:57.729699 4097533 build_cinn_pass.cc:562] insert internal var: batch_norm_11.tmp_2
I1018 08:53:57.729702 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_47.tmp_1
I1018 08:53:57.729707 4097533 build_cinn_pass.cc:742] Cluster Ops: (fill_constant, scale, reshape2, scale, reshape2, elementwise_pow, elementwise_mul, elementwise_mul, scale, reshape2, reshape2, reshape2, elementwise_pow, reshape2, elementwise_mul, reshape2, elementwise_mul, elementwise_pow, reshape2, elementwise_sub, elementwise_add, elementwise_add, elementwise_sub, elementwise_add, elementwise_max, fill_constant, assign, fill_constant, scale, elementwise_pow, assign, fill_constant, fill_constant, )
I1018 08:53:57.729709 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_10.b_0, batch_norm2d_10.w_1, batch_norm2d_12.w_1, conv2d_12.tmp_0, batch_norm2d_12.w_2, batch_norm2d_10.w_0, batch_norm2d_12.w_0, batch_norm2d_10.w_2, batch_norm2d_12.b_0, conv2d_11.tmp_0, )
I1018 08:53:57.729712 4097533 build_cinn_pass.cc:744] Cluster output vars: (assign_49.tmp_0, relu_10.tmp_0, assign_45.tmp_0, )
I1018 08:53:57.729714 4097533 build_cinn_pass.cc:745] Cluster internal vars: (reshape2_47.tmp_1, batch_norm_11.tmp_2, reshape2_44.tmp_1, fill_constant_87.tmp_0, reshape2_47.tmp_0, elementwise_pow_48.tmp_0, reshape2_49.tmp_1, elementwise_pow_46.tmp_0, reshape2_49.tmp_0, tmp_84, reshape2_50.tmp_0, reshape2_51.tmp_0, tmp_76, elementwise_pow_50.tmp_0, reshape2_51.tmp_1, tmp_74, tmp_81, fill_constant_91.tmp_0, fill_constant_99.tmp_0, tmp_4, fill_constant_93.tmp_0, batch_norm_12.tmp_2, reshape2_48.tmp_1, fill_constant_97.tmp_0, tmp_75, reshape2_48.tmp_0, reshape2_45.tmp_1, reshape2_45.tmp_0, elementwise_pow_44.tmp_0, tmp_80, reshape2_44.tmp_0, reshape2_50.tmp_1, reshape2_46.tmp_0, tmp_78, tmp_82, tmp_83, reshape2_46.tmp_1, tmp_77, )
I1018 08:53:57.729723 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.730077 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: conv2d_12.tmp_0
I1018 08:53:57.730082 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: conv2d_11.tmp_0
I1018 08:53:57.730085 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_12.b_0
I1018 08:53:57.730089 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_10.w_2
I1018 08:53:57.730093 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_12.w_0
I1018 08:53:57.730095 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_10.w_0
I1018 08:53:57.730099 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_12.w_1
I1018 08:53:57.730103 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_10.b_0
I1018 08:53:57.730106 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_10.w_1
I1018 08:53:57.730109 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_12.w_2
I1018 08:53:57.730113 4097533 build_cinn_pass.cc:274] Add Output Var Node: assign_45.tmp_0
I1018 08:53:57.730116 4097533 build_cinn_pass.cc:274] Add Output Var Node: assign_49.tmp_0
I1018 08:53:57.730119 4097533 build_cinn_pass.cc:274] Add Output Var Node: relu_10.tmp_0
I1018 08:53:57.730149 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_96[label="reshape2_48.tmp_0
[1,256,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_95[label="batch_norm_12.tmp_2
[1,256,14,14]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_94[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_92[label="fill_constant_93.tmp_0
[1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_93[label="batch_norm2d_10.w_0
[256]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_91[label="reshape2_44.tmp_1
[0,256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_83[label="tmp_76
[1,256,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_89[label="batch_norm2d_12.w_2
[256]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_80[label="tmp_4
[1,256,14,14]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_79[label="conv2d_12.tmp_0
[1,256,14,14]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_78[label="reshape2_47.tmp_0
[1,256,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_77[label="fill_constant_87.tmp_0
[1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_84[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_76[label="batch_norm2d_12.w_1
[256]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_75[label="tmp_81
[1,256,14,14]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_73[label="reshape2_51.tmp_0
[1,256,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_72[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_71[label="reshape2_49.tmp_0
[1,256,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_70[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_66[label="reshape2_47.tmp_1
[0,256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_63[label="elementwise_mul" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_62[label="reshape2_48.tmp_1
[0,256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_60[label="elementwise_sub" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_59[label="elementwise_mul" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_82[label="reshape2_50.tmp_0
[1,256,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_29[label="elementwise_max" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_85[label="scale" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_25[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_22[label="reshape2_45.tmp_1
[0,256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_40[label="tmp_84
[1,256,14,14]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_20[label="reshape2_50.tmp_1
[0,256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_18[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_13[label="tmp_77
[1,256,14,14]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_34[label="reshape2_49.tmp_1
[0,256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_41[label="elementwise_add" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_87[label="reshape2_51.tmp_1
[0,256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_1[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_19[label="reshape2_46.tmp_0
[1,256,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_10[label="batch_norm2d_10.w_2
[256]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_36[label="elementwise_pow" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_9[label="batch_norm2d_12.b_0
[256]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_4[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_5[label="assign_45.tmp_0
[256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_16[label="tmp_82
[1,256,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_90[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_69[label="tmp_80
[256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_64[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_58[label="scale" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_11[label="batch_norm2d_10.b_0
[256]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_68[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_28[label="reshape2_45.tmp_0
[1,256,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_26[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_23[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_33[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_6[label="relu_10.tmp_0
[1,256,14,14]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_7[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_81[label="tmp_75
[1,256,14,14]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_49[label="elementwise_pow" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_8[label="conv2d_11.tmp_0
[1,256,14,14]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_38[label="elementwise_mul" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_21[label="reshape2_44.tmp_0
[1,256,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_3[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_88[label="fill_constant_91.tmp_0
[256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_30[label="assign_49.tmp_0
[256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_2[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_31[label="elementwise_pow_44.tmp_0
[256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_17[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_32[label="elementwise_add" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_86[label="fill_constant_99.tmp_0
[1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_35[label="elementwise_add" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_53[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_24[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_37[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_39[label="tmp_74
[256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_42[label="fill_constant_97.tmp_0
[256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_43[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_15[label="tmp_83
[1,256,14,14]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_44[label="elementwise_sub" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_65[label="elementwise_pow" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_45[label="tmp_78
[1,256,14,14]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_46[label="scale" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_74[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_27[label="batch_norm_11.tmp_2
[1,256,14,14]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_47[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_48[label="batch_norm2d_12.w_0
[256]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_50[label="scale" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_67[label="batch_norm2d_10.w_1
[256]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_12[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_51[label="elementwise_pow_46.tmp_0
[1,256,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_52[label="elementwise_pow_48.tmp_0
[256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_61[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_14[label="reshape2_46.tmp_1
[0,256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_54[label="elementwise_mul" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_55[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_56[label="elementwise_pow_50.tmp_0
[1,256,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_57[label="elementwise_pow" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_67
   node_2->node_76
   node_3->node_93
   node_4->node_79
   node_5->node_24
   node_6->node_17
   node_7->node_11
   node_8->node_44
   node_9->node_37
   node_10->node_43
   node_11->node_61
   node_12->node_9
   node_13->node_38
   node_15->node_63
   node_16->node_57
   node_18->node_48
   node_19->node_38
   node_21->node_44
   node_23->node_86
   node_25->node_82
   node_25->node_20
   node_26->node_30
   node_27->node_32
   node_28->node_58
   node_29->node_6
   node_30->node_0
   node_31->node_68
   node_32->node_80
   node_33->node_8
   node_35->node_27
   node_36->node_52
   node_37->node_78
   node_37->node_66
   node_38->node_45
   node_39->node_65
   node_40->node_41
   node_41->node_95
   node_42->node_46
   node_43->node_71
   node_43->node_34
   node_44->node_81
   node_45->node_35
   node_46->node_69
   node_47->node_28
   node_47->node_22
   node_48->node_55
   node_49->node_51
   node_50->node_16
   node_51->node_59
   node_52->node_26
   node_53->node_96
   node_53->node_62
   node_54->node_15
   node_55->node_19
   node_55->node_14
   node_56->node_54
   node_57->node_56
   node_58->node_83
   node_59->node_13
   node_60->node_75
   node_61->node_73
   node_61->node_87
   node_63->node_40
   node_64->node_21
   node_64->node_91
   node_65->node_31
   node_67->node_53
   node_68->node_5
   node_69->node_36
   node_70->node_77
   node_71->node_50
   node_72->node_10
   node_73->node_41
   node_74->node_92
   node_75->node_54
   node_76->node_64
   node_77->node_49
   node_77->node_65
   node_78->node_35
   node_79->node_60
   node_80->node_29
   node_81->node_59
   node_82->node_63
   node_83->node_49
   node_84->node_89
   node_85->node_39
   node_86->node_29
   node_88->node_85
   node_89->node_47
   node_90->node_88
   node_92->node_57
   node_92->node_36
   node_93->node_25
   node_94->node_42
   node_95->node_32
   node_96->node_60
} // end G
I1018 08:53:57.730635 4097533 var_desc.cc:415] Flush  assign_45.tmp_0 1
I1018 08:53:57.730638 4097533 var_desc.cc:415] Flush  relu_10.tmp_0 1
I1018 08:53:57.730641 4097533 var_desc.cc:415] Flush  conv2d_11.tmp_0 1
I1018 08:53:57.730644 4097533 var_desc.cc:415] Flush  batch_norm2d_12.b_0 1
I1018 08:53:57.730648 4097533 var_desc.cc:415] Flush  batch_norm2d_10.w_2 1
I1018 08:53:57.730650 4097533 var_desc.cc:415] Flush  batch_norm2d_10.b_0 1
I1018 08:53:57.730652 4097533 var_desc.cc:415] Flush  tmp_77 1
I1018 08:53:57.730655 4097533 var_desc.cc:415] Flush  reshape2_46.tmp_1 1
I1018 08:53:57.730657 4097533 var_desc.cc:415] Flush  tmp_83 1
I1018 08:53:57.730661 4097533 var_desc.cc:415] Flush  tmp_82 1
I1018 08:53:57.730664 4097533 var_desc.cc:415] Flush  reshape2_46.tmp_0 1
I1018 08:53:57.730666 4097533 var_desc.cc:415] Flush  reshape2_50.tmp_1 1
I1018 08:53:57.730669 4097533 var_desc.cc:415] Flush  reshape2_44.tmp_0 1
I1018 08:53:57.730671 4097533 var_desc.cc:415] Flush  reshape2_45.tmp_1 1
I1018 08:53:57.730674 4097533 var_desc.cc:415] Flush  batch_norm_11.tmp_2 1
I1018 08:53:57.730677 4097533 var_desc.cc:415] Flush  reshape2_45.tmp_0 1
I1018 08:53:57.730679 4097533 var_desc.cc:415] Flush  assign_49.tmp_0 1
I1018 08:53:57.730682 4097533 var_desc.cc:415] Flush  elementwise_pow_44.tmp_0 1
I1018 08:53:57.730685 4097533 var_desc.cc:415] Flush  reshape2_49.tmp_1 1
I1018 08:53:57.730688 4097533 var_desc.cc:415] Flush  tmp_74 1
I1018 08:53:57.730691 4097533 var_desc.cc:415] Flush  tmp_84 1
I1018 08:53:57.730693 4097533 var_desc.cc:415] Flush  fill_constant_97.tmp_0 1
I1018 08:53:57.730696 4097533 var_desc.cc:415] Flush  tmp_78 1
I1018 08:53:57.730698 4097533 var_desc.cc:415] Flush  batch_norm2d_12.w_0 1
I1018 08:53:57.730701 4097533 var_desc.cc:415] Flush  elementwise_pow_46.tmp_0 1
I1018 08:53:57.730705 4097533 var_desc.cc:415] Flush  elementwise_pow_48.tmp_0 1
I1018 08:53:57.730706 4097533 var_desc.cc:415] Flush  elementwise_pow_50.tmp_0 1
I1018 08:53:57.730710 4097533 var_desc.cc:415] Flush  reshape2_48.tmp_1 1
I1018 08:53:57.730712 4097533 var_desc.cc:415] Flush  reshape2_47.tmp_1 1
I1018 08:53:57.730715 4097533 var_desc.cc:415] Flush  batch_norm2d_10.w_1 1
I1018 08:53:57.730717 4097533 var_desc.cc:415] Flush  tmp_80 1
I1018 08:53:57.730719 4097533 var_desc.cc:415] Flush  reshape2_49.tmp_0 1
I1018 08:53:57.730722 4097533 var_desc.cc:415] Flush  reshape2_51.tmp_0 1
I1018 08:53:57.730726 4097533 var_desc.cc:415] Flush  tmp_81 1
I1018 08:53:57.730728 4097533 var_desc.cc:415] Flush  batch_norm2d_12.w_1 1
I1018 08:53:57.730731 4097533 var_desc.cc:415] Flush  fill_constant_87.tmp_0 1
I1018 08:53:57.730733 4097533 var_desc.cc:415] Flush  reshape2_47.tmp_0 1
I1018 08:53:57.730736 4097533 var_desc.cc:415] Flush  conv2d_12.tmp_0 1
I1018 08:53:57.730738 4097533 var_desc.cc:415] Flush  tmp_4 1
I1018 08:53:57.730741 4097533 var_desc.cc:415] Flush  tmp_75 1
I1018 08:53:57.730743 4097533 var_desc.cc:415] Flush  reshape2_50.tmp_0 1
I1018 08:53:57.730746 4097533 var_desc.cc:415] Flush  tmp_76 1
I1018 08:53:57.730748 4097533 var_desc.cc:415] Flush  fill_constant_99.tmp_0 1
I1018 08:53:57.730751 4097533 var_desc.cc:415] Flush  reshape2_51.tmp_1 1
I1018 08:53:57.730753 4097533 var_desc.cc:415] Flush  fill_constant_91.tmp_0 1
I1018 08:53:57.730756 4097533 var_desc.cc:415] Flush  batch_norm2d_12.w_2 1
I1018 08:53:57.730758 4097533 var_desc.cc:415] Flush  reshape2_44.tmp_1 1
I1018 08:53:57.730762 4097533 var_desc.cc:415] Flush  fill_constant_93.tmp_0 1
I1018 08:53:57.730763 4097533 var_desc.cc:415] Flush  batch_norm2d_10.w_0 1
I1018 08:53:57.730768 4097533 var_desc.cc:415] Flush  batch_norm_12.tmp_2 1
I1018 08:53:57.730770 4097533 var_desc.cc:415] Flush  reshape2_48.tmp_0 1
I1018 08:53:57.730796 4097533 graph_helper.h:104] adj assign0x55b7d0beddc0 -> fetch0x55b7d0c39e20  via assign_49.tmp_00x55b7d0b9f2e0
I1018 08:53:57.730805 4097533 graph_helper.h:104] adj elementwise_max0x55b7d09c4330 -> fetch0x55b7d0c3a1f0  via relu_10.tmp_00x55b7d0b9f610
I1018 08:53:57.730809 4097533 graph_helper.h:104] adj assign0x55b7d0f4c670 -> fetch0x55b7d0c39a70  via assign_45.tmp_00x55b7d0b9f950
I1018 08:53:57.730813 4097533 graph_helper.h:104] adj feed0x55b7d0b66040 -> reshape20x55b7d0fcbf40  via batch_norm2d_10.w_00x55b7d0c2ca90
I1018 08:53:57.730823 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0f7c280 -> assign0x55b7d0beddc0  via elementwise_pow_48.tmp_00x55b7d0b96610
I1018 08:53:57.730829 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0b64a40 -> elementwise_max0x55b7d09c4330  via tmp_40x55b7d1006f90
I1018 08:53:57.730834 4097533 graph_helper.h:104] adj fill_constant0x55b7d0ba9260 -> elementwise_max0x55b7d09c4330  via fill_constant_99.tmp_00x55b7d1006c20
I1018 08:53:57.730840 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0c40c60 -> elementwise_add0x55b7d0b64a40  via batch_norm_11.tmp_20x55b7d0b960d0
I1018 08:53:57.730845 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0c38810 -> elementwise_add0x55b7d0b64a40  via batch_norm_12.tmp_20x55b7d1007640
I1018 08:53:57.730851 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0ab5cb0 -> elementwise_add0x55b7d0c40c60  via tmp_780x55b7d0be4050
I1018 08:53:57.730856 4097533 graph_helper.h:104] adj reshape20x55b7d099e3c0 -> elementwise_add0x55b7d0c40c60  via reshape2_47.tmp_00x55b7d0b964d0
I1018 08:53:57.730862 4097533 graph_helper.h:104] adj scale0x55b7d0c277d0 -> elementwise_pow0x55b7d0f7c280  via tmp_800x55b7d0a606a0
I1018 08:53:57.730867 4097533 graph_helper.h:104] adj fill_constant0x55b7d10976a0 -> elementwise_pow0x55b7d0f7c280  via fill_constant_93.tmp_00x55b7d10072d0
I1018 08:53:57.730873 4097533 graph_helper.h:104] adj feed0x55b7d0b65590 -> reshape20x55b7d099e3c0  via batch_norm2d_12.b_00x55b7d0c2d4e0
I1018 08:53:57.730880 4097533 graph_helper.h:104] adj reshape20x55b7d0fcd1d0 -> elementwise_mul0x55b7d0ab5cb0  via reshape2_46.tmp_00x55b7d0be3cd0
I1018 08:53:57.730885 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0afcc70 -> elementwise_mul0x55b7d0ab5cb0  via tmp_770x55b7d0be4d80
I1018 08:53:57.730890 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0c28a40 -> elementwise_add0x55b7d0c38810  via tmp_840x55b7d0b29ae0
I1018 08:53:57.730896 4097533 graph_helper.h:104] adj reshape20x55b7d0c24370 -> elementwise_add0x55b7d0c38810  via reshape2_51.tmp_00x55b7d0b2a0a0
I1018 08:53:57.730901 4097533 graph_helper.h:104] adj feed0x55b7d0b65920 -> reshape20x55b7d0c1dfa0  via batch_norm2d_10.w_20x55b7d0c2d170
I1018 08:53:57.730908 4097533 graph_helper.h:104] adj reshape20x55b7d0c21b20 -> elementwise_sub0x55b7d0c538e0  via reshape2_44.tmp_00x55b7d0a609d0
I1018 08:53:57.730914 4097533 graph_helper.h:104] adj feed0x55b7d0b65200 -> elementwise_sub0x55b7d0c538e0  via conv2d_11.tmp_00x55b7d0c2d850
I1018 08:53:57.730919 4097533 graph_helper.h:104] adj fill_constant0x55b7d0c2aad0 -> scale0x55b7d0c277d0  via fill_constant_97.tmp_00x55b7d1007d30
I1018 08:53:57.730926 4097533 graph_helper.h:104] adj feed0x55b7d0c396c0 -> reshape20x55b7d0c1ddc0  via batch_norm2d_12.w_20x55b7d0c2c720
I1018 08:53:57.730932 4097533 graph_helper.h:104] adj scale0x55b7d1008390 -> elementwise_pow0x55b7d0c49a60  via tmp_760x55b7d0b2a3a0
I1018 08:53:57.730938 4097533 graph_helper.h:104] adj fill_constant0x55b7d108c790 -> elementwise_pow0x55b7d0c49a60  via fill_constant_87.tmp_00x55b7d0b96370
I1018 08:53:57.730943 4097533 graph_helper.h:104] adj reshape20x55b7d0c1dfa0 -> scale0x55b7d0c20e40  via reshape2_49.tmp_00x55b7d0c0f200
I1018 08:53:57.730949 4097533 graph_helper.h:104] adj feed0x55b7d0c392d0 -> reshape20x55b7d0c47790  via batch_norm2d_10.w_10x55b7d0be5430
I1018 08:53:57.730957 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d0c2ae60 -> elementwise_mul0x55b7d0c13380  via tmp_810x55b7d0b2b120
I1018 08:53:57.730963 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0c41810 -> elementwise_mul0x55b7d0c13380  via elementwise_pow_50.tmp_00x55b7d0b2a6e0
I1018 08:53:57.730969 4097533 graph_helper.h:104] adj feed0x55b7d0b65cb0 -> reshape20x55b7d0fcd1d0  via batch_norm2d_12.w_00x55b7d0c2ce00
I1018 08:53:57.730975 4097533 graph_helper.h:104] adj scale0x55b7d0c20e40 -> elementwise_pow0x55b7d0c41810  via tmp_820x55b7d0be4390
I1018 08:53:57.730981 4097533 graph_helper.h:104] adj fill_constant0x55b7d10976a0 -> elementwise_pow0x55b7d0c41810  via fill_constant_93.tmp_00x55b7d10072d0
I1018 08:53:57.730986 4097533 graph_helper.h:104] adj reshape20x55b7d0c1ddc0 -> scale0x55b7d1008390  via reshape2_45.tmp_00x55b7d0a5ff90
I1018 08:53:57.730993 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d0c538e0 -> elementwise_mul0x55b7d0afcc70  via tmp_750x55b7d10080a0
I1018 08:53:57.730998 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0c49a60 -> elementwise_mul0x55b7d0afcc70  via elementwise_pow_46.tmp_00x55b7d0c0eee0
I1018 08:53:57.731003 4097533 graph_helper.h:104] adj reshape20x55b7d0c47790 -> elementwise_sub0x55b7d0c2ae60  via reshape2_48.tmp_00x55b7d0a5f8a0
I1018 08:53:57.731009 4097533 graph_helper.h:104] adj feed0x55b7d0b64e90 -> elementwise_sub0x55b7d0c2ae60  via conv2d_12.tmp_00x55b7d0c2c3e0
I1018 08:53:57.731014 4097533 graph_helper.h:104] adj feed0x55b7d0c38ee0 -> reshape20x55b7d0c24370  via batch_norm2d_10.b_00x55b7d0be50c0
I1018 08:53:57.731020 4097533 graph_helper.h:104] adj reshape20x55b7d0fcbf40 -> elementwise_mul0x55b7d0c28a40  via reshape2_50.tmp_00x55b7d0b29da0
I1018 08:53:57.731025 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0c13380 -> elementwise_mul0x55b7d0c28a40  via tmp_830x55b7d0be46d0
I1018 08:53:57.731031 4097533 graph_helper.h:104] adj feed0x55b7d0b663d0 -> reshape20x55b7d0c21b20  via batch_norm2d_12.w_10x55b7d0c2c090
I1018 08:53:57.731038 4097533 graph_helper.h:104] adj scale0x55b7d0ff0ee0 -> elementwise_pow0x55b7d0f86f70  via tmp_740x55b7d0b2adf0
I1018 08:53:57.731043 4097533 graph_helper.h:104] adj fill_constant0x55b7d108c790 -> elementwise_pow0x55b7d0f86f70  via fill_constant_87.tmp_00x55b7d0b96370
I1018 08:53:57.731050 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0f86f70 -> assign0x55b7d0f4c670  via elementwise_pow_44.tmp_00x55b7d0a60310
I1018 08:53:57.731055 4097533 graph_helper.h:104] adj fill_constant0x55b7d0987c60 -> scale0x55b7d0ff0ee0  via fill_constant_91.tmp_00x55b7d10068b0
I1018 08:53:57.731309 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.731312 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731321 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.731325 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731339 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.731341 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731357 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.731360 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731367 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.731370 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731392 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.731395 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731402 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.731405 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731412 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.731415 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731418 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.731421 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731459 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.731462 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731468 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.731472 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731482 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.731484 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731487 4097533 graph_helper.cc:698] convert op node to desc elementwise_sub
I1018 08:53:57.731490 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731498 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.731500 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731508 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.731509 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731513 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.731515 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731523 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.731525 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731597 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.731600 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731607 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.731609 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731612 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.731616 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731622 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.731626 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731628 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.731631 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731638 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.731640 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731647 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.731650 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731657 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.731659 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731662 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.731665 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731673 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.731674 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731678 4097533 graph_helper.cc:698] convert op node to desc elementwise_sub
I1018 08:53:57.731680 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731688 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.731690 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731696 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.731699 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731705 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.731709 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731715 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.731717 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731720 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.731724 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731855 4097533 graph_helper.cc:698] convert op node to desc elementwise_add
I1018 08:53:57.731858 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731865 4097533 graph_helper.cc:698] convert op node to desc elementwise_add
I1018 08:53:57.731868 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731875 4097533 graph_helper.cc:698] convert op node to desc elementwise_add
I1018 08:53:57.731878 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731884 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.731887 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731894 4097533 graph_helper.cc:698] convert op node to desc elementwise_max
I1018 08:53:57.731897 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731904 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.731906 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731912 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.731915 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731925 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.731927 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731933 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.731936 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731942 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.731945 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731952 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.731954 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731958 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.731961 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.731963 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.731966 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.733126 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.734006 4097533 block_desc.cc:205] vars in desc 51
I1018 08:53:57.734014 4097533 block_desc.cc:209] Flush assign_45.tmp_0
I1018 08:53:57.734017 4097533 var_desc.cc:415] Flush  assign_45.tmp_0 1
I1018 08:53:57.734020 4097533 block_desc.cc:209] Flush relu_10.tmp_0
I1018 08:53:57.734022 4097533 var_desc.cc:415] Flush  relu_10.tmp_0 1
I1018 08:53:57.734025 4097533 block_desc.cc:209] Flush conv2d_11.tmp_0
I1018 08:53:57.734027 4097533 var_desc.cc:415] Flush  conv2d_11.tmp_0 1
I1018 08:53:57.734030 4097533 block_desc.cc:209] Flush batch_norm2d_12.b_0
I1018 08:53:57.734032 4097533 var_desc.cc:415] Flush  batch_norm2d_12.b_0 1
I1018 08:53:57.734035 4097533 block_desc.cc:209] Flush batch_norm2d_10.w_2
I1018 08:53:57.734037 4097533 var_desc.cc:415] Flush  batch_norm2d_10.w_2 1
I1018 08:53:57.734040 4097533 block_desc.cc:209] Flush batch_norm2d_10.b_0
I1018 08:53:57.734042 4097533 var_desc.cc:415] Flush  batch_norm2d_10.b_0 1
I1018 08:53:57.734045 4097533 block_desc.cc:209] Flush tmp_77
I1018 08:53:57.734047 4097533 var_desc.cc:415] Flush  tmp_77 1
I1018 08:53:57.734050 4097533 block_desc.cc:209] Flush reshape2_46.tmp_1
I1018 08:53:57.734052 4097533 var_desc.cc:415] Flush  reshape2_46.tmp_1 1
I1018 08:53:57.734054 4097533 block_desc.cc:209] Flush tmp_83
I1018 08:53:57.734057 4097533 var_desc.cc:415] Flush  tmp_83 1
I1018 08:53:57.734059 4097533 block_desc.cc:209] Flush tmp_82
I1018 08:53:57.734061 4097533 var_desc.cc:415] Flush  tmp_82 1
I1018 08:53:57.734064 4097533 block_desc.cc:209] Flush reshape2_46.tmp_0
I1018 08:53:57.734066 4097533 var_desc.cc:415] Flush  reshape2_46.tmp_0 1
I1018 08:53:57.734069 4097533 block_desc.cc:209] Flush reshape2_50.tmp_1
I1018 08:53:57.734071 4097533 var_desc.cc:415] Flush  reshape2_50.tmp_1 1
I1018 08:53:57.734073 4097533 block_desc.cc:209] Flush reshape2_44.tmp_0
I1018 08:53:57.734076 4097533 var_desc.cc:415] Flush  reshape2_44.tmp_0 1
I1018 08:53:57.734078 4097533 block_desc.cc:209] Flush reshape2_45.tmp_1
I1018 08:53:57.734081 4097533 var_desc.cc:415] Flush  reshape2_45.tmp_1 1
I1018 08:53:57.734082 4097533 block_desc.cc:209] Flush batch_norm_11.tmp_2
I1018 08:53:57.734086 4097533 var_desc.cc:415] Flush  batch_norm_11.tmp_2 1
I1018 08:53:57.734087 4097533 block_desc.cc:209] Flush reshape2_45.tmp_0
I1018 08:53:57.734089 4097533 var_desc.cc:415] Flush  reshape2_45.tmp_0 1
I1018 08:53:57.734092 4097533 block_desc.cc:209] Flush assign_49.tmp_0
I1018 08:53:57.734094 4097533 var_desc.cc:415] Flush  assign_49.tmp_0 1
I1018 08:53:57.734097 4097533 block_desc.cc:209] Flush elementwise_pow_44.tmp_0
I1018 08:53:57.734099 4097533 var_desc.cc:415] Flush  elementwise_pow_44.tmp_0 1
I1018 08:53:57.734102 4097533 block_desc.cc:209] Flush reshape2_49.tmp_1
I1018 08:53:57.734103 4097533 var_desc.cc:415] Flush  reshape2_49.tmp_1 1
I1018 08:53:57.734107 4097533 block_desc.cc:209] Flush tmp_74
I1018 08:53:57.734108 4097533 var_desc.cc:415] Flush  tmp_74 1
I1018 08:53:57.734112 4097533 block_desc.cc:209] Flush tmp_84
I1018 08:53:57.734113 4097533 var_desc.cc:415] Flush  tmp_84 1
I1018 08:53:57.734115 4097533 block_desc.cc:209] Flush fill_constant_97.tmp_0
I1018 08:53:57.734118 4097533 var_desc.cc:415] Flush  fill_constant_97.tmp_0 1
I1018 08:53:57.734124 4097533 block_desc.cc:209] Flush tmp_78
I1018 08:53:57.734126 4097533 var_desc.cc:415] Flush  tmp_78 1
I1018 08:53:57.734129 4097533 block_desc.cc:209] Flush batch_norm2d_12.w_0
I1018 08:53:57.734131 4097533 var_desc.cc:415] Flush  batch_norm2d_12.w_0 1
I1018 08:53:57.734133 4097533 block_desc.cc:209] Flush elementwise_pow_46.tmp_0
I1018 08:53:57.734135 4097533 var_desc.cc:415] Flush  elementwise_pow_46.tmp_0 1
I1018 08:53:57.734138 4097533 block_desc.cc:209] Flush elementwise_pow_48.tmp_0
I1018 08:53:57.734140 4097533 var_desc.cc:415] Flush  elementwise_pow_48.tmp_0 1
I1018 08:53:57.734143 4097533 block_desc.cc:209] Flush elementwise_pow_50.tmp_0
I1018 08:53:57.734144 4097533 var_desc.cc:415] Flush  elementwise_pow_50.tmp_0 1
I1018 08:53:57.734148 4097533 block_desc.cc:209] Flush reshape2_48.tmp_1
I1018 08:53:57.734149 4097533 var_desc.cc:415] Flush  reshape2_48.tmp_1 1
I1018 08:53:57.734153 4097533 block_desc.cc:209] Flush reshape2_47.tmp_1
I1018 08:53:57.734154 4097533 var_desc.cc:415] Flush  reshape2_47.tmp_1 1
I1018 08:53:57.734156 4097533 block_desc.cc:209] Flush batch_norm2d_10.w_1
I1018 08:53:57.734158 4097533 var_desc.cc:415] Flush  batch_norm2d_10.w_1 1
I1018 08:53:57.734161 4097533 block_desc.cc:209] Flush tmp_80
I1018 08:53:57.734163 4097533 var_desc.cc:415] Flush  tmp_80 1
I1018 08:53:57.734165 4097533 block_desc.cc:209] Flush reshape2_49.tmp_0
I1018 08:53:57.734169 4097533 var_desc.cc:415] Flush  reshape2_49.tmp_0 1
I1018 08:53:57.734170 4097533 block_desc.cc:209] Flush reshape2_51.tmp_0
I1018 08:53:57.734172 4097533 var_desc.cc:415] Flush  reshape2_51.tmp_0 1
I1018 08:53:57.734175 4097533 block_desc.cc:209] Flush tmp_81
I1018 08:53:57.734177 4097533 var_desc.cc:415] Flush  tmp_81 1
I1018 08:53:57.734179 4097533 block_desc.cc:209] Flush batch_norm2d_12.w_1
I1018 08:53:57.734181 4097533 var_desc.cc:415] Flush  batch_norm2d_12.w_1 1
I1018 08:53:57.734184 4097533 block_desc.cc:209] Flush fill_constant_87.tmp_0
I1018 08:53:57.734186 4097533 var_desc.cc:415] Flush  fill_constant_87.tmp_0 1
I1018 08:53:57.734189 4097533 block_desc.cc:209] Flush reshape2_47.tmp_0
I1018 08:53:57.734191 4097533 var_desc.cc:415] Flush  reshape2_47.tmp_0 1
I1018 08:53:57.734194 4097533 block_desc.cc:209] Flush conv2d_12.tmp_0
I1018 08:53:57.734195 4097533 var_desc.cc:415] Flush  conv2d_12.tmp_0 1
I1018 08:53:57.734198 4097533 block_desc.cc:209] Flush tmp_4
I1018 08:53:57.734200 4097533 var_desc.cc:415] Flush  tmp_4 1
I1018 08:53:57.734202 4097533 block_desc.cc:209] Flush tmp_75
I1018 08:53:57.734205 4097533 var_desc.cc:415] Flush  tmp_75 1
I1018 08:53:57.734207 4097533 block_desc.cc:209] Flush reshape2_50.tmp_0
I1018 08:53:57.734210 4097533 var_desc.cc:415] Flush  reshape2_50.tmp_0 1
I1018 08:53:57.734212 4097533 block_desc.cc:209] Flush tmp_76
I1018 08:53:57.734215 4097533 var_desc.cc:415] Flush  tmp_76 1
I1018 08:53:57.734216 4097533 block_desc.cc:209] Flush fill_constant_99.tmp_0
I1018 08:53:57.734220 4097533 var_desc.cc:415] Flush  fill_constant_99.tmp_0 1
I1018 08:53:57.734221 4097533 block_desc.cc:209] Flush reshape2_51.tmp_1
I1018 08:53:57.734223 4097533 var_desc.cc:415] Flush  reshape2_51.tmp_1 1
I1018 08:53:57.734226 4097533 block_desc.cc:209] Flush fill_constant_91.tmp_0
I1018 08:53:57.734228 4097533 var_desc.cc:415] Flush  fill_constant_91.tmp_0 1
I1018 08:53:57.734230 4097533 block_desc.cc:209] Flush batch_norm2d_12.w_2
I1018 08:53:57.734232 4097533 var_desc.cc:415] Flush  batch_norm2d_12.w_2 1
I1018 08:53:57.734236 4097533 block_desc.cc:209] Flush reshape2_44.tmp_1
I1018 08:53:57.734237 4097533 var_desc.cc:415] Flush  reshape2_44.tmp_1 1
I1018 08:53:57.734241 4097533 block_desc.cc:209] Flush fill_constant_93.tmp_0
I1018 08:53:57.734242 4097533 var_desc.cc:415] Flush  fill_constant_93.tmp_0 1
I1018 08:53:57.734244 4097533 block_desc.cc:209] Flush batch_norm2d_10.w_0
I1018 08:53:57.734246 4097533 var_desc.cc:415] Flush  batch_norm2d_10.w_0 1
I1018 08:53:57.734249 4097533 block_desc.cc:209] Flush batch_norm_12.tmp_2
I1018 08:53:57.734251 4097533 var_desc.cc:415] Flush  batch_norm_12.tmp_2 1
I1018 08:53:57.734256 4097533 block_desc.cc:209] Flush reshape2_48.tmp_0
I1018 08:53:57.734257 4097533 var_desc.cc:415] Flush  reshape2_48.tmp_0 1
I1018 08:53:57.730626 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "assign_45.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "relu_10.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 14
          dims: 14
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "conv2d_11.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 14
          dims: 14
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_12.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_10.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "batch_norm2d_10.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "tmp_77"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 14
          dims: 14
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "reshape2_46.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "tmp_83"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 14
          dims: 14
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "tmp_82"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  vars {
    name: "reshape2_46.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_50.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_44.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_45.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "batch_norm_11.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 14
          dims: 14
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_45.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "assign_49.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "elementwise_pow_44.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_49.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_74"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  vars {
    name: "tmp_84"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 14
          dims: 14
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "fill_constant_97.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_78"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 14
          dims: 14
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_12.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "elementwise_pow_46.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "elementwise_pow_48.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "elementwise_pow_50.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_48.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_47.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_10.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "tmp_80"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  vars {
    name: "reshape2_49.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_51.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "tmp_81"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 14
          dims: 14
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_12.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "fill_constant_87.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_47.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "conv2d_12.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 14
          dims: 14
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "tmp_4"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 14
          dims: 14
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "tmp_75"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 14
          dims: 14
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "reshape2_50.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "tmp_76"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  vars {
    name: "fill_constant_99.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_51.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "fill_constant_91.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "batch_norm2d_12.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "reshape2_44.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_93.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "batch_norm2d_10.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "batch_norm_12.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 14
          dims: 14
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_48.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  ops {
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "ShapeTensorList"
    }
    inputs {
      parameter: "ValueTensor"
    }
    outputs {
      parameter: "Out"
      arguments: "fill_constant_97.tmp_0"
    }
    type: "fill_constant"
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "dtype"
      type: INT
      i: 5
    }
    attrs {
      name: "force_cpu"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 118, in composite_batchnorm"
      strings: "    batch_var = zeros(run_var.shape, run_var.dtype)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 1117, in zeros"
      strings: "    return fill_constant(value=0.0, shape=shape, dtype=dtype, name=name)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 977, in fill_constant"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "place_type"
      type: INT
      i: -1
    }
    attrs {
      name: "shape"
      type: LONGS
      longs: 256
    }
    attrs {
      name: "str_value"
      type: STRING
      s: "0.0"
    }
    attrs {
      name: "value"
      type: FLOAT
      f: 0
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "BiasTensor"
    }
    inputs {
      parameter: "ScaleTensor"
    }
    inputs {
      parameter: "X"
      arguments: "fill_constant_97.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_80"
    }
    type: "scale"
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "bias"
      type: FLOAT
      f: 1e-05
    }
    attrs {
      name: "bias_after_scale"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 119, in composite_batchnorm"
      strings: "    inv_std = pow((batch_var + epsilon), half)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 557, in __impl__"
      strings: "    return scalar_method(self, other_var)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 530, in _scalar_add_"
      strings: "    return _scalar_op_(var, 1.0, value)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 451, in _scalar_op_"
      strings: "    block.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "scale"
      type: FLOAT
      f: 1
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_12.w_2"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_12.w_2"
    }
    outputs {
      parameter: "Out"
      arguments: "reshape2_45.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_45.tmp_1"
    }
    type: "reshape2"
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchma
I1018 08:53:57.736572 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.736765 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_101.tmp_0, is_only_used_internal: 1
I1018 08:53:57.736769 4097533 build_cinn_pass.cc:557] var_node->outputs[1]: fill_constant_101.tmp_0, is_only_used_internal: 1
I1018 08:53:57.736771 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_101.tmp_0
I1018 08:53:57.736774 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: elementwise_pow_52.tmp_0, is_only_used_internal: 1
I1018 08:53:57.736776 4097533 build_cinn_pass.cc:562] insert internal var: elementwise_pow_52.tmp_0
I1018 08:53:57.736778 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_86, is_only_used_internal: 1
I1018 08:53:57.736781 4097533 build_cinn_pass.cc:562] insert internal var: tmp_86
I1018 08:53:57.736783 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_87, is_only_used_internal: 1
I1018 08:53:57.736785 4097533 build_cinn_pass.cc:562] insert internal var: tmp_87
I1018 08:53:57.736788 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_53.tmp_0, is_only_used_internal: 1
I1018 08:53:57.736789 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_53.tmp_0
I1018 08:53:57.736792 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_88, is_only_used_internal: 1
I1018 08:53:57.736794 4097533 build_cinn_pass.cc:562] insert internal var: tmp_88
I1018 08:53:57.736796 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_89, is_only_used_internal: 1
I1018 08:53:57.736799 4097533 build_cinn_pass.cc:562] insert internal var: tmp_89
I1018 08:53:57.736800 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_53.tmp_1
I1018 08:53:57.736804 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_107.tmp_0, is_only_used_internal: 1
I1018 08:53:57.736805 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_107.tmp_0
I1018 08:53:57.736807 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_52.tmp_0, is_only_used_internal: 1
I1018 08:53:57.736809 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_52.tmp_0
I1018 08:53:57.736811 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_105.tmp_0, is_only_used_internal: 1
I1018 08:53:57.736814 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_105.tmp_0
I1018 08:53:57.736816 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_52.tmp_1
I1018 08:53:57.736819 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: elementwise_pow_54.tmp_0, is_only_used_internal: 1
I1018 08:53:57.736824 4097533 build_cinn_pass.cc:562] insert internal var: elementwise_pow_54.tmp_0
I1018 08:53:57.736825 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_54.tmp_1
I1018 08:53:57.736828 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_54.tmp_0, is_only_used_internal: 1
I1018 08:53:57.736830 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_54.tmp_0
I1018 08:53:57.736832 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_90, is_only_used_internal: 1
I1018 08:53:57.736835 4097533 build_cinn_pass.cc:562] insert internal var: tmp_90
I1018 08:53:57.736837 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_55.tmp_1
I1018 08:53:57.736840 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_55.tmp_0, is_only_used_internal: 1
I1018 08:53:57.736841 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_55.tmp_0
I1018 08:53:57.736843 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: batch_norm_13.tmp_2, is_only_used_internal: 1
I1018 08:53:57.736845 4097533 build_cinn_pass.cc:562] insert internal var: batch_norm_13.tmp_2
I1018 08:53:57.736848 4097533 build_cinn_pass.cc:742] Cluster Ops: (scale, reshape2, elementwise_pow, elementwise_mul, reshape2, elementwise_mul, fill_constant, reshape2, elementwise_max, elementwise_add, fill_constant, reshape2, elementwise_sub, scale, elementwise_pow, assign, fill_constant, )
I1018 08:53:57.736852 4097533 build_cinn_pass.cc:743] Cluster input vars: (conv2d_13.tmp_0, batch_norm2d_13.w_2, batch_norm2d_13.w_1, batch_norm2d_13.w_0, batch_norm2d_13.b_0, )
I1018 08:53:57.736855 4097533 build_cinn_pass.cc:744] Cluster output vars: (assign_53.tmp_0, relu_11.tmp_0, )
I1018 08:53:57.736856 4097533 build_cinn_pass.cc:745] Cluster internal vars: (batch_norm_13.tmp_2, reshape2_55.tmp_0, reshape2_55.tmp_1, tmp_90, reshape2_54.tmp_0, reshape2_54.tmp_1, fill_constant_101.tmp_0, elementwise_pow_52.tmp_0, tmp_86, tmp_88, tmp_89, reshape2_53.tmp_1, fill_constant_107.tmp_0, tmp_87, reshape2_53.tmp_0, reshape2_52.tmp_0, fill_constant_105.tmp_0, reshape2_52.tmp_1, elementwise_pow_54.tmp_0, )
I1018 08:53:57.736865 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.737097 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: conv2d_13.tmp_0
I1018 08:53:57.737102 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_13.w_2
I1018 08:53:57.737105 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_13.b_0
I1018 08:53:57.737108 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_13.w_0
I1018 08:53:57.737111 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_13.w_1
I1018 08:53:57.737115 4097533 build_cinn_pass.cc:274] Add Output Var Node: assign_53.tmp_0
I1018 08:53:57.737119 4097533 build_cinn_pass.cc:274] Add Output Var Node: relu_11.tmp_0
I1018 08:53:57.737140 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_48[label="fill_constant_101.tmp_0
[1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_47[label="tmp_89
[1,256,14,14]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_46[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_45[label="conv2d_13.tmp_0
[1,256,14,14]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_44[label="reshape2_54.tmp_1
[0,256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_43[label="batch_norm2d_13.b_0
[256]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_42[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_39[label="reshape2_54.tmp_0
[1,256,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_37[label="relu_11.tmp_0
[1,256,14,14]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_35[label="reshape2_55.tmp_0
[1,256,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_32[label="elementwise_pow" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_31[label="fill_constant_107.tmp_0
[1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_30[label="elementwise_pow" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="batch_norm2d_13.w_0
[256]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_21[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_38[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_8[label="reshape2_53.tmp_0
[1,256,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_49[label="tmp_88
[1,256,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_7[label="fill_constant_105.tmp_0
[256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_6[label="reshape2_52.tmp_1
[0,256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_33[label="tmp_90
[1,256,14,14]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_26[label="tmp_86
[256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_11[label="elementwise_add" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_16[label="elementwise_max" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_12[label="elementwise_mul" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_5[label="elementwise_pow_54.tmp_0
[1,256,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_4[label="batch_norm2d_13.w_2
[256]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_24[label="elementwise_mul" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_27[label="scale" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_9[label="tmp_87
[1,256,14,14]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_36[label="reshape2_55.tmp_1
[0,256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_17[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_10[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_19[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_41[label="reshape2_53.tmp_1
[0,256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_34[label="batch_norm2d_13.w_1
[256]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_13[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_23[label="batch_norm_13.tmp_2
[1,256,14,14]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_14[label="assign_53.tmp_0
[256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_2[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_15[label="elementwise_sub" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_18[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_20[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_40[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_22[label="reshape2_52.tmp_0
[1,256,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_25[label="scale" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_29[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_28[label="elementwise_pow_52.tmp_0
[256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_2->node_43
   node_3->node_20
   node_4->node_10
   node_5->node_24
   node_7->node_27
   node_8->node_25
   node_9->node_24
   node_10->node_8
   node_10->node_41
   node_11->node_23
   node_12->node_33
   node_13->node_7
   node_14->node_1
   node_15->node_9
   node_16->node_37
   node_17->node_4
   node_18->node_45
   node_19->node_35
   node_19->node_36
   node_20->node_39
   node_20->node_44
   node_21->node_22
   node_21->node_6
   node_22->node_15
   node_23->node_16
   node_24->node_47
   node_25->node_49
   node_26->node_30
   node_27->node_26
   node_28->node_38
   node_29->node_31
   node_30->node_28
   node_31->node_16
   node_32->node_5
   node_33->node_11
   node_34->node_21
   node_35->node_11
   node_37->node_0
   node_38->node_14
   node_39->node_12
   node_40->node_34
   node_42->node_3
   node_43->node_19
   node_45->node_15
   node_46->node_48
   node_47->node_12
   node_48->node_32
   node_48->node_30
   node_49->node_32
} // end G
I1018 08:53:57.737421 4097533 var_desc.cc:415] Flush  batch_norm2d_13.w_0 1
I1018 08:53:57.737423 4097533 var_desc.cc:415] Flush  batch_norm2d_13.w_2 1
I1018 08:53:57.737426 4097533 var_desc.cc:415] Flush  elementwise_pow_54.tmp_0 1
I1018 08:53:57.737429 4097533 var_desc.cc:415] Flush  reshape2_52.tmp_1 1
I1018 08:53:57.737432 4097533 var_desc.cc:415] Flush  fill_constant_105.tmp_0 1
I1018 08:53:57.737434 4097533 var_desc.cc:415] Flush  reshape2_53.tmp_0 1
I1018 08:53:57.737437 4097533 var_desc.cc:415] Flush  tmp_87 1
I1018 08:53:57.737440 4097533 var_desc.cc:415] Flush  assign_53.tmp_0 1
I1018 08:53:57.737442 4097533 var_desc.cc:415] Flush  reshape2_52.tmp_0 1
I1018 08:53:57.737445 4097533 var_desc.cc:415] Flush  batch_norm_13.tmp_2 1
I1018 08:53:57.737448 4097533 var_desc.cc:415] Flush  tmp_86 1
I1018 08:53:57.737452 4097533 var_desc.cc:415] Flush  elementwise_pow_52.tmp_0 1
I1018 08:53:57.737454 4097533 var_desc.cc:415] Flush  fill_constant_107.tmp_0 1
I1018 08:53:57.737457 4097533 var_desc.cc:415] Flush  tmp_90 1
I1018 08:53:57.737459 4097533 var_desc.cc:415] Flush  batch_norm2d_13.w_1 1
I1018 08:53:57.737461 4097533 var_desc.cc:415] Flush  reshape2_55.tmp_0 1
I1018 08:53:57.737464 4097533 var_desc.cc:415] Flush  reshape2_55.tmp_1 1
I1018 08:53:57.737468 4097533 var_desc.cc:415] Flush  relu_11.tmp_0 1
I1018 08:53:57.737470 4097533 var_desc.cc:415] Flush  reshape2_54.tmp_0 1
I1018 08:53:57.737473 4097533 var_desc.cc:415] Flush  reshape2_53.tmp_1 1
I1018 08:53:57.737475 4097533 var_desc.cc:415] Flush  batch_norm2d_13.b_0 1
I1018 08:53:57.737478 4097533 var_desc.cc:415] Flush  reshape2_54.tmp_1 1
I1018 08:53:57.737480 4097533 var_desc.cc:415] Flush  conv2d_13.tmp_0 1
I1018 08:53:57.737483 4097533 var_desc.cc:415] Flush  tmp_89 1
I1018 08:53:57.737485 4097533 var_desc.cc:415] Flush  fill_constant_101.tmp_0 1
I1018 08:53:57.737488 4097533 var_desc.cc:415] Flush  tmp_88 1
I1018 08:53:57.737505 4097533 graph_helper.h:104] adj elementwise_max0x55b7d0a84e30 -> fetch0x55b7d0c75fb0  via relu_11.tmp_00x55b7d09b2ab0
I1018 08:53:57.737512 4097533 graph_helper.h:104] adj assign0x55b7d0c68d70 -> fetch0x55b7d0ca5020  via assign_53.tmp_00x55b7d09b2780
I1018 08:53:57.737516 4097533 graph_helper.h:104] adj feed0x55b7d0b841e0 -> reshape20x55b7d0c9a780  via batch_norm2d_13.w_20x55b7d104b2d0
I1018 08:53:57.737525 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0c590c0 -> elementwise_add0x55b7d0cb9390  via tmp_900x55b7d0c66420
I1018 08:53:57.737533 4097533 graph_helper.h:104] adj reshape20x55b7d0cd45f0 -> elementwise_add0x55b7d0cb9390  via reshape2_55.tmp_00x55b7d0c662a0
I1018 08:53:57.737538 4097533 graph_helper.h:104] adj reshape20x55b7d0c8c2f0 -> elementwise_mul0x55b7d0c590c0  via reshape2_54.tmp_00x55b7d0c664e0
I1018 08:53:57.737545 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0c659a0 -> elementwise_mul0x55b7d0c590c0  via tmp_890x55b7d0c69e20
I1018 08:53:57.737550 4097533 graph_helper.h:104] adj reshape20x55b7d0cd5790 -> elementwise_sub0x55b7d0cbe600  via reshape2_52.tmp_00x55b7d0bd8610
I1018 08:53:57.737555 4097533 graph_helper.h:104] adj feed0x55b7d0b83e30 -> elementwise_sub0x55b7d0cbe600  via conv2d_13.tmp_00x55b7d104af90
I1018 08:53:57.737560 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0cb9390 -> elementwise_max0x55b7d0a84e30  via batch_norm_13.tmp_20x55b7d0c661e0
I1018 08:53:57.737566 4097533 graph_helper.h:104] adj fill_constant0x55b7d0c979e0 -> elementwise_max0x55b7d0a84e30  via fill_constant_107.tmp_00x55b7d0bd7d40
I1018 08:53:57.737572 4097533 graph_helper.h:104] adj feed0x55b7d0ca45b0 -> reshape20x55b7d0cd45f0  via batch_norm2d_13.b_00x55b7d09b2410
I1018 08:53:57.737578 4097533 graph_helper.h:104] adj feed0x55b7d0ca4940 -> reshape20x55b7d0c8c2f0  via batch_norm2d_13.w_00x55b7d09b20a0
I1018 08:53:57.737586 4097533 graph_helper.h:104] adj feed0x55b7d0ca4cd0 -> reshape20x55b7d0cd5790  via batch_norm2d_13.w_10x55b7d09b1d60
I1018 08:53:57.737591 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d0cbe600 -> elementwise_mul0x55b7d0c659a0  via tmp_870x55b7d0bd8010
I1018 08:53:57.737596 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0c62920 -> elementwise_mul0x55b7d0c659a0  via elementwise_pow_54.tmp_00x55b7d104abf0
I1018 08:53:57.737602 4097533 graph_helper.h:104] adj reshape20x55b7d0c9a780 -> scale0x55b7d0ccac70  via reshape2_53.tmp_00x55b7d0bd8310
I1018 08:53:57.737608 4097533 graph_helper.h:104] adj fill_constant0x55b7d0c741b0 -> scale0x55b7d0cab0e0  via fill_constant_105.tmp_00x55b7d0bd8930
I1018 08:53:57.737614 4097533 graph_helper.h:104] adj scale0x55b7d0cab0e0 -> elementwise_pow0x55b7d1070df0  via tmp_860x55b7d0c69950
I1018 08:53:57.737620 4097533 graph_helper.h:104] adj fill_constant0x55b7d0bd6d80 -> elementwise_pow0x55b7d1070df0  via fill_constant_101.tmp_00x55b7d0c666d0
I1018 08:53:57.737625 4097533 graph_helper.h:104] adj scale0x55b7d0ccac70 -> elementwise_pow0x55b7d0c62920  via tmp_880x55b7d0c69ba0
I1018 08:53:57.737630 4097533 graph_helper.h:104] adj fill_constant0x55b7d0bd6d80 -> elementwise_pow0x55b7d0c62920  via fill_constant_101.tmp_00x55b7d0c666d0
I1018 08:53:57.737636 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d1070df0 -> assign0x55b7d0c68d70  via elementwise_pow_52.tmp_00x55b7d0c66870
I1018 08:53:57.737771 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.737774 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.737779 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.737782 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.737792 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.737794 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.737808 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.737811 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.737814 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.737818 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.737838 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.737841 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.737848 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.737851 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.737859 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.737861 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.737864 4097533 graph_helper.cc:698] convert op node to desc elementwise_sub
I1018 08:53:57.737869 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.737910 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.737912 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.737919 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.737921 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.737926 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.737928 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.737936 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.737938 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.737946 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.737948 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.737957 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.737958 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.737962 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.737964 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.737972 4097533 graph_helper.cc:698] convert op node to desc elementwise_add
I1018 08:53:57.737974 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.738046 4097533 graph_helper.cc:698] convert op node to desc elementwise_max
I1018 08:53:57.738049 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.738056 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.738058 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.738066 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.738070 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.738076 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.738080 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.738085 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.738088 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.738095 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.738098 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.738101 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.738103 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.738750 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.739207 4097533 block_desc.cc:205] vars in desc 26
I1018 08:53:57.739212 4097533 block_desc.cc:209] Flush batch_norm2d_13.w_0
I1018 08:53:57.739214 4097533 var_desc.cc:415] Flush  batch_norm2d_13.w_0 1
I1018 08:53:57.739217 4097533 block_desc.cc:209] Flush batch_norm2d_13.w_2
I1018 08:53:57.739219 4097533 var_desc.cc:415] Flush  batch_norm2d_13.w_2 1
I1018 08:53:57.739223 4097533 block_desc.cc:209] Flush elementwise_pow_54.tmp_0
I1018 08:53:57.739224 4097533 var_desc.cc:415] Flush  elementwise_pow_54.tmp_0 1
I1018 08:53:57.739228 4097533 block_desc.cc:209] Flush reshape2_52.tmp_1
I1018 08:53:57.739229 4097533 var_desc.cc:415] Flush  reshape2_52.tmp_1 1
I1018 08:53:57.739231 4097533 block_desc.cc:209] Flush fill_constant_105.tmp_0
I1018 08:53:57.739233 4097533 var_desc.cc:415] Flush  fill_constant_105.tmp_0 1
I1018 08:53:57.739236 4097533 block_desc.cc:209] Flush reshape2_53.tmp_0
I1018 08:53:57.739238 4097533 var_desc.cc:415] Flush  reshape2_53.tmp_0 1
I1018 08:53:57.739241 4097533 block_desc.cc:209] Flush tmp_87
I1018 08:53:57.739243 4097533 var_desc.cc:415] Flush  tmp_87 1
I1018 08:53:57.739245 4097533 block_desc.cc:209] Flush assign_53.tmp_0
I1018 08:53:57.739248 4097533 var_desc.cc:415] Flush  assign_53.tmp_0 1
I1018 08:53:57.739250 4097533 block_desc.cc:209] Flush reshape2_52.tmp_0
I1018 08:53:57.739252 4097533 var_desc.cc:415] Flush  reshape2_52.tmp_0 1
I1018 08:53:57.739255 4097533 block_desc.cc:209] Flush batch_norm_13.tmp_2
I1018 08:53:57.739257 4097533 var_desc.cc:415] Flush  batch_norm_13.tmp_2 1
I1018 08:53:57.739259 4097533 block_desc.cc:209] Flush tmp_86
I1018 08:53:57.739262 4097533 var_desc.cc:415] Flush  tmp_86 1
I1018 08:53:57.739264 4097533 block_desc.cc:209] Flush elementwise_pow_52.tmp_0
I1018 08:53:57.739266 4097533 var_desc.cc:415] Flush  elementwise_pow_52.tmp_0 1
I1018 08:53:57.739269 4097533 block_desc.cc:209] Flush fill_constant_107.tmp_0
I1018 08:53:57.739275 4097533 var_desc.cc:415] Flush  fill_constant_107.tmp_0 1
I1018 08:53:57.739277 4097533 block_desc.cc:209] Flush tmp_90
I1018 08:53:57.739279 4097533 var_desc.cc:415] Flush  tmp_90 1
I1018 08:53:57.739282 4097533 block_desc.cc:209] Flush batch_norm2d_13.w_1
I1018 08:53:57.739284 4097533 var_desc.cc:415] Flush  batch_norm2d_13.w_1 1
I1018 08:53:57.739286 4097533 block_desc.cc:209] Flush reshape2_55.tmp_0
I1018 08:53:57.739289 4097533 var_desc.cc:415] Flush  reshape2_55.tmp_0 1
I1018 08:53:57.739291 4097533 block_desc.cc:209] Flush reshape2_55.tmp_1
I1018 08:53:57.739293 4097533 var_desc.cc:415] Flush  reshape2_55.tmp_1 1
I1018 08:53:57.739296 4097533 block_desc.cc:209] Flush relu_11.tmp_0
I1018 08:53:57.739298 4097533 var_desc.cc:415] Flush  relu_11.tmp_0 1
I1018 08:53:57.739301 4097533 block_desc.cc:209] Flush reshape2_54.tmp_0
I1018 08:53:57.739303 4097533 var_desc.cc:415] Flush  reshape2_54.tmp_0 1
I1018 08:53:57.739305 4097533 block_desc.cc:209] Flush reshape2_53.tmp_1
I1018 08:53:57.739307 4097533 var_desc.cc:415] Flush  reshape2_53.tmp_1 1
I1018 08:53:57.739310 4097533 block_desc.cc:209] Flush batch_norm2d_13.b_0
I1018 08:53:57.739312 4097533 var_desc.cc:415] Flush  batch_norm2d_13.b_0 1
I1018 08:53:57.739315 4097533 block_desc.cc:209] Flush reshape2_54.tmp_1
I1018 08:53:57.739317 4097533 var_desc.cc:415] Flush  reshape2_54.tmp_1 1
I1018 08:53:57.739320 4097533 block_desc.cc:209] Flush conv2d_13.tmp_0
I1018 08:53:57.739321 4097533 var_desc.cc:415] Flush  conv2d_13.tmp_0 1
I1018 08:53:57.739324 4097533 block_desc.cc:209] Flush tmp_89
I1018 08:53:57.739326 4097533 var_desc.cc:415] Flush  tmp_89 1
I1018 08:53:57.739328 4097533 block_desc.cc:209] Flush fill_constant_101.tmp_0
I1018 08:53:57.739331 4097533 var_desc.cc:415] Flush  fill_constant_101.tmp_0 1
I1018 08:53:57.739333 4097533 block_desc.cc:209] Flush tmp_88
I1018 08:53:57.739336 4097533 var_desc.cc:415] Flush  tmp_88 1
I1018 08:53:57.737414 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_13.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_13.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "elementwise_pow_54.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_52.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_105.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_53.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_87"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 14
          dims: 14
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "assign_53.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_52.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "batch_norm_13.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 14
          dims: 14
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "tmp_86"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  vars {
    name: "elementwise_pow_52.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_107.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_90"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 14
          dims: 14
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_13.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "reshape2_55.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_55.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "relu_11.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 14
          dims: 14
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_54.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_53.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "batch_norm2d_13.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "reshape2_54.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "conv2d_13.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 14
          dims: 14
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "tmp_89"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 14
          dims: 14
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "fill_constant_101.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_88"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_13.w_2"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_13.w_2"
    }
    outputs {
      parameter: "Out"
      arguments: "reshape2_53.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_53.tmp_1"
    }
    type: "reshape2"
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 124, in composite_batchnorm"
      strings: "    (reshape(run_var, stats_shape) + epsilon), half"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/manipulation.py\", line 4518, in reshape"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 1
      ints: 256
      ints: 1
      ints: 1
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "BiasTensor"
    }
    inputs {
      parameter: "ScaleTensor"
    }
    inputs {
      parameter: "X"
      arguments: "reshape2_53.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_88"
    }
    type: "scale"
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "bias"
      type: FLOAT
      f: 1e-05
    }
    attrs {
      name: "bias_after_scale"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 124, in composite_batchnorm"
      strings: "    (reshape(run_var, stats_shape) + epsilon), half"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 557, in __impl__"
      strings: "    return scalar_method(self, other_var)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 530, in _scalar_add_"
      strings: "    return _scalar_op_(var, 1.0, value)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 451, in _scalar_op_"
      strings: "    block.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "scale"
      type: FLOAT
      f: 1
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_13.w_1"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_13.w_1"
    }
    outputs {
      parameter: "Out"
      arguments: "reshape2_52.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_52.tmp_1"
    }
    type: "reshape2"
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 123, in composite_batchnorm"
      strings: "    x_hat = (x - reshape(run_mean, stats_shape)) * pow("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/manipulation.py\", line 4518, in reshape"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 1
      ints: 256
      ints: 1
      ints: 1
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
I1018 08:53:57.740514 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.740654 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_57.tmp_1
I1018 08:53:57.740658 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_57.tmp_0, is_only_used_internal: 1
I1018 08:53:57.740660 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_57.tmp_0
I1018 08:53:57.740662 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_109.tmp_0, is_only_used_internal: 1
I1018 08:53:57.740665 4097533 build_cinn_pass.cc:557] var_node->outputs[1]: fill_constant_109.tmp_0, is_only_used_internal: 1
I1018 08:53:57.740667 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_109.tmp_0
I1018 08:53:57.740669 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: elementwise_pow_56.tmp_0, is_only_used_internal: 1
I1018 08:53:57.740674 4097533 build_cinn_pass.cc:562] insert internal var: elementwise_pow_56.tmp_0
I1018 08:53:57.740677 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_92, is_only_used_internal: 1
I1018 08:53:57.740679 4097533 build_cinn_pass.cc:562] insert internal var: tmp_92
I1018 08:53:57.740681 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: elementwise_pow_58.tmp_0, is_only_used_internal: 1
I1018 08:53:57.740684 4097533 build_cinn_pass.cc:562] insert internal var: elementwise_pow_58.tmp_0
I1018 08:53:57.740685 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_56.tmp_0, is_only_used_internal: 1
I1018 08:53:57.740689 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_56.tmp_0
I1018 08:53:57.740690 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_113.tmp_0, is_only_used_internal: 1
I1018 08:53:57.740692 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_113.tmp_0
I1018 08:53:57.740694 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_56.tmp_1
I1018 08:53:57.740697 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_58.tmp_1
I1018 08:53:57.740700 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_95, is_only_used_internal: 1
I1018 08:53:57.740701 4097533 build_cinn_pass.cc:562] insert internal var: tmp_95
I1018 08:53:57.740703 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_94, is_only_used_internal: 1
I1018 08:53:57.740705 4097533 build_cinn_pass.cc:562] insert internal var: tmp_94
I1018 08:53:57.740708 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_59.tmp_1
I1018 08:53:57.740710 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_115.tmp_0, is_only_used_internal: 1
I1018 08:53:57.740712 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_115.tmp_0
I1018 08:53:57.740715 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_59.tmp_0, is_only_used_internal: 1
I1018 08:53:57.740717 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_59.tmp_0
I1018 08:53:57.740720 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: batch_norm_14.tmp_2, is_only_used_internal: 1
I1018 08:53:57.740721 4097533 build_cinn_pass.cc:562] insert internal var: batch_norm_14.tmp_2
I1018 08:53:57.740723 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_93, is_only_used_internal: 1
I1018 08:53:57.740725 4097533 build_cinn_pass.cc:562] insert internal var: tmp_93
I1018 08:53:57.740727 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_5, is_only_used_internal: 1
I1018 08:53:57.740729 4097533 build_cinn_pass.cc:562] insert internal var: tmp_5
I1018 08:53:57.740732 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_58.tmp_0, is_only_used_internal: 1
I1018 08:53:57.740734 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_58.tmp_0
I1018 08:53:57.740736 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_96, is_only_used_internal: 1
I1018 08:53:57.740738 4097533 build_cinn_pass.cc:562] insert internal var: tmp_96
I1018 08:53:57.740741 4097533 build_cinn_pass.cc:742] Cluster Ops: (reshape2, reshape2, elementwise_mul, scale, reshape2, elementwise_add, elementwise_add, elementwise_mul, elementwise_max, elementwise_sub, elementwise_pow, fill_constant, scale, elementwise_pow, reshape2, fill_constant, assign, fill_constant, )
I1018 08:53:57.740744 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_14.w_2, conv2d_14.tmp_0, batch_norm2d_14.w_0, relu_10.tmp_0, batch_norm2d_14.b_0, batch_norm2d_14.w_1, )
I1018 08:53:57.740747 4097533 build_cinn_pass.cc:744] Cluster output vars: (assign_57.tmp_0, relu_12.tmp_0, )
I1018 08:53:57.740749 4097533 build_cinn_pass.cc:745] Cluster internal vars: (tmp_96, reshape2_58.tmp_0, tmp_5, tmp_93, batch_norm_14.tmp_2, reshape2_59.tmp_0, fill_constant_115.tmp_0, reshape2_57.tmp_1, tmp_92, tmp_95, tmp_94, reshape2_57.tmp_0, fill_constant_109.tmp_0, elementwise_pow_56.tmp_0, elementwise_pow_58.tmp_0, reshape2_58.tmp_1, reshape2_56.tmp_0, fill_constant_113.tmp_0, reshape2_56.tmp_1, reshape2_59.tmp_1, )
I1018 08:53:57.740761 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.740978 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: conv2d_14.tmp_0
I1018 08:53:57.740981 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: relu_10.tmp_0
I1018 08:53:57.740985 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_14.w_2
I1018 08:53:57.740989 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_14.b_0
I1018 08:53:57.740993 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_14.w_0
I1018 08:53:57.740995 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_14.w_1
I1018 08:53:57.740999 4097533 build_cinn_pass.cc:274] Add Output Var Node: assign_57.tmp_0
I1018 08:53:57.741003 4097533 build_cinn_pass.cc:274] Add Output Var Node: relu_12.tmp_0
I1018 08:53:57.741024 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_52[label="reshape2_59.tmp_0
[1,256,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_51[label="tmp_5
[1,256,14,14]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_50[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_48[label="reshape2_58.tmp_0
[1,256,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_47[label="tmp_95
[1,256,14,14]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_46[label="reshape2_57.tmp_1
[0,256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_45[label="batch_norm_14.tmp_2
[1,256,14,14]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_44[label="scale" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_43[label="elementwise_pow" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_42[label="tmp_94
[1,256,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_39[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_37[label="elementwise_add" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_53[label="fill_constant_115.tmp_0
[1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_35[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_32[label="elementwise_pow" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_31[label="tmp_96
[1,256,14,14]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_30[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="relu_10.tmp_0
[1,256,14,14]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_21[label="tmp_93
[1,256,14,14]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_38[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_8[label="reshape2_59.tmp_1
[0,256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_49[label="elementwise_max" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_7[label="batch_norm2d_14.w_2
[256]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_6[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_33[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_26[label="elementwise_mul" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_11[label="elementwise_pow_58.tmp_0
[1,256,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_16[label="elementwise_add" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_12[label="elementwise_pow_56.tmp_0
[256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_5[label="conv2d_14.tmp_0
[1,256,14,14]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_4[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_24[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_27[label="elementwise_mul" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_9[label="reshape2_56.tmp_1
[0,256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_36[label="relu_12.tmp_0
[1,256,14,14]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_17[label="assign_57.tmp_0
[256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_10[label="reshape2_56.tmp_0
[1,256,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_19[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_41[label="reshape2_58.tmp_1
[0,256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_34[label="elementwise_sub" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_13[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1[label="batch_norm2d_14.w_1
[256]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_23[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_14[label="fill_constant_109.tmp_0
[1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_2[label="batch_norm2d_14.b_0
[256]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_15[label="reshape2_57.tmp_0
[1,256,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_18[label="scale" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_20[label="batch_norm2d_14.w_0
[256]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_40[label="tmp_92
[256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_22[label="fill_constant_113.tmp_0
[256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_25[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_29[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_28[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0->node_20
   node_1->node_50
   node_2->node_19
   node_3->node_37
   node_4->node_1
   node_5->node_34
   node_6->node_2
   node_7->node_39
   node_10->node_34
   node_11->node_27
   node_12->node_30
   node_13->node_7
   node_14->node_32
   node_14->node_43
   node_15->node_44
   node_16->node_45
   node_17->node_25
   node_18->node_40
   node_19->node_52
   node_19->node_8
   node_20->node_28
   node_21->node_27
   node_22->node_18
   node_23->node_53
   node_26->node_31
   node_27->node_47
   node_28->node_48
   node_28->node_41
   node_29->node_22
   node_30->node_17
   node_31->node_16
   node_32->node_11
   node_33->node_3
   node_34->node_21
   node_35->node_5
   node_36->node_24
   node_37->node_51
   node_38->node_14
   node_39->node_15
   node_39->node_46
   node_40->node_43
   node_42->node_32
   node_43->node_12
   node_44->node_42
   node_45->node_37
   node_47->node_26
   node_48->node_26
   node_49->node_36
   node_50->node_10
   node_50->node_9
   node_51->node_49
   node_52->node_16
   node_53->node_49
} // end G
I1018 08:53:57.741312 4097533 var_desc.cc:415] Flush  batch_norm2d_14.w_1 1
I1018 08:53:57.741315 4097533 var_desc.cc:415] Flush  batch_norm2d_14.b_0 1
I1018 08:53:57.741318 4097533 var_desc.cc:415] Flush  relu_10.tmp_0 1
I1018 08:53:57.741322 4097533 var_desc.cc:415] Flush  conv2d_14.tmp_0 1
I1018 08:53:57.741323 4097533 var_desc.cc:415] Flush  batch_norm2d_14.w_2 1
I1018 08:53:57.741326 4097533 var_desc.cc:415] Flush  reshape2_59.tmp_1 1
I1018 08:53:57.741328 4097533 var_desc.cc:415] Flush  reshape2_56.tmp_1 1
I1018 08:53:57.741331 4097533 var_desc.cc:415] Flush  reshape2_56.tmp_0 1
I1018 08:53:57.741333 4097533 var_desc.cc:415] Flush  elementwise_pow_58.tmp_0 1
I1018 08:53:57.741338 4097533 var_desc.cc:415] Flush  elementwise_pow_56.tmp_0 1
I1018 08:53:57.741340 4097533 var_desc.cc:415] Flush  fill_constant_109.tmp_0 1
I1018 08:53:57.741343 4097533 var_desc.cc:415] Flush  reshape2_57.tmp_0 1
I1018 08:53:57.741345 4097533 var_desc.cc:415] Flush  assign_57.tmp_0 1
I1018 08:53:57.741348 4097533 var_desc.cc:415] Flush  batch_norm2d_14.w_0 1
I1018 08:53:57.741350 4097533 var_desc.cc:415] Flush  tmp_93 1
I1018 08:53:57.741353 4097533 var_desc.cc:415] Flush  fill_constant_113.tmp_0 1
I1018 08:53:57.741355 4097533 var_desc.cc:415] Flush  tmp_96 1
I1018 08:53:57.741359 4097533 var_desc.cc:415] Flush  relu_12.tmp_0 1
I1018 08:53:57.741361 4097533 var_desc.cc:415] Flush  tmp_92 1
I1018 08:53:57.741364 4097533 var_desc.cc:415] Flush  reshape2_58.tmp_1 1
I1018 08:53:57.741366 4097533 var_desc.cc:415] Flush  tmp_94 1
I1018 08:53:57.741369 4097533 var_desc.cc:415] Flush  batch_norm_14.tmp_2 1
I1018 08:53:57.741372 4097533 var_desc.cc:415] Flush  reshape2_57.tmp_1 1
I1018 08:53:57.741374 4097533 var_desc.cc:415] Flush  tmp_95 1
I1018 08:53:57.741377 4097533 var_desc.cc:415] Flush  reshape2_58.tmp_0 1
I1018 08:53:57.741379 4097533 var_desc.cc:415] Flush  tmp_5 1
I1018 08:53:57.741382 4097533 var_desc.cc:415] Flush  reshape2_59.tmp_0 1
I1018 08:53:57.741385 4097533 var_desc.cc:415] Flush  fill_constant_115.tmp_0 1
I1018 08:53:57.741405 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0d13630 -> elementwise_add0x55b7d0cdec40  via tmp_960x55b7d0c810c0
I1018 08:53:57.741412 4097533 graph_helper.h:104] adj reshape20x55b7d0d11e70 -> elementwise_add0x55b7d0cdec40  via reshape2_59.tmp_00x55b7d0c81620
I1018 08:53:57.741418 4097533 graph_helper.h:104] adj fill_constant0x55b7d0ce5870 -> scale0x55b7d0d14350  via fill_constant_113.tmp_00x55b7d0cae4c0
I1018 08:53:57.741425 4097533 graph_helper.h:104] adj feed0x55b7d0c61810 -> reshape20x55b7d0d11e70  via batch_norm2d_14.b_00x55b7d0cf0bf0
I1018 08:53:57.741433 4097533 graph_helper.h:104] adj elementwise_max0x55b7d0f68e70 -> fetch0x55b7d0c62630  via relu_12.tmp_00x55b7d0cf1600
I1018 08:53:57.741437 4097533 graph_helper.h:104] adj assign0x55b7d0ba3ec0 -> fetch0x55b7d0c62280  via assign_57.tmp_00x55b7d0cf12d0
I1018 08:53:57.741441 4097533 graph_helper.h:104] adj reshape20x55b7d0cf1eb0 -> elementwise_mul0x55b7d0d13630  via reshape2_58.tmp_00x55b7d0c811a0
I1018 08:53:57.741447 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0ce84a0 -> elementwise_mul0x55b7d0d13630  via tmp_950x55b7d0b0da10
I1018 08:53:57.741453 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d0ca6750 -> elementwise_mul0x55b7d0ce84a0  via tmp_930x55b7d0c813e0
I1018 08:53:57.741458 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0f79880 -> elementwise_mul0x55b7d0ce84a0  via elementwise_pow_58.tmp_00x55b7d1085360
I1018 08:53:57.741463 4097533 graph_helper.h:104] adj feed0x55b7d0c61ba0 -> reshape20x55b7d0cf1eb0  via batch_norm2d_14.w_00x55b7d0cf0540
I1018 08:53:57.741470 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d097bb50 -> assign0x55b7d0ba3ec0  via elementwise_pow_56.tmp_00x55b7d1084fd0
I1018 08:53:57.741477 4097533 graph_helper.h:104] adj scale0x55b7d0ceb520 -> elementwise_pow0x55b7d0f79880  via tmp_940x55b7d0b0dcd0
I1018 08:53:57.741483 4097533 graph_helper.h:104] adj fill_constant0x55b7d105c890 -> elementwise_pow0x55b7d0f79880  via fill_constant_109.tmp_00x55b7d1084c60
I1018 08:53:57.741490 4097533 graph_helper.h:104] adj reshape20x55b7d0cf51b0 -> elementwise_sub0x55b7d0ca6750  via reshape2_56.tmp_00x55b7d0cae170
I1018 08:53:57.741495 4097533 graph_helper.h:104] adj feed0x55b7d0c7cfd0 -> elementwise_sub0x55b7d0ca6750  via conv2d_14.tmp_00x55b7d0caf280
I1018 08:53:57.741501 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0cdec40 -> elementwise_add0x55b7d0ca93d0  via batch_norm_14.tmp_20x55b7d0c814e0
I1018 08:53:57.741506 4097533 graph_helper.h:104] adj feed0x55b7d0c7d340 -> elementwise_add0x55b7d0ca93d0  via relu_10.tmp_00x55b7d0cf08b0
I1018 08:53:57.741513 4097533 graph_helper.h:104] adj feed0x55b7d0c7d6d0 -> reshape20x55b7d0f3ec10  via batch_norm2d_14.w_20x55b7d0caef10
I1018 08:53:57.741519 4097533 graph_helper.h:104] adj scale0x55b7d0d14350 -> elementwise_pow0x55b7d097bb50  via tmp_920x55b7d0b0d760
I1018 08:53:57.741524 4097533 graph_helper.h:104] adj fill_constant0x55b7d105c890 -> elementwise_pow0x55b7d097bb50  via fill_constant_109.tmp_00x55b7d1084c60
I1018 08:53:57.741530 4097533 graph_helper.h:104] adj reshape20x55b7d0f3ec10 -> scale0x55b7d0ceb520  via reshape2_57.tmp_00x55b7d1084960
I1018 08:53:57.741536 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0ca93d0 -> elementwise_max0x55b7d0f68e70  via tmp_50x55b7d0c812e0
I1018 08:53:57.741542 4097533 graph_helper.h:104] adj fill_constant0x55b7d0cfc6d0 -> elementwise_max0x55b7d0f68e70  via fill_constant_115.tmp_00x55b7d0c81830
I1018 08:53:57.741547 4097533 graph_helper.h:104] adj feed0x55b7d0c61f30 -> reshape20x55b7d0cf51b0  via batch_norm2d_14.w_10x55b7d0cf0f60
I1018 08:53:57.741691 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.741694 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.741699 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.741703 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.741710 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.741714 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.741724 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.741727 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.741734 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.741736 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.741755 4097533 graph_helper.cc:698] convert op node to desc elementwise_sub
I1018 08:53:57.741757 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.741765 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.741766 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.741770 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.741772 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.741780 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.741783 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.741816 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.741819 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.741827 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.741829 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.741837 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.741840 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.741847 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.741850 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.741853 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.741856 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.741863 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.741866 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.741873 4097533 graph_helper.cc:698] convert op node to desc elementwise_add
I1018 08:53:57.741876 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.741883 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.741887 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.741956 4097533 graph_helper.cc:698] convert op node to desc elementwise_add
I1018 08:53:57.741959 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.741964 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.741967 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.741976 4097533 graph_helper.cc:698] convert op node to desc elementwise_max
I1018 08:53:57.741977 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.741984 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.741987 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.741994 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.741997 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.742004 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.742007 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.742013 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.742017 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.742022 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.742025 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.742028 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.742031 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.742700 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.743178 4097533 block_desc.cc:205] vars in desc 28
I1018 08:53:57.743183 4097533 block_desc.cc:209] Flush batch_norm2d_14.w_1
I1018 08:53:57.743186 4097533 var_desc.cc:415] Flush  batch_norm2d_14.w_1 1
I1018 08:53:57.743189 4097533 block_desc.cc:209] Flush batch_norm2d_14.b_0
I1018 08:53:57.743191 4097533 var_desc.cc:415] Flush  batch_norm2d_14.b_0 1
I1018 08:53:57.743194 4097533 block_desc.cc:209] Flush relu_10.tmp_0
I1018 08:53:57.743196 4097533 var_desc.cc:415] Flush  relu_10.tmp_0 1
I1018 08:53:57.743199 4097533 block_desc.cc:209] Flush conv2d_14.tmp_0
I1018 08:53:57.743201 4097533 var_desc.cc:415] Flush  conv2d_14.tmp_0 1
I1018 08:53:57.743203 4097533 block_desc.cc:209] Flush batch_norm2d_14.w_2
I1018 08:53:57.743206 4097533 var_desc.cc:415] Flush  batch_norm2d_14.w_2 1
I1018 08:53:57.743208 4097533 block_desc.cc:209] Flush reshape2_59.tmp_1
I1018 08:53:57.743211 4097533 var_desc.cc:415] Flush  reshape2_59.tmp_1 1
I1018 08:53:57.743213 4097533 block_desc.cc:209] Flush reshape2_56.tmp_1
I1018 08:53:57.743216 4097533 var_desc.cc:415] Flush  reshape2_56.tmp_1 1
I1018 08:53:57.743217 4097533 block_desc.cc:209] Flush reshape2_56.tmp_0
I1018 08:53:57.743219 4097533 var_desc.cc:415] Flush  reshape2_56.tmp_0 1
I1018 08:53:57.743222 4097533 block_desc.cc:209] Flush elementwise_pow_58.tmp_0
I1018 08:53:57.743224 4097533 var_desc.cc:415] Flush  elementwise_pow_58.tmp_0 1
I1018 08:53:57.743227 4097533 block_desc.cc:209] Flush elementwise_pow_56.tmp_0
I1018 08:53:57.743228 4097533 var_desc.cc:415] Flush  elementwise_pow_56.tmp_0 1
I1018 08:53:57.743232 4097533 block_desc.cc:209] Flush fill_constant_109.tmp_0
I1018 08:53:57.743233 4097533 var_desc.cc:415] Flush  fill_constant_109.tmp_0 1
I1018 08:53:57.743235 4097533 block_desc.cc:209] Flush reshape2_57.tmp_0
I1018 08:53:57.743238 4097533 var_desc.cc:415] Flush  reshape2_57.tmp_0 1
I1018 08:53:57.743240 4097533 block_desc.cc:209] Flush assign_57.tmp_0
I1018 08:53:57.743242 4097533 var_desc.cc:415] Flush  assign_57.tmp_0 1
I1018 08:53:57.743245 4097533 block_desc.cc:209] Flush batch_norm2d_14.w_0
I1018 08:53:57.743247 4097533 var_desc.cc:415] Flush  batch_norm2d_14.w_0 1
I1018 08:53:57.743249 4097533 block_desc.cc:209] Flush tmp_93
I1018 08:53:57.743252 4097533 var_desc.cc:415] Flush  tmp_93 1
I1018 08:53:57.743254 4097533 block_desc.cc:209] Flush fill_constant_113.tmp_0
I1018 08:53:57.743256 4097533 var_desc.cc:415] Flush  fill_constant_113.tmp_0 1
I1018 08:53:57.743259 4097533 block_desc.cc:209] Flush tmp_96
I1018 08:53:57.743261 4097533 var_desc.cc:415] Flush  tmp_96 1
I1018 08:53:57.743263 4097533 block_desc.cc:209] Flush relu_12.tmp_0
I1018 08:53:57.743266 4097533 var_desc.cc:415] Flush  relu_12.tmp_0 1
I1018 08:53:57.743273 4097533 block_desc.cc:209] Flush tmp_92
I1018 08:53:57.743274 4097533 var_desc.cc:415] Flush  tmp_92 1
I1018 08:53:57.743278 4097533 block_desc.cc:209] Flush reshape2_58.tmp_1
I1018 08:53:57.743279 4097533 var_desc.cc:415] Flush  reshape2_58.tmp_1 1
I1018 08:53:57.743281 4097533 block_desc.cc:209] Flush tmp_94
I1018 08:53:57.743284 4097533 var_desc.cc:415] Flush  tmp_94 1
I1018 08:53:57.743286 4097533 block_desc.cc:209] Flush batch_norm_14.tmp_2
I1018 08:53:57.743288 4097533 var_desc.cc:415] Flush  batch_norm_14.tmp_2 1
I1018 08:53:57.743291 4097533 block_desc.cc:209] Flush reshape2_57.tmp_1
I1018 08:53:57.743294 4097533 var_desc.cc:415] Flush  reshape2_57.tmp_1 1
I1018 08:53:57.743295 4097533 block_desc.cc:209] Flush tmp_95
I1018 08:53:57.743297 4097533 var_desc.cc:415] Flush  tmp_95 1
I1018 08:53:57.743300 4097533 block_desc.cc:209] Flush reshape2_58.tmp_0
I1018 08:53:57.743302 4097533 var_desc.cc:415] Flush  reshape2_58.tmp_0 1
I1018 08:53:57.743304 4097533 block_desc.cc:209] Flush tmp_5
I1018 08:53:57.743307 4097533 var_desc.cc:415] Flush  tmp_5 1
I1018 08:53:57.743309 4097533 block_desc.cc:209] Flush reshape2_59.tmp_0
I1018 08:53:57.743311 4097533 var_desc.cc:415] Flush  reshape2_59.tmp_0 1
I1018 08:53:57.743314 4097533 block_desc.cc:209] Flush fill_constant_115.tmp_0
I1018 08:53:57.743316 4097533 var_desc.cc:415] Flush  fill_constant_115.tmp_0 1
I1018 08:53:57.741305 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_14.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "batch_norm2d_14.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "relu_10.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 14
          dims: 14
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "conv2d_14.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 14
          dims: 14
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_14.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "reshape2_59.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_56.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_56.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "elementwise_pow_58.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "elementwise_pow_56.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_109.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_57.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "assign_57.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "batch_norm2d_14.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "tmp_93"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 14
          dims: 14
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "fill_constant_113.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_96"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 14
          dims: 14
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "relu_12.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 14
          dims: 14
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "tmp_92"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  vars {
    name: "reshape2_58.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "tmp_94"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  vars {
    name: "batch_norm_14.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 14
          dims: 14
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_57.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_95"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 14
          dims: 14
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "reshape2_58.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "tmp_5"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 14
          dims: 14
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "reshape2_59.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 256
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "fill_constant_115.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_14.w_1"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_14.w_1"
    }
    outputs {
      parameter: "Out"
      arguments: "reshape2_56.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_56.tmp_1"
    }
    type: "reshape2"
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 123, in composite_batchnorm"
      strings: "    x_hat = (x - reshape(run_mean, stats_shape)) * pow("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/manipulation.py\", line 4518, in reshape"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 1
      ints: 256
      ints: 1
      ints: 1
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_14.w_0"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_14.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "reshape2_58.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_58.tmp_1"
    }
    type: "reshape2"
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 129, in composite_batchnorm"
      strings: "    y = reshape(scale, stats_shape) * x_hat + reshape(bias, stats_shape)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/manipulation.py\", line 4518, in reshape"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 1
      ints: 256
      ints: 1
      ints: 1
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "conv2d_14.tmp_0"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "conv2d_14.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "reshape2_56.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_93"
    }
    type: "elementwise_sub"
    attrs {
      name: "Scale_out"
      type: FLOAT
      f: 1
    }
    attrs {
      name: "Scale_x"
      type: FLOAT
      f: 1
    }
    attrs {
      name: "Scale_y"
      type: FLOAT
      f: 1
    }
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "axis"
      type: INT
      i: -1
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 123, in composite_batchnorm"
      strings: "    x_hat = (x - reshape(run_mean, stats_shape)) * pow("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 657, in __impl__"
      strings: "    current_block(self).append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  Fil
I1018 08:53:57.744553 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.744688 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_117.tmp_0, is_only_used_internal: 1
I1018 08:53:57.744691 4097533 build_cinn_pass.cc:557] var_node->outputs[1]: fill_constant_117.tmp_0, is_only_used_internal: 1
I1018 08:53:57.744694 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_117.tmp_0
I1018 08:53:57.744696 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: elementwise_pow_60.tmp_0, is_only_used_internal: 1
I1018 08:53:57.744699 4097533 build_cinn_pass.cc:562] insert internal var: elementwise_pow_60.tmp_0
I1018 08:53:57.744701 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_98, is_only_used_internal: 1
I1018 08:53:57.744704 4097533 build_cinn_pass.cc:562] insert internal var: tmp_98
I1018 08:53:57.744705 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_99, is_only_used_internal: 1
I1018 08:53:57.744707 4097533 build_cinn_pass.cc:562] insert internal var: tmp_99
I1018 08:53:57.744709 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_61.tmp_0, is_only_used_internal: 1
I1018 08:53:57.744711 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_61.tmp_0
I1018 08:53:57.744714 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_100, is_only_used_internal: 1
I1018 08:53:57.744719 4097533 build_cinn_pass.cc:562] insert internal var: tmp_100
I1018 08:53:57.744721 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_101, is_only_used_internal: 1
I1018 08:53:57.744724 4097533 build_cinn_pass.cc:562] insert internal var: tmp_101
I1018 08:53:57.744725 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_61.tmp_1
I1018 08:53:57.744727 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_123.tmp_0, is_only_used_internal: 1
I1018 08:53:57.744730 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_123.tmp_0
I1018 08:53:57.744732 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_60.tmp_0, is_only_used_internal: 1
I1018 08:53:57.744735 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_60.tmp_0
I1018 08:53:57.744736 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_121.tmp_0, is_only_used_internal: 1
I1018 08:53:57.744738 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_121.tmp_0
I1018 08:53:57.744741 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_60.tmp_1
I1018 08:53:57.744743 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: elementwise_pow_62.tmp_0, is_only_used_internal: 1
I1018 08:53:57.744745 4097533 build_cinn_pass.cc:562] insert internal var: elementwise_pow_62.tmp_0
I1018 08:53:57.744747 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_62.tmp_1
I1018 08:53:57.744750 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_62.tmp_0, is_only_used_internal: 1
I1018 08:53:57.744752 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_62.tmp_0
I1018 08:53:57.744755 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_102, is_only_used_internal: 1
I1018 08:53:57.744756 4097533 build_cinn_pass.cc:562] insert internal var: tmp_102
I1018 08:53:57.744758 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_63.tmp_1
I1018 08:53:57.744761 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_63.tmp_0, is_only_used_internal: 1
I1018 08:53:57.744763 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_63.tmp_0
I1018 08:53:57.744765 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: batch_norm_15.tmp_2, is_only_used_internal: 1
I1018 08:53:57.744767 4097533 build_cinn_pass.cc:562] insert internal var: batch_norm_15.tmp_2
I1018 08:53:57.744771 4097533 build_cinn_pass.cc:742] Cluster Ops: (scale, reshape2, elementwise_pow, elementwise_mul, reshape2, elementwise_mul, fill_constant, reshape2, elementwise_max, elementwise_add, fill_constant, reshape2, elementwise_sub, scale, elementwise_pow, assign, fill_constant, )
I1018 08:53:57.744773 4097533 build_cinn_pass.cc:743] Cluster input vars: (conv2d_15.tmp_0, batch_norm2d_16.w_2, batch_norm2d_16.w_1, batch_norm2d_16.w_0, batch_norm2d_16.b_0, )
I1018 08:53:57.744776 4097533 build_cinn_pass.cc:744] Cluster output vars: (assign_61.tmp_0, relu_13.tmp_0, )
I1018 08:53:57.744778 4097533 build_cinn_pass.cc:745] Cluster internal vars: (batch_norm_15.tmp_2, reshape2_63.tmp_0, reshape2_63.tmp_1, tmp_102, reshape2_62.tmp_0, reshape2_62.tmp_1, fill_constant_117.tmp_0, elementwise_pow_60.tmp_0, tmp_98, tmp_100, tmp_101, reshape2_61.tmp_1, fill_constant_123.tmp_0, tmp_99, reshape2_61.tmp_0, reshape2_60.tmp_0, fill_constant_121.tmp_0, reshape2_60.tmp_1, elementwise_pow_62.tmp_0, )
I1018 08:53:57.744786 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.744995 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: conv2d_15.tmp_0
I1018 08:53:57.744999 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_16.w_2
I1018 08:53:57.745003 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_16.b_0
I1018 08:53:57.745007 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_16.w_0
I1018 08:53:57.745010 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_16.w_1
I1018 08:53:57.745013 4097533 build_cinn_pass.cc:274] Add Output Var Node: assign_61.tmp_0
I1018 08:53:57.745019 4097533 build_cinn_pass.cc:274] Add Output Var Node: relu_13.tmp_0
I1018 08:53:57.745040 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_48[label="reshape2_62.tmp_1
[0,512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_47[label="reshape2_63.tmp_1
[0,512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_46[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_45[label="reshape2_61.tmp_1
[0,512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_44[label="elementwise_pow_60.tmp_0
[512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_43[label="reshape2_62.tmp_0
[1,512,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_42[label="tmp_100
[1,512,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_39[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_37[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_35[label="tmp_99
[1,512,7,7]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_32[label="scale" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_31[label="batch_norm2d_16.w_2
[512]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_30[label="scale" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="assign_61.tmp_0
[512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_21[label="reshape2_60.tmp_1
[0,512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_38[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_8[label="fill_constant_123.tmp_0
[1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_49[label="fill_constant_117.tmp_0
[1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_7[label="reshape2_60.tmp_0
[1,512,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_6[label="fill_constant_121.tmp_0
[512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_33[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_26[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_11[label="elementwise_max" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_16[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_12[label="elementwise_add" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_5[label="batch_norm2d_16.w_1
[512]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_4[label="batch_norm2d_16.w_0
[512]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_24[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_27[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_9[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_36[label="elementwise_pow" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_17[label="elementwise_mul" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_10[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_19[label="conv2d_15.tmp_0
[1,512,7,7]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_41[label="batch_norm2d_16.b_0
[512]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_34[label="batch_norm_15.tmp_2
[1,512,7,7]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_13[label="tmp_98
[512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_1[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_23[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_14[label="reshape2_61.tmp_0
[1,512,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_2[label="relu_13.tmp_0
[1,512,7,7]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_15[label="tmp_101
[1,512,7,7]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_18[label="elementwise_pow" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_20[label="elementwise_sub" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_40[label="reshape2_63.tmp_0
[1,512,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_22[label="elementwise_mul" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_25[label="tmp_102
[1,512,7,7]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_29[label="elementwise_pow_62.tmp_0
[1,512,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_28[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_5
   node_2->node_46
   node_3->node_0
   node_4->node_24
   node_5->node_28
   node_6->node_30
   node_7->node_20
   node_8->node_11
   node_9->node_14
   node_9->node_45
   node_10->node_41
   node_11->node_2
   node_12->node_34
   node_13->node_36
   node_14->node_32
   node_15->node_22
   node_16->node_40
   node_16->node_47
   node_17->node_15
   node_18->node_29
   node_19->node_20
   node_20->node_35
   node_22->node_25
   node_23->node_8
   node_24->node_43
   node_24->node_48
   node_25->node_12
   node_26->node_31
   node_27->node_19
   node_28->node_7
   node_28->node_21
   node_29->node_17
   node_30->node_13
   node_31->node_9
   node_32->node_42
   node_33->node_6
   node_34->node_11
   node_35->node_17
   node_36->node_44
   node_37->node_3
   node_38->node_4
   node_39->node_49
   node_40->node_12
   node_41->node_16
   node_42->node_18
   node_43->node_22
   node_44->node_37
   node_49->node_18
   node_49->node_36
} // end G
I1018 08:53:57.745312 4097533 var_desc.cc:415] Flush  relu_13.tmp_0 1
I1018 08:53:57.745316 4097533 var_desc.cc:415] Flush  assign_61.tmp_0 1
I1018 08:53:57.745319 4097533 var_desc.cc:415] Flush  batch_norm2d_16.w_0 1
I1018 08:53:57.745321 4097533 var_desc.cc:415] Flush  batch_norm2d_16.w_1 1
I1018 08:53:57.745324 4097533 var_desc.cc:415] Flush  fill_constant_121.tmp_0 1
I1018 08:53:57.745326 4097533 var_desc.cc:415] Flush  reshape2_60.tmp_0 1
I1018 08:53:57.745329 4097533 var_desc.cc:415] Flush  fill_constant_123.tmp_0 1
I1018 08:53:57.745332 4097533 var_desc.cc:415] Flush  tmp_98 1
I1018 08:53:57.745334 4097533 var_desc.cc:415] Flush  reshape2_61.tmp_0 1
I1018 08:53:57.745338 4097533 var_desc.cc:415] Flush  tmp_101 1
I1018 08:53:57.745340 4097533 var_desc.cc:415] Flush  conv2d_15.tmp_0 1
I1018 08:53:57.745343 4097533 var_desc.cc:415] Flush  reshape2_60.tmp_1 1
I1018 08:53:57.745347 4097533 var_desc.cc:415] Flush  tmp_102 1
I1018 08:53:57.745348 4097533 var_desc.cc:415] Flush  elementwise_pow_62.tmp_0 1
I1018 08:53:57.745352 4097533 var_desc.cc:415] Flush  batch_norm2d_16.w_2 1
I1018 08:53:57.745354 4097533 var_desc.cc:415] Flush  batch_norm_15.tmp_2 1
I1018 08:53:57.745358 4097533 var_desc.cc:415] Flush  tmp_99 1
I1018 08:53:57.745361 4097533 var_desc.cc:415] Flush  reshape2_63.tmp_0 1
I1018 08:53:57.745364 4097533 var_desc.cc:415] Flush  batch_norm2d_16.b_0 1
I1018 08:53:57.745366 4097533 var_desc.cc:415] Flush  tmp_100 1
I1018 08:53:57.745369 4097533 var_desc.cc:415] Flush  reshape2_62.tmp_0 1
I1018 08:53:57.745373 4097533 var_desc.cc:415] Flush  elementwise_pow_60.tmp_0 1
I1018 08:53:57.745374 4097533 var_desc.cc:415] Flush  reshape2_61.tmp_1 1
I1018 08:53:57.745378 4097533 var_desc.cc:415] Flush  reshape2_63.tmp_1 1
I1018 08:53:57.745379 4097533 var_desc.cc:415] Flush  reshape2_62.tmp_1 1
I1018 08:53:57.745383 4097533 var_desc.cc:415] Flush  fill_constant_117.tmp_0 1
I1018 08:53:57.745400 4097533 graph_helper.h:104] adj assign0x55b7d0a843e0 -> fetch0x55b7d0d2ca20  via assign_61.tmp_00x55b7d0d646c0
I1018 08:53:57.745407 4097533 graph_helper.h:104] adj feed0x55b7d0d2bc20 -> reshape20x55b7d0d338a0  via batch_norm2d_16.w_20x55b7d0d01040
I1018 08:53:57.745416 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0d51020 -> elementwise_max0x55b7d0c679f0  via batch_norm_15.tmp_20x55b7d0b1fea0
I1018 08:53:57.745422 4097533 graph_helper.h:104] adj fill_constant0x55b7d0becce0 -> elementwise_max0x55b7d0c679f0  via fill_constant_123.tmp_00x55b7d0cd9a80
I1018 08:53:57.745429 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0d1e240 -> elementwise_add0x55b7d0d51020  via tmp_1020x55b7d0b20260
I1018 08:53:57.745433 4097533 graph_helper.h:104] adj reshape20x55b7d0d37ec0 -> elementwise_add0x55b7d0d51020  via reshape2_63.tmp_00x55b7d0b1ffc0
I1018 08:53:57.745440 4097533 graph_helper.h:104] adj feed0x55b7d0d2bfb0 -> reshape20x55b7d0d37ec0  via batch_norm2d_16.b_00x55b7d0d01a90
I1018 08:53:57.745445 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d0d1c020 -> elementwise_mul0x55b7d0d569d0  via tmp_990x55b7d0d35060
I1018 08:53:57.745451 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0d614d0 -> elementwise_mul0x55b7d0d569d0  via elementwise_pow_62.tmp_00x55b7d0d36180
I1018 08:53:57.745456 4097533 graph_helper.h:104] adj scale0x55b7d0d30080 -> elementwise_pow0x55b7d0d614d0  via tmp_1000x55b7d0cd9190
I1018 08:53:57.745461 4097533 graph_helper.h:104] adj fill_constant0x55b7d0d0ac90 -> elementwise_pow0x55b7d0d614d0  via fill_constant_117.tmp_00x55b7d0b20670
I1018 08:53:57.745467 4097533 graph_helper.h:104] adj reshape20x55b7d0d39d10 -> elementwise_sub0x55b7d0d1c020  via reshape2_60.tmp_00x55b7d0d35720
I1018 08:53:57.745472 4097533 graph_helper.h:104] adj feed0x55b7d0d65600 -> elementwise_sub0x55b7d0d1c020  via conv2d_15.tmp_00x55b7d0d00d00
I1018 08:53:57.745478 4097533 graph_helper.h:104] adj reshape20x55b7d0d237a0 -> elementwise_mul0x55b7d0d1e240  via reshape2_62.tmp_00x55b7d0b20360
I1018 08:53:57.745483 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0d569d0 -> elementwise_mul0x55b7d0d1e240  via tmp_1010x55b7d0cd9450
I1018 08:53:57.745489 4097533 graph_helper.h:104] adj feed0x55b7d0d2c340 -> reshape20x55b7d0d237a0  via batch_norm2d_16.w_00x55b7d0d01720
I1018 08:53:57.745496 4097533 graph_helper.h:104] adj feed0x55b7d0d2c6d0 -> reshape20x55b7d0d39d10  via batch_norm2d_16.w_10x55b7d0d013b0
I1018 08:53:57.745502 4097533 graph_helper.h:104] adj fill_constant0x55b7d0d61ee0 -> scale0x55b7d0cffb40  via fill_constant_121.tmp_00x55b7d0d35aa0
I1018 08:53:57.745509 4097533 graph_helper.h:104] adj reshape20x55b7d0d338a0 -> scale0x55b7d0d30080  via reshape2_61.tmp_00x55b7d0d353a0
I1018 08:53:57.745515 4097533 graph_helper.h:104] adj scale0x55b7d0cffb40 -> elementwise_pow0x55b7d0f25740  via tmp_980x55b7d0cd8ee0
I1018 08:53:57.745522 4097533 graph_helper.h:104] adj fill_constant0x55b7d0d0ac90 -> elementwise_pow0x55b7d0f25740  via fill_constant_117.tmp_00x55b7d0b20670
I1018 08:53:57.745527 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0f25740 -> assign0x55b7d0a843e0  via elementwise_pow_60.tmp_00x55b7d0cd8c30
I1018 08:53:57.745532 4097533 graph_helper.h:104] adj elementwise_max0x55b7d0c679f0 -> fetch0x55b7d0d2cd70  via relu_13.tmp_00x55b7d0d649f0
I1018 08:53:57.745667 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.745671 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.745676 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.745678 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.745687 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.745690 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.745704 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.745707 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.745710 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.745713 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.745734 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.745738 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.745744 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.745747 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.745755 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.745757 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.745761 4097533 graph_helper.cc:698] convert op node to desc elementwise_sub
I1018 08:53:57.745764 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.745802 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.745805 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.745812 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.745815 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.745817 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.745820 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.745828 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.745831 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.745838 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.745841 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.745847 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.745851 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.745853 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.745857 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.745863 4097533 graph_helper.cc:698] convert op node to desc elementwise_add
I1018 08:53:57.745867 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.745939 4097533 graph_helper.cc:698] convert op node to desc elementwise_max
I1018 08:53:57.745941 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.745949 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.745951 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.745958 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.745961 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.745968 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.745971 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.745978 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.745981 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.745987 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.745990 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.745993 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.745996 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.746637 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.747094 4097533 block_desc.cc:205] vars in desc 26
I1018 08:53:57.747100 4097533 block_desc.cc:209] Flush relu_13.tmp_0
I1018 08:53:57.747102 4097533 var_desc.cc:415] Flush  relu_13.tmp_0 1
I1018 08:53:57.747105 4097533 block_desc.cc:209] Flush assign_61.tmp_0
I1018 08:53:57.747108 4097533 var_desc.cc:415] Flush  assign_61.tmp_0 1
I1018 08:53:57.747110 4097533 block_desc.cc:209] Flush batch_norm2d_16.w_0
I1018 08:53:57.747113 4097533 var_desc.cc:415] Flush  batch_norm2d_16.w_0 1
I1018 08:53:57.747115 4097533 block_desc.cc:209] Flush batch_norm2d_16.w_1
I1018 08:53:57.747118 4097533 var_desc.cc:415] Flush  batch_norm2d_16.w_1 1
I1018 08:53:57.747124 4097533 block_desc.cc:209] Flush fill_constant_121.tmp_0
I1018 08:53:57.747126 4097533 var_desc.cc:415] Flush  fill_constant_121.tmp_0 1
I1018 08:53:57.747128 4097533 block_desc.cc:209] Flush reshape2_60.tmp_0
I1018 08:53:57.747131 4097533 var_desc.cc:415] Flush  reshape2_60.tmp_0 1
I1018 08:53:57.747133 4097533 block_desc.cc:209] Flush fill_constant_123.tmp_0
I1018 08:53:57.747135 4097533 var_desc.cc:415] Flush  fill_constant_123.tmp_0 1
I1018 08:53:57.747138 4097533 block_desc.cc:209] Flush tmp_98
I1018 08:53:57.747140 4097533 var_desc.cc:415] Flush  tmp_98 1
I1018 08:53:57.747143 4097533 block_desc.cc:209] Flush reshape2_61.tmp_0
I1018 08:53:57.747145 4097533 var_desc.cc:415] Flush  reshape2_61.tmp_0 1
I1018 08:53:57.747148 4097533 block_desc.cc:209] Flush tmp_101
I1018 08:53:57.747149 4097533 var_desc.cc:415] Flush  tmp_101 1
I1018 08:53:57.747152 4097533 block_desc.cc:209] Flush conv2d_15.tmp_0
I1018 08:53:57.747154 4097533 var_desc.cc:415] Flush  conv2d_15.tmp_0 1
I1018 08:53:57.747157 4097533 block_desc.cc:209] Flush reshape2_60.tmp_1
I1018 08:53:57.747159 4097533 var_desc.cc:415] Flush  reshape2_60.tmp_1 1
I1018 08:53:57.747161 4097533 block_desc.cc:209] Flush tmp_102
I1018 08:53:57.747164 4097533 var_desc.cc:415] Flush  tmp_102 1
I1018 08:53:57.747166 4097533 block_desc.cc:209] Flush elementwise_pow_62.tmp_0
I1018 08:53:57.747169 4097533 var_desc.cc:415] Flush  elementwise_pow_62.tmp_0 1
I1018 08:53:57.747170 4097533 block_desc.cc:209] Flush batch_norm2d_16.w_2
I1018 08:53:57.747174 4097533 var_desc.cc:415] Flush  batch_norm2d_16.w_2 1
I1018 08:53:57.747175 4097533 block_desc.cc:209] Flush batch_norm_15.tmp_2
I1018 08:53:57.747177 4097533 var_desc.cc:415] Flush  batch_norm_15.tmp_2 1
I1018 08:53:57.747179 4097533 block_desc.cc:209] Flush tmp_99
I1018 08:53:57.747182 4097533 var_desc.cc:415] Flush  tmp_99 1
I1018 08:53:57.747184 4097533 block_desc.cc:209] Flush reshape2_63.tmp_0
I1018 08:53:57.747186 4097533 var_desc.cc:415] Flush  reshape2_63.tmp_0 1
I1018 08:53:57.747189 4097533 block_desc.cc:209] Flush batch_norm2d_16.b_0
I1018 08:53:57.747191 4097533 var_desc.cc:415] Flush  batch_norm2d_16.b_0 1
I1018 08:53:57.747193 4097533 block_desc.cc:209] Flush tmp_100
I1018 08:53:57.747196 4097533 var_desc.cc:415] Flush  tmp_100 1
I1018 08:53:57.747198 4097533 block_desc.cc:209] Flush reshape2_62.tmp_0
I1018 08:53:57.747200 4097533 var_desc.cc:415] Flush  reshape2_62.tmp_0 1
I1018 08:53:57.747202 4097533 block_desc.cc:209] Flush elementwise_pow_60.tmp_0
I1018 08:53:57.747205 4097533 var_desc.cc:415] Flush  elementwise_pow_60.tmp_0 1
I1018 08:53:57.747207 4097533 block_desc.cc:209] Flush reshape2_61.tmp_1
I1018 08:53:57.747210 4097533 var_desc.cc:415] Flush  reshape2_61.tmp_1 1
I1018 08:53:57.747212 4097533 block_desc.cc:209] Flush reshape2_63.tmp_1
I1018 08:53:57.747215 4097533 var_desc.cc:415] Flush  reshape2_63.tmp_1 1
I1018 08:53:57.747216 4097533 block_desc.cc:209] Flush reshape2_62.tmp_1
I1018 08:53:57.747218 4097533 var_desc.cc:415] Flush  reshape2_62.tmp_1 1
I1018 08:53:57.747221 4097533 block_desc.cc:209] Flush fill_constant_117.tmp_0
I1018 08:53:57.747223 4097533 var_desc.cc:415] Flush  fill_constant_117.tmp_0 1
I1018 08:53:57.745306 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "relu_13.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 7
          dims: 7
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "assign_61.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "batch_norm2d_16.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_16.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "fill_constant_121.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_60.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_123.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_98"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  vars {
    name: "reshape2_61.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_101"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 7
          dims: 7
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "conv2d_15.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 7
          dims: 7
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_60.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_102"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 7
          dims: 7
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "elementwise_pow_62.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "batch_norm2d_16.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "batch_norm_15.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 7
          dims: 7
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "tmp_99"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 7
          dims: 7
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "reshape2_63.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_16.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "tmp_100"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  vars {
    name: "reshape2_62.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "elementwise_pow_60.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_61.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_63.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_62.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "fill_constant_117.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_16.w_2"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_16.w_2"
    }
    outputs {
      parameter: "Out"
      arguments: "reshape2_61.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_61.tmp_1"
    }
    type: "reshape2"
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 124, in composite_batchnorm"
      strings: "    (reshape(run_var, stats_shape) + epsilon), half"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/manipulation.py\", line 4518, in reshape"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 1
      ints: 512
      ints: 1
      ints: 1
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "BiasTensor"
    }
    inputs {
      parameter: "ScaleTensor"
    }
    inputs {
      parameter: "X"
      arguments: "reshape2_61.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_100"
    }
    type: "scale"
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "bias"
      type: FLOAT
      f: 1e-05
    }
    attrs {
      name: "bias_after_scale"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 124, in composite_batchnorm"
      strings: "    (reshape(run_var, stats_shape) + epsilon), half"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 557, in __impl__"
      strings: "    return scalar_method(self, other_var)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 530, in _scalar_add_"
      strings: "    return _scalar_op_(var, 1.0, value)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 451, in _scalar_op_"
      strings: "    block.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "scale"
      type: FLOAT
      f: 1
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_16.w_1"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_16.w_1"
    }
    outputs {
      parameter: "Out"
      arguments: "reshape2_60.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_60.tmp_1"
    }
    type: "reshape2"
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 123, in composite_batchnorm"
      strings: "    x_hat = (x - reshape(run_mean, stats_shape)) * pow("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/manipulation.py\", line 4518, in reshape"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 1
      ints: 512
      ints: 1
      ints: 1
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inpu
I1018 08:53:57.748396 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.748531 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_137.tmp_0, is_only_used_internal: 1
I1018 08:53:57.748535 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_137.tmp_0
I1018 08:53:57.748538 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_129.tmp_0, is_only_used_internal: 1
I1018 08:53:57.748540 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_129.tmp_0
I1018 08:53:57.748543 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_111, is_only_used_internal: 1
I1018 08:53:57.748544 4097533 build_cinn_pass.cc:562] insert internal var: tmp_111
I1018 08:53:57.748548 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_6, is_only_used_internal: 1
I1018 08:53:57.748549 4097533 build_cinn_pass.cc:562] insert internal var: tmp_6
I1018 08:53:57.748551 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_104, is_only_used_internal: 1
I1018 08:53:57.748553 4097533 build_cinn_pass.cc:562] insert internal var: tmp_104
I1018 08:53:57.748555 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_71.tmp_1
I1018 08:53:57.748558 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_71.tmp_0, is_only_used_internal: 1
I1018 08:53:57.748560 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_71.tmp_0
I1018 08:53:57.748562 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_114, is_only_used_internal: 1
I1018 08:53:57.748564 4097533 build_cinn_pass.cc:562] insert internal var: tmp_114
I1018 08:53:57.748566 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_70.tmp_0, is_only_used_internal: 1
I1018 08:53:57.748569 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_70.tmp_0
I1018 08:53:57.748570 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: elementwise_pow_70.tmp_0, is_only_used_internal: 1
I1018 08:53:57.748572 4097533 build_cinn_pass.cc:562] insert internal var: elementwise_pow_70.tmp_0
I1018 08:53:57.748574 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_106, is_only_used_internal: 1
I1018 08:53:57.748577 4097533 build_cinn_pass.cc:562] insert internal var: tmp_106
I1018 08:53:57.748579 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_131.tmp_0, is_only_used_internal: 1
I1018 08:53:57.748581 4097533 build_cinn_pass.cc:557] var_node->outputs[1]: fill_constant_131.tmp_0, is_only_used_internal: 1
I1018 08:53:57.748584 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_131.tmp_0
I1018 08:53:57.748585 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: batch_norm_17.tmp_2, is_only_used_internal: 1
I1018 08:53:57.748587 4097533 build_cinn_pass.cc:562] insert internal var: batch_norm_17.tmp_2
I1018 08:53:57.748589 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_68.tmp_1
I1018 08:53:57.748592 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_135.tmp_0, is_only_used_internal: 1
I1018 08:53:57.748594 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_135.tmp_0
I1018 08:53:57.748596 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_105, is_only_used_internal: 1
I1018 08:53:57.748598 4097533 build_cinn_pass.cc:562] insert internal var: tmp_105
I1018 08:53:57.748601 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_68.tmp_0, is_only_used_internal: 1
I1018 08:53:57.748603 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_68.tmp_0
I1018 08:53:57.748605 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_65.tmp_1
I1018 08:53:57.748607 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_65.tmp_0, is_only_used_internal: 1
I1018 08:53:57.748612 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_65.tmp_0
I1018 08:53:57.748615 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: elementwise_pow_64.tmp_0, is_only_used_internal: 1
I1018 08:53:57.748616 4097533 build_cinn_pass.cc:562] insert internal var: elementwise_pow_64.tmp_0
I1018 08:53:57.748618 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_110, is_only_used_internal: 1
I1018 08:53:57.748620 4097533 build_cinn_pass.cc:562] insert internal var: tmp_110
I1018 08:53:57.748622 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_64.tmp_0, is_only_used_internal: 1
I1018 08:53:57.748625 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_64.tmp_0
I1018 08:53:57.748627 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_70.tmp_1
I1018 08:53:57.748629 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_66.tmp_0, is_only_used_internal: 1
I1018 08:53:57.748631 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_66.tmp_0
I1018 08:53:57.748633 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_112, is_only_used_internal: 1
I1018 08:53:57.748636 4097533 build_cinn_pass.cc:562] insert internal var: tmp_112
I1018 08:53:57.748637 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_108, is_only_used_internal: 1
I1018 08:53:57.748639 4097533 build_cinn_pass.cc:562] insert internal var: tmp_108
I1018 08:53:57.748642 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_113, is_only_used_internal: 1
I1018 08:53:57.748644 4097533 build_cinn_pass.cc:562] insert internal var: tmp_113
I1018 08:53:57.748646 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_66.tmp_1
I1018 08:53:57.748648 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_107, is_only_used_internal: 1
I1018 08:53:57.748651 4097533 build_cinn_pass.cc:562] insert internal var: tmp_107
I1018 08:53:57.748652 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_69.tmp_0, is_only_used_internal: 1
I1018 08:53:57.748654 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_69.tmp_0
I1018 08:53:57.748660 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: elementwise_pow_66.tmp_0, is_only_used_internal: 1
I1018 08:53:57.748662 4097533 build_cinn_pass.cc:562] insert internal var: elementwise_pow_66.tmp_0
I1018 08:53:57.748664 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_69.tmp_1
I1018 08:53:57.748667 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: elementwise_pow_68.tmp_0, is_only_used_internal: 1
I1018 08:53:57.748668 4097533 build_cinn_pass.cc:562] insert internal var: elementwise_pow_68.tmp_0
I1018 08:53:57.748672 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_67.tmp_0, is_only_used_internal: 1
I1018 08:53:57.748673 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_67.tmp_0
I1018 08:53:57.748675 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_125.tmp_0, is_only_used_internal: 1
I1018 08:53:57.748677 4097533 build_cinn_pass.cc:557] var_node->outputs[1]: fill_constant_125.tmp_0, is_only_used_internal: 1
I1018 08:53:57.748679 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_125.tmp_0
I1018 08:53:57.748682 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_64.tmp_1
I1018 08:53:57.748684 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: batch_norm_16.tmp_2, is_only_used_internal: 1
I1018 08:53:57.748687 4097533 build_cinn_pass.cc:562] insert internal var: batch_norm_16.tmp_2
I1018 08:53:57.748688 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_67.tmp_1
I1018 08:53:57.748692 4097533 build_cinn_pass.cc:742] Cluster Ops: (fill_constant, scale, reshape2, scale, reshape2, elementwise_pow, elementwise_mul, elementwise_mul, scale, reshape2, reshape2, reshape2, elementwise_pow, reshape2, elementwise_mul, reshape2, elementwise_mul, elementwise_pow, reshape2, elementwise_sub, elementwise_add, elementwise_add, elementwise_sub, elementwise_add, elementwise_max, fill_constant, assign, fill_constant, scale, elementwise_pow, assign, fill_constant, fill_constant, )
I1018 08:53:57.748698 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_15.b_0, batch_norm2d_15.w_1, batch_norm2d_17.w_1, conv2d_17.tmp_0, batch_norm2d_17.w_2, batch_norm2d_15.w_0, batch_norm2d_17.w_0, batch_norm2d_15.w_2, batch_norm2d_17.b_0, conv2d_16.tmp_0, )
I1018 08:53:57.748701 4097533 build_cinn_pass.cc:744] Cluster output vars: (assign_69.tmp_0, relu_14.tmp_0, assign_65.tmp_0, )
I1018 08:53:57.748703 4097533 build_cinn_pass.cc:745] Cluster internal vars: (reshape2_67.tmp_1, batch_norm_16.tmp_2, reshape2_64.tmp_1, fill_constant_125.tmp_0, reshape2_67.tmp_0, elementwise_pow_68.tmp_0, reshape2_69.tmp_1, elementwise_pow_66.tmp_0, reshape2_69.tmp_0, tmp_114, reshape2_70.tmp_0, reshape2_71.tmp_0, tmp_106, elementwise_pow_70.tmp_0, reshape2_71.tmp_1, tmp_104, tmp_111, fill_constant_129.tmp_0, fill_constant_137.tmp_0, tmp_6, fill_constant_131.tmp_0, batch_norm_17.tmp_2, reshape2_68.tmp_1, fill_constant_135.tmp_0, tmp_105, reshape2_68.tmp_0, reshape2_65.tmp_1, reshape2_65.tmp_0, elementwise_pow_64.tmp_0, tmp_110, reshape2_64.tmp_0, reshape2_70.tmp_1, reshape2_66.tmp_0, tmp_108, tmp_112, tmp_113, reshape2_66.tmp_1, tmp_107, )
I1018 08:53:57.748713 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.749066 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: conv2d_17.tmp_0
I1018 08:53:57.749070 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: conv2d_16.tmp_0
I1018 08:53:57.749074 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_17.b_0
I1018 08:53:57.749078 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_15.w_2
I1018 08:53:57.749081 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_17.w_0
I1018 08:53:57.749084 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_15.w_0
I1018 08:53:57.749087 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_17.w_1
I1018 08:53:57.749091 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_15.b_0
I1018 08:53:57.749094 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_15.w_1
I1018 08:53:57.749097 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_17.w_2
I1018 08:53:57.749101 4097533 build_cinn_pass.cc:274] Add Output Var Node: assign_65.tmp_0
I1018 08:53:57.749104 4097533 build_cinn_pass.cc:274] Add Output Var Node: assign_69.tmp_0
I1018 08:53:57.749109 4097533 build_cinn_pass.cc:274] Add Output Var Node: relu_14.tmp_0
I1018 08:53:57.749137 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_96[label="tmp_105
[1,512,7,7]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_95[label="fill_constant_135.tmp_0
[512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_94[label="tmp_106
[1,512,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_92[label="batch_norm_17.tmp_2
[1,512,7,7]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_93[label="elementwise_pow" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_91[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_83[label="elementwise_pow_66.tmp_0
[1,512,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_89[label="reshape2_71.tmp_1
[0,512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_80[label="scale" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_79[label="reshape2_68.tmp_1
[0,512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_78[label="reshape2_70.tmp_0
[1,512,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_77[label="reshape2_67.tmp_0
[1,512,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_84[label="reshape2_69.tmp_0
[1,512,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_76[label="batch_norm_16.tmp_2
[1,512,7,7]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_75[label="tmp_111
[1,512,7,7]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_73[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_72[label="elementwise_pow_68.tmp_0
[512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_71[label="batch_norm2d_15.w_2
[512]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_70[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_66[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_63[label="elementwise_add" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_62[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_60[label="scale" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_59[label="elementwise_mul" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_82[label="tmp_104
[512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_29[label="reshape2_65.tmp_0
[1,512,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_85[label="fill_constant_137.tmp_0
[1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_25[label="reshape2_70.tmp_1
[0,512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_22[label="tmp_112
[1,512,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_40[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_20[label="reshape2_66.tmp_1
[0,512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_18[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_13[label="batch_norm2d_15.w_0
[512]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_34[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_41[label="elementwise_pow" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_87[label="scale" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_19[label="batch_norm2d_15.b_0
[512]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_10[label="conv2d_16.tmp_0
[1,512,7,7]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_36[label="elementwise_mul" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_9[label="assign_69.tmp_0
[512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_4[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_5[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_16[label="batch_norm2d_17.w_1
[512]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_90[label="fill_constant_129.tmp_0
[512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_69[label="elementwise_pow" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_64[label="reshape2_68.tmp_0
[1,512,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_58[label="tmp_107
[1,512,7,7]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_11[label="batch_norm2d_17.b_0
[512]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_68[label="scale" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_28[label="elementwise_pow_64.tmp_0
[512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_26[label="reshape2_64.tmp_0
[1,512,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_23[label="tmp_108
[1,512,7,7]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_33[label="elementwise_sub" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_6[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_7[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_81[label="reshape2_69.tmp_1
[0,512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_49[label="elementwise_pow" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_8[label="assign_65.tmp_0
[512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_38[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_21[label="tmp_113
[1,512,7,7]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_3[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_88[label="elementwise_pow_70.tmp_0
[1,512,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_30[label="reshape2_65.tmp_1
[0,512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_2[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_31[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_17[label="batch_norm2d_15.w_1
[512]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_32[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_86[label="reshape2_71.tmp_0
[1,512,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_35[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_53[label="elementwise_mul" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_24[label="reshape2_66.tmp_0
[1,512,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_37[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_39[label="elementwise_max" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_42[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_43[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_15[label="conv2d_17.tmp_0
[1,512,7,7]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_44[label="tmp_114
[1,512,7,7]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_65[label="elementwise_add" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_45[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_46[label="fill_constant_131.tmp_0
[1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_74[label="fill_constant_125.tmp_0
[1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_27[label="tmp_110
[512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_47[label="elementwise_add" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_48[label="elementwise_sub" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_50[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_67[label="reshape2_67.tmp_1
[0,512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_12[label="batch_norm2d_17.w_0
[512]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_51[label="tmp_6
[1,512,7,7]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_52[label="reshape2_64.tmp_1
[0,512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_61[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_14[label="batch_norm2d_17.w_2
[512]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_54[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_55[label="relu_14.tmp_0
[1,512,7,7]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_56[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_57[label="elementwise_mul" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3->node_14
   node_4->node_17
   node_5->node_19
   node_6->node_11
   node_7->node_15
   node_8->node_2
   node_9->node_1
   node_10->node_48
   node_11->node_34
   node_12->node_38
   node_13->node_43
   node_14->node_66
   node_15->node_33
   node_16->node_56
   node_17->node_54
   node_18->node_16
   node_19->node_32
   node_21->node_59
   node_22->node_41
   node_23->node_63
   node_24->node_36
   node_26->node_48
   node_27->node_49
   node_28->node_70
   node_29->node_60
   node_31->node_85
   node_32->node_86
   node_32->node_89
   node_33->node_75
   node_34->node_77
   node_34->node_67
   node_35->node_71
   node_36->node_23
   node_37->node_10
   node_38->node_24
   node_38->node_20
   node_39->node_55
   node_40->node_9
   node_41->node_88
   node_42->node_95
   node_43->node_78
   node_43->node_25
   node_44->node_47
   node_45->node_12
   node_46->node_41
   node_46->node_49
   node_47->node_92
   node_48->node_96
   node_49->node_72
   node_50->node_90
   node_51->node_39
   node_53->node_58
   node_54->node_64
   node_54->node_79
   node_55->node_0
   node_56->node_26
   node_56->node_52
   node_57->node_21
   node_58->node_36
   node_59->node_44
   node_60->node_94
   node_61->node_13
   node_62->node_46
   node_63->node_76
   node_64->node_33
   node_65->node_51
   node_66->node_29
   node_66->node_30
   node_68->node_22
   node_69->node_83
   node_70->node_8
   node_71->node_91
   node_72->node_40
   node_73->node_74
   node_74->node_69
   node_74->node_93
   node_75->node_57
   node_76->node_65
   node_77->node_63
   node_78->node_59
   node_80->node_82
   node_82->node_93
   node_83->node_53
   node_84->node_68
   node_85->node_39
   node_86->node_47
   node_87->node_27
   node_88->node_57
   node_90->node_80
   node_91->node_84
   node_91->node_81
   node_92->node_65
   node_93->node_28
   node_94->node_69
   node_95->node_87
   node_96->node_53
} // end G
I1018 08:53:57.749625 4097533 var_desc.cc:415] Flush  assign_65.tmp_0 1
I1018 08:53:57.749629 4097533 var_desc.cc:415] Flush  assign_69.tmp_0 1
I1018 08:53:57.749632 4097533 var_desc.cc:415] Flush  conv2d_16.tmp_0 1
I1018 08:53:57.749634 4097533 var_desc.cc:415] Flush  batch_norm2d_17.b_0 1
I1018 08:53:57.749639 4097533 var_desc.cc:415] Flush  batch_norm2d_17.w_0 1
I1018 08:53:57.749642 4097533 var_desc.cc:415] Flush  batch_norm2d_15.w_0 1
I1018 08:53:57.749645 4097533 var_desc.cc:415] Flush  batch_norm2d_17.w_2 1
I1018 08:53:57.749647 4097533 var_desc.cc:415] Flush  conv2d_17.tmp_0 1
I1018 08:53:57.749650 4097533 var_desc.cc:415] Flush  batch_norm2d_17.w_1 1
I1018 08:53:57.749653 4097533 var_desc.cc:415] Flush  batch_norm2d_15.w_1 1
I1018 08:53:57.749655 4097533 var_desc.cc:415] Flush  batch_norm2d_15.b_0 1
I1018 08:53:57.749658 4097533 var_desc.cc:415] Flush  reshape2_66.tmp_1 1
I1018 08:53:57.749660 4097533 var_desc.cc:415] Flush  tmp_113 1
I1018 08:53:57.749663 4097533 var_desc.cc:415] Flush  tmp_112 1
I1018 08:53:57.749666 4097533 var_desc.cc:415] Flush  tmp_108 1
I1018 08:53:57.749668 4097533 var_desc.cc:415] Flush  reshape2_66.tmp_0 1
I1018 08:53:57.749671 4097533 var_desc.cc:415] Flush  reshape2_70.tmp_1 1
I1018 08:53:57.749675 4097533 var_desc.cc:415] Flush  reshape2_64.tmp_0 1
I1018 08:53:57.749677 4097533 var_desc.cc:415] Flush  tmp_110 1
I1018 08:53:57.749679 4097533 var_desc.cc:415] Flush  elementwise_pow_64.tmp_0 1
I1018 08:53:57.749682 4097533 var_desc.cc:415] Flush  reshape2_65.tmp_0 1
I1018 08:53:57.749684 4097533 var_desc.cc:415] Flush  reshape2_65.tmp_1 1
I1018 08:53:57.749687 4097533 var_desc.cc:415] Flush  tmp_114 1
I1018 08:53:57.749691 4097533 var_desc.cc:415] Flush  fill_constant_131.tmp_0 1
I1018 08:53:57.749692 4097533 var_desc.cc:415] Flush  tmp_6 1
I1018 08:53:57.749696 4097533 var_desc.cc:415] Flush  reshape2_64.tmp_1 1
I1018 08:53:57.749697 4097533 var_desc.cc:415] Flush  relu_14.tmp_0 1
I1018 08:53:57.749701 4097533 var_desc.cc:415] Flush  tmp_107 1
I1018 08:53:57.749702 4097533 var_desc.cc:415] Flush  reshape2_68.tmp_0 1
I1018 08:53:57.749706 4097533 var_desc.cc:415] Flush  reshape2_67.tmp_1 1
I1018 08:53:57.749708 4097533 var_desc.cc:415] Flush  batch_norm2d_15.w_2 1
I1018 08:53:57.749711 4097533 var_desc.cc:415] Flush  elementwise_pow_68.tmp_0 1
I1018 08:53:57.749713 4097533 var_desc.cc:415] Flush  fill_constant_125.tmp_0 1
I1018 08:53:57.749716 4097533 var_desc.cc:415] Flush  tmp_111 1
I1018 08:53:57.749719 4097533 var_desc.cc:415] Flush  batch_norm_16.tmp_2 1
I1018 08:53:57.749722 4097533 var_desc.cc:415] Flush  reshape2_67.tmp_0 1
I1018 08:53:57.749724 4097533 var_desc.cc:415] Flush  reshape2_70.tmp_0 1
I1018 08:53:57.749727 4097533 var_desc.cc:415] Flush  reshape2_68.tmp_1 1
I1018 08:53:57.749729 4097533 var_desc.cc:415] Flush  reshape2_69.tmp_1 1
I1018 08:53:57.749732 4097533 var_desc.cc:415] Flush  tmp_104 1
I1018 08:53:57.749734 4097533 var_desc.cc:415] Flush  elementwise_pow_66.tmp_0 1
I1018 08:53:57.749737 4097533 var_desc.cc:415] Flush  reshape2_69.tmp_0 1
I1018 08:53:57.749739 4097533 var_desc.cc:415] Flush  fill_constant_137.tmp_0 1
I1018 08:53:57.749742 4097533 var_desc.cc:415] Flush  reshape2_71.tmp_0 1
I1018 08:53:57.749744 4097533 var_desc.cc:415] Flush  elementwise_pow_70.tmp_0 1
I1018 08:53:57.749747 4097533 var_desc.cc:415] Flush  reshape2_71.tmp_1 1
I1018 08:53:57.749749 4097533 var_desc.cc:415] Flush  fill_constant_129.tmp_0 1
I1018 08:53:57.749751 4097533 var_desc.cc:415] Flush  batch_norm_17.tmp_2 1
I1018 08:53:57.749754 4097533 var_desc.cc:415] Flush  tmp_106 1
I1018 08:53:57.749758 4097533 var_desc.cc:415] Flush  fill_constant_135.tmp_0 1
I1018 08:53:57.749759 4097533 var_desc.cc:415] Flush  tmp_105 1
I1018 08:53:57.749786 4097533 graph_helper.h:104] adj elementwise_max0x55b7d0b68e00 -> fetch0x55b7d0cbcd10  via relu_14.tmp_00x55b7d0d3c780
I1018 08:53:57.749794 4097533 graph_helper.h:104] adj assign0x55b7d0d469d0 -> fetch0x55b7d0cbc940  via assign_69.tmp_00x55b7d0d3c450
I1018 08:53:57.749797 4097533 graph_helper.h:104] adj assign0x55b7d0ba5af0 -> fetch0x55b7d0cbc590  via assign_65.tmp_00x55b7d0d3cac0
I1018 08:53:57.749802 4097533 graph_helper.h:104] adj feed0x55b7d0c7ea70 -> reshape20x55b7d0d72a80  via batch_norm2d_15.b_00x55b7d0d82db0
I1018 08:53:57.749811 4097533 graph_helper.h:104] adj reshape20x55b7d0d77260 -> elementwise_sub0x55b7d104bde0  via reshape2_68.tmp_00x55b7d0c71c00
I1018 08:53:57.749819 4097533 graph_helper.h:104] adj feed0x55b7d0c205f0 -> elementwise_sub0x55b7d104bde0  via conv2d_17.tmp_00x55b7d0d83800
I1018 08:53:57.749825 4097533 graph_helper.h:104] adj feed0x55b7d0c7d880 -> reshape20x55b7d0d4f380  via batch_norm2d_17.b_00x55b7d0d3bda0
I1018 08:53:57.749832 4097533 graph_helper.h:104] adj reshape20x55b7d0b4b440 -> elementwise_mul0x55b7d0c7a440  via reshape2_66.tmp_00x55b7d0cb0410
I1018 08:53:57.749837 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0d7ae30 -> elementwise_mul0x55b7d0c7a440  via tmp_1070x55b7d0d82a70
I1018 08:53:57.749843 4097533 graph_helper.h:104] adj feed0x55b7d0c7df60 -> reshape20x55b7d0b4b440  via batch_norm2d_17.w_00x55b7d0d3b6c0
I1018 08:53:57.749850 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0d8d9f0 -> elementwise_max0x55b7d0b68e00  via tmp_60x55b7d0cf7cc0
I1018 08:53:57.749855 4097533 graph_helper.h:104] adj fill_constant0x55b7d0d0d650 -> elementwise_max0x55b7d0b68e00  via fill_constant_137.tmp_00x55b7d0cf7950
I1018 08:53:57.749861 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0d453c0 -> assign0x55b7d0d469d0  via elementwise_pow_68.tmp_00x55b7d0fa4480
I1018 08:53:57.749866 4097533 graph_helper.h:104] adj scale0x55b7d0d741e0 -> elementwise_pow0x55b7d0d9dbb0  via tmp_1120x55b7d0cb0ad0
I1018 08:53:57.749872 4097533 graph_helper.h:104] adj fill_constant0x55b7d104e000 -> elementwise_pow0x55b7d0d9dbb0  via fill_constant_131.tmp_00x55b7d0cf8000
I1018 08:53:57.749877 4097533 graph_helper.h:104] adj feed0x55b7d0c7e2f0 -> reshape20x55b7d1050310  via batch_norm2d_15.w_00x55b7d0d3b350
I1018 08:53:57.749884 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0d3a390 -> elementwise_add0x55b7d0beb9d0  via tmp_1140x55b7d0d1f690
I1018 08:53:57.749890 4097533 graph_helper.h:104] adj reshape20x55b7d0d72a80 -> elementwise_add0x55b7d0beb9d0  via reshape2_71.tmp_00x55b7d0d1fc50
I1018 08:53:57.749895 4097533 graph_helper.h:104] adj reshape20x55b7d1088d20 -> elementwise_sub0x55b7d0d72e00  via reshape2_64.tmp_00x55b7d0cafd20
I1018 08:53:57.749902 4097533 graph_helper.h:104] adj feed0x55b7d0c20960 -> elementwise_sub0x55b7d0d72e00  via conv2d_16.tmp_00x55b7d0d3c110
I1018 08:53:57.749907 4097533 graph_helper.h:104] adj scale0x55b7d0d80ef0 -> elementwise_pow0x55b7d0d453c0  via tmp_1100x55b7d0caf9f0
I1018 08:53:57.749912 4097533 graph_helper.h:104] adj fill_constant0x55b7d104e000 -> elementwise_pow0x55b7d0d453c0  via fill_constant_131.tmp_00x55b7d0cf8000
I1018 08:53:57.749917 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d0d72e00 -> elementwise_mul0x55b7d0d7ae30  via tmp_1050x55b7d0c718c0
I1018 08:53:57.749923 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0ce7a60 -> elementwise_mul0x55b7d0d7ae30  via elementwise_pow_66.tmp_00x55b7d0d1f0a0
I1018 08:53:57.749928 4097533 graph_helper.h:104] adj feed0x55b7d0c7ee60 -> reshape20x55b7d0d77260  via batch_norm2d_15.w_10x55b7d0d83120
I1018 08:53:57.749935 4097533 graph_helper.h:104] adj feed0x55b7d0c7e680 -> reshape20x55b7d1088d20  via batch_norm2d_17.w_10x55b7d0d83490
I1018 08:53:57.749941 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d104bde0 -> elementwise_mul0x55b7d0d6a980  via tmp_1110x55b7d0cf72a0
I1018 08:53:57.749946 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0d9dbb0 -> elementwise_mul0x55b7d0d6a980  via elementwise_pow_70.tmp_00x55b7d0cf6860
I1018 08:53:57.749953 4097533 graph_helper.h:104] adj reshape20x55b7d1050310 -> elementwise_mul0x55b7d0d3a390  via reshape2_70.tmp_00x55b7d0d1f950
I1018 08:53:57.749958 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0d6a980 -> elementwise_mul0x55b7d0d3a390  via tmp_1130x55b7d0cb0e10
I1018 08:53:57.749962 4097533 graph_helper.h:104] adj reshape20x55b7d0d7dbf0 -> scale0x55b7d0f3f6e0  via reshape2_65.tmp_00x55b7d0c722f0
I1018 08:53:57.749969 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0c7a440 -> elementwise_add0x55b7d0ce15f0  via tmp_1080x55b7d0cb0790
I1018 08:53:57.749975 4097533 graph_helper.h:104] adj reshape20x55b7d0d4f380 -> elementwise_add0x55b7d0ce15f0  via reshape2_67.tmp_00x55b7d0fa4340
I1018 08:53:57.749982 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0ce15f0 -> elementwise_add0x55b7d0d8d9f0  via batch_norm_16.tmp_20x55b7d0fa3f40
I1018 08:53:57.749987 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0beb9d0 -> elementwise_add0x55b7d0d8d9f0  via batch_norm_17.tmp_20x55b7d0c70e60
I1018 08:53:57.749994 4097533 graph_helper.h:104] adj feed0x55b7d0cbc200 -> reshape20x55b7d0d7dbf0  via batch_norm2d_17.w_20x55b7d0d83b40
I1018 08:53:57.750000 4097533 graph_helper.h:104] adj reshape20x55b7d0d4e280 -> scale0x55b7d0d741e0  via reshape2_69.tmp_00x55b7d0d1f390
I1018 08:53:57.750005 4097533 graph_helper.h:104] adj scale0x55b7d0f3f6e0 -> elementwise_pow0x55b7d0ce7a60  via tmp_1060x55b7d0d1ff50
I1018 08:53:57.750011 4097533 graph_helper.h:104] adj fill_constant0x55b7d0f691e0 -> elementwise_pow0x55b7d0ce7a60  via fill_constant_125.tmp_00x55b7d0fa41e0
I1018 08:53:57.750016 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0b9c030 -> assign0x55b7d0ba5af0  via elementwise_pow_64.tmp_00x55b7d0caf660
I1018 08:53:57.750022 4097533 graph_helper.h:104] adj fill_constant0x55b7d0c6f5f0 -> scale0x55b7d111c6b0  via fill_constant_129.tmp_00x55b7d0cf75e0
I1018 08:53:57.750028 4097533 graph_helper.h:104] adj fill_constant0x55b7d0d70fb0 -> scale0x55b7d0d80ef0  via fill_constant_135.tmp_00x55b7d0c71550
I1018 08:53:57.750034 4097533 graph_helper.h:104] adj feed0x55b7d0c7dbd0 -> reshape20x55b7d0d4e280  via batch_norm2d_15.w_20x55b7d0d3ba30
I1018 08:53:57.750041 4097533 graph_helper.h:104] adj scale0x55b7d111c6b0 -> elementwise_pow0x55b7d0b9c030  via tmp_1040x55b7d0cf6f70
I1018 08:53:57.750046 4097533 graph_helper.h:104] adj fill_constant0x55b7d0f691e0 -> elementwise_pow0x55b7d0b9c030  via fill_constant_125.tmp_00x55b7d0fa41e0
I1018 08:53:57.750298 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.750301 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750311 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.750314 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750329 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.750331 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750348 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.750351 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750358 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.750361 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750383 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.750386 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750394 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.750397 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750404 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.750407 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750411 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.750413 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750452 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.750454 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750461 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.750464 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750473 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.750475 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750478 4097533 graph_helper.cc:698] convert op node to desc elementwise_sub
I1018 08:53:57.750481 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750489 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.750492 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750499 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.750501 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750504 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.750507 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750514 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.750517 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750591 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.750593 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750602 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.750603 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750607 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.750609 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750617 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.750619 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750622 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.750625 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750631 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.750634 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750641 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.750644 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750651 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.750653 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750656 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.750659 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750666 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.750669 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750671 4097533 graph_helper.cc:698] convert op node to desc elementwise_sub
I1018 08:53:57.750674 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750681 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.750684 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750690 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.750694 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750699 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.750702 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750708 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.750711 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750715 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.750716 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750850 4097533 graph_helper.cc:698] convert op node to desc elementwise_add
I1018 08:53:57.750852 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750860 4097533 graph_helper.cc:698] convert op node to desc elementwise_add
I1018 08:53:57.750862 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750870 4097533 graph_helper.cc:698] convert op node to desc elementwise_add
I1018 08:53:57.750873 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750880 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.750881 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750888 4097533 graph_helper.cc:698] convert op node to desc elementwise_max
I1018 08:53:57.750891 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750897 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.750900 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750907 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.750910 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750916 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.750919 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750926 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.750928 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750934 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.750937 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750943 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.750946 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750950 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.750952 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.750955 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.750958 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.752125 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.753000 4097533 block_desc.cc:205] vars in desc 51
I1018 08:53:57.753005 4097533 block_desc.cc:209] Flush assign_65.tmp_0
I1018 08:53:57.753008 4097533 var_desc.cc:415] Flush  assign_65.tmp_0 1
I1018 08:53:57.753011 4097533 block_desc.cc:209] Flush assign_69.tmp_0
I1018 08:53:57.753013 4097533 var_desc.cc:415] Flush  assign_69.tmp_0 1
I1018 08:53:57.753016 4097533 block_desc.cc:209] Flush conv2d_16.tmp_0
I1018 08:53:57.753018 4097533 var_desc.cc:415] Flush  conv2d_16.tmp_0 1
I1018 08:53:57.753021 4097533 block_desc.cc:209] Flush batch_norm2d_17.b_0
I1018 08:53:57.753023 4097533 var_desc.cc:415] Flush  batch_norm2d_17.b_0 1
I1018 08:53:57.753026 4097533 block_desc.cc:209] Flush batch_norm2d_17.w_0
I1018 08:53:57.753028 4097533 var_desc.cc:415] Flush  batch_norm2d_17.w_0 1
I1018 08:53:57.753031 4097533 block_desc.cc:209] Flush batch_norm2d_15.w_0
I1018 08:53:57.753032 4097533 var_desc.cc:415] Flush  batch_norm2d_15.w_0 1
I1018 08:53:57.753036 4097533 block_desc.cc:209] Flush batch_norm2d_17.w_2
I1018 08:53:57.753037 4097533 var_desc.cc:415] Flush  batch_norm2d_17.w_2 1
I1018 08:53:57.753039 4097533 block_desc.cc:209] Flush conv2d_17.tmp_0
I1018 08:53:57.753042 4097533 var_desc.cc:415] Flush  conv2d_17.tmp_0 1
I1018 08:53:57.753044 4097533 block_desc.cc:209] Flush batch_norm2d_17.w_1
I1018 08:53:57.753046 4097533 var_desc.cc:415] Flush  batch_norm2d_17.w_1 1
I1018 08:53:57.753049 4097533 block_desc.cc:209] Flush batch_norm2d_15.w_1
I1018 08:53:57.753051 4097533 var_desc.cc:415] Flush  batch_norm2d_15.w_1 1
I1018 08:53:57.753053 4097533 block_desc.cc:209] Flush batch_norm2d_15.b_0
I1018 08:53:57.753055 4097533 var_desc.cc:415] Flush  batch_norm2d_15.b_0 1
I1018 08:53:57.753058 4097533 block_desc.cc:209] Flush reshape2_66.tmp_1
I1018 08:53:57.753060 4097533 var_desc.cc:415] Flush  reshape2_66.tmp_1 1
I1018 08:53:57.753063 4097533 block_desc.cc:209] Flush tmp_113
I1018 08:53:57.753065 4097533 var_desc.cc:415] Flush  tmp_113 1
I1018 08:53:57.753067 4097533 block_desc.cc:209] Flush tmp_112
I1018 08:53:57.753070 4097533 var_desc.cc:415] Flush  tmp_112 1
I1018 08:53:57.753072 4097533 block_desc.cc:209] Flush tmp_108
I1018 08:53:57.753074 4097533 var_desc.cc:415] Flush  tmp_108 1
I1018 08:53:57.753077 4097533 block_desc.cc:209] Flush reshape2_66.tmp_0
I1018 08:53:57.753079 4097533 var_desc.cc:415] Flush  reshape2_66.tmp_0 1
I1018 08:53:57.753082 4097533 block_desc.cc:209] Flush reshape2_70.tmp_1
I1018 08:53:57.753084 4097533 var_desc.cc:415] Flush  reshape2_70.tmp_1 1
I1018 08:53:57.753086 4097533 block_desc.cc:209] Flush reshape2_64.tmp_0
I1018 08:53:57.753088 4097533 var_desc.cc:415] Flush  reshape2_64.tmp_0 1
I1018 08:53:57.753091 4097533 block_desc.cc:209] Flush tmp_110
I1018 08:53:57.753093 4097533 var_desc.cc:415] Flush  tmp_110 1
I1018 08:53:57.753095 4097533 block_desc.cc:209] Flush elementwise_pow_64.tmp_0
I1018 08:53:57.753098 4097533 var_desc.cc:415] Flush  elementwise_pow_64.tmp_0 1
I1018 08:53:57.753100 4097533 block_desc.cc:209] Flush reshape2_65.tmp_0
I1018 08:53:57.753103 4097533 var_desc.cc:415] Flush  reshape2_65.tmp_0 1
I1018 08:53:57.753105 4097533 block_desc.cc:209] Flush reshape2_65.tmp_1
I1018 08:53:57.753108 4097533 var_desc.cc:415] Flush  reshape2_65.tmp_1 1
I1018 08:53:57.753109 4097533 block_desc.cc:209] Flush tmp_114
I1018 08:53:57.753111 4097533 var_desc.cc:415] Flush  tmp_114 1
I1018 08:53:57.753114 4097533 block_desc.cc:209] Flush fill_constant_131.tmp_0
I1018 08:53:57.753116 4097533 var_desc.cc:415] Flush  fill_constant_131.tmp_0 1
I1018 08:53:57.753119 4097533 block_desc.cc:209] Flush tmp_6
I1018 08:53:57.753121 4097533 var_desc.cc:415] Flush  tmp_6 1
I1018 08:53:57.753123 4097533 block_desc.cc:209] Flush reshape2_64.tmp_1
I1018 08:53:57.753125 4097533 var_desc.cc:415] Flush  reshape2_64.tmp_1 1
I1018 08:53:57.753129 4097533 block_desc.cc:209] Flush relu_14.tmp_0
I1018 08:53:57.753130 4097533 var_desc.cc:415] Flush  relu_14.tmp_0 1
I1018 08:53:57.753134 4097533 block_desc.cc:209] Flush tmp_107
I1018 08:53:57.753139 4097533 var_desc.cc:415] Flush  tmp_107 1
I1018 08:53:57.753141 4097533 block_desc.cc:209] Flush reshape2_68.tmp_0
I1018 08:53:57.753144 4097533 var_desc.cc:415] Flush  reshape2_68.tmp_0 1
I1018 08:53:57.753146 4097533 block_desc.cc:209] Flush reshape2_67.tmp_1
I1018 08:53:57.753149 4097533 var_desc.cc:415] Flush  reshape2_67.tmp_1 1
I1018 08:53:57.753150 4097533 block_desc.cc:209] Flush batch_norm2d_15.w_2
I1018 08:53:57.753152 4097533 var_desc.cc:415] Flush  batch_norm2d_15.w_2 1
I1018 08:53:57.753155 4097533 block_desc.cc:209] Flush elementwise_pow_68.tmp_0
I1018 08:53:57.753157 4097533 var_desc.cc:415] Flush  elementwise_pow_68.tmp_0 1
I1018 08:53:57.753160 4097533 block_desc.cc:209] Flush fill_constant_125.tmp_0
I1018 08:53:57.753162 4097533 var_desc.cc:415] Flush  fill_constant_125.tmp_0 1
I1018 08:53:57.753165 4097533 block_desc.cc:209] Flush tmp_111
I1018 08:53:57.753166 4097533 var_desc.cc:415] Flush  tmp_111 1
I1018 08:53:57.753170 4097533 block_desc.cc:209] Flush batch_norm_16.tmp_2
I1018 08:53:57.753171 4097533 var_desc.cc:415] Flush  batch_norm_16.tmp_2 1
I1018 08:53:57.753173 4097533 block_desc.cc:209] Flush reshape2_67.tmp_0
I1018 08:53:57.753176 4097533 var_desc.cc:415] Flush  reshape2_67.tmp_0 1
I1018 08:53:57.753178 4097533 block_desc.cc:209] Flush reshape2_70.tmp_0
I1018 08:53:57.753180 4097533 var_desc.cc:415] Flush  reshape2_70.tmp_0 1
I1018 08:53:57.753183 4097533 block_desc.cc:209] Flush reshape2_68.tmp_1
I1018 08:53:57.753185 4097533 var_desc.cc:415] Flush  reshape2_68.tmp_1 1
I1018 08:53:57.753187 4097533 block_desc.cc:209] Flush reshape2_69.tmp_1
I1018 08:53:57.753190 4097533 var_desc.cc:415] Flush  reshape2_69.tmp_1 1
I1018 08:53:57.753192 4097533 block_desc.cc:209] Flush tmp_104
I1018 08:53:57.753194 4097533 var_desc.cc:415] Flush  tmp_104 1
I1018 08:53:57.753197 4097533 block_desc.cc:209] Flush elementwise_pow_66.tmp_0
I1018 08:53:57.753199 4097533 var_desc.cc:415] Flush  elementwise_pow_66.tmp_0 1
I1018 08:53:57.753201 4097533 block_desc.cc:209] Flush reshape2_69.tmp_0
I1018 08:53:57.753203 4097533 var_desc.cc:415] Flush  reshape2_69.tmp_0 1
I1018 08:53:57.753211 4097533 block_desc.cc:209] Flush fill_constant_137.tmp_0
I1018 08:53:57.753213 4097533 var_desc.cc:415] Flush  fill_constant_137.tmp_0 1
I1018 08:53:57.753216 4097533 block_desc.cc:209] Flush reshape2_71.tmp_0
I1018 08:53:57.753218 4097533 var_desc.cc:415] Flush  reshape2_71.tmp_0 1
I1018 08:53:57.753221 4097533 block_desc.cc:209] Flush elementwise_pow_70.tmp_0
I1018 08:53:57.753223 4097533 var_desc.cc:415] Flush  elementwise_pow_70.tmp_0 1
I1018 08:53:57.753225 4097533 block_desc.cc:209] Flush reshape2_71.tmp_1
I1018 08:53:57.753227 4097533 var_desc.cc:415] Flush  reshape2_71.tmp_1 1
I1018 08:53:57.753230 4097533 block_desc.cc:209] Flush fill_constant_129.tmp_0
I1018 08:53:57.753232 4097533 var_desc.cc:415] Flush  fill_constant_129.tmp_0 1
I1018 08:53:57.753234 4097533 block_desc.cc:209] Flush batch_norm_17.tmp_2
I1018 08:53:57.753237 4097533 var_desc.cc:415] Flush  batch_norm_17.tmp_2 1
I1018 08:53:57.753239 4097533 block_desc.cc:209] Flush tmp_106
I1018 08:53:57.753242 4097533 var_desc.cc:415] Flush  tmp_106 1
I1018 08:53:57.753243 4097533 block_desc.cc:209] Flush fill_constant_135.tmp_0
I1018 08:53:57.753247 4097533 var_desc.cc:415] Flush  fill_constant_135.tmp_0 1
I1018 08:53:57.753248 4097533 block_desc.cc:209] Flush tmp_105
I1018 08:53:57.753250 4097533 var_desc.cc:415] Flush  tmp_105 1
I1018 08:53:57.749616 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "assign_65.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "assign_69.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "conv2d_16.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 7
          dims: 7
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_17.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_17.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_15.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_17.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "conv2d_17.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 7
          dims: 7
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_17.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "batch_norm2d_15.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "batch_norm2d_15.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "reshape2_66.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "tmp_113"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 7
          dims: 7
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "tmp_112"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  vars {
    name: "tmp_108"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 7
          dims: 7
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "reshape2_66.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_70.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_64.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_110"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  vars {
    name: "elementwise_pow_64.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_65.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_65.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_114"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 7
          dims: 7
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "fill_constant_131.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_6"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 7
          dims: 7
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "reshape2_64.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "relu_14.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 7
          dims: 7
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "tmp_107"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 7
          dims: 7
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "reshape2_68.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_67.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_15.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "elementwise_pow_68.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_125.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_111"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 7
          dims: 7
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "batch_norm_16.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 7
          dims: 7
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_67.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_70.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_68.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_69.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_104"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  vars {
    name: "elementwise_pow_66.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_69.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_137.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_71.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "elementwise_pow_70.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_71.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "fill_constant_129.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "batch_norm_17.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 7
          dims: 7
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "tmp_106"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  vars {
    name: "fill_constant_135.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_105"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 7
          dims: 7
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  ops {
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "ShapeTensorList"
    }
    inputs {
      parameter: "ValueTensor"
    }
    outputs {
      parameter: "Out"
      arguments: "fill_constant_135.tmp_0"
    }
    type: "fill_constant"
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "dtype"
      type: INT
      i: 5
    }
    attrs {
      name: "force_cpu"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 118, in composite_batchnorm"
      strings: "    batch_var = zeros(run_var.shape, run_var.dtype)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 1117, in zeros"
      strings: "    return fill_constant(value=0.0, shape=shape, dtype=dtype, name=name)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 977, in fill_constant"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "place_type"
      type: INT
      i: -1
    }
    attrs {
      name: "shape"
      type: LONGS
      longs: 512
    }
    attrs {
      name: "str_value"
      type: STRING
      s: "0.0"
    }
    attrs {
      name: "value"
      type: FLOAT
      f: 0
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "BiasTensor"
    }
    inputs {
      parameter: "ScaleTensor"
    }
    inputs {
      parameter: "X"
      arguments: "fill_constant_135.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_110"
    }
    type: "scale"
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "bias"
      type: FLOAT
      f: 1e-05
    }
    attrs {
      name: "bias_after_scale"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 119, in composite_batchnorm"
      strings: "    inv_std = pow((batch_var + epsilon), half)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 557, in __impl__"
      strings: "    return scalar_method(self, other_var)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 530, in _scalar_add_"
      strings: "    return _scalar_op_(var, 1.0, value)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 451, in _scalar_op_"
      strings: "    block.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "scale"
      type: FLOAT
      f: 1
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_17.w_2"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_17.w_2"
    }
    outputs {
      parameter: "Out"
      arguments: "reshape2_65.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_65.tmp_1"
    }
    type: "reshape2"
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use
I1018 08:53:57.755563 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.755736 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_139.tmp_0, is_only_used_internal: 1
I1018 08:53:57.755739 4097533 build_cinn_pass.cc:557] var_node->outputs[1]: fill_constant_139.tmp_0, is_only_used_internal: 1
I1018 08:53:57.755741 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_139.tmp_0
I1018 08:53:57.755744 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: elementwise_pow_72.tmp_0, is_only_used_internal: 1
I1018 08:53:57.755746 4097533 build_cinn_pass.cc:562] insert internal var: elementwise_pow_72.tmp_0
I1018 08:53:57.755748 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_116, is_only_used_internal: 1
I1018 08:53:57.755751 4097533 build_cinn_pass.cc:562] insert internal var: tmp_116
I1018 08:53:57.755753 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_117, is_only_used_internal: 1
I1018 08:53:57.755755 4097533 build_cinn_pass.cc:562] insert internal var: tmp_117
I1018 08:53:57.755757 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_73.tmp_0, is_only_used_internal: 1
I1018 08:53:57.755759 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_73.tmp_0
I1018 08:53:57.755761 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_118, is_only_used_internal: 1
I1018 08:53:57.755764 4097533 build_cinn_pass.cc:562] insert internal var: tmp_118
I1018 08:53:57.755766 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_119, is_only_used_internal: 1
I1018 08:53:57.755769 4097533 build_cinn_pass.cc:562] insert internal var: tmp_119
I1018 08:53:57.755770 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_73.tmp_1
I1018 08:53:57.755772 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_145.tmp_0, is_only_used_internal: 1
I1018 08:53:57.755774 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_145.tmp_0
I1018 08:53:57.755777 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_72.tmp_0, is_only_used_internal: 1
I1018 08:53:57.755779 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_72.tmp_0
I1018 08:53:57.755781 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_143.tmp_0, is_only_used_internal: 1
I1018 08:53:57.755784 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_143.tmp_0
I1018 08:53:57.755785 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_72.tmp_1
I1018 08:53:57.755788 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: elementwise_pow_74.tmp_0, is_only_used_internal: 1
I1018 08:53:57.755790 4097533 build_cinn_pass.cc:562] insert internal var: elementwise_pow_74.tmp_0
I1018 08:53:57.755792 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_74.tmp_1
I1018 08:53:57.755795 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_74.tmp_0, is_only_used_internal: 1
I1018 08:53:57.755797 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_74.tmp_0
I1018 08:53:57.755800 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_120, is_only_used_internal: 1
I1018 08:53:57.755801 4097533 build_cinn_pass.cc:562] insert internal var: tmp_120
I1018 08:53:57.755803 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_75.tmp_1
I1018 08:53:57.755805 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_75.tmp_0, is_only_used_internal: 1
I1018 08:53:57.755810 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_75.tmp_0
I1018 08:53:57.755813 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: batch_norm_18.tmp_2, is_only_used_internal: 1
I1018 08:53:57.755815 4097533 build_cinn_pass.cc:562] insert internal var: batch_norm_18.tmp_2
I1018 08:53:57.755818 4097533 build_cinn_pass.cc:742] Cluster Ops: (scale, reshape2, elementwise_pow, elementwise_mul, reshape2, elementwise_mul, fill_constant, reshape2, elementwise_max, elementwise_add, fill_constant, reshape2, elementwise_sub, scale, elementwise_pow, assign, fill_constant, )
I1018 08:53:57.755821 4097533 build_cinn_pass.cc:743] Cluster input vars: (conv2d_18.tmp_0, batch_norm2d_18.w_2, batch_norm2d_18.w_1, batch_norm2d_18.w_0, batch_norm2d_18.b_0, )
I1018 08:53:57.755823 4097533 build_cinn_pass.cc:744] Cluster output vars: (assign_73.tmp_0, relu_15.tmp_0, )
I1018 08:53:57.755826 4097533 build_cinn_pass.cc:745] Cluster internal vars: (batch_norm_18.tmp_2, reshape2_75.tmp_0, reshape2_75.tmp_1, tmp_120, reshape2_74.tmp_0, reshape2_74.tmp_1, fill_constant_139.tmp_0, elementwise_pow_72.tmp_0, tmp_116, tmp_118, tmp_119, reshape2_73.tmp_1, fill_constant_145.tmp_0, tmp_117, reshape2_73.tmp_0, reshape2_72.tmp_0, fill_constant_143.tmp_0, reshape2_72.tmp_1, elementwise_pow_74.tmp_0, )
I1018 08:53:57.755834 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.756067 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: conv2d_18.tmp_0
I1018 08:53:57.756070 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_18.w_2
I1018 08:53:57.756073 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_18.b_0
I1018 08:53:57.756078 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_18.w_0
I1018 08:53:57.756080 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_18.w_1
I1018 08:53:57.756084 4097533 build_cinn_pass.cc:274] Add Output Var Node: assign_73.tmp_0
I1018 08:53:57.756088 4097533 build_cinn_pass.cc:274] Add Output Var Node: relu_15.tmp_0
I1018 08:53:57.756108 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_48[label="elementwise_pow_72.tmp_0
[512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_47[label="batch_norm2d_18.w_2
[512]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_46[label="tmp_119
[1,512,7,7]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_45[label="fill_constant_139.tmp_0
[1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_44[label="tmp_116
[512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_43[label="reshape2_74.tmp_1
[0,512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_42[label="elementwise_sub" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_39[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_37[label="scale" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_35[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_32[label="batch_norm_18.tmp_2
[1,512,7,7]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_31[label="batch_norm2d_18.b_0
[512]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_30[label="elementwise_mul" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="batch_norm2d_18.w_1
[512]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_21[label="elementwise_mul" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_38[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_8[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_49[label="reshape2_73.tmp_1
[0,512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_7[label="reshape2_72.tmp_1
[0,512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_6[label="assign_73.tmp_0
[512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_33[label="relu_15.tmp_0
[1,512,7,7]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_26[label="tmp_118
[1,512,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_11[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_16[label="elementwise_add" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_12[label="elementwise_max" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_5[label="elementwise_pow_74.tmp_0
[1,512,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_4[label="conv2d_18.tmp_0
[1,512,7,7]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_24[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_27[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_9[label="reshape2_72.tmp_0
[1,512,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_36[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_17[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_10[label="tmp_117
[1,512,7,7]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_19[label="elementwise_pow" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_41[label="fill_constant_143.tmp_0
[512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_34[label="reshape2_74.tmp_0
[1,512,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_13[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_23[label="reshape2_75.tmp_1
[0,512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_14[label="tmp_120
[1,512,7,7]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_2[label="batch_norm2d_18.w_0
[512]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_15[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_18[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_20[label="reshape2_73.tmp_0
[1,512,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_40[label="reshape2_75.tmp_0
[1,512,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_22[label="fill_constant_145.tmp_0
[1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_25[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_29[label="scale" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_28[label="elementwise_pow" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_3
   node_2->node_35
   node_3->node_36
   node_4->node_42
   node_5->node_30
   node_6->node_0
   node_9->node_42
   node_10->node_30
   node_11->node_41
   node_12->node_33
   node_13->node_40
   node_13->node_23
   node_14->node_16
   node_15->node_22
   node_16->node_32
   node_17->node_47
   node_18->node_4
   node_19->node_48
   node_20->node_29
   node_21->node_14
   node_22->node_12
   node_24->node_31
   node_25->node_20
   node_25->node_49
   node_26->node_28
   node_27->node_2
   node_28->node_5
   node_29->node_26
   node_30->node_46
   node_31->node_13
   node_32->node_12
   node_33->node_8
   node_34->node_21
   node_35->node_34
   node_35->node_43
   node_36->node_9
   node_36->node_7
   node_37->node_44
   node_38->node_6
   node_39->node_45
   node_40->node_16
   node_41->node_37
   node_42->node_10
   node_44->node_19
   node_45->node_28
   node_45->node_19
   node_46->node_21
   node_47->node_25
   node_48->node_38
} // end G
I1018 08:53:57.756367 4097533 var_desc.cc:415] Flush  batch_norm2d_18.w_0 1
I1018 08:53:57.756371 4097533 var_desc.cc:415] Flush  batch_norm2d_18.w_1 1
I1018 08:53:57.756374 4097533 var_desc.cc:415] Flush  conv2d_18.tmp_0 1
I1018 08:53:57.756376 4097533 var_desc.cc:415] Flush  elementwise_pow_74.tmp_0 1
I1018 08:53:57.756379 4097533 var_desc.cc:415] Flush  assign_73.tmp_0 1
I1018 08:53:57.756382 4097533 var_desc.cc:415] Flush  reshape2_72.tmp_1 1
I1018 08:53:57.756384 4097533 var_desc.cc:415] Flush  reshape2_72.tmp_0 1
I1018 08:53:57.756387 4097533 var_desc.cc:415] Flush  tmp_117 1
I1018 08:53:57.756390 4097533 var_desc.cc:415] Flush  tmp_120 1
I1018 08:53:57.756393 4097533 var_desc.cc:415] Flush  reshape2_73.tmp_0 1
I1018 08:53:57.756397 4097533 var_desc.cc:415] Flush  fill_constant_145.tmp_0 1
I1018 08:53:57.756398 4097533 var_desc.cc:415] Flush  reshape2_75.tmp_1 1
I1018 08:53:57.756402 4097533 var_desc.cc:415] Flush  tmp_118 1
I1018 08:53:57.756404 4097533 var_desc.cc:415] Flush  batch_norm2d_18.b_0 1
I1018 08:53:57.756407 4097533 var_desc.cc:415] Flush  batch_norm_18.tmp_2 1
I1018 08:53:57.756409 4097533 var_desc.cc:415] Flush  relu_15.tmp_0 1
I1018 08:53:57.756412 4097533 var_desc.cc:415] Flush  reshape2_74.tmp_0 1
I1018 08:53:57.756415 4097533 var_desc.cc:415] Flush  reshape2_75.tmp_0 1
I1018 08:53:57.756417 4097533 var_desc.cc:415] Flush  fill_constant_143.tmp_0 1
I1018 08:53:57.756420 4097533 var_desc.cc:415] Flush  reshape2_74.tmp_1 1
I1018 08:53:57.756423 4097533 var_desc.cc:415] Flush  tmp_116 1
I1018 08:53:57.756425 4097533 var_desc.cc:415] Flush  fill_constant_139.tmp_0 1
I1018 08:53:57.756428 4097533 var_desc.cc:415] Flush  tmp_119 1
I1018 08:53:57.756430 4097533 var_desc.cc:415] Flush  batch_norm2d_18.w_2 1
I1018 08:53:57.756433 4097533 var_desc.cc:415] Flush  elementwise_pow_72.tmp_0 1
I1018 08:53:57.756436 4097533 var_desc.cc:415] Flush  reshape2_73.tmp_1 1
I1018 08:53:57.756453 4097533 graph_helper.h:104] adj assign0x55b7d0fba110 -> fetch0x55b7d0ca2970  via assign_73.tmp_00x55b7d0e0afe0
I1018 08:53:57.756460 4097533 graph_helper.h:104] adj elementwise_max0x55b7d0d39940 -> fetch0x55b7d0ca2cc0  via relu_15.tmp_00x55b7d0e0b310
I1018 08:53:57.756464 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0dd0df0 -> elementwise_max0x55b7d0d39940  via batch_norm_18.tmp_20x55b7d0dfeec0
I1018 08:53:57.756471 4097533 graph_helper.h:104] adj fill_constant0x55b7d0dede00 -> elementwise_max0x55b7d0d39940  via fill_constant_145.tmp_00x55b7d0dbc820
I1018 08:53:57.756477 4097533 graph_helper.h:104] adj feed0x55b7d0d94e20 -> reshape20x55b7d0de3930  via batch_norm2d_18.b_00x55b7d0dcd240
I1018 08:53:57.756484 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0daf4e0 -> elementwise_add0x55b7d0dd0df0  via tmp_1200x55b7d0dbf8d0
I1018 08:53:57.756490 4097533 graph_helper.h:104] adj reshape20x55b7d0de3930 -> elementwise_add0x55b7d0dd0df0  via reshape2_75.tmp_00x55b7d0dbf750
I1018 08:53:57.756496 4097533 graph_helper.h:104] adj scale0x55b7d0e01600 -> elementwise_pow0x55b7d0e2b280  via tmp_1160x55b7d0dbbe80
I1018 08:53:57.756502 4097533 graph_helper.h:104] adj fill_constant0x55b7d0d93d90 -> elementwise_pow0x55b7d0e2b280  via fill_constant_139.tmp_00x55b7d0dbfb80
I1018 08:53:57.756511 4097533 graph_helper.h:104] adj reshape20x55b7d0de2710 -> elementwise_mul0x55b7d0daf4e0  via reshape2_74.tmp_00x55b7d0dbf990
I1018 08:53:57.756517 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0dbbdc0 -> elementwise_mul0x55b7d0daf4e0  via tmp_1190x55b7d0dbc350
I1018 08:53:57.756523 4097533 graph_helper.h:104] adj feed0x55b7d0d94a90 -> reshape20x55b7d0df0d70  via batch_norm2d_18.w_20x55b7d0dcc7f0
I1018 08:53:57.756531 4097533 graph_helper.h:104] adj scale0x55b7d0e21090 -> elementwise_pow0x55b7d0db8d40  via tmp_1180x55b7d0dbc0d0
I1018 08:53:57.756536 4097533 graph_helper.h:104] adj fill_constant0x55b7d0d93d90 -> elementwise_pow0x55b7d0db8d40  via fill_constant_139.tmp_00x55b7d0dbfb80
I1018 08:53:57.756541 4097533 graph_helper.h:104] adj reshape20x55b7d0df0d70 -> scale0x55b7d0e21090  via reshape2_73.tmp_00x55b7d0dfa700
I1018 08:53:57.756547 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d0e14b00 -> elementwise_mul0x55b7d0dbbdc0  via tmp_1170x55b7d0dbcb30
I1018 08:53:57.756552 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0db8d40 -> elementwise_mul0x55b7d0dbbdc0  via elementwise_pow_74.tmp_00x55b7d0dfb4e0
I1018 08:53:57.756558 4097533 graph_helper.h:104] adj feed0x55b7d0d951b0 -> reshape20x55b7d0de2710  via batch_norm2d_18.w_00x55b7d0dcced0
I1018 08:53:57.756564 4097533 graph_helper.h:104] adj feed0x55b7d0ca2620 -> reshape20x55b7d0e2bbb0  via batch_norm2d_18.w_10x55b7d0dccb60
I1018 08:53:57.756572 4097533 graph_helper.h:104] adj fill_constant0x55b7d0dca2a0 -> scale0x55b7d0e01600  via fill_constant_143.tmp_00x55b7d0dfae00
I1018 08:53:57.756577 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0e2b280 -> assign0x55b7d0fba110  via elementwise_pow_72.tmp_00x55b7d0dbfd20
I1018 08:53:57.756582 4097533 graph_helper.h:104] adj reshape20x55b7d0e2bbb0 -> elementwise_sub0x55b7d0e14b00  via reshape2_72.tmp_00x55b7d0dfaa80
I1018 08:53:57.756588 4097533 graph_helper.h:104] adj feed0x55b7d0d946e0 -> elementwise_sub0x55b7d0e14b00  via conv2d_18.tmp_00x55b7d0dcc4b0
I1018 08:53:57.756724 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.756727 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.756732 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.756736 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.756743 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.756747 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.756762 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.756764 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.756767 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.756770 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.756791 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.756793 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.756801 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.756804 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.756811 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.756814 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.756817 4097533 graph_helper.cc:698] convert op node to desc elementwise_sub
I1018 08:53:57.756820 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.756858 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.756861 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.756868 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.756870 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.756873 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.756876 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.756884 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.756887 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.756893 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.756896 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.756903 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.756911 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.756913 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.756916 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.756923 4097533 graph_helper.cc:698] convert op node to desc elementwise_add
I1018 08:53:57.756927 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.756999 4097533 graph_helper.cc:698] convert op node to desc elementwise_max
I1018 08:53:57.757001 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.757009 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.757011 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.757018 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.757021 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.757028 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.757031 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.757038 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.757040 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.757047 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.757050 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.757053 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.757056 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.757714 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.758169 4097533 block_desc.cc:205] vars in desc 26
I1018 08:53:57.758175 4097533 block_desc.cc:209] Flush batch_norm2d_18.w_0
I1018 08:53:57.758177 4097533 var_desc.cc:415] Flush  batch_norm2d_18.w_0 1
I1018 08:53:57.758180 4097533 block_desc.cc:209] Flush batch_norm2d_18.w_1
I1018 08:53:57.758183 4097533 var_desc.cc:415] Flush  batch_norm2d_18.w_1 1
I1018 08:53:57.758185 4097533 block_desc.cc:209] Flush conv2d_18.tmp_0
I1018 08:53:57.758188 4097533 var_desc.cc:415] Flush  conv2d_18.tmp_0 1
I1018 08:53:57.758190 4097533 block_desc.cc:209] Flush elementwise_pow_74.tmp_0
I1018 08:53:57.758193 4097533 var_desc.cc:415] Flush  elementwise_pow_74.tmp_0 1
I1018 08:53:57.758195 4097533 block_desc.cc:209] Flush assign_73.tmp_0
I1018 08:53:57.758198 4097533 var_desc.cc:415] Flush  assign_73.tmp_0 1
I1018 08:53:57.758199 4097533 block_desc.cc:209] Flush reshape2_72.tmp_1
I1018 08:53:57.758203 4097533 var_desc.cc:415] Flush  reshape2_72.tmp_1 1
I1018 08:53:57.758204 4097533 block_desc.cc:209] Flush reshape2_72.tmp_0
I1018 08:53:57.758206 4097533 var_desc.cc:415] Flush  reshape2_72.tmp_0 1
I1018 08:53:57.758209 4097533 block_desc.cc:209] Flush tmp_117
I1018 08:53:57.758211 4097533 var_desc.cc:415] Flush  tmp_117 1
I1018 08:53:57.758213 4097533 block_desc.cc:209] Flush tmp_120
I1018 08:53:57.758216 4097533 var_desc.cc:415] Flush  tmp_120 1
I1018 08:53:57.758219 4097533 block_desc.cc:209] Flush reshape2_73.tmp_0
I1018 08:53:57.758220 4097533 var_desc.cc:415] Flush  reshape2_73.tmp_0 1
I1018 08:53:57.758224 4097533 block_desc.cc:209] Flush fill_constant_145.tmp_0
I1018 08:53:57.758225 4097533 var_desc.cc:415] Flush  fill_constant_145.tmp_0 1
I1018 08:53:57.758227 4097533 block_desc.cc:209] Flush reshape2_75.tmp_1
I1018 08:53:57.758229 4097533 var_desc.cc:415] Flush  reshape2_75.tmp_1 1
I1018 08:53:57.758232 4097533 block_desc.cc:209] Flush tmp_118
I1018 08:53:57.758234 4097533 var_desc.cc:415] Flush  tmp_118 1
I1018 08:53:57.758237 4097533 block_desc.cc:209] Flush batch_norm2d_18.b_0
I1018 08:53:57.758239 4097533 var_desc.cc:415] Flush  batch_norm2d_18.b_0 1
I1018 08:53:57.758241 4097533 block_desc.cc:209] Flush batch_norm_18.tmp_2
I1018 08:53:57.758244 4097533 var_desc.cc:415] Flush  batch_norm_18.tmp_2 1
I1018 08:53:57.758246 4097533 block_desc.cc:209] Flush relu_15.tmp_0
I1018 08:53:57.758248 4097533 var_desc.cc:415] Flush  relu_15.tmp_0 1
I1018 08:53:57.758251 4097533 block_desc.cc:209] Flush reshape2_74.tmp_0
I1018 08:53:57.758253 4097533 var_desc.cc:415] Flush  reshape2_74.tmp_0 1
I1018 08:53:57.758255 4097533 block_desc.cc:209] Flush reshape2_75.tmp_0
I1018 08:53:57.758257 4097533 var_desc.cc:415] Flush  reshape2_75.tmp_0 1
I1018 08:53:57.758265 4097533 block_desc.cc:209] Flush fill_constant_143.tmp_0
I1018 08:53:57.758266 4097533 var_desc.cc:415] Flush  fill_constant_143.tmp_0 1
I1018 08:53:57.758270 4097533 block_desc.cc:209] Flush reshape2_74.tmp_1
I1018 08:53:57.758271 4097533 var_desc.cc:415] Flush  reshape2_74.tmp_1 1
I1018 08:53:57.758273 4097533 block_desc.cc:209] Flush tmp_116
I1018 08:53:57.758275 4097533 var_desc.cc:415] Flush  tmp_116 1
I1018 08:53:57.758278 4097533 block_desc.cc:209] Flush fill_constant_139.tmp_0
I1018 08:53:57.758280 4097533 var_desc.cc:415] Flush  fill_constant_139.tmp_0 1
I1018 08:53:57.758282 4097533 block_desc.cc:209] Flush tmp_119
I1018 08:53:57.758285 4097533 var_desc.cc:415] Flush  tmp_119 1
I1018 08:53:57.758287 4097533 block_desc.cc:209] Flush batch_norm2d_18.w_2
I1018 08:53:57.758289 4097533 var_desc.cc:415] Flush  batch_norm2d_18.w_2 1
I1018 08:53:57.758292 4097533 block_desc.cc:209] Flush elementwise_pow_72.tmp_0
I1018 08:53:57.758294 4097533 var_desc.cc:415] Flush  elementwise_pow_72.tmp_0 1
I1018 08:53:57.758296 4097533 block_desc.cc:209] Flush reshape2_73.tmp_1
I1018 08:53:57.758298 4097533 var_desc.cc:415] Flush  reshape2_73.tmp_1 1
I1018 08:53:57.756361 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_18.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_18.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "conv2d_18.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 7
          dims: 7
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "elementwise_pow_74.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "assign_73.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_72.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_72.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_117"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 7
          dims: 7
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "tmp_120"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 7
          dims: 7
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "reshape2_73.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_145.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_75.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "tmp_118"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  vars {
    name: "batch_norm2d_18.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "batch_norm_18.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 7
          dims: 7
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "relu_15.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 7
          dims: 7
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_74.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_75.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "fill_constant_143.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_74.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "tmp_116"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  vars {
    name: "fill_constant_139.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_119"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 7
          dims: 7
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_18.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "elementwise_pow_72.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_73.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_18.w_2"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_18.w_2"
    }
    outputs {
      parameter: "Out"
      arguments: "reshape2_73.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_73.tmp_1"
    }
    type: "reshape2"
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 124, in composite_batchnorm"
      strings: "    (reshape(run_var, stats_shape) + epsilon), half"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/manipulation.py\", line 4518, in reshape"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 1
      ints: 512
      ints: 1
      ints: 1
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "BiasTensor"
    }
    inputs {
      parameter: "ScaleTensor"
    }
    inputs {
      parameter: "X"
      arguments: "reshape2_73.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_118"
    }
    type: "scale"
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "bias"
      type: FLOAT
      f: 1e-05
    }
    attrs {
      name: "bias_after_scale"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 124, in composite_batchnorm"
      strings: "    (reshape(run_var, stats_shape) + epsilon), half"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 557, in __impl__"
      strings: "    return scalar_method(self, other_var)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 530, in _scalar_add_"
      strings: "    return _scalar_op_(var, 1.0, value)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 451, in _scalar_op_"
      strings: "    block.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "scale"
      type: FLOAT
      f: 1
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_18.w_1"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_18.w_1"
    }
    outputs {
      parameter: "Out"
      arguments: "reshape2_72.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_72.tmp_1"
    }
    type: "reshape2"
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 123, in composite_batchnorm"
      strings: "    x_hat = (x - reshape(run_mean, stats_shape)) * pow("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/manipulation.py\", line 4518, in reshape"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 1
      ints: 512
      ints: 1
      ints: 1
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    in
I1018 08:53:57.759476 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.759594 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_122, is_only_used_internal: 1
I1018 08:53:57.759598 4097533 build_cinn_pass.cc:562] insert internal var: tmp_122
I1018 08:53:57.759600 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_123, is_only_used_internal: 1
I1018 08:53:57.759603 4097533 build_cinn_pass.cc:562] insert internal var: tmp_123
I1018 08:53:57.759605 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_147.tmp_0, is_only_used_internal: 1
I1018 08:53:57.759608 4097533 build_cinn_pass.cc:557] var_node->outputs[1]: fill_constant_147.tmp_0, is_only_used_internal: 1
I1018 08:53:57.759609 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_147.tmp_0
I1018 08:53:57.759613 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: elementwise_pow_76.tmp_0, is_only_used_internal: 1
I1018 08:53:57.759614 4097533 build_cinn_pass.cc:562] insert internal var: elementwise_pow_76.tmp_0
I1018 08:53:57.759616 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_125, is_only_used_internal: 1
I1018 08:53:57.759618 4097533 build_cinn_pass.cc:562] insert internal var: tmp_125
I1018 08:53:57.759621 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_80.tmp_1
I1018 08:53:57.759624 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: flatten_0.tmp_0, is_only_used_internal: 1
I1018 08:53:57.759625 4097533 build_cinn_pass.cc:562] insert internal var: flatten_0.tmp_0
I1018 08:53:57.759627 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_79.tmp_1
I1018 08:53:57.759632 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_124, is_only_used_internal: 1
I1018 08:53:57.759634 4097533 build_cinn_pass.cc:562] insert internal var: tmp_124
I1018 08:53:57.759636 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: linear_0.tmp_0, is_only_used_internal: 1
I1018 08:53:57.759639 4097533 build_cinn_pass.cc:562] insert internal var: linear_0.tmp_0
I1018 08:53:57.759641 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_77.tmp_1
I1018 08:53:57.759644 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_76.tmp_0, is_only_used_internal: 1
I1018 08:53:57.759645 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_76.tmp_0
I1018 08:53:57.759649 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_151.tmp_0, is_only_used_internal: 1
I1018 08:53:57.759650 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_151.tmp_0
I1018 08:53:57.759652 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_76.tmp_1
I1018 08:53:57.759654 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: elementwise_pow_78.tmp_0, is_only_used_internal: 1
I1018 08:53:57.759657 4097533 build_cinn_pass.cc:562] insert internal var: elementwise_pow_78.tmp_0
I1018 08:53:57.759660 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_78.tmp_1
I1018 08:53:57.759661 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: relu_16.tmp_0, is_only_used_internal: 1
I1018 08:53:57.759663 4097533 build_cinn_pass.cc:562] insert internal var: relu_16.tmp_0
I1018 08:53:57.759665 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_78.tmp_0, is_only_used_internal: 1
I1018 08:53:57.759667 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_78.tmp_0
I1018 08:53:57.759670 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_126, is_only_used_internal: 1
I1018 08:53:57.759672 4097533 build_cinn_pass.cc:562] insert internal var: tmp_126
I1018 08:53:57.759675 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: batch_norm_19.tmp_2, is_only_used_internal: 1
I1018 08:53:57.759676 4097533 build_cinn_pass.cc:562] insert internal var: batch_norm_19.tmp_2
I1018 08:53:57.759678 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_79.tmp_0, is_only_used_internal: 1
I1018 08:53:57.759680 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_79.tmp_0
I1018 08:53:57.759683 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_153.tmp_0, is_only_used_internal: 1
I1018 08:53:57.759685 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_153.tmp_0
I1018 08:53:57.759687 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: reshape2_77.tmp_0, is_only_used_internal: 1
I1018 08:53:57.759689 4097533 build_cinn_pass.cc:562] insert internal var: reshape2_77.tmp_0
I1018 08:53:57.759691 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: tmp_7, is_only_used_internal: 1
I1018 08:53:57.759693 4097533 build_cinn_pass.cc:562] insert internal var: tmp_7
I1018 08:53:57.759696 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: pool2d_1.tmp_0, is_only_used_internal: 1
I1018 08:53:57.759698 4097533 build_cinn_pass.cc:562] insert internal var: pool2d_1.tmp_0
I1018 08:53:57.759701 4097533 build_cinn_pass.cc:742] Cluster Ops: (scale, reshape2, elementwise_pow, reshape2, elementwise_mul, elementwise_add, fill_constant, elementwise_max, reshape2, pool2d, reshape2, reshape2, matmul_v2, elementwise_add, fill_constant, elementwise_mul, elementwise_add, elementwise_pow, elementwise_sub, assign, scale, fill_constant, )
I1018 08:53:57.759706 4097533 build_cinn_pass.cc:743] Cluster input vars: (conv2d_19.tmp_0, linear_0.b_0, linear_0.w_0, batch_norm2d_19.w_1, batch_norm2d_19.w_2, batch_norm2d_19.w_0, batch_norm2d_19.b_0, relu_14.tmp_0, )
I1018 08:53:57.759707 4097533 build_cinn_pass.cc:744] Cluster output vars: (assign_77.tmp_0, linear_0.tmp_1, )
I1018 08:53:57.759711 4097533 build_cinn_pass.cc:745] Cluster internal vars: (pool2d_1.tmp_0, tmp_7, reshape2_77.tmp_0, fill_constant_153.tmp_0, reshape2_79.tmp_0, batch_norm_19.tmp_2, tmp_126, reshape2_78.tmp_0, relu_16.tmp_0, reshape2_78.tmp_1, elementwise_pow_78.tmp_0, tmp_122, tmp_125, tmp_124, tmp_123, reshape2_76.tmp_1, fill_constant_151.tmp_0, fill_constant_147.tmp_0, elementwise_pow_76.tmp_0, flatten_0.tmp_0, reshape2_80.tmp_1, reshape2_79.tmp_1, reshape2_77.tmp_1, linear_0.tmp_0, reshape2_76.tmp_0, )
I1018 08:53:57.759721 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.759964 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: conv2d_19.tmp_0
I1018 08:53:57.759968 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: relu_14.tmp_0
I1018 08:53:57.759972 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: linear_0.b_0
I1018 08:53:57.759976 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: linear_0.w_0
I1018 08:53:57.759979 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_19.b_0
I1018 08:53:57.759982 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_19.w_2
I1018 08:53:57.759986 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_19.w_0
I1018 08:53:57.759989 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_19.w_1
I1018 08:53:57.759994 4097533 build_cinn_pass.cc:274] Add Output Var Node: assign_77.tmp_0
I1018 08:53:57.759996 4097533 build_cinn_pass.cc:274] Add Output Var Node: linear_0.tmp_1
I1018 08:53:57.760020 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_66[label="batch_norm2d_19.b_0
[512]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_63[label="elementwise_add" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_62[label="linear_0.w_0
[512,1000]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_60[label="matmul_v2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_59[label="reshape2_77.tmp_1
[0,512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_29[label="batch_norm2d_19.w_1
[512]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_25[label="scale" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_22[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_40[label="pool2d" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_20[label="elementwise_mul" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_18[label="elementwise_pow" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_13[label="tmp_125
[1,512,7,7]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_34[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_41[label="relu_16.tmp_0
[1,512,7,7]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_1[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_19[label="tmp_124
[1,512,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_10[label="elementwise_add" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_36[label="reshape2_77.tmp_0
[1,512,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_9[label="linear_0.tmp_1
[1,1000]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_4[label="tmp_126
[1,512,7,7]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_5[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_16[label="reshape2_79.tmp_0
[1,512,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_64[label="batch_norm2d_19.w_0
[512]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_58[label="reshape2_79.tmp_1
[0,512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_11[label="pool2d_1.tmp_0
[1,512,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_28[label="elementwise_mul" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_26[label="linear_0.tmp_0
[1,1000]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_23[label="reshape2_78.tmp_0
[1,512,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_33[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_6[label="fill_constant_153.tmp_0
[1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_7[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_49[label="tmp_123
[1,512,7,7]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_8[label="reshape2_76.tmp_0
[1,512,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_38[label="scale" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_21[label="batch_norm_19.tmp_2
[1,512,7,7]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_3[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_30[label="elementwise_max" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_2[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_31[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_17[label="reshape2_80.tmp_1
[0,1,512,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_32[label="conv2d_19.tmp_0
[1,512,7,7]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_35[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_53[label="fill_constant_151.tmp_0
[512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_24[label="linear_0.b_0
[1000]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_37[label="elementwise_add" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_39[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_42[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_43[label="reshape2_78.tmp_1
[0,512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_15[label="elementwise_sub" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_44[label="batch_norm2d_19.w_2
[512]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_65[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_45[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_46[label="elementwise_pow_78.tmp_0
[1,512,1,1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_27[label="assign_77.tmp_0
[512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_47[label="relu_14.tmp_0
[1,512,7,7]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_48[label="tmp_122
[512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_50[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_12[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_51[label="elementwise_pow" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_52[label="reshape2_76.tmp_1
[0,512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_61[label="reshape2" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_14[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_54[label="fill_constant_147.tmp_0
[1]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_55[label="tmp_7
[1,512,7,7]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_56[label="elementwise_pow_76.tmp_0
[512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_57[label="flatten_0.tmp_0
[1,512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_0->node_29
   node_1->node_44
   node_2->node_62
   node_3->node_24
   node_4->node_37
   node_5->node_16
   node_5->node_58
   node_6->node_30
   node_8->node_15
   node_9->node_7
   node_10->node_55
   node_11->node_39
   node_12->node_27
   node_13->node_28
   node_14->node_66
   node_15->node_49
   node_16->node_37
   node_18->node_56
   node_19->node_51
   node_20->node_13
   node_21->node_10
   node_22->node_23
   node_22->node_43
   node_23->node_28
   node_24->node_63
   node_25->node_48
   node_26->node_63
   node_27->node_50
   node_28->node_4
   node_29->node_34
   node_30->node_41
   node_31->node_54
   node_32->node_15
   node_33->node_53
   node_34->node_8
   node_34->node_52
   node_35->node_47
   node_36->node_38
   node_37->node_21
   node_38->node_19
   node_39->node_57
   node_39->node_17
   node_40->node_11
   node_41->node_40
   node_42->node_6
   node_44->node_61
   node_45->node_32
   node_46->node_20
   node_47->node_10
   node_48->node_18
   node_49->node_20
   node_51->node_46
   node_53->node_25
   node_54->node_51
   node_54->node_18
   node_55->node_30
   node_56->node_12
   node_57->node_60
   node_60->node_26
   node_61->node_36
   node_61->node_59
   node_62->node_60
   node_63->node_9
   node_64->node_22
   node_65->node_64
   node_66->node_5
} // end G
I1018 08:53:57.760363 4097533 var_desc.cc:415] Flush  tmp_126 1
I1018 08:53:57.760366 4097533 var_desc.cc:415] Flush  fill_constant_153.tmp_0 1
I1018 08:53:57.760370 4097533 var_desc.cc:415] Flush  reshape2_76.tmp_0 1
I1018 08:53:57.760372 4097533 var_desc.cc:415] Flush  linear_0.tmp_1 1
I1018 08:53:57.760375 4097533 var_desc.cc:415] Flush  pool2d_1.tmp_0 1
I1018 08:53:57.760377 4097533 var_desc.cc:415] Flush  tmp_125 1
I1018 08:53:57.760380 4097533 var_desc.cc:415] Flush  reshape2_79.tmp_0 1
I1018 08:53:57.760383 4097533 var_desc.cc:415] Flush  reshape2_80.tmp_1 1
I1018 08:53:57.760386 4097533 var_desc.cc:415] Flush  tmp_124 1
I1018 08:53:57.760389 4097533 var_desc.cc:415] Flush  batch_norm_19.tmp_2 1
I1018 08:53:57.760391 4097533 var_desc.cc:415] Flush  reshape2_78.tmp_0 1
I1018 08:53:57.760394 4097533 var_desc.cc:415] Flush  linear_0.b_0 1
I1018 08:53:57.760397 4097533 var_desc.cc:415] Flush  linear_0.tmp_0 1
I1018 08:53:57.760399 4097533 var_desc.cc:415] Flush  assign_77.tmp_0 1
I1018 08:53:57.760402 4097533 var_desc.cc:415] Flush  batch_norm2d_19.w_1 1
I1018 08:53:57.760406 4097533 var_desc.cc:415] Flush  conv2d_19.tmp_0 1
I1018 08:53:57.760407 4097533 var_desc.cc:415] Flush  reshape2_77.tmp_0 1
I1018 08:53:57.760411 4097533 var_desc.cc:415] Flush  relu_16.tmp_0 1
I1018 08:53:57.760413 4097533 var_desc.cc:415] Flush  reshape2_78.tmp_1 1
I1018 08:53:57.760416 4097533 var_desc.cc:415] Flush  batch_norm2d_19.w_2 1
I1018 08:53:57.760418 4097533 var_desc.cc:415] Flush  elementwise_pow_78.tmp_0 1
I1018 08:53:57.760421 4097533 var_desc.cc:415] Flush  relu_14.tmp_0 1
I1018 08:53:57.760426 4097533 var_desc.cc:415] Flush  tmp_122 1
I1018 08:53:57.760428 4097533 var_desc.cc:415] Flush  tmp_123 1
I1018 08:53:57.760432 4097533 var_desc.cc:415] Flush  reshape2_76.tmp_1 1
I1018 08:53:57.760434 4097533 var_desc.cc:415] Flush  fill_constant_151.tmp_0 1
I1018 08:53:57.760437 4097533 var_desc.cc:415] Flush  fill_constant_147.tmp_0 1
I1018 08:53:57.760438 4097533 var_desc.cc:415] Flush  tmp_7 1
I1018 08:53:57.760442 4097533 var_desc.cc:415] Flush  elementwise_pow_76.tmp_0 1
I1018 08:53:57.760444 4097533 var_desc.cc:415] Flush  flatten_0.tmp_0 1
I1018 08:53:57.760447 4097533 var_desc.cc:415] Flush  reshape2_79.tmp_1 1
I1018 08:53:57.760449 4097533 var_desc.cc:415] Flush  reshape2_77.tmp_1 1
I1018 08:53:57.760452 4097533 var_desc.cc:415] Flush  linear_0.w_0 1
I1018 08:53:57.760455 4097533 var_desc.cc:415] Flush  batch_norm2d_19.w_0 1
I1018 08:53:57.760457 4097533 var_desc.cc:415] Flush  batch_norm2d_19.b_0 1
I1018 08:53:57.760479 4097533 graph_helper.h:104] adj feed0x55b7d0ddb920 -> reshape20x55b7d0e3b750  via batch_norm2d_19.b_00x55b7d0e732d0
I1018 08:53:57.760491 4097533 graph_helper.h:104] adj elementwise_add0x55b7d1018a40 -> fetch0x55b7d0dd3200  via linear_0.tmp_10x55b7d0e60be0
I1018 08:53:57.760495 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0e524f0 -> elementwise_add0x55b7d0e35060  via batch_norm_19.tmp_20x55b7d0e3da50
I1018 08:53:57.760501 4097533 graph_helper.h:104] adj feed0x55b7d0ddaef0 -> elementwise_add0x55b7d0e35060  via relu_14.tmp_00x55b7d0e73640
I1018 08:53:57.760507 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0d801f0 -> assign0x55b7d0dc6ac0  via elementwise_pow_76.tmp_00x55b7d0e4c170
I1018 08:53:57.760514 4097533 graph_helper.h:104] adj reshape20x55b7d0e482d0 -> elementwise_sub0x55b7d0e3f490  via reshape2_76.tmp_00x55b7d0f10210
I1018 08:53:57.760519 4097533 graph_helper.h:104] adj feed0x55b7d0e61ae0 -> elementwise_sub0x55b7d0e3f490  via conv2d_19.tmp_00x55b7d0f10590
I1018 08:53:57.760524 4097533 graph_helper.h:104] adj scale0x55b7d0bfb440 -> elementwise_pow0x55b7d0d801f0  via tmp_1220x55b7d0f2e440
I1018 08:53:57.760529 4097533 graph_helper.h:104] adj fill_constant0x55b7d0df7680 -> elementwise_pow0x55b7d0d801f0  via fill_constant_147.tmp_00x55b7d0e4be00
I1018 08:53:57.760535 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d0e3f490 -> elementwise_mul0x55b7d0f279c0  via tmp_1230x55b7d0f2ed70
I1018 08:53:57.760540 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0e3e8c0 -> elementwise_mul0x55b7d0f279c0  via elementwise_pow_78.tmp_00x55b7d0f2e120
I1018 08:53:57.760545 4097533 graph_helper.h:104] adj feed0x55b7d0dd2710 -> reshape20x55b7d0e41940  via batch_norm2d_19.w_00x55b7d0e72f60
I1018 08:53:57.760552 4097533 graph_helper.h:104] adj fill_constant0x55b7d0dabd90 -> scale0x55b7d0bfb440  via fill_constant_151.tmp_00x55b7d0e4ba90
I1018 08:53:57.760558 4097533 graph_helper.h:104] adj reshape20x55b7d0e41940 -> elementwise_mul0x55b7d0e68290  via reshape2_78.tmp_00x55b7d0e3de40
I1018 08:53:57.760564 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0f279c0 -> elementwise_mul0x55b7d0e68290  via tmp_1250x55b7d0f2e6f0
I1018 08:53:57.760569 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0e35060 -> elementwise_max0x55b7d0e19cb0  via tmp_70x55b7d0df7420
I1018 08:53:57.760574 4097533 graph_helper.h:104] adj fill_constant0x55b7d0e637b0 -> elementwise_max0x55b7d0e19cb0  via fill_constant_153.tmp_00x55b7d0e3d7b0
I1018 08:53:57.760581 4097533 graph_helper.h:104] adj feed0x55b7d0dd2aa0 -> reshape20x55b7d0e482d0  via batch_norm2d_19.w_10x55b7d0e72880
I1018 08:53:57.760587 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d0e68290 -> elementwise_add0x55b7d0e524f0  via tmp_1260x55b7d0e3dc30
I1018 08:53:57.760593 4097533 graph_helper.h:104] adj reshape20x55b7d0e3b750 -> elementwise_add0x55b7d0e524f0  via reshape2_79.tmp_00x55b7d0e3d910
I1018 08:53:57.760598 4097533 graph_helper.h:104] adj reshape20x55b7d0e62750 -> scale0x55b7d0e4b5d0  via reshape2_77.tmp_00x55b7d0df7520
I1018 08:53:57.760604 4097533 graph_helper.h:104] adj pool2d0x55b7d0db12a0 -> reshape20x55b7d0daaa10  via pool2d_1.tmp_00x55b7d0df7340
I1018 08:53:57.760614 4097533 graph_helper.h:104] adj elementwise_max0x55b7d0e19cb0 -> pool2d0x55b7d0db12a0  via relu_16.tmp_00x55b7d0f2dba0
I1018 08:53:57.760620 4097533 graph_helper.h:104] adj assign0x55b7d0dc6ac0 -> fetch0x55b7d0dd2e50  via assign_77.tmp_00x55b7d0e608b0
I1018 08:53:57.760624 4097533 graph_helper.h:104] adj scale0x55b7d0e4b5d0 -> elementwise_pow0x55b7d0e3e8c0  via tmp_1240x55b7d0f2ea30
I1018 08:53:57.760630 4097533 graph_helper.h:104] adj fill_constant0x55b7d0df7680 -> elementwise_pow0x55b7d0e3e8c0  via fill_constant_147.tmp_00x55b7d0e4be00
I1018 08:53:57.760635 4097533 graph_helper.h:104] adj reshape20x55b7d0daaa10 -> matmul_v20x55b7d0e2fdb0  via flatten_0.tmp_00x55b7d0e4c500
I1018 08:53:57.760640 4097533 graph_helper.h:104] adj feed0x55b7d0ddb590 -> matmul_v20x55b7d0e2fdb0  via linear_0.w_00x55b7d0e72550
I1018 08:53:57.760646 4097533 graph_helper.h:104] adj feed0x55b7d0ddbcb0 -> reshape20x55b7d0e62750  via batch_norm2d_19.w_20x55b7d0e72bf0
I1018 08:53:57.760653 4097533 graph_helper.h:104] adj matmul_v20x55b7d0e2fdb0 -> elementwise_add0x55b7d1018a40  via linear_0.tmp_00x55b7d0f0fee0
I1018 08:53:57.760658 4097533 graph_helper.h:104] adj feed0x55b7d0ddb240 -> elementwise_add0x55b7d1018a40  via linear_0.b_00x55b7d0f108d0
I1018 08:53:57.760830 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.760833 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.760838 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.760841 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.760849 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.760852 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.760866 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.760869 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.760872 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.760875 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.760895 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.760898 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.760906 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.760910 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.760916 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.760918 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.760922 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.760924 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.760963 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.760965 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.760969 4097533 graph_helper.cc:698] convert op node to desc elementwise_sub
I1018 08:53:57.760972 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.760978 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.760982 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.760989 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.760993 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.761000 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.761003 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.761005 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.761008 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.761015 4097533 graph_helper.cc:698] convert op node to desc elementwise_add
I1018 08:53:57.761018 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.761025 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.761027 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.761096 4097533 graph_helper.cc:698] convert op node to desc elementwise_add
I1018 08:53:57.761098 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.761104 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.761106 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.761114 4097533 graph_helper.cc:698] convert op node to desc elementwise_max
I1018 08:53:57.761117 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.761127 4097533 graph_helper.cc:698] convert op node to desc pool2d
I1018 08:53:57.761129 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.761137 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.761139 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.761147 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.761148 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.761152 4097533 graph_helper.cc:698] convert op node to desc matmul_v2
I1018 08:53:57.761154 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.761160 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.761162 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.761165 4097533 graph_helper.cc:698] convert op node to desc elementwise_add
I1018 08:53:57.761168 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.761174 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.761176 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.761183 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.761186 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.761193 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.761195 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.761202 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.761211 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.761219 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.761221 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.761224 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.761227 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.762020 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.762596 4097533 block_desc.cc:205] vars in desc 35
I1018 08:53:57.762602 4097533 block_desc.cc:209] Flush tmp_126
I1018 08:53:57.762605 4097533 var_desc.cc:415] Flush  tmp_126 1
I1018 08:53:57.762609 4097533 block_desc.cc:209] Flush fill_constant_153.tmp_0
I1018 08:53:57.762610 4097533 var_desc.cc:415] Flush  fill_constant_153.tmp_0 1
I1018 08:53:57.762614 4097533 block_desc.cc:209] Flush reshape2_76.tmp_0
I1018 08:53:57.762615 4097533 var_desc.cc:415] Flush  reshape2_76.tmp_0 1
I1018 08:53:57.762619 4097533 block_desc.cc:209] Flush linear_0.tmp_1
I1018 08:53:57.762620 4097533 var_desc.cc:415] Flush  linear_0.tmp_1 1
I1018 08:53:57.762624 4097533 block_desc.cc:209] Flush pool2d_1.tmp_0
I1018 08:53:57.762625 4097533 var_desc.cc:415] Flush  pool2d_1.tmp_0 1
I1018 08:53:57.762629 4097533 block_desc.cc:209] Flush tmp_125
I1018 08:53:57.762630 4097533 var_desc.cc:415] Flush  tmp_125 1
I1018 08:53:57.762634 4097533 block_desc.cc:209] Flush reshape2_79.tmp_0
I1018 08:53:57.762635 4097533 var_desc.cc:415] Flush  reshape2_79.tmp_0 1
I1018 08:53:57.762637 4097533 block_desc.cc:209] Flush reshape2_80.tmp_1
I1018 08:53:57.762639 4097533 var_desc.cc:415] Flush  reshape2_80.tmp_1 1
I1018 08:53:57.762642 4097533 block_desc.cc:209] Flush tmp_124
I1018 08:53:57.762645 4097533 var_desc.cc:415] Flush  tmp_124 1
I1018 08:53:57.762647 4097533 block_desc.cc:209] Flush batch_norm_19.tmp_2
I1018 08:53:57.762650 4097533 var_desc.cc:415] Flush  batch_norm_19.tmp_2 1
I1018 08:53:57.762651 4097533 block_desc.cc:209] Flush reshape2_78.tmp_0
I1018 08:53:57.762653 4097533 var_desc.cc:415] Flush  reshape2_78.tmp_0 1
I1018 08:53:57.762656 4097533 block_desc.cc:209] Flush linear_0.b_0
I1018 08:53:57.762658 4097533 var_desc.cc:415] Flush  linear_0.b_0 1
I1018 08:53:57.762661 4097533 block_desc.cc:209] Flush linear_0.tmp_0
I1018 08:53:57.762663 4097533 var_desc.cc:415] Flush  linear_0.tmp_0 1
I1018 08:53:57.762665 4097533 block_desc.cc:209] Flush assign_77.tmp_0
I1018 08:53:57.762667 4097533 var_desc.cc:415] Flush  assign_77.tmp_0 1
I1018 08:53:57.762670 4097533 block_desc.cc:209] Flush batch_norm2d_19.w_1
I1018 08:53:57.762672 4097533 var_desc.cc:415] Flush  batch_norm2d_19.w_1 1
I1018 08:53:57.762676 4097533 block_desc.cc:209] Flush conv2d_19.tmp_0
I1018 08:53:57.762677 4097533 var_desc.cc:415] Flush  conv2d_19.tmp_0 1
I1018 08:53:57.762683 4097533 block_desc.cc:209] Flush reshape2_77.tmp_0
I1018 08:53:57.762686 4097533 var_desc.cc:415] Flush  reshape2_77.tmp_0 1
I1018 08:53:57.762688 4097533 block_desc.cc:209] Flush relu_16.tmp_0
I1018 08:53:57.762691 4097533 var_desc.cc:415] Flush  relu_16.tmp_0 1
I1018 08:53:57.762693 4097533 block_desc.cc:209] Flush reshape2_78.tmp_1
I1018 08:53:57.762696 4097533 var_desc.cc:415] Flush  reshape2_78.tmp_1 1
I1018 08:53:57.762697 4097533 block_desc.cc:209] Flush batch_norm2d_19.w_2
I1018 08:53:57.762699 4097533 var_desc.cc:415] Flush  batch_norm2d_19.w_2 1
I1018 08:53:57.762702 4097533 block_desc.cc:209] Flush elementwise_pow_78.tmp_0
I1018 08:53:57.762704 4097533 var_desc.cc:415] Flush  elementwise_pow_78.tmp_0 1
I1018 08:53:57.762707 4097533 block_desc.cc:209] Flush relu_14.tmp_0
I1018 08:53:57.762709 4097533 var_desc.cc:415] Flush  relu_14.tmp_0 1
I1018 08:53:57.762712 4097533 block_desc.cc:209] Flush tmp_122
I1018 08:53:57.762713 4097533 var_desc.cc:415] Flush  tmp_122 1
I1018 08:53:57.762717 4097533 block_desc.cc:209] Flush tmp_123
I1018 08:53:57.762718 4097533 var_desc.cc:415] Flush  tmp_123 1
I1018 08:53:57.762720 4097533 block_desc.cc:209] Flush reshape2_76.tmp_1
I1018 08:53:57.762723 4097533 var_desc.cc:415] Flush  reshape2_76.tmp_1 1
I1018 08:53:57.762725 4097533 block_desc.cc:209] Flush fill_constant_151.tmp_0
I1018 08:53:57.762727 4097533 var_desc.cc:415] Flush  fill_constant_151.tmp_0 1
I1018 08:53:57.762730 4097533 block_desc.cc:209] Flush fill_constant_147.tmp_0
I1018 08:53:57.762732 4097533 var_desc.cc:415] Flush  fill_constant_147.tmp_0 1
I1018 08:53:57.762734 4097533 block_desc.cc:209] Flush tmp_7
I1018 08:53:57.762737 4097533 var_desc.cc:415] Flush  tmp_7 1
I1018 08:53:57.762739 4097533 block_desc.cc:209] Flush elementwise_pow_76.tmp_0
I1018 08:53:57.762741 4097533 var_desc.cc:415] Flush  elementwise_pow_76.tmp_0 1
I1018 08:53:57.762744 4097533 block_desc.cc:209] Flush flatten_0.tmp_0
I1018 08:53:57.762746 4097533 var_desc.cc:415] Flush  flatten_0.tmp_0 1
I1018 08:53:57.762748 4097533 block_desc.cc:209] Flush reshape2_79.tmp_1
I1018 08:53:57.762751 4097533 var_desc.cc:415] Flush  reshape2_79.tmp_1 1
I1018 08:53:57.762753 4097533 block_desc.cc:209] Flush reshape2_77.tmp_1
I1018 08:53:57.762755 4097533 var_desc.cc:415] Flush  reshape2_77.tmp_1 1
I1018 08:53:57.762758 4097533 block_desc.cc:209] Flush linear_0.w_0
I1018 08:53:57.762760 4097533 var_desc.cc:415] Flush  linear_0.w_0 1
I1018 08:53:57.762763 4097533 block_desc.cc:209] Flush batch_norm2d_19.w_0
I1018 08:53:57.762765 4097533 var_desc.cc:415] Flush  batch_norm2d_19.w_0 1
I1018 08:53:57.762768 4097533 block_desc.cc:209] Flush batch_norm2d_19.b_0
I1018 08:53:57.762769 4097533 var_desc.cc:415] Flush  batch_norm2d_19.b_0 1
I1018 08:53:57.760355 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "tmp_126"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 7
          dims: 7
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "fill_constant_153.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_76.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "linear_0.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 1000
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "pool2d_1.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "tmp_125"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 7
          dims: 7
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "reshape2_79.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_80.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 1
          dims: 512
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "tmp_124"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  vars {
    name: "batch_norm_19.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 7
          dims: 7
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_78.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "linear_0.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1000
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "linear_0.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 1000
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "assign_77.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "batch_norm2d_19.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "conv2d_19.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 7
          dims: 7
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_77.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "relu_16.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 7
          dims: 7
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_78.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_19.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "elementwise_pow_78.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "relu_14.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 7
          dims: 7
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "tmp_122"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  vars {
    name: "tmp_123"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 7
          dims: 7
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "reshape2_76.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_151.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_147.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_7"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
          dims: 7
          dims: 7
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "elementwise_pow_76.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "flatten_0.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_79.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_77.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "linear_0.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
          dims: 1000
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_19.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "batch_norm2d_19.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_19.w_2"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_19.w_2"
    }
    outputs {
      parameter: "Out"
      arguments: "reshape2_77.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_77.tmp_1"
    }
    type: "reshape2"
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 124, in composite_batchnorm"
      strings: "    (reshape(run_var, stats_shape) + epsilon), half"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/manipulation.py\", line 4518, in reshape"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 1
      ints: 512
      ints: 1
      ints: 1
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "BiasTensor"
    }
    inputs {
      parameter: "ScaleTensor"
    }
    inputs {
      parameter: "X"
      arguments: "reshape2_77.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_124"
    }
    type: "scale"
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "bias"
      type: FLOAT
      f: 1e-05
    }
    attrs {
      name: "bias_after_scale"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 124, in composite_batchnorm"
      strings: "    (reshape(run_var, stats_shape) + epsilon), half"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 557, in __impl__"
      strings: "    return scalar_method(self, other_var)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 530, in _scalar_add_"
      strings: "    return _scalar_op_(var, 1.0, value)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 451, in _scalar_op_"
      strings: "    block.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "scale"
      type: FLOAT
      f: 1
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_19.w_1"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_19.w_1"
    }
    outputs {
      parameter: "Out"
      arguments: "reshape2_76.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_76.tmp_1"
    }
    type: "reshape2"
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __imp
I1018 08:53:57.764231 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.764348 4097533 build_cinn_pass.cc:742] Cluster Ops: (assign, )
I1018 08:53:57.764350 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_0.w_1, )
I1018 08:53:57.764353 4097533 build_cinn_pass.cc:744] Cluster output vars: (batch_norm2d_0.w_1, )
I1018 08:53:57.764354 4097533 build_cinn_pass.cc:745] Cluster internal vars: ()
I1018 08:53:57.764361 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.764412 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_0.w_1
I1018 08:53:57.764416 4097533 build_cinn_pass.cc:274] Add Output Var Node: batch_norm2d_0.w_1
I1018 08:53:57.764426 4097533 build_cinn_pass.cc:493] Inplace var in cluster are: [batch_norm2d_0.w_1, ]
I1018 08:53:57.764431 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="batch_norm2d_0.w_1
[64]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_2[label="batch_norm2d_0.w_1
[64]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_1[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_3
   node_2->node_0
   node_3->node_4
   node_4->node_2
} // end G
I1018 08:53:57.764468 4097533 var_desc.cc:415] Flush  batch_norm2d_0.w_1 1
I1018 08:53:57.764472 4097533 var_desc.cc:415] Flush  batch_norm2d_0.w_1 1
I1018 08:53:57.764477 4097533 graph_helper.h:104] adj assign0x55b7d0ea6e70 -> fetch0x55b7d0e83420  via batch_norm2d_0.w_10x55b7d0e8ce30
I1018 08:53:57.764484 4097533 graph_helper.h:104] adj feed0x55b7d0eacdf0 -> assign0x55b7d0ea6e70  via batch_norm2d_0.w_10x55b7d0e864a0
I1018 08:53:57.764503 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.764504 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.764509 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.764513 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.764521 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.764524 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.764577 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.764614 4097533 block_desc.cc:205] vars in desc 1
I1018 08:53:57.764617 4097533 block_desc.cc:209] Flush batch_norm2d_0.w_1
I1018 08:53:57.764619 4097533 var_desc.cc:415] Flush  batch_norm2d_0.w_1 1
I1018 08:53:57.764464 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_0.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_0.w_1"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_0.w_1"
    }
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_0.w_1"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 136, in composite_batchnorm"
      strings: "    run_mean_ = assign(run_mean)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_0.w_1"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.764693 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.764716 4097533 build_cinn_pass.cc:742] Cluster Ops: (assign, )
I1018 08:53:57.764719 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_0.w_2, )
I1018 08:53:57.764721 4097533 build_cinn_pass.cc:744] Cluster output vars: (batch_norm2d_0.w_2, )
I1018 08:53:57.764724 4097533 build_cinn_pass.cc:745] Cluster internal vars: ()
I1018 08:53:57.764729 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.764752 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_0.w_2
I1018 08:53:57.764756 4097533 build_cinn_pass.cc:274] Add Output Var Node: batch_norm2d_0.w_2
I1018 08:53:57.764765 4097533 build_cinn_pass.cc:493] Inplace var in cluster are: [batch_norm2d_0.w_2, ]
I1018 08:53:57.764770 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="batch_norm2d_0.w_2
[64]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_2[label="batch_norm2d_0.w_2
[64]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_1[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_3
   node_2->node_0
   node_3->node_4
   node_4->node_2
} // end G
I1018 08:53:57.764799 4097533 var_desc.cc:415] Flush  batch_norm2d_0.w_2 1
I1018 08:53:57.764802 4097533 var_desc.cc:415] Flush  batch_norm2d_0.w_2 1
I1018 08:53:57.764807 4097533 graph_helper.h:104] adj assign0x55b7d0992f10 -> fetch0x55b7d0e7ac70  via batch_norm2d_0.w_20x55b7d0e7cd70
I1018 08:53:57.764812 4097533 graph_helper.h:104] adj feed0x55b7d0e7ce30 -> assign0x55b7d0992f10  via batch_norm2d_0.w_20x55b7d0c15040
I1018 08:53:57.764829 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.764835 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.764839 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.764842 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.764850 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.764853 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.764900 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.764935 4097533 block_desc.cc:205] vars in desc 1
I1018 08:53:57.764936 4097533 block_desc.cc:209] Flush batch_norm2d_0.w_2
I1018 08:53:57.764940 4097533 var_desc.cc:415] Flush  batch_norm2d_0.w_2 1
I1018 08:53:57.764797 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_0.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_0.w_2"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_0.w_2"
    }
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_0.w_2"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 137, in composite_batchnorm"
      strings: "    run_var_ = assign(run_var)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_0.w_2"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.765002 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.765025 4097533 build_cinn_pass.cc:742] Cluster Ops: (assign, )
I1018 08:53:57.765028 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_1.w_1, )
I1018 08:53:57.765030 4097533 build_cinn_pass.cc:744] Cluster output vars: (batch_norm2d_1.w_1, )
I1018 08:53:57.765033 4097533 build_cinn_pass.cc:745] Cluster internal vars: ()
I1018 08:53:57.765038 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.765060 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_1.w_1
I1018 08:53:57.765064 4097533 build_cinn_pass.cc:274] Add Output Var Node: batch_norm2d_1.w_1
I1018 08:53:57.765072 4097533 build_cinn_pass.cc:493] Inplace var in cluster are: [batch_norm2d_1.w_1, ]
I1018 08:53:57.765076 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="batch_norm2d_1.w_1
[64]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_2[label="batch_norm2d_1.w_1
[64]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_1[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_3
   node_2->node_0
   node_3->node_4
   node_4->node_2
} // end G
I1018 08:53:57.765106 4097533 var_desc.cc:415] Flush  batch_norm2d_1.w_1 1
I1018 08:53:57.765110 4097533 var_desc.cc:415] Flush  batch_norm2d_1.w_1 1
I1018 08:53:57.765113 4097533 graph_helper.h:104] adj assign0x55b7d0995b70 -> fetch0x55b7d0eae0c0  via batch_norm2d_1.w_10x55b7d0ea1950
I1018 08:53:57.765120 4097533 graph_helper.h:104] adj feed0x55b7d0eae000 -> assign0x55b7d0995b70  via batch_norm2d_1.w_10x55b7d0e8c9f0
I1018 08:53:57.765136 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.765137 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.765142 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.765144 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.765152 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.765154 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.765202 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.765249 4097533 block_desc.cc:205] vars in desc 1
I1018 08:53:57.765251 4097533 block_desc.cc:209] Flush batch_norm2d_1.w_1
I1018 08:53:57.765254 4097533 var_desc.cc:415] Flush  batch_norm2d_1.w_1 1
I1018 08:53:57.765105 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_1.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_1.w_1"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_1.w_1"
    }
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_1.w_1"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 136, in composite_batchnorm"
      strings: "    run_mean_ = assign(run_mean)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_1.w_1"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.765322 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.765344 4097533 build_cinn_pass.cc:742] Cluster Ops: (assign, )
I1018 08:53:57.765347 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_1.w_2, )
I1018 08:53:57.765348 4097533 build_cinn_pass.cc:744] Cluster output vars: (batch_norm2d_1.w_2, )
I1018 08:53:57.765350 4097533 build_cinn_pass.cc:745] Cluster internal vars: ()
I1018 08:53:57.765357 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.765377 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_1.w_2
I1018 08:53:57.765381 4097533 build_cinn_pass.cc:274] Add Output Var Node: batch_norm2d_1.w_2
I1018 08:53:57.765390 4097533 build_cinn_pass.cc:493] Inplace var in cluster are: [batch_norm2d_1.w_2, ]
I1018 08:53:57.765394 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="batch_norm2d_1.w_2
[64]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_2[label="batch_norm2d_1.w_2
[64]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_1[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_3
   node_2->node_0
   node_3->node_4
   node_4->node_2
} // end G
I1018 08:53:57.765424 4097533 var_desc.cc:415] Flush  batch_norm2d_1.w_2 1
I1018 08:53:57.765427 4097533 var_desc.cc:415] Flush  batch_norm2d_1.w_2 1
I1018 08:53:57.765431 4097533 graph_helper.h:104] adj assign0x55b7d09da280 -> fetch0x55b7d09db080  via batch_norm2d_1.w_20x55b7d0e8dda0
I1018 08:53:57.765437 4097533 graph_helper.h:104] adj feed0x55b7d1048790 -> assign0x55b7d09da280  via batch_norm2d_1.w_20x55b7d0996730
I1018 08:53:57.765453 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.765455 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.765460 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.765465 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.765472 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.765475 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.765522 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.765553 4097533 block_desc.cc:205] vars in desc 1
I1018 08:53:57.765556 4097533 block_desc.cc:209] Flush batch_norm2d_1.w_2
I1018 08:53:57.765558 4097533 var_desc.cc:415] Flush  batch_norm2d_1.w_2 1
I1018 08:53:57.765422 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_1.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_1.w_2"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_1.w_2"
    }
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_1.w_2"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 137, in composite_batchnorm"
      strings: "    run_var_ = assign(run_var)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_1.w_2"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.765621 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.765640 4097533 build_cinn_pass.cc:742] Cluster Ops: (assign, )
I1018 08:53:57.765642 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_2.w_1, )
I1018 08:53:57.765646 4097533 build_cinn_pass.cc:744] Cluster output vars: (batch_norm2d_2.w_1, )
I1018 08:53:57.765648 4097533 build_cinn_pass.cc:745] Cluster internal vars: ()
I1018 08:53:57.765655 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.765676 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_2.w_1
I1018 08:53:57.765679 4097533 build_cinn_pass.cc:274] Add Output Var Node: batch_norm2d_2.w_1
I1018 08:53:57.765687 4097533 build_cinn_pass.cc:493] Inplace var in cluster are: [batch_norm2d_2.w_1, ]
I1018 08:53:57.765692 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="batch_norm2d_2.w_1
[64]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_2[label="batch_norm2d_2.w_1
[64]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_1[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_3
   node_2->node_0
   node_3->node_4
   node_4->node_2
} // end G
I1018 08:53:57.765720 4097533 var_desc.cc:415] Flush  batch_norm2d_2.w_1 1
I1018 08:53:57.765723 4097533 var_desc.cc:415] Flush  batch_norm2d_2.w_1 1
I1018 08:53:57.765727 4097533 graph_helper.h:104] adj assign0x55b7d09dcee0 -> fetch0x55b7d0c22750  via batch_norm2d_2.w_10x55b7d0e89910
I1018 08:53:57.765733 4097533 graph_helper.h:104] adj feed0x55b7d09dda60 -> assign0x55b7d09dcee0  via batch_norm2d_2.w_10x55b7d0ec5070
I1018 08:53:57.765748 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.765750 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.765754 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.765758 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.765764 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.765766 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.765813 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.765844 4097533 block_desc.cc:205] vars in desc 1
I1018 08:53:57.765847 4097533 block_desc.cc:209] Flush batch_norm2d_2.w_1
I1018 08:53:57.765849 4097533 var_desc.cc:415] Flush  batch_norm2d_2.w_1 1
I1018 08:53:57.765718 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_2.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_2.w_1"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_2.w_1"
    }
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_2.w_1"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 136, in composite_batchnorm"
      strings: "    run_mean_ = assign(run_mean)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_2.w_1"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.765913 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.765930 4097533 build_cinn_pass.cc:742] Cluster Ops: (assign, )
I1018 08:53:57.765933 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_2.w_2, )
I1018 08:53:57.765935 4097533 build_cinn_pass.cc:744] Cluster output vars: (batch_norm2d_2.w_2, )
I1018 08:53:57.765937 4097533 build_cinn_pass.cc:745] Cluster internal vars: ()
I1018 08:53:57.765942 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.765964 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_2.w_2
I1018 08:53:57.765969 4097533 build_cinn_pass.cc:274] Add Output Var Node: batch_norm2d_2.w_2
I1018 08:53:57.765975 4097533 build_cinn_pass.cc:493] Inplace var in cluster are: [batch_norm2d_2.w_2, ]
I1018 08:53:57.765980 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="batch_norm2d_2.w_2
[64]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_2[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1[label="batch_norm2d_2.w_2
[64]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_0
   node_2->node_3
   node_3->node_4
   node_4->node_1
} // end G
I1018 08:53:57.766008 4097533 var_desc.cc:415] Flush  batch_norm2d_2.w_2 1
I1018 08:53:57.766011 4097533 var_desc.cc:415] Flush  batch_norm2d_2.w_2 1
I1018 08:53:57.766016 4097533 graph_helper.h:104] adj assign0x55b7d0a1f5d0 -> fetch0x55b7d0997550  via batch_norm2d_2.w_20x55b7d09de670
I1018 08:53:57.766021 4097533 graph_helper.h:104] adj feed0x55b7d1098180 -> assign0x55b7d0a1f5d0  via batch_norm2d_2.w_20x55b7d09de5b0
I1018 08:53:57.766036 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.766039 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.766043 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.766045 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.766052 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.766058 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.766104 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.766134 4097533 block_desc.cc:205] vars in desc 1
I1018 08:53:57.766137 4097533 block_desc.cc:209] Flush batch_norm2d_2.w_2
I1018 08:53:57.766139 4097533 var_desc.cc:415] Flush  batch_norm2d_2.w_2 1
I1018 08:53:57.766006 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_2.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_2.w_2"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_2.w_2"
    }
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_2.w_2"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 137, in composite_batchnorm"
      strings: "    run_var_ = assign(run_var)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_2.w_2"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.766199 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.766216 4097533 build_cinn_pass.cc:742] Cluster Ops: (assign, )
I1018 08:53:57.766217 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_3.w_1, )
I1018 08:53:57.766220 4097533 build_cinn_pass.cc:744] Cluster output vars: (batch_norm2d_3.w_1, )
I1018 08:53:57.766227 4097533 build_cinn_pass.cc:745] Cluster internal vars: ()
I1018 08:53:57.766232 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.766252 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_3.w_1
I1018 08:53:57.766256 4097533 build_cinn_pass.cc:274] Add Output Var Node: batch_norm2d_3.w_1
I1018 08:53:57.766263 4097533 build_cinn_pass.cc:493] Inplace var in cluster are: [batch_norm2d_3.w_1, ]
I1018 08:53:57.766268 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="batch_norm2d_3.w_1
[64]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_2[label="batch_norm2d_3.w_1
[64]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_1[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_3
   node_2->node_0
   node_3->node_4
   node_4->node_2
} // end G
I1018 08:53:57.766296 4097533 var_desc.cc:415] Flush  batch_norm2d_3.w_1 1
I1018 08:53:57.766299 4097533 var_desc.cc:415] Flush  batch_norm2d_3.w_1 1
I1018 08:53:57.766304 4097533 graph_helper.h:104] adj assign0x55b7d0a22230 -> fetch0x55b7d09dd2c0  via batch_norm2d_3.w_10x55b7d0f2b560
I1018 08:53:57.766309 4097533 graph_helper.h:104] adj feed0x55b7d0dd42c0 -> assign0x55b7d0a22230  via batch_norm2d_3.w_10x55b7d0e81d20
I1018 08:53:57.766324 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.766326 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.766330 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.766332 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.766340 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.766342 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.766386 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.766415 4097533 block_desc.cc:205] vars in desc 1
I1018 08:53:57.766418 4097533 block_desc.cc:209] Flush batch_norm2d_3.w_1
I1018 08:53:57.766420 4097533 var_desc.cc:415] Flush  batch_norm2d_3.w_1 1
I1018 08:53:57.766294 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_3.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_3.w_1"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_3.w_1"
    }
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_3.w_1"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 136, in composite_batchnorm"
      strings: "    run_mean_ = assign(run_mean)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_3.w_1"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.766481 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.766497 4097533 build_cinn_pass.cc:742] Cluster Ops: (assign, )
I1018 08:53:57.766499 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_3.w_2, )
I1018 08:53:57.766501 4097533 build_cinn_pass.cc:744] Cluster output vars: (batch_norm2d_3.w_2, )
I1018 08:53:57.766505 4097533 build_cinn_pass.cc:745] Cluster internal vars: ()
I1018 08:53:57.766508 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.766530 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_3.w_2
I1018 08:53:57.766533 4097533 build_cinn_pass.cc:274] Add Output Var Node: batch_norm2d_3.w_2
I1018 08:53:57.766541 4097533 build_cinn_pass.cc:493] Inplace var in cluster are: [batch_norm2d_3.w_2, ]
I1018 08:53:57.766544 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="batch_norm2d_3.w_2
[64]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_2[label="batch_norm2d_3.w_2
[64]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_1[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_3
   node_2->node_0
   node_3->node_4
   node_4->node_2
} // end G
I1018 08:53:57.766572 4097533 var_desc.cc:415] Flush  batch_norm2d_3.w_2 1
I1018 08:53:57.766575 4097533 var_desc.cc:415] Flush  batch_norm2d_3.w_2 1
I1018 08:53:57.766579 4097533 graph_helper.h:104] adj assign0x55b7d0a667d0 -> fetch0x55b7d0ec0d80  via batch_norm2d_3.w_20x55b7d0f2d720
I1018 08:53:57.766584 4097533 graph_helper.h:104] adj feed0x55b7d0f2d880 -> assign0x55b7d0a667d0  via batch_norm2d_3.w_20x55b7d0e056c0
I1018 08:53:57.766600 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.766602 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.766606 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.766608 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.766615 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.766618 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.766664 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.766693 4097533 block_desc.cc:205] vars in desc 1
I1018 08:53:57.766696 4097533 block_desc.cc:209] Flush batch_norm2d_3.w_2
I1018 08:53:57.766698 4097533 var_desc.cc:415] Flush  batch_norm2d_3.w_2 1
I1018 08:53:57.766570 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_3.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_3.w_2"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_3.w_2"
    }
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_3.w_2"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 137, in composite_batchnorm"
      strings: "    run_var_ = assign(run_var)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_3.w_2"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.766758 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.766773 4097533 build_cinn_pass.cc:742] Cluster Ops: (assign, )
I1018 08:53:57.766775 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_4.w_1, )
I1018 08:53:57.766777 4097533 build_cinn_pass.cc:744] Cluster output vars: (batch_norm2d_4.w_1, )
I1018 08:53:57.766779 4097533 build_cinn_pass.cc:745] Cluster internal vars: ()
I1018 08:53:57.766788 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.766808 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_4.w_1
I1018 08:53:57.766813 4097533 build_cinn_pass.cc:274] Add Output Var Node: batch_norm2d_4.w_1
I1018 08:53:57.766819 4097533 build_cinn_pass.cc:493] Inplace var in cluster are: [batch_norm2d_4.w_1, ]
I1018 08:53:57.766824 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="batch_norm2d_4.w_1
[64]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_2[label="batch_norm2d_4.w_1
[64]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_1[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_3
   node_2->node_0
   node_3->node_4
   node_4->node_2
} // end G
I1018 08:53:57.766852 4097533 var_desc.cc:415] Flush  batch_norm2d_4.w_1 1
I1018 08:53:57.766855 4097533 var_desc.cc:415] Flush  batch_norm2d_4.w_1 1
I1018 08:53:57.766860 4097533 graph_helper.h:104] adj assign0x55b7d0a69410 -> fetch0x55b7d0d089a0  via batch_norm2d_4.w_10x55b7d0eaf470
I1018 08:53:57.766865 4097533 graph_helper.h:104] adj feed0x55b7d0a69f90 -> assign0x55b7d0a69410  via batch_norm2d_4.w_10x55b7d0ec0980
I1018 08:53:57.766880 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.766881 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.766885 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.766888 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.766894 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.766897 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.766942 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.766970 4097533 block_desc.cc:205] vars in desc 1
I1018 08:53:57.766973 4097533 block_desc.cc:209] Flush batch_norm2d_4.w_1
I1018 08:53:57.766975 4097533 var_desc.cc:415] Flush  batch_norm2d_4.w_1 1
I1018 08:53:57.766849 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_4.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_4.w_1"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_4.w_1"
    }
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_4.w_1"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 136, in composite_batchnorm"
      strings: "    run_mean_ = assign(run_mean)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_4.w_1"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.767036 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.767051 4097533 build_cinn_pass.cc:742] Cluster Ops: (assign, )
I1018 08:53:57.767053 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_4.w_2, )
I1018 08:53:57.767056 4097533 build_cinn_pass.cc:744] Cluster output vars: (batch_norm2d_4.w_2, )
I1018 08:53:57.767058 4097533 build_cinn_pass.cc:745] Cluster internal vars: ()
I1018 08:53:57.767063 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.767083 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_4.w_2
I1018 08:53:57.767086 4097533 build_cinn_pass.cc:274] Add Output Var Node: batch_norm2d_4.w_2
I1018 08:53:57.767093 4097533 build_cinn_pass.cc:493] Inplace var in cluster are: [batch_norm2d_4.w_2, ]
I1018 08:53:57.767098 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="batch_norm2d_4.w_2
[64]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_2[label="batch_norm2d_4.w_2
[64]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_1[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_3
   node_2->node_0
   node_3->node_4
   node_4->node_2
} // end G
I1018 08:53:57.767127 4097533 var_desc.cc:415] Flush  batch_norm2d_4.w_2 1
I1018 08:53:57.767128 4097533 var_desc.cc:415] Flush  batch_norm2d_4.w_2 1
I1018 08:53:57.767132 4097533 graph_helper.h:104] adj assign0x55b7d0aabc10 -> fetch0x55b7d1098740  via batch_norm2d_4.w_20x55b7d0e94a30
I1018 08:53:57.767138 4097533 graph_helper.h:104] adj feed0x55b7d0eb8370 -> assign0x55b7d0aabc10  via batch_norm2d_4.w_20x55b7d0a215d0
I1018 08:53:57.767153 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.767156 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.767160 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.767163 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.767169 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.767171 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.767217 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.767253 4097533 block_desc.cc:205] vars in desc 1
I1018 08:53:57.767256 4097533 block_desc.cc:209] Flush batch_norm2d_4.w_2
I1018 08:53:57.767258 4097533 var_desc.cc:415] Flush  batch_norm2d_4.w_2 1
I1018 08:53:57.767123 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_4.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_4.w_2"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_4.w_2"
    }
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_4.w_2"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 137, in composite_batchnorm"
      strings: "    run_var_ = assign(run_var)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_4.w_2"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.767319 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.767336 4097533 build_cinn_pass.cc:742] Cluster Ops: (assign, )
I1018 08:53:57.767338 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_6.w_1, )
I1018 08:53:57.767340 4097533 build_cinn_pass.cc:744] Cluster output vars: (batch_norm2d_6.w_1, )
I1018 08:53:57.767342 4097533 build_cinn_pass.cc:745] Cluster internal vars: ()
I1018 08:53:57.767347 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.767370 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_6.w_1
I1018 08:53:57.767374 4097533 build_cinn_pass.cc:274] Add Output Var Node: batch_norm2d_6.w_1
I1018 08:53:57.767381 4097533 build_cinn_pass.cc:493] Inplace var in cluster are: [batch_norm2d_6.w_1, ]
I1018 08:53:57.767385 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="batch_norm2d_6.w_1
[128]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_2[label="batch_norm2d_6.w_1
[128]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_1[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0->node_3
   node_2->node_1
   node_3->node_4
   node_4->node_2
} // end G
I1018 08:53:57.767414 4097533 var_desc.cc:415] Flush  batch_norm2d_6.w_1 1
I1018 08:53:57.767417 4097533 var_desc.cc:415] Flush  batch_norm2d_6.w_1 1
I1018 08:53:57.767421 4097533 graph_helper.h:104] adj assign0x55b7d0aae850 -> fetch0x55b7d0a23900  via batch_norm2d_6.w_10x55b7d0a6ab50
I1018 08:53:57.767426 4097533 graph_helper.h:104] adj feed0x55b7d0a6ad10 -> assign0x55b7d0aae850  via batch_norm2d_6.w_10x55b7d0e12d50
I1018 08:53:57.767442 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.767444 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.767448 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.767452 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.767457 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.767460 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.767504 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.767534 4097533 block_desc.cc:205] vars in desc 1
I1018 08:53:57.767537 4097533 block_desc.cc:209] Flush batch_norm2d_6.w_1
I1018 08:53:57.767539 4097533 var_desc.cc:415] Flush  batch_norm2d_6.w_1 1
I1018 08:53:57.767412 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_6.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_6.w_1"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_6.w_1"
    }
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_6.w_1"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 136, in composite_batchnorm"
      strings: "    run_mean_ = assign(run_mean)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_6.w_1"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.767601 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.767618 4097533 build_cinn_pass.cc:742] Cluster Ops: (assign, )
I1018 08:53:57.767621 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_6.w_2, )
I1018 08:53:57.767622 4097533 build_cinn_pass.cc:744] Cluster output vars: (batch_norm2d_6.w_2, )
I1018 08:53:57.767624 4097533 build_cinn_pass.cc:745] Cluster internal vars: ()
I1018 08:53:57.767629 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.767650 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_6.w_2
I1018 08:53:57.767653 4097533 build_cinn_pass.cc:274] Add Output Var Node: batch_norm2d_6.w_2
I1018 08:53:57.767661 4097533 build_cinn_pass.cc:493] Inplace var in cluster are: [batch_norm2d_6.w_2, ]
I1018 08:53:57.767665 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="batch_norm2d_6.w_2
[128]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_2[label="batch_norm2d_6.w_2
[128]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_1[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_3
   node_2->node_0
   node_3->node_4
   node_4->node_2
} // end G
I1018 08:53:57.767692 4097533 var_desc.cc:415] Flush  batch_norm2d_6.w_2 1
I1018 08:53:57.767695 4097533 var_desc.cc:415] Flush  batch_norm2d_6.w_2 1
I1018 08:53:57.767699 4097533 graph_helper.h:104] adj assign0x55b7d0af2df0 -> fetch0x55b7d09df4e0  via batch_norm2d_6.w_20x55b7d0aaf0b0
I1018 08:53:57.767705 4097533 graph_helper.h:104] adj feed0x55b7d0eaebb0 -> assign0x55b7d0af2df0  via batch_norm2d_6.w_20x55b7d0ead480
I1018 08:53:57.767720 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.767724 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.767727 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.767729 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.767736 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.767738 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.767782 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.767812 4097533 block_desc.cc:205] vars in desc 1
I1018 08:53:57.767813 4097533 block_desc.cc:209] Flush batch_norm2d_6.w_2
I1018 08:53:57.767818 4097533 var_desc.cc:415] Flush  batch_norm2d_6.w_2 1
I1018 08:53:57.767690 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_6.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_6.w_2"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_6.w_2"
    }
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_6.w_2"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 137, in composite_batchnorm"
      strings: "    run_var_ = assign(run_var)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_6.w_2"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.767877 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.767894 4097533 build_cinn_pass.cc:742] Cluster Ops: (assign, )
I1018 08:53:57.767895 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_7.w_1, )
I1018 08:53:57.767897 4097533 build_cinn_pass.cc:744] Cluster output vars: (batch_norm2d_7.w_1, )
I1018 08:53:57.767899 4097533 build_cinn_pass.cc:745] Cluster internal vars: ()
I1018 08:53:57.767904 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.767925 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_7.w_1
I1018 08:53:57.767931 4097533 build_cinn_pass.cc:274] Add Output Var Node: batch_norm2d_7.w_1
I1018 08:53:57.767938 4097533 build_cinn_pass.cc:493] Inplace var in cluster are: [batch_norm2d_7.w_1, ]
I1018 08:53:57.767943 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="batch_norm2d_7.w_1
[128]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_2[label="batch_norm2d_7.w_1
[128]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_1[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_3
   node_2->node_0
   node_3->node_4
   node_4->node_2
} // end G
I1018 08:53:57.767971 4097533 var_desc.cc:415] Flush  batch_norm2d_7.w_1 1
I1018 08:53:57.767973 4097533 var_desc.cc:415] Flush  batch_norm2d_7.w_1 1
I1018 08:53:57.767977 4097533 graph_helper.h:104] adj assign0x55b7d0af5a30 -> fetch0x55b7d0dd5220  via batch_norm2d_7.w_10x55b7d0af65b0
I1018 08:53:57.767982 4097533 graph_helper.h:104] adj feed0x55b7d0cc7e10 -> assign0x55b7d0af5a30  via batch_norm2d_7.w_10x55b7d0f0b970
I1018 08:53:57.767998 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.768000 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.768004 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.768007 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.768013 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.768016 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.768060 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.768090 4097533 block_desc.cc:205] vars in desc 1
I1018 08:53:57.768091 4097533 block_desc.cc:209] Flush batch_norm2d_7.w_1
I1018 08:53:57.768093 4097533 var_desc.cc:415] Flush  batch_norm2d_7.w_1 1
I1018 08:53:57.767968 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_7.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_7.w_1"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_7.w_1"
    }
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_7.w_1"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 136, in composite_batchnorm"
      strings: "    run_mean_ = assign(run_mean)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_7.w_1"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.768154 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.768170 4097533 build_cinn_pass.cc:742] Cluster Ops: (assign, )
I1018 08:53:57.768173 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_7.w_2, )
I1018 08:53:57.768175 4097533 build_cinn_pass.cc:744] Cluster output vars: (batch_norm2d_7.w_2, )
I1018 08:53:57.768177 4097533 build_cinn_pass.cc:745] Cluster internal vars: ()
I1018 08:53:57.768182 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.768201 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_7.w_2
I1018 08:53:57.768205 4097533 build_cinn_pass.cc:274] Add Output Var Node: batch_norm2d_7.w_2
I1018 08:53:57.768213 4097533 build_cinn_pass.cc:493] Inplace var in cluster are: [batch_norm2d_7.w_2, ]
I1018 08:53:57.768216 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="batch_norm2d_7.w_2
[128]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_2[label="batch_norm2d_7.w_2
[128]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_1[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0->node_3
   node_2->node_1
   node_3->node_4
   node_4->node_2
} // end G
I1018 08:53:57.768244 4097533 var_desc.cc:415] Flush  batch_norm2d_7.w_2 1
I1018 08:53:57.768247 4097533 var_desc.cc:415] Flush  batch_norm2d_7.w_2 1
I1018 08:53:57.768251 4097533 graph_helper.h:104] adj assign0x55b7d0b37950 -> fetch0x55b7d0f0c050  via batch_norm2d_7.w_20x55b7d0b38730
I1018 08:53:57.768256 4097533 graph_helper.h:104] adj feed0x55b7d0f0bed0 -> assign0x55b7d0b37950  via batch_norm2d_7.w_20x55b7d0dbb2c0
I1018 08:53:57.768272 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.768275 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.768278 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.768280 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.768287 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.768290 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.768334 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.768362 4097533 block_desc.cc:205] vars in desc 1
I1018 08:53:57.768365 4097533 block_desc.cc:209] Flush batch_norm2d_7.w_2
I1018 08:53:57.768368 4097533 var_desc.cc:415] Flush  batch_norm2d_7.w_2 1
I1018 08:53:57.768242 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_7.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_7.w_2"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_7.w_2"
    }
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_7.w_2"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 137, in composite_batchnorm"
      strings: "    run_var_ = assign(run_var)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_7.w_2"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.768429 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.768443 4097533 build_cinn_pass.cc:742] Cluster Ops: (assign, )
I1018 08:53:57.768446 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_5.w_1, )
I1018 08:53:57.768448 4097533 build_cinn_pass.cc:744] Cluster output vars: (batch_norm2d_5.w_1, )
I1018 08:53:57.768450 4097533 build_cinn_pass.cc:745] Cluster internal vars: ()
I1018 08:53:57.768455 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.768474 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_5.w_1
I1018 08:53:57.768478 4097533 build_cinn_pass.cc:274] Add Output Var Node: batch_norm2d_5.w_1
I1018 08:53:57.768489 4097533 build_cinn_pass.cc:493] Inplace var in cluster are: [batch_norm2d_5.w_1, ]
I1018 08:53:57.768493 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="batch_norm2d_5.w_1
[128]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_2[label="batch_norm2d_5.w_1
[128]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_1[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_3
   node_2->node_0
   node_3->node_4
   node_4->node_2
} // end G
I1018 08:53:57.768520 4097533 var_desc.cc:415] Flush  batch_norm2d_5.w_1 1
I1018 08:53:57.768523 4097533 var_desc.cc:415] Flush  batch_norm2d_5.w_1 1
I1018 08:53:57.768527 4097533 graph_helper.h:104] adj assign0x55b7d0b3a590 -> fetch0x55b7d0c23d50  via batch_norm2d_5.w_10x55b7d0b38430
I1018 08:53:57.768532 4097533 graph_helper.h:104] adj feed0x55b7d0af7f90 -> assign0x55b7d0b3a590  via batch_norm2d_5.w_10x55b7d0f0b2e0
I1018 08:53:57.768548 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.768550 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.768554 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.768558 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.768563 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.768566 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.768610 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.768638 4097533 block_desc.cc:205] vars in desc 1
I1018 08:53:57.768641 4097533 block_desc.cc:209] Flush batch_norm2d_5.w_1
I1018 08:53:57.768643 4097533 var_desc.cc:415] Flush  batch_norm2d_5.w_1 1
I1018 08:53:57.768518 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_5.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_5.w_1"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_5.w_1"
    }
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_5.w_1"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 136, in composite_batchnorm"
      strings: "    run_mean_ = assign(run_mean)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_5.w_1"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.768703 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.768718 4097533 build_cinn_pass.cc:742] Cluster Ops: (assign, )
I1018 08:53:57.768720 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_5.w_2, )
I1018 08:53:57.768723 4097533 build_cinn_pass.cc:744] Cluster output vars: (batch_norm2d_5.w_2, )
I1018 08:53:57.768724 4097533 build_cinn_pass.cc:745] Cluster internal vars: ()
I1018 08:53:57.768729 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.768749 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_5.w_2
I1018 08:53:57.768754 4097533 build_cinn_pass.cc:274] Add Output Var Node: batch_norm2d_5.w_2
I1018 08:53:57.768760 4097533 build_cinn_pass.cc:493] Inplace var in cluster are: [batch_norm2d_5.w_2, ]
I1018 08:53:57.768764 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="batch_norm2d_5.w_2
[128]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_2[label="batch_norm2d_5.w_2
[128]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_1[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_2
   node_2->node_4
   node_3->node_0
   node_4->node_3
} // end G
I1018 08:53:57.768792 4097533 var_desc.cc:415] Flush  batch_norm2d_5.w_2 1
I1018 08:53:57.768795 4097533 var_desc.cc:415] Flush  batch_norm2d_5.w_2 1
I1018 08:53:57.768798 4097533 graph_helper.h:104] adj assign0x55b7d0b762d0 -> fetch0x55b7d0c236a0  via batch_norm2d_5.w_20x55b7d0b77170
I1018 08:53:57.768805 4097533 graph_helper.h:104] adj feed0x55b7d0e834e0 -> assign0x55b7d0b762d0  via batch_norm2d_5.w_20x55b7d0b770b0
I1018 08:53:57.768819 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.768822 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.768826 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.768828 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.768836 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.768837 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.768882 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.768909 4097533 block_desc.cc:205] vars in desc 1
I1018 08:53:57.768913 4097533 block_desc.cc:209] Flush batch_norm2d_5.w_2
I1018 08:53:57.768914 4097533 var_desc.cc:415] Flush  batch_norm2d_5.w_2 1
I1018 08:53:57.768790 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_5.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_5.w_2"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_5.w_2"
    }
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_5.w_2"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 137, in composite_batchnorm"
      strings: "    run_var_ = assign(run_var)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_5.w_2"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.768975 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.768990 4097533 build_cinn_pass.cc:742] Cluster Ops: (assign, )
I1018 08:53:57.768991 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_8.w_1, )
I1018 08:53:57.768993 4097533 build_cinn_pass.cc:744] Cluster output vars: (batch_norm2d_8.w_1, )
I1018 08:53:57.768996 4097533 build_cinn_pass.cc:745] Cluster internal vars: ()
I1018 08:53:57.769001 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.769021 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_8.w_1
I1018 08:53:57.769023 4097533 build_cinn_pass.cc:274] Add Output Var Node: batch_norm2d_8.w_1
I1018 08:53:57.769032 4097533 build_cinn_pass.cc:493] Inplace var in cluster are: [batch_norm2d_8.w_1, ]
I1018 08:53:57.769037 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_2[label="batch_norm2d_8.w_1
[128]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_1[label="batch_norm2d_8.w_1
[128]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_0
   node_2->node_4
   node_3->node_2
   node_4->node_1
} // end G
I1018 08:53:57.769065 4097533 var_desc.cc:415] Flush  batch_norm2d_8.w_1 1
I1018 08:53:57.769068 4097533 var_desc.cc:415] Flush  batch_norm2d_8.w_1 1
I1018 08:53:57.769073 4097533 graph_helper.h:104] adj assign0x55b7d0b78f10 -> fetch0x55b7d0af4cd0  via batch_norm2d_8.w_10x55b7d0c23040
I1018 08:53:57.769078 4097533 graph_helper.h:104] adj feed0x55b7d0af4b50 -> assign0x55b7d0b78f10  via batch_norm2d_8.w_10x55b7d0c22f80
I1018 08:53:57.769093 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.769095 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.769099 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.769101 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.769109 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.769110 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.769155 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.769182 4097533 block_desc.cc:205] vars in desc 1
I1018 08:53:57.769186 4097533 block_desc.cc:209] Flush batch_norm2d_8.w_1
I1018 08:53:57.769187 4097533 var_desc.cc:415] Flush  batch_norm2d_8.w_1 1
I1018 08:53:57.769062 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_8.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_8.w_1"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_8.w_1"
    }
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_8.w_1"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 136, in composite_batchnorm"
      strings: "    run_mean_ = assign(run_mean)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_8.w_1"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.769258 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.769272 4097533 build_cinn_pass.cc:742] Cluster Ops: (assign, )
I1018 08:53:57.769275 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_8.w_2, )
I1018 08:53:57.769277 4097533 build_cinn_pass.cc:744] Cluster output vars: (batch_norm2d_8.w_2, )
I1018 08:53:57.769279 4097533 build_cinn_pass.cc:745] Cluster internal vars: ()
I1018 08:53:57.769284 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.769304 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_8.w_2
I1018 08:53:57.769308 4097533 build_cinn_pass.cc:274] Add Output Var Node: batch_norm2d_8.w_2
I1018 08:53:57.769315 4097533 build_cinn_pass.cc:493] Inplace var in cluster are: [batch_norm2d_8.w_2, ]
I1018 08:53:57.769320 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="batch_norm2d_8.w_2
[128]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_2[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1[label="batch_norm2d_8.w_2
[128]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_0
   node_2->node_3
   node_3->node_4
   node_4->node_1
} // end G
I1018 08:53:57.769347 4097533 var_desc.cc:415] Flush  batch_norm2d_8.w_2 1
I1018 08:53:57.769351 4097533 var_desc.cc:415] Flush  batch_norm2d_8.w_2 1
I1018 08:53:57.769354 4097533 graph_helper.h:104] adj assign0x55b7d0bbd4b0 -> fetch0x55b7d0bbde30  via batch_norm2d_8.w_20x55b7d0bbfd20
I1018 08:53:57.769361 4097533 graph_helper.h:104] adj feed0x55b7d0bbdcb0 -> assign0x55b7d0bbd4b0  via batch_norm2d_8.w_20x55b7d0bbfc60
I1018 08:53:57.769376 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.769378 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.769381 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.769384 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.769392 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.769393 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.769436 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.769464 4097533 block_desc.cc:205] vars in desc 1
I1018 08:53:57.769467 4097533 block_desc.cc:209] Flush batch_norm2d_8.w_2
I1018 08:53:57.769469 4097533 var_desc.cc:415] Flush  batch_norm2d_8.w_2 1
I1018 08:53:57.769345 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_8.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_8.w_2"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_8.w_2"
    }
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_8.w_2"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 137, in composite_batchnorm"
      strings: "    run_var_ = assign(run_var)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_8.w_2"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.769531 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.769544 4097533 build_cinn_pass.cc:742] Cluster Ops: (assign, )
I1018 08:53:57.769546 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_9.w_1, )
I1018 08:53:57.769548 4097533 build_cinn_pass.cc:744] Cluster output vars: (batch_norm2d_9.w_1, )
I1018 08:53:57.769551 4097533 build_cinn_pass.cc:745] Cluster internal vars: ()
I1018 08:53:57.769555 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.769577 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_9.w_1
I1018 08:53:57.769580 4097533 build_cinn_pass.cc:274] Add Output Var Node: batch_norm2d_9.w_1
I1018 08:53:57.769587 4097533 build_cinn_pass.cc:493] Inplace var in cluster are: [batch_norm2d_9.w_1, ]
I1018 08:53:57.769591 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="batch_norm2d_9.w_1
[128]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_2[label="batch_norm2d_9.w_1
[128]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_1[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_3
   node_2->node_0
   node_3->node_4
   node_4->node_2
} // end G
I1018 08:53:57.769621 4097533 var_desc.cc:415] Flush  batch_norm2d_9.w_1 1
I1018 08:53:57.769624 4097533 var_desc.cc:415] Flush  batch_norm2d_9.w_1 1
I1018 08:53:57.769629 4097533 graph_helper.h:104] adj assign0x55b7d0bc00f0 -> fetch0x55b7d0bc0c70  via batch_norm2d_9.w_10x55b7d1034e40
I1018 08:53:57.769634 4097533 graph_helper.h:104] adj feed0x55b7d0b78520 -> assign0x55b7d0bc00f0  via batch_norm2d_9.w_10x55b7d1034d80
I1018 08:53:57.769649 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.769652 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.769655 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.769658 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.769665 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.769667 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.769711 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.769738 4097533 block_desc.cc:205] vars in desc 1
I1018 08:53:57.769742 4097533 block_desc.cc:209] Flush batch_norm2d_9.w_1
I1018 08:53:57.769743 4097533 var_desc.cc:415] Flush  batch_norm2d_9.w_1 1
I1018 08:53:57.769619 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_9.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_9.w_1"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_9.w_1"
    }
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_9.w_1"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 136, in composite_batchnorm"
      strings: "    run_mean_ = assign(run_mean)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_9.w_1"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.769804 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.769819 4097533 build_cinn_pass.cc:742] Cluster Ops: (assign, )
I1018 08:53:57.769821 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_9.w_2, )
I1018 08:53:57.769824 4097533 build_cinn_pass.cc:744] Cluster output vars: (batch_norm2d_9.w_2, )
I1018 08:53:57.769826 4097533 build_cinn_pass.cc:745] Cluster internal vars: ()
I1018 08:53:57.769830 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.769850 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_9.w_2
I1018 08:53:57.769855 4097533 build_cinn_pass.cc:274] Add Output Var Node: batch_norm2d_9.w_2
I1018 08:53:57.769861 4097533 build_cinn_pass.cc:493] Inplace var in cluster are: [batch_norm2d_9.w_2, ]
I1018 08:53:57.769865 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="batch_norm2d_9.w_2
[128]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_2[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="batch_norm2d_9.w_2
[128]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_0->node_1
   node_2->node_3
   node_3->node_4
   node_4->node_0
} // end G
I1018 08:53:57.769894 4097533 var_desc.cc:415] Flush  batch_norm2d_9.w_2 1
I1018 08:53:57.769897 4097533 var_desc.cc:415] Flush  batch_norm2d_9.w_2 1
I1018 08:53:57.769901 4097533 graph_helper.h:104] adj assign0x55b7d0c02010 -> fetch0x55b7d0c029b0  via batch_norm2d_9.w_20x55b7d0e36600
I1018 08:53:57.769906 4097533 graph_helper.h:104] adj feed0x55b7d0c02810 -> assign0x55b7d0c02010  via batch_norm2d_9.w_20x55b7d0effba0
I1018 08:53:57.769922 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.769924 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.769928 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.769932 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.769937 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.769940 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.769985 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.770015 4097533 block_desc.cc:205] vars in desc 1
I1018 08:53:57.770017 4097533 block_desc.cc:209] Flush batch_norm2d_9.w_2
I1018 08:53:57.770020 4097533 var_desc.cc:415] Flush  batch_norm2d_9.w_2 1
I1018 08:53:57.769891 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_9.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_9.w_2"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_9.w_2"
    }
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_9.w_2"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 137, in composite_batchnorm"
      strings: "    run_var_ = assign(run_var)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_9.w_2"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.770083 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.770100 4097533 build_cinn_pass.cc:742] Cluster Ops: (assign, )
I1018 08:53:57.770102 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_11.w_1, )
I1018 08:53:57.770104 4097533 build_cinn_pass.cc:744] Cluster output vars: (batch_norm2d_11.w_1, )
I1018 08:53:57.770107 4097533 build_cinn_pass.cc:745] Cluster internal vars: ()
I1018 08:53:57.770112 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.770131 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_11.w_1
I1018 08:53:57.770135 4097533 build_cinn_pass.cc:274] Add Output Var Node: batch_norm2d_11.w_1
I1018 08:53:57.770143 4097533 build_cinn_pass.cc:493] Inplace var in cluster are: [batch_norm2d_11.w_1, ]
I1018 08:53:57.770148 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="batch_norm2d_11.w_1
[256]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_2[label="batch_norm2d_11.w_1
[256]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_1[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_3
   node_2->node_0
   node_3->node_4
   node_4->node_2
} // end G
I1018 08:53:57.770179 4097533 var_desc.cc:415] Flush  batch_norm2d_11.w_1 1
I1018 08:53:57.770182 4097533 var_desc.cc:415] Flush  batch_norm2d_11.w_1 1
I1018 08:53:57.770186 4097533 graph_helper.h:104] adj assign0x55b7d0c04c50 -> fetch0x55b7d0eb7330  via batch_norm2d_11.w_10x55b7d0ec0430
I1018 08:53:57.770191 4097533 graph_helper.h:104] adj feed0x55b7d0eb71b0 -> assign0x55b7d0c04c50  via batch_norm2d_11.w_10x55b7d0ea5cc0
I1018 08:53:57.770207 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.770210 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.770213 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.770216 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.770222 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.770226 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.770268 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.770298 4097533 block_desc.cc:205] vars in desc 1
I1018 08:53:57.770301 4097533 block_desc.cc:209] Flush batch_norm2d_11.w_1
I1018 08:53:57.770303 4097533 var_desc.cc:415] Flush  batch_norm2d_11.w_1 1
I1018 08:53:57.770176 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_11.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_11.w_1"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_11.w_1"
    }
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_11.w_1"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 136, in composite_batchnorm"
      strings: "    run_mean_ = assign(run_mean)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_11.w_1"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.770367 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.770385 4097533 build_cinn_pass.cc:742] Cluster Ops: (assign, )
I1018 08:53:57.770386 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_11.w_2, )
I1018 08:53:57.770388 4097533 build_cinn_pass.cc:744] Cluster output vars: (batch_norm2d_11.w_2, )
I1018 08:53:57.770390 4097533 build_cinn_pass.cc:745] Cluster internal vars: ()
I1018 08:53:57.770395 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.770416 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_11.w_2
I1018 08:53:57.770421 4097533 build_cinn_pass.cc:274] Add Output Var Node: batch_norm2d_11.w_2
I1018 08:53:57.770427 4097533 build_cinn_pass.cc:493] Inplace var in cluster are: [batch_norm2d_11.w_2, ]
I1018 08:53:57.770431 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="batch_norm2d_11.w_2
[256]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_2[label="batch_norm2d_11.w_2
[256]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_1[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_2
   node_2->node_4
   node_3->node_0
   node_4->node_3
} // end G
I1018 08:53:57.770459 4097533 var_desc.cc:415] Flush  batch_norm2d_11.w_2 1
I1018 08:53:57.770462 4097533 var_desc.cc:415] Flush  batch_norm2d_11.w_2 1
I1018 08:53:57.770466 4097533 graph_helper.h:104] adj assign0x55b7d0c4a3d0 -> fetch0x55b7d0c04040  via batch_norm2d_11.w_20x55b7d0c4b270
I1018 08:53:57.770471 4097533 graph_helper.h:104] adj feed0x55b7d0c03da0 -> assign0x55b7d0c4a3d0  via batch_norm2d_11.w_20x55b7d0c4b1b0
I1018 08:53:57.770486 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.770488 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.770493 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.770495 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.770502 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.770504 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.770548 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.770577 4097533 block_desc.cc:205] vars in desc 1
I1018 08:53:57.770579 4097533 block_desc.cc:209] Flush batch_norm2d_11.w_2
I1018 08:53:57.770581 4097533 var_desc.cc:415] Flush  batch_norm2d_11.w_2 1
I1018 08:53:57.770457 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_11.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_11.w_2"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_11.w_2"
    }
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_11.w_2"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 137, in composite_batchnorm"
      strings: "    run_var_ = assign(run_var)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_11.w_2"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.770645 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.770661 4097533 build_cinn_pass.cc:742] Cluster Ops: (assign, )
I1018 08:53:57.770663 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_12.w_1, )
I1018 08:53:57.770666 4097533 build_cinn_pass.cc:744] Cluster output vars: (batch_norm2d_12.w_1, )
I1018 08:53:57.770668 4097533 build_cinn_pass.cc:745] Cluster internal vars: ()
I1018 08:53:57.770673 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.770694 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_12.w_1
I1018 08:53:57.770697 4097533 build_cinn_pass.cc:274] Add Output Var Node: batch_norm2d_12.w_1
I1018 08:53:57.770704 4097533 build_cinn_pass.cc:493] Inplace var in cluster are: [batch_norm2d_12.w_1, ]
I1018 08:53:57.770709 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="batch_norm2d_12.w_1
[256]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_2[label="batch_norm2d_12.w_1
[256]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_1[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_3
   node_2->node_0
   node_3->node_4
   node_4->node_2
} // end G
I1018 08:53:57.770738 4097533 var_desc.cc:415] Flush  batch_norm2d_12.w_1 1
I1018 08:53:57.770741 4097533 var_desc.cc:415] Flush  batch_norm2d_12.w_1 1
I1018 08:53:57.770746 4097533 graph_helper.h:104] adj assign0x55b7d0c4d010 -> fetch0x55b7d0ea63e0  via batch_norm2d_12.w_10x55b7d0c4bcb0
I1018 08:53:57.770751 4097533 graph_helper.h:104] adj feed0x55b7d0ea6260 -> assign0x55b7d0c4d010  via batch_norm2d_12.w_10x55b7d0c4bbf0
I1018 08:53:57.770766 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.770769 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.770773 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.770776 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.770782 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.770784 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.770828 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.770857 4097533 block_desc.cc:205] vars in desc 1
I1018 08:53:57.770860 4097533 block_desc.cc:209] Flush batch_norm2d_12.w_1
I1018 08:53:57.770862 4097533 var_desc.cc:415] Flush  batch_norm2d_12.w_1 1
I1018 08:53:57.770736 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_12.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_12.w_1"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_12.w_1"
    }
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_12.w_1"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 136, in composite_batchnorm"
      strings: "    run_mean_ = assign(run_mean)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_12.w_1"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.770923 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.770941 4097533 build_cinn_pass.cc:742] Cluster Ops: (assign, )
I1018 08:53:57.770942 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_12.w_2, )
I1018 08:53:57.770944 4097533 build_cinn_pass.cc:744] Cluster output vars: (batch_norm2d_12.w_2, )
I1018 08:53:57.770946 4097533 build_cinn_pass.cc:745] Cluster internal vars: ()
I1018 08:53:57.770951 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.770972 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_12.w_2
I1018 08:53:57.770975 4097533 build_cinn_pass.cc:274] Add Output Var Node: batch_norm2d_12.w_2
I1018 08:53:57.770982 4097533 build_cinn_pass.cc:493] Inplace var in cluster are: [batch_norm2d_12.w_2, ]
I1018 08:53:57.770987 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="batch_norm2d_12.w_2
[256]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_2[label="batch_norm2d_12.w_2
[256]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_1[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_3
   node_2->node_0
   node_3->node_4
   node_4->node_2
} // end G
I1018 08:53:57.771014 4097533 var_desc.cc:415] Flush  batch_norm2d_12.w_2 1
I1018 08:53:57.771018 4097533 var_desc.cc:415] Flush  batch_norm2d_12.w_2 1
I1018 08:53:57.771021 4097533 graph_helper.h:104] adj assign0x55b7d0c8ef30 -> fetch0x55b7d0b7ae40  via batch_norm2d_12.w_20x55b7d0b7b000
I1018 08:53:57.771026 4097533 graph_helper.h:104] adj feed0x55b7d0b7acc0 -> assign0x55b7d0c8ef30  via batch_norm2d_12.w_20x55b7d0d53a70
I1018 08:53:57.771042 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.771044 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.771049 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.771050 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.771057 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.771059 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.771103 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.771132 4097533 block_desc.cc:205] vars in desc 1
I1018 08:53:57.771134 4097533 block_desc.cc:209] Flush batch_norm2d_12.w_2
I1018 08:53:57.771136 4097533 var_desc.cc:415] Flush  batch_norm2d_12.w_2 1
I1018 08:53:57.771013 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_12.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_12.w_2"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_12.w_2"
    }
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_12.w_2"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 137, in composite_batchnorm"
      strings: "    run_var_ = assign(run_var)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_12.w_2"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.771198 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.771214 4097533 build_cinn_pass.cc:742] Cluster Ops: (assign, )
I1018 08:53:57.771216 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_10.w_1, )
I1018 08:53:57.771219 4097533 build_cinn_pass.cc:744] Cluster output vars: (batch_norm2d_10.w_1, )
I1018 08:53:57.771220 4097533 build_cinn_pass.cc:745] Cluster internal vars: ()
I1018 08:53:57.771225 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.771246 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_10.w_1
I1018 08:53:57.771250 4097533 build_cinn_pass.cc:274] Add Output Var Node: batch_norm2d_10.w_1
I1018 08:53:57.771257 4097533 build_cinn_pass.cc:493] Inplace var in cluster are: [batch_norm2d_10.w_1, ]
I1018 08:53:57.771261 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="batch_norm2d_10.w_1
[256]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_2[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1[label="batch_norm2d_10.w_1
[256]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_0[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0->node_3
   node_1->node_2
   node_3->node_4
   node_4->node_1
} // end G
I1018 08:53:57.771291 4097533 var_desc.cc:415] Flush  batch_norm2d_10.w_1 1
I1018 08:53:57.771294 4097533 var_desc.cc:415] Flush  batch_norm2d_10.w_1 1
I1018 08:53:57.771298 4097533 graph_helper.h:104] adj assign0x55b7d0c91b70 -> fetch0x55b7d0e90450  via batch_norm2d_10.w_10x55b7d0e0fa80
I1018 08:53:57.771304 4097533 graph_helper.h:104] adj feed0x55b7d0e902d0 -> assign0x55b7d0c91b70  via batch_norm2d_10.w_10x55b7d0c4e6e0
I1018 08:53:57.771319 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.771322 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.771325 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.771328 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.771334 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.771337 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.771380 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.771409 4097533 block_desc.cc:205] vars in desc 1
I1018 08:53:57.771411 4097533 block_desc.cc:209] Flush batch_norm2d_10.w_1
I1018 08:53:57.771413 4097533 var_desc.cc:415] Flush  batch_norm2d_10.w_1 1
I1018 08:53:57.771289 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_10.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_10.w_1"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_10.w_1"
    }
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_10.w_1"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 136, in composite_batchnorm"
      strings: "    run_mean_ = assign(run_mean)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_10.w_1"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.771474 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.771490 4097533 build_cinn_pass.cc:742] Cluster Ops: (assign, )
I1018 08:53:57.771492 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_10.w_2, )
I1018 08:53:57.771494 4097533 build_cinn_pass.cc:744] Cluster output vars: (batch_norm2d_10.w_2, )
I1018 08:53:57.771497 4097533 build_cinn_pass.cc:745] Cluster internal vars: ()
I1018 08:53:57.771502 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.771521 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_10.w_2
I1018 08:53:57.771525 4097533 build_cinn_pass.cc:274] Add Output Var Node: batch_norm2d_10.w_2
I1018 08:53:57.771533 4097533 build_cinn_pass.cc:493] Inplace var in cluster are: [batch_norm2d_10.w_2, ]
I1018 08:53:57.771536 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="batch_norm2d_10.w_2
[256]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_2[label="batch_norm2d_10.w_2
[256]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_1[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_3
   node_2->node_0
   node_3->node_4
   node_4->node_2
} // end G
I1018 08:53:57.771564 4097533 var_desc.cc:415] Flush  batch_norm2d_10.w_2 1
I1018 08:53:57.771567 4097533 var_desc.cc:415] Flush  batch_norm2d_10.w_2 1
I1018 08:53:57.771571 4097533 graph_helper.h:104] adj assign0x55b7d0ccd8b0 -> fetch0x55b7d0c4f290  via batch_norm2d_10.w_20x55b7d0c93390
I1018 08:53:57.771576 4097533 graph_helper.h:104] adj feed0x55b7d0c4f110 -> assign0x55b7d0ccd8b0  via batch_norm2d_10.w_20x55b7d1074760
I1018 08:53:57.771592 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.771595 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.771598 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.771601 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.771607 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.771610 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.771652 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.771682 4097533 block_desc.cc:205] vars in desc 1
I1018 08:53:57.771684 4097533 block_desc.cc:209] Flush batch_norm2d_10.w_2
I1018 08:53:57.771687 4097533 var_desc.cc:415] Flush  batch_norm2d_10.w_2 1
I1018 08:53:57.771562 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_10.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_10.w_2"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_10.w_2"
    }
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_10.w_2"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 137, in composite_batchnorm"
      strings: "    run_var_ = assign(run_var)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_10.w_2"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.771746 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.771761 4097533 build_cinn_pass.cc:742] Cluster Ops: (assign, )
I1018 08:53:57.771764 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_13.w_1, )
I1018 08:53:57.771765 4097533 build_cinn_pass.cc:744] Cluster output vars: (batch_norm2d_13.w_1, )
I1018 08:53:57.771768 4097533 build_cinn_pass.cc:745] Cluster internal vars: ()
I1018 08:53:57.771772 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.771792 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_13.w_1
I1018 08:53:57.771795 4097533 build_cinn_pass.cc:274] Add Output Var Node: batch_norm2d_13.w_1
I1018 08:53:57.771803 4097533 build_cinn_pass.cc:493] Inplace var in cluster are: [batch_norm2d_13.w_1, ]
I1018 08:53:57.771807 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="batch_norm2d_13.w_1
[256]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_2[label="batch_norm2d_13.w_1
[256]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_1[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0->node_3
   node_2->node_1
   node_3->node_4
   node_4->node_2
} // end G
I1018 08:53:57.771839 4097533 var_desc.cc:415] Flush  batch_norm2d_13.w_1 1
I1018 08:53:57.771842 4097533 var_desc.cc:415] Flush  batch_norm2d_13.w_1 1
I1018 08:53:57.771847 4097533 graph_helper.h:104] adj assign0x55b7d0cd04f0 -> fetch0x55b7d09f7730  via batch_norm2d_13.w_10x55b7d0cd1130
I1018 08:53:57.771852 4097533 graph_helper.h:104] adj feed0x55b7d0fe6cc0 -> assign0x55b7d0cd04f0  via batch_norm2d_13.w_10x55b7d0cd1070
I1018 08:53:57.771867 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.771869 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.771873 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.771875 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.771883 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.771884 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.771927 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.771955 4097533 block_desc.cc:205] vars in desc 1
I1018 08:53:57.771957 4097533 block_desc.cc:209] Flush batch_norm2d_13.w_1
I1018 08:53:57.771960 4097533 var_desc.cc:415] Flush  batch_norm2d_13.w_1 1
I1018 08:53:57.771836 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_13.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_13.w_1"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_13.w_1"
    }
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_13.w_1"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 136, in composite_batchnorm"
      strings: "    run_mean_ = assign(run_mean)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_13.w_1"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.772019 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.772034 4097533 build_cinn_pass.cc:742] Cluster Ops: (assign, )
I1018 08:53:57.772037 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_13.w_2, )
I1018 08:53:57.772038 4097533 build_cinn_pass.cc:744] Cluster output vars: (batch_norm2d_13.w_2, )
I1018 08:53:57.772040 4097533 build_cinn_pass.cc:745] Cluster internal vars: ()
I1018 08:53:57.772045 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.772065 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_13.w_2
I1018 08:53:57.772069 4097533 build_cinn_pass.cc:274] Add Output Var Node: batch_norm2d_13.w_2
I1018 08:53:57.772076 4097533 build_cinn_pass.cc:493] Inplace var in cluster are: [batch_norm2d_13.w_2, ]
I1018 08:53:57.772080 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="batch_norm2d_13.w_2
[256]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_2[label="batch_norm2d_13.w_2
[256]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_1[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_3
   node_2->node_0
   node_3->node_4
   node_4->node_2
} // end G
I1018 08:53:57.772109 4097533 var_desc.cc:415] Flush  batch_norm2d_13.w_2 1
I1018 08:53:57.772110 4097533 var_desc.cc:415] Flush  batch_norm2d_13.w_2 1
I1018 08:53:57.772115 4097533 graph_helper.h:104] adj assign0x55b7d0d14ab0 -> fetch0x55b7d0ec7080  via batch_norm2d_13.w_20x55b7d0cd08f0
I1018 08:53:57.772120 4097533 graph_helper.h:104] adj feed0x55b7d0ec6f00 -> assign0x55b7d0d14ab0  via batch_norm2d_13.w_20x55b7d0cd0830
I1018 08:53:57.772135 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.772138 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.772141 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.772145 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.772150 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.772153 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.772195 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.772223 4097533 block_desc.cc:205] vars in desc 1
I1018 08:53:57.772225 4097533 block_desc.cc:209] Flush batch_norm2d_13.w_2
I1018 08:53:57.772228 4097533 var_desc.cc:415] Flush  batch_norm2d_13.w_2 1
I1018 08:53:57.772105 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_13.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_13.w_2"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_13.w_2"
    }
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_13.w_2"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 137, in composite_batchnorm"
      strings: "    run_var_ = assign(run_var)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_13.w_2"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.772289 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.772302 4097533 build_cinn_pass.cc:742] Cluster Ops: (assign, )
I1018 08:53:57.772305 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_14.w_1, )
I1018 08:53:57.772307 4097533 build_cinn_pass.cc:744] Cluster output vars: (batch_norm2d_14.w_1, )
I1018 08:53:57.772310 4097533 build_cinn_pass.cc:745] Cluster internal vars: ()
I1018 08:53:57.772313 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.772334 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_14.w_1
I1018 08:53:57.772338 4097533 build_cinn_pass.cc:274] Add Output Var Node: batch_norm2d_14.w_1
I1018 08:53:57.772346 4097533 build_cinn_pass.cc:493] Inplace var in cluster are: [batch_norm2d_14.w_1, ]
I1018 08:53:57.772349 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="batch_norm2d_14.w_1
[256]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_2[label="batch_norm2d_14.w_1
[256]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_1[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_3
   node_2->node_0
   node_3->node_4
   node_4->node_2
} // end G
I1018 08:53:57.772380 4097533 var_desc.cc:415] Flush  batch_norm2d_14.w_1 1
I1018 08:53:57.772383 4097533 var_desc.cc:415] Flush  batch_norm2d_14.w_1 1
I1018 08:53:57.772387 4097533 graph_helper.h:104] adj assign0x55b7d0d176f0 -> fetch0x55b7d0e58740  via batch_norm2d_14.w_10x55b7d0fe5c30
I1018 08:53:57.772392 4097533 graph_helper.h:104] adj feed0x55b7d0fe5dd0 -> assign0x55b7d0d176f0  via batch_norm2d_14.w_10x55b7d0d18320
I1018 08:53:57.772408 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.772411 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.772415 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.772418 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.772424 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.772426 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.772470 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.772500 4097533 block_desc.cc:205] vars in desc 1
I1018 08:53:57.772503 4097533 block_desc.cc:209] Flush batch_norm2d_14.w_1
I1018 08:53:57.772505 4097533 var_desc.cc:415] Flush  batch_norm2d_14.w_1 1
I1018 08:53:57.772378 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_14.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_14.w_1"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_14.w_1"
    }
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_14.w_1"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 136, in composite_batchnorm"
      strings: "    run_mean_ = assign(run_mean)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_14.w_1"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.772567 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.772584 4097533 build_cinn_pass.cc:742] Cluster Ops: (assign, )
I1018 08:53:57.772586 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_14.w_2, )
I1018 08:53:57.772588 4097533 build_cinn_pass.cc:744] Cluster output vars: (batch_norm2d_14.w_2, )
I1018 08:53:57.772591 4097533 build_cinn_pass.cc:745] Cluster internal vars: ()
I1018 08:53:57.772595 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.772615 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_14.w_2
I1018 08:53:57.772619 4097533 build_cinn_pass.cc:274] Add Output Var Node: batch_norm2d_14.w_2
I1018 08:53:57.772626 4097533 build_cinn_pass.cc:493] Inplace var in cluster are: [batch_norm2d_14.w_2, ]
I1018 08:53:57.772630 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="batch_norm2d_14.w_2
[256]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_2[label="batch_norm2d_14.w_2
[256]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_1[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_3
   node_2->node_0
   node_3->node_4
   node_4->node_2
} // end G
I1018 08:53:57.772658 4097533 var_desc.cc:415] Flush  batch_norm2d_14.w_2 1
I1018 08:53:57.772661 4097533 var_desc.cc:415] Flush  batch_norm2d_14.w_2 1
I1018 08:53:57.772665 4097533 graph_helper.h:104] adj assign0x55b7d0d59610 -> fetch0x55b7d0e57ad0  via batch_norm2d_14.w_20x55b7d0d5a4f0
I1018 08:53:57.772671 4097533 graph_helper.h:104] adj feed0x55b7d0e57950 -> assign0x55b7d0d59610  via batch_norm2d_14.w_20x55b7d0d5a430
I1018 08:53:57.772686 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.772689 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.772692 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.772696 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.772702 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.772704 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.772747 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.772776 4097533 block_desc.cc:205] vars in desc 1
I1018 08:53:57.772779 4097533 block_desc.cc:209] Flush batch_norm2d_14.w_2
I1018 08:53:57.772781 4097533 var_desc.cc:415] Flush  batch_norm2d_14.w_2 1
I1018 08:53:57.772656 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_14.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_14.w_2"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_14.w_2"
    }
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_14.w_2"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 137, in composite_batchnorm"
      strings: "    run_var_ = assign(run_var)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_14.w_2"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.772843 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.772861 4097533 build_cinn_pass.cc:742] Cluster Ops: (assign, )
I1018 08:53:57.772862 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_16.w_1, )
I1018 08:53:57.772864 4097533 build_cinn_pass.cc:744] Cluster output vars: (batch_norm2d_16.w_1, )
I1018 08:53:57.772867 4097533 build_cinn_pass.cc:745] Cluster internal vars: ()
I1018 08:53:57.772871 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.772892 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_16.w_1
I1018 08:53:57.772897 4097533 build_cinn_pass.cc:274] Add Output Var Node: batch_norm2d_16.w_1
I1018 08:53:57.772903 4097533 build_cinn_pass.cc:493] Inplace var in cluster are: [batch_norm2d_16.w_1, ]
I1018 08:53:57.772907 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="batch_norm2d_16.w_1
[512]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_2[label="batch_norm2d_16.w_1
[512]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_1[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_3
   node_2->node_0
   node_3->node_4
   node_4->node_2
} // end G
I1018 08:53:57.772938 4097533 var_desc.cc:415] Flush  batch_norm2d_16.w_1 1
I1018 08:53:57.772941 4097533 var_desc.cc:415] Flush  batch_norm2d_16.w_1 1
I1018 08:53:57.772945 4097533 graph_helper.h:104] adj assign0x55b7d0d5c250 -> fetch0x55b7d0cad440  via batch_norm2d_16.w_10x55b7d10faf00
I1018 08:53:57.772950 4097533 graph_helper.h:104] adj feed0x55b7d0cad2c0 -> assign0x55b7d0d5c250  via batch_norm2d_16.w_10x55b7d0d5b130
I1018 08:53:57.772965 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.772967 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.772971 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.772974 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.772980 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.772984 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.773026 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.773056 4097533 block_desc.cc:205] vars in desc 1
I1018 08:53:57.773057 4097533 block_desc.cc:209] Flush batch_norm2d_16.w_1
I1018 08:53:57.773059 4097533 var_desc.cc:415] Flush  batch_norm2d_16.w_1 1
I1018 08:53:57.772936 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_16.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_16.w_1"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_16.w_1"
    }
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_16.w_1"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 136, in composite_batchnorm"
      strings: "    run_mean_ = assign(run_mean)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_16.w_1"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.773120 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.773137 4097533 build_cinn_pass.cc:742] Cluster Ops: (assign, )
I1018 08:53:57.773139 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_16.w_2, )
I1018 08:53:57.773141 4097533 build_cinn_pass.cc:744] Cluster output vars: (batch_norm2d_16.w_2, )
I1018 08:53:57.773144 4097533 build_cinn_pass.cc:745] Cluster internal vars: ()
I1018 08:53:57.773149 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.773169 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_16.w_2
I1018 08:53:57.773172 4097533 build_cinn_pass.cc:274] Add Output Var Node: batch_norm2d_16.w_2
I1018 08:53:57.773180 4097533 build_cinn_pass.cc:493] Inplace var in cluster are: [batch_norm2d_16.w_2, ]
I1018 08:53:57.773183 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="batch_norm2d_16.w_2
[512]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_2[label="batch_norm2d_16.w_2
[512]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_1[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_3
   node_2->node_0
   node_3->node_4
   node_4->node_2
} // end G
I1018 08:53:57.773218 4097533 var_desc.cc:415] Flush  batch_norm2d_16.w_2 1
I1018 08:53:57.773221 4097533 var_desc.cc:415] Flush  batch_norm2d_16.w_2 1
I1018 08:53:57.773226 4097533 graph_helper.h:104] adj assign0x55b7d0da07f0 -> fetch0x55b7d0dcdfa0  via batch_norm2d_16.w_20x55b7d0da15d0
I1018 08:53:57.773231 4097533 graph_helper.h:104] adj feed0x55b7d0dcde20 -> assign0x55b7d0da07f0  via batch_norm2d_16.w_20x55b7d0d5c860
I1018 08:53:57.773247 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.773248 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.773252 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.773255 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.773262 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.773263 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.773306 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.773335 4097533 block_desc.cc:205] vars in desc 1
I1018 08:53:57.773339 4097533 block_desc.cc:209] Flush batch_norm2d_16.w_2
I1018 08:53:57.773340 4097533 var_desc.cc:415] Flush  batch_norm2d_16.w_2 1
I1018 08:53:57.773216 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_16.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_16.w_2"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_16.w_2"
    }
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_16.w_2"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 137, in composite_batchnorm"
      strings: "    run_var_ = assign(run_var)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_16.w_2"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.773401 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.773417 4097533 build_cinn_pass.cc:742] Cluster Ops: (assign, )
I1018 08:53:57.773419 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_17.w_1, )
I1018 08:53:57.773422 4097533 build_cinn_pass.cc:744] Cluster output vars: (batch_norm2d_17.w_1, )
I1018 08:53:57.773423 4097533 build_cinn_pass.cc:745] Cluster internal vars: ()
I1018 08:53:57.773428 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.773449 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_17.w_1
I1018 08:53:57.773452 4097533 build_cinn_pass.cc:274] Add Output Var Node: batch_norm2d_17.w_1
I1018 08:53:57.773459 4097533 build_cinn_pass.cc:493] Inplace var in cluster are: [batch_norm2d_17.w_1, ]
I1018 08:53:57.773463 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="batch_norm2d_17.w_1
[512]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_2[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1[label="batch_norm2d_17.w_1
[512]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_0[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0->node_3
   node_1->node_2
   node_3->node_4
   node_4->node_1
} // end G
I1018 08:53:57.773495 4097533 var_desc.cc:415] Flush  batch_norm2d_17.w_1 1
I1018 08:53:57.773499 4097533 var_desc.cc:415] Flush  batch_norm2d_17.w_1 1
I1018 08:53:57.773502 4097533 graph_helper.h:104] adj assign0x55b7d0da3430 -> fetch0x55b7d1054bc0  via batch_norm2d_17.w_10x55b7d0da2630
I1018 08:53:57.773509 4097533 graph_helper.h:104] adj feed0x55b7d1054a40 -> assign0x55b7d0da3430  via batch_norm2d_17.w_10x55b7d0dce980
I1018 08:53:57.773523 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.773525 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.773530 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.773532 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.773538 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.773541 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.773584 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.773613 4097533 block_desc.cc:205] vars in desc 1
I1018 08:53:57.773615 4097533 block_desc.cc:209] Flush batch_norm2d_17.w_1
I1018 08:53:57.773618 4097533 var_desc.cc:415] Flush  batch_norm2d_17.w_1 1
I1018 08:53:57.773494 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_17.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_17.w_1"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_17.w_1"
    }
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_17.w_1"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 136, in composite_batchnorm"
      strings: "    run_mean_ = assign(run_mean)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_17.w_1"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.773679 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.773694 4097533 build_cinn_pass.cc:742] Cluster Ops: (assign, )
I1018 08:53:57.773696 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_17.w_2, )
I1018 08:53:57.773698 4097533 build_cinn_pass.cc:744] Cluster output vars: (batch_norm2d_17.w_2, )
I1018 08:53:57.773700 4097533 build_cinn_pass.cc:745] Cluster internal vars: ()
I1018 08:53:57.773705 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.773725 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_17.w_2
I1018 08:53:57.773728 4097533 build_cinn_pass.cc:274] Add Output Var Node: batch_norm2d_17.w_2
I1018 08:53:57.773736 4097533 build_cinn_pass.cc:493] Inplace var in cluster are: [batch_norm2d_17.w_2, ]
I1018 08:53:57.773739 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="batch_norm2d_17.w_2
[512]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_2[label="batch_norm2d_17.w_2
[512]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_1[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_3
   node_2->node_0
   node_3->node_4
   node_4->node_2
} // end G
I1018 08:53:57.773767 4097533 var_desc.cc:415] Flush  batch_norm2d_17.w_2 1
I1018 08:53:57.773770 4097533 var_desc.cc:415] Flush  batch_norm2d_17.w_2 1
I1018 08:53:57.773774 4097533 graph_helper.h:104] adj assign0x55b7d0de5350 -> fetch0x55b7d0da52f0  via batch_norm2d_17.w_20x55b7d0f2bc20
I1018 08:53:57.773780 4097533 graph_helper.h:104] adj feed0x55b7d0da5170 -> assign0x55b7d0de5350  via batch_norm2d_17.w_20x55b7d0f2bb60
I1018 08:53:57.773795 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.773797 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.773801 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.773803 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.773810 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.773813 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.773854 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.773883 4097533 block_desc.cc:205] vars in desc 1
I1018 08:53:57.773886 4097533 block_desc.cc:209] Flush batch_norm2d_17.w_2
I1018 08:53:57.773888 4097533 var_desc.cc:415] Flush  batch_norm2d_17.w_2 1
I1018 08:53:57.773766 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_17.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_17.w_2"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_17.w_2"
    }
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_17.w_2"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 137, in composite_batchnorm"
      strings: "    run_var_ = assign(run_var)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_17.w_2"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.773948 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.773962 4097533 build_cinn_pass.cc:742] Cluster Ops: (assign, )
I1018 08:53:57.773965 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_15.w_1, )
I1018 08:53:57.773967 4097533 build_cinn_pass.cc:744] Cluster output vars: (batch_norm2d_15.w_1, )
I1018 08:53:57.773969 4097533 build_cinn_pass.cc:745] Cluster internal vars: ()
I1018 08:53:57.773973 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.773994 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_15.w_1
I1018 08:53:57.773998 4097533 build_cinn_pass.cc:274] Add Output Var Node: batch_norm2d_15.w_1
I1018 08:53:57.774004 4097533 build_cinn_pass.cc:493] Inplace var in cluster are: [batch_norm2d_15.w_1, ]
I1018 08:53:57.774008 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="batch_norm2d_15.w_1
[512]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_2[label="batch_norm2d_15.w_1
[512]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_1[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_3
   node_2->node_0
   node_3->node_4
   node_4->node_2
} // end G
I1018 08:53:57.774039 4097533 var_desc.cc:415] Flush  batch_norm2d_15.w_1 1
I1018 08:53:57.774042 4097533 var_desc.cc:415] Flush  batch_norm2d_15.w_1 1
I1018 08:53:57.774046 4097533 graph_helper.h:104] adj assign0x55b7d0de7f90 -> fetch0x55b7d0d08220  via batch_norm2d_15.w_10x55b7d0a121b0
I1018 08:53:57.774051 4097533 graph_helper.h:104] adj feed0x55b7d0d08080 -> assign0x55b7d0de7f90  via batch_norm2d_15.w_10x55b7d0a120f0
I1018 08:53:57.774066 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.774070 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.774072 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.774075 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.774081 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.774084 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.774125 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.774154 4097533 block_desc.cc:205] vars in desc 1
I1018 08:53:57.774156 4097533 block_desc.cc:209] Flush batch_norm2d_15.w_1
I1018 08:53:57.774158 4097533 var_desc.cc:415] Flush  batch_norm2d_15.w_1 1
I1018 08:53:57.774036 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_15.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_15.w_1"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_15.w_1"
    }
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_15.w_1"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 136, in composite_batchnorm"
      strings: "    run_mean_ = assign(run_mean)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_15.w_1"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.774221 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.774236 4097533 build_cinn_pass.cc:742] Cluster Ops: (assign, )
I1018 08:53:57.774238 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_15.w_2, )
I1018 08:53:57.774240 4097533 build_cinn_pass.cc:744] Cluster output vars: (batch_norm2d_15.w_2, )
I1018 08:53:57.774242 4097533 build_cinn_pass.cc:745] Cluster internal vars: ()
I1018 08:53:57.774247 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.774267 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_15.w_2
I1018 08:53:57.774271 4097533 build_cinn_pass.cc:274] Add Output Var Node: batch_norm2d_15.w_2
I1018 08:53:57.774278 4097533 build_cinn_pass.cc:493] Inplace var in cluster are: [batch_norm2d_15.w_2, ]
I1018 08:53:57.774282 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_2[label="batch_norm2d_15.w_2
[512]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_1[label="batch_norm2d_15.w_2
[512]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_0
   node_2->node_4
   node_3->node_2
   node_4->node_1
} // end G
I1018 08:53:57.774310 4097533 var_desc.cc:415] Flush  batch_norm2d_15.w_2 1
I1018 08:53:57.774313 4097533 var_desc.cc:415] Flush  batch_norm2d_15.w_2 1
I1018 08:53:57.774317 4097533 graph_helper.h:104] adj assign0x55b7d0e23cd0 -> fetch0x55b7d0dea690  via batch_norm2d_15.w_20x55b7d0de71d0
I1018 08:53:57.774322 4097533 graph_helper.h:104] adj feed0x55b7d0dea510 -> assign0x55b7d0e23cd0  via batch_norm2d_15.w_20x55b7d0f2c300
I1018 08:53:57.774338 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.774340 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.774344 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.774348 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.774353 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.774356 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.774397 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.774425 4097533 block_desc.cc:205] vars in desc 1
I1018 08:53:57.774428 4097533 block_desc.cc:209] Flush batch_norm2d_15.w_2
I1018 08:53:57.774430 4097533 var_desc.cc:415] Flush  batch_norm2d_15.w_2 1
I1018 08:53:57.774308 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_15.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_15.w_2"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_15.w_2"
    }
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_15.w_2"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 137, in composite_batchnorm"
      strings: "    run_var_ = assign(run_var)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_15.w_2"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.774490 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.774504 4097533 build_cinn_pass.cc:742] Cluster Ops: (assign, )
I1018 08:53:57.774506 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_18.w_1, )
I1018 08:53:57.774508 4097533 build_cinn_pass.cc:744] Cluster output vars: (batch_norm2d_18.w_1, )
I1018 08:53:57.774511 4097533 build_cinn_pass.cc:745] Cluster internal vars: ()
I1018 08:53:57.774515 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.774535 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_18.w_1
I1018 08:53:57.774539 4097533 build_cinn_pass.cc:274] Add Output Var Node: batch_norm2d_18.w_1
I1018 08:53:57.774546 4097533 build_cinn_pass.cc:493] Inplace var in cluster are: [batch_norm2d_18.w_1, ]
I1018 08:53:57.774549 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="batch_norm2d_18.w_1
[512]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_2[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1[label="batch_norm2d_18.w_1
[512]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_0[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0->node_3
   node_1->node_2
   node_3->node_4
   node_4->node_1
} // end G
I1018 08:53:57.774580 4097533 var_desc.cc:415] Flush  batch_norm2d_18.w_1 1
I1018 08:53:57.774583 4097533 var_desc.cc:415] Flush  batch_norm2d_18.w_1 1
I1018 08:53:57.774587 4097533 graph_helper.h:104] adj assign0x55b7d0e26910 -> fetch0x55b7d0dc82b0  via batch_norm2d_18.w_10x55b7d0dc7cd0
I1018 08:53:57.774592 4097533 graph_helper.h:104] adj feed0x55b7d0dc7e30 -> assign0x55b7d0e26910  via batch_norm2d_18.w_10x55b7d0dc8110
I1018 08:53:57.774607 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.774610 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.774614 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.774616 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.774622 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.774626 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.774667 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.774696 4097533 block_desc.cc:205] vars in desc 1
I1018 08:53:57.774699 4097533 block_desc.cc:209] Flush batch_norm2d_18.w_1
I1018 08:53:57.774701 4097533 var_desc.cc:415] Flush  batch_norm2d_18.w_1 1
I1018 08:53:57.774578 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_18.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_18.w_1"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_18.w_1"
    }
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_18.w_1"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 136, in composite_batchnorm"
      strings: "    run_mean_ = assign(run_mean)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_18.w_1"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.774762 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.774780 4097533 build_cinn_pass.cc:742] Cluster Ops: (assign, )
I1018 08:53:57.774781 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_18.w_2, )
I1018 08:53:57.774783 4097533 build_cinn_pass.cc:744] Cluster output vars: (batch_norm2d_18.w_2, )
I1018 08:53:57.774785 4097533 build_cinn_pass.cc:745] Cluster internal vars: ()
I1018 08:53:57.774789 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.774811 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_18.w_2
I1018 08:53:57.774813 4097533 build_cinn_pass.cc:274] Add Output Var Node: batch_norm2d_18.w_2
I1018 08:53:57.774821 4097533 build_cinn_pass.cc:493] Inplace var in cluster are: [batch_norm2d_18.w_2, ]
I1018 08:53:57.774825 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="batch_norm2d_18.w_2
[512]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_2[label="batch_norm2d_18.w_2
[512]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_1[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_3
   node_2->node_0
   node_3->node_4
   node_4->node_2
} // end G
I1018 08:53:57.774854 4097533 var_desc.cc:415] Flush  batch_norm2d_18.w_2 1
I1018 08:53:57.774857 4097533 var_desc.cc:415] Flush  batch_norm2d_18.w_2 1
I1018 08:53:57.774861 4097533 graph_helper.h:104] adj assign0x55b7d0e6aed0 -> fetch0x55b7d0e6d800  via batch_norm2d_18.w_20x55b7d0cf4a40
I1018 08:53:57.774866 4097533 graph_helper.h:104] adj feed0x55b7d0e6d680 -> assign0x55b7d0e6aed0  via batch_norm2d_18.w_20x55b7d0e2c690
I1018 08:53:57.774883 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.774884 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.774888 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.774891 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.774897 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.774899 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.774943 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.774972 4097533 block_desc.cc:205] vars in desc 1
I1018 08:53:57.774974 4097533 block_desc.cc:209] Flush batch_norm2d_18.w_2
I1018 08:53:57.774976 4097533 var_desc.cc:415] Flush  batch_norm2d_18.w_2 1
I1018 08:53:57.774852 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_18.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_18.w_2"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_18.w_2"
    }
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_18.w_2"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 137, in composite_batchnorm"
      strings: "    run_var_ = assign(run_var)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_18.w_2"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.775041 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.775058 4097533 build_cinn_pass.cc:742] Cluster Ops: (assign, )
I1018 08:53:57.775060 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_19.w_1, )
I1018 08:53:57.775063 4097533 build_cinn_pass.cc:744] Cluster output vars: (batch_norm2d_19.w_1, )
I1018 08:53:57.775065 4097533 build_cinn_pass.cc:745] Cluster internal vars: ()
I1018 08:53:57.775069 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.775090 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_19.w_1
I1018 08:53:57.775094 4097533 build_cinn_pass.cc:274] Add Output Var Node: batch_norm2d_19.w_1
I1018 08:53:57.775100 4097533 build_cinn_pass.cc:493] Inplace var in cluster are: [batch_norm2d_19.w_1, ]
I1018 08:53:57.775104 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="batch_norm2d_19.w_1
[512]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_2[label="batch_norm2d_19.w_1
[512]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_1[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_3
   node_2->node_0
   node_3->node_4
   node_4->node_2
} // end G
I1018 08:53:57.775137 4097533 var_desc.cc:415] Flush  batch_norm2d_19.w_1 1
I1018 08:53:57.775141 4097533 var_desc.cc:415] Flush  batch_norm2d_19.w_1 1
I1018 08:53:57.775144 4097533 graph_helper.h:104] adj assign0x55b7d0e6db10 -> fetch0x55b7d0c0b730  via batch_norm2d_19.w_10x55b7d0c0b890
I1018 08:53:57.775149 4097533 graph_helper.h:104] adj feed0x55b7d0c0b590 -> assign0x55b7d0e6db10  via batch_norm2d_19.w_10x55b7d0e6c5c0
I1018 08:53:57.775166 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.775167 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.775171 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.775174 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.775180 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.775183 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.775228 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.775255 4097533 block_desc.cc:205] vars in desc 1
I1018 08:53:57.775259 4097533 block_desc.cc:209] Flush batch_norm2d_19.w_1
I1018 08:53:57.775260 4097533 var_desc.cc:415] Flush  batch_norm2d_19.w_1 1
I1018 08:53:57.775135 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_19.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_19.w_1"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_19.w_1"
    }
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_19.w_1"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 136, in composite_batchnorm"
      strings: "    run_mean_ = assign(run_mean)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_19.w_1"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.775321 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.775337 4097533 build_cinn_pass.cc:742] Cluster Ops: (assign, )
I1018 08:53:57.775341 4097533 build_cinn_pass.cc:743] Cluster input vars: (batch_norm2d_19.w_2, )
I1018 08:53:57.775342 4097533 build_cinn_pass.cc:744] Cluster output vars: (batch_norm2d_19.w_2, )
I1018 08:53:57.775344 4097533 build_cinn_pass.cc:745] Cluster internal vars: ()
I1018 08:53:57.775349 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.775369 4097533 build_cinn_pass.cc:242] Add Feed Op before the input var: batch_norm2d_19.w_2
I1018 08:53:57.775372 4097533 build_cinn_pass.cc:274] Add Output Var Node: batch_norm2d_19.w_2
I1018 08:53:57.775379 4097533 build_cinn_pass.cc:493] Inplace var in cluster are: [batch_norm2d_19.w_2, ]
I1018 08:53:57.775383 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="batch_norm2d_19.w_2
[512]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_2[label="batch_norm2d_19.w_2
[512]
FP32" shape="box" style="rounded,filled,bold" color="#148b97" fontcolor="#ffffff"]
   node_1[label="feed" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_3
   node_2->node_0
   node_3->node_4
   node_4->node_2
} // end G
I1018 08:53:57.775413 4097533 var_desc.cc:415] Flush  batch_norm2d_19.w_2 1
I1018 08:53:57.775415 4097533 var_desc.cc:415] Flush  batch_norm2d_19.w_2 1
I1018 08:53:57.775419 4097533 graph_helper.h:104] adj assign0x55b7d0eafa30 -> fetch0x55b7d0e701f0  via batch_norm2d_19.w_20x55b7d0cc2f00
I1018 08:53:57.775424 4097533 graph_helper.h:104] adj feed0x55b7d0e70090 -> assign0x55b7d0eafa30  via batch_norm2d_19.w_20x55b7d0e6e390
I1018 08:53:57.775439 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.775442 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.775446 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.775449 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.775455 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.775457 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.775501 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.775529 4097533 block_desc.cc:205] vars in desc 1
I1018 08:53:57.775532 4097533 block_desc.cc:209] Flush batch_norm2d_19.w_2
I1018 08:53:57.775534 4097533 var_desc.cc:415] Flush  batch_norm2d_19.w_2 1
I1018 08:53:57.775409 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "batch_norm2d_19.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_19.w_2"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_19.w_2"
    }
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_19.w_2"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 137, in composite_batchnorm"
      strings: "    run_var_ = assign(run_var)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_19.w_2"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.775595 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.775611 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_3.tmp_0, is_only_used_internal: 1
I1018 08:53:57.775614 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_3.tmp_0
I1018 08:53:57.775616 4097533 build_cinn_pass.cc:742] Cluster Ops: (fill_constant, assign, )
I1018 08:53:57.775619 4097533 build_cinn_pass.cc:743] Cluster input vars: ()
I1018 08:53:57.775621 4097533 build_cinn_pass.cc:744] Cluster output vars: (assign_0.tmp_0, )
I1018 08:53:57.775624 4097533 build_cinn_pass.cc:745] Cluster internal vars: (fill_constant_3.tmp_0, )
I1018 08:53:57.775628 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.775660 4097533 build_cinn_pass.cc:274] Add Output Var Node: assign_0.tmp_0
I1018 08:53:57.775668 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_2[label="fill_constant_3.tmp_0
[64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_1[label="assign_0.tmp_0
[64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_0
   node_2->node_3
   node_3->node_1
   node_4->node_2
} // end G
I1018 08:53:57.775697 4097533 var_desc.cc:415] Flush  assign_0.tmp_0 1
I1018 08:53:57.775702 4097533 var_desc.cc:415] Flush  fill_constant_3.tmp_0 1
I1018 08:53:57.775707 4097533 graph_helper.h:104] adj assign0x55b7d0eb2ec0 -> fetch0x55b7d0e5f9e0  via assign_0.tmp_00x55b7d0e5f820
I1018 08:53:57.775712 4097533 graph_helper.h:104] adj fill_constant0x55b7d0eb2670 -> assign0x55b7d0eb2ec0  via fill_constant_3.tmp_00x55b7d0e5f760
I1018 08:53:57.775734 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.775736 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.775744 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.775748 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.775760 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.775763 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.775854 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.775910 4097533 block_desc.cc:205] vars in desc 2
I1018 08:53:57.775913 4097533 block_desc.cc:209] Flush assign_0.tmp_0
I1018 08:53:57.775915 4097533 var_desc.cc:415] Flush  assign_0.tmp_0 1
I1018 08:53:57.775918 4097533 block_desc.cc:209] Flush fill_constant_3.tmp_0
I1018 08:53:57.775920 4097533 var_desc.cc:415] Flush  fill_constant_3.tmp_0 1
I1018 08:53:57.775694 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "assign_0.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_3.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  ops {
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "ShapeTensorList"
    }
    inputs {
      parameter: "ValueTensor"
    }
    outputs {
      parameter: "Out"
      arguments: "fill_constant_3.tmp_0"
    }
    type: "fill_constant"
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "dtype"
      type: INT
      i: 5
    }
    attrs {
      name: "force_cpu"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 117, in composite_batchnorm"
      strings: "    batch_mean = zeros(run_mean.shape, run_mean.dtype)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 1117, in zeros"
      strings: "    return fill_constant(value=0.0, shape=shape, dtype=dtype, name=name)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 977, in fill_constant"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "place_type"
      type: INT
      i: -1
    }
    attrs {
      name: "shape"
      type: LONGS
      longs: 64
    }
    attrs {
      name: "str_value"
      type: STRING
      s: "0.0"
    }
    attrs {
      name: "value"
      type: FLOAT
      f: 0
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fill_constant_3.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "assign_0.tmp_0"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 134, in composite_batchnorm"
      strings: "    batch_mean_ = assign(batch_mean)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "assign_0.tmp_0"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.776037 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.776065 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_11.tmp_0, is_only_used_internal: 1
I1018 08:53:57.776067 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_11.tmp_0
I1018 08:53:57.776070 4097533 build_cinn_pass.cc:742] Cluster Ops: (fill_constant, assign, )
I1018 08:53:57.776072 4097533 build_cinn_pass.cc:743] Cluster input vars: ()
I1018 08:53:57.776074 4097533 build_cinn_pass.cc:744] Cluster output vars: (assign_4.tmp_0, )
I1018 08:53:57.776077 4097533 build_cinn_pass.cc:745] Cluster internal vars: (fill_constant_11.tmp_0, )
I1018 08:53:57.776082 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.776121 4097533 build_cinn_pass.cc:274] Add Output Var Node: assign_4.tmp_0
I1018 08:53:57.776131 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_2[label="fill_constant_11.tmp_0
[64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_1[label="assign_4.tmp_0
[64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_0
   node_2->node_3
   node_3->node_1
   node_4->node_2
} // end G
I1018 08:53:57.776160 4097533 var_desc.cc:415] Flush  assign_4.tmp_0 1
I1018 08:53:57.776163 4097533 var_desc.cc:415] Flush  fill_constant_11.tmp_0 1
I1018 08:53:57.776168 4097533 graph_helper.h:104] adj assign0x55b7b78735d0 -> fetch0x55b7d0961520  via assign_4.tmp_00x55b7d0e63c50
I1018 08:53:57.776175 4097533 graph_helper.h:104] adj fill_constant0x55b7d0963570 -> assign0x55b7b78735d0  via fill_constant_11.tmp_00x55b7d098d7b0
I1018 08:53:57.776194 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.776197 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.776206 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.776207 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.776221 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.776223 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.776312 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.776368 4097533 block_desc.cc:205] vars in desc 2
I1018 08:53:57.776371 4097533 block_desc.cc:209] Flush assign_4.tmp_0
I1018 08:53:57.776373 4097533 var_desc.cc:415] Flush  assign_4.tmp_0 1
I1018 08:53:57.776376 4097533 block_desc.cc:209] Flush fill_constant_11.tmp_0
I1018 08:53:57.776378 4097533 var_desc.cc:415] Flush  fill_constant_11.tmp_0 1
I1018 08:53:57.776158 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "assign_4.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_11.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  ops {
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "ShapeTensorList"
    }
    inputs {
      parameter: "ValueTensor"
    }
    outputs {
      parameter: "Out"
      arguments: "fill_constant_11.tmp_0"
    }
    type: "fill_constant"
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "dtype"
      type: INT
      i: 5
    }
    attrs {
      name: "force_cpu"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 117, in composite_batchnorm"
      strings: "    batch_mean = zeros(run_mean.shape, run_mean.dtype)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 1117, in zeros"
      strings: "    return fill_constant(value=0.0, shape=shape, dtype=dtype, name=name)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 977, in fill_constant"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "place_type"
      type: INT
      i: -1
    }
    attrs {
      name: "shape"
      type: LONGS
      longs: 64
    }
    attrs {
      name: "str_value"
      type: STRING
      s: "0.0"
    }
    attrs {
      name: "value"
      type: FLOAT
      f: 0
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fill_constant_11.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "assign_4.tmp_0"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 134, in composite_batchnorm"
      strings: "    batch_mean_ = assign(batch_mean)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "assign_4.tmp_0"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.776496 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.776522 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_19.tmp_0, is_only_used_internal: 1
I1018 08:53:57.776525 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_19.tmp_0
I1018 08:53:57.776527 4097533 build_cinn_pass.cc:742] Cluster Ops: (fill_constant, assign, )
I1018 08:53:57.776530 4097533 build_cinn_pass.cc:743] Cluster input vars: ()
I1018 08:53:57.776531 4097533 build_cinn_pass.cc:744] Cluster output vars: (assign_8.tmp_0, )
I1018 08:53:57.776533 4097533 build_cinn_pass.cc:745] Cluster internal vars: (fill_constant_19.tmp_0, )
I1018 08:53:57.776539 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.776578 4097533 build_cinn_pass.cc:274] Add Output Var Node: assign_8.tmp_0
I1018 08:53:57.776590 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_2[label="fill_constant_19.tmp_0
[64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_1[label="assign_8.tmp_0
[64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_0
   node_2->node_3
   node_3->node_1
   node_4->node_2
} // end G
I1018 08:53:57.776621 4097533 var_desc.cc:415] Flush  assign_8.tmp_0 1
I1018 08:53:57.776624 4097533 var_desc.cc:415] Flush  fill_constant_19.tmp_0 1
I1018 08:53:57.776629 4097533 graph_helper.h:104] adj assign0x55b7d09a7890 -> fetch0x55b7d0962000  via assign_8.tmp_00x55b7d0961e40
I1018 08:53:57.776634 4097533 graph_helper.h:104] adj fill_constant0x55b7d09aa870 -> assign0x55b7d09a7890  via fill_constant_19.tmp_00x55b7d09d4b20
I1018 08:53:57.776656 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.776659 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.776670 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.776674 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.776687 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.776690 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.776782 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.776837 4097533 block_desc.cc:205] vars in desc 2
I1018 08:53:57.776840 4097533 block_desc.cc:209] Flush assign_8.tmp_0
I1018 08:53:57.776842 4097533 var_desc.cc:415] Flush  assign_8.tmp_0 1
I1018 08:53:57.776844 4097533 block_desc.cc:209] Flush fill_constant_19.tmp_0
I1018 08:53:57.776846 4097533 var_desc.cc:415] Flush  fill_constant_19.tmp_0 1
I1018 08:53:57.776618 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "assign_8.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_19.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  ops {
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "ShapeTensorList"
    }
    inputs {
      parameter: "ValueTensor"
    }
    outputs {
      parameter: "Out"
      arguments: "fill_constant_19.tmp_0"
    }
    type: "fill_constant"
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "dtype"
      type: INT
      i: 5
    }
    attrs {
      name: "force_cpu"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 117, in composite_batchnorm"
      strings: "    batch_mean = zeros(run_mean.shape, run_mean.dtype)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 1117, in zeros"
      strings: "    return fill_constant(value=0.0, shape=shape, dtype=dtype, name=name)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 977, in fill_constant"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "place_type"
      type: INT
      i: -1
    }
    attrs {
      name: "shape"
      type: LONGS
      longs: 64
    }
    attrs {
      name: "str_value"
      type: STRING
      s: "0.0"
    }
    attrs {
      name: "value"
      type: FLOAT
      f: 0
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fill_constant_19.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "assign_8.tmp_0"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 134, in composite_batchnorm"
      strings: "    batch_mean_ = assign(batch_mean)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "assign_8.tmp_0"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.776960 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.776986 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_27.tmp_0, is_only_used_internal: 1
I1018 08:53:57.776988 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_27.tmp_0
I1018 08:53:57.776991 4097533 build_cinn_pass.cc:742] Cluster Ops: (fill_constant, assign, )
I1018 08:53:57.776995 4097533 build_cinn_pass.cc:743] Cluster input vars: ()
I1018 08:53:57.776997 4097533 build_cinn_pass.cc:744] Cluster output vars: (assign_12.tmp_0, )
I1018 08:53:57.776999 4097533 build_cinn_pass.cc:745] Cluster internal vars: (fill_constant_27.tmp_0, )
I1018 08:53:57.777006 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.777043 4097533 build_cinn_pass.cc:274] Add Output Var Node: assign_12.tmp_0
I1018 08:53:57.777053 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_2[label="fill_constant_27.tmp_0
[64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_1[label="assign_12.tmp_0
[64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_0
   node_2->node_3
   node_3->node_1
   node_4->node_2
} // end G
I1018 08:53:57.777084 4097533 var_desc.cc:415] Flush  assign_12.tmp_0 1
I1018 08:53:57.777086 4097533 var_desc.cc:415] Flush  fill_constant_27.tmp_0 1
I1018 08:53:57.777091 4097533 graph_helper.h:104] adj assign0x55b7d09ec390 -> fetch0x55b7d0d4ca20  via assign_12.tmp_00x55b7d0d4cb30
I1018 08:53:57.777097 4097533 graph_helper.h:104] adj fill_constant0x55b7d09ef370 -> assign0x55b7d09ec390  via fill_constant_27.tmp_00x55b7d0a19e70
I1018 08:53:57.777117 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.777119 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.777127 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.777130 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.777143 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.777146 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.777247 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.777303 4097533 block_desc.cc:205] vars in desc 2
I1018 08:53:57.777307 4097533 block_desc.cc:209] Flush assign_12.tmp_0
I1018 08:53:57.777308 4097533 var_desc.cc:415] Flush  assign_12.tmp_0 1
I1018 08:53:57.777312 4097533 block_desc.cc:209] Flush fill_constant_27.tmp_0
I1018 08:53:57.777313 4097533 var_desc.cc:415] Flush  fill_constant_27.tmp_0 1
I1018 08:53:57.777081 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "assign_12.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_27.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  ops {
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "ShapeTensorList"
    }
    inputs {
      parameter: "ValueTensor"
    }
    outputs {
      parameter: "Out"
      arguments: "fill_constant_27.tmp_0"
    }
    type: "fill_constant"
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "dtype"
      type: INT
      i: 5
    }
    attrs {
      name: "force_cpu"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 117, in composite_batchnorm"
      strings: "    batch_mean = zeros(run_mean.shape, run_mean.dtype)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 1117, in zeros"
      strings: "    return fill_constant(value=0.0, shape=shape, dtype=dtype, name=name)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 977, in fill_constant"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "place_type"
      type: INT
      i: -1
    }
    attrs {
      name: "shape"
      type: LONGS
      longs: 64
    }
    attrs {
      name: "str_value"
      type: STRING
      s: "0.0"
    }
    attrs {
      name: "value"
      type: FLOAT
      f: 0
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fill_constant_27.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "assign_12.tmp_0"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 134, in composite_batchnorm"
      strings: "    batch_mean_ = assign(batch_mean)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "assign_12.tmp_0"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.777429 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.777456 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_35.tmp_0, is_only_used_internal: 1
I1018 08:53:57.777458 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_35.tmp_0
I1018 08:53:57.777462 4097533 build_cinn_pass.cc:742] Cluster Ops: (fill_constant, assign, )
I1018 08:53:57.777463 4097533 build_cinn_pass.cc:743] Cluster input vars: ()
I1018 08:53:57.777465 4097533 build_cinn_pass.cc:744] Cluster output vars: (assign_16.tmp_0, )
I1018 08:53:57.777467 4097533 build_cinn_pass.cc:745] Cluster internal vars: (fill_constant_35.tmp_0, )
I1018 08:53:57.777474 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.777511 4097533 build_cinn_pass.cc:274] Add Output Var Node: assign_16.tmp_0
I1018 08:53:57.777521 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_2[label="fill_constant_35.tmp_0
[64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_1[label="assign_16.tmp_0
[64]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_0
   node_2->node_3
   node_3->node_1
   node_4->node_2
} // end G
I1018 08:53:57.777552 4097533 var_desc.cc:415] Flush  assign_16.tmp_0 1
I1018 08:53:57.777554 4097533 var_desc.cc:415] Flush  fill_constant_35.tmp_0 1
I1018 08:53:57.777560 4097533 graph_helper.h:104] adj assign0x55b7d0a33d60 -> fetch0x55b7d0d75e00  via assign_16.tmp_00x55b7d09d4d50
I1018 08:53:57.777565 4097533 graph_helper.h:104] adj fill_constant0x55b7d0a36d40 -> assign0x55b7d0a33d60  via fill_constant_35.tmp_00x55b7d0a61020
I1018 08:53:57.777585 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.777587 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.777595 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.777598 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.777611 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.777613 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.777702 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.777758 4097533 block_desc.cc:205] vars in desc 2
I1018 08:53:57.777761 4097533 block_desc.cc:209] Flush assign_16.tmp_0
I1018 08:53:57.777766 4097533 var_desc.cc:415] Flush  assign_16.tmp_0 1
I1018 08:53:57.777769 4097533 block_desc.cc:209] Flush fill_constant_35.tmp_0
I1018 08:53:57.777771 4097533 var_desc.cc:415] Flush  fill_constant_35.tmp_0 1
I1018 08:53:57.777549 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "assign_16.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_35.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  ops {
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "ShapeTensorList"
    }
    inputs {
      parameter: "ValueTensor"
    }
    outputs {
      parameter: "Out"
      arguments: "fill_constant_35.tmp_0"
    }
    type: "fill_constant"
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "dtype"
      type: INT
      i: 5
    }
    attrs {
      name: "force_cpu"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 117, in composite_batchnorm"
      strings: "    batch_mean = zeros(run_mean.shape, run_mean.dtype)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 1117, in zeros"
      strings: "    return fill_constant(value=0.0, shape=shape, dtype=dtype, name=name)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 977, in fill_constant"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "place_type"
      type: INT
      i: -1
    }
    attrs {
      name: "shape"
      type: LONGS
      longs: 64
    }
    attrs {
      name: "str_value"
      type: STRING
      s: "0.0"
    }
    attrs {
      name: "value"
      type: FLOAT
      f: 0
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fill_constant_35.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "assign_16.tmp_0"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 134, in composite_batchnorm"
      strings: "    batch_mean_ = assign(batch_mean)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "assign_16.tmp_0"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.777886 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.777912 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_43.tmp_0, is_only_used_internal: 1
I1018 08:53:57.777915 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_43.tmp_0
I1018 08:53:57.777916 4097533 build_cinn_pass.cc:742] Cluster Ops: (fill_constant, assign, )
I1018 08:53:57.777920 4097533 build_cinn_pass.cc:743] Cluster input vars: ()
I1018 08:53:57.777921 4097533 build_cinn_pass.cc:744] Cluster output vars: (assign_20.tmp_0, )
I1018 08:53:57.777923 4097533 build_cinn_pass.cc:745] Cluster internal vars: (fill_constant_43.tmp_0, )
I1018 08:53:57.777930 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.777967 4097533 build_cinn_pass.cc:274] Add Output Var Node: assign_20.tmp_0
I1018 08:53:57.777980 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_2[label="fill_constant_43.tmp_0
[128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_1[label="assign_20.tmp_0
[128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_0
   node_2->node_3
   node_3->node_1
   node_4->node_2
} // end G
I1018 08:53:57.778010 4097533 var_desc.cc:415] Flush  assign_20.tmp_0 1
I1018 08:53:57.778013 4097533 var_desc.cc:415] Flush  fill_constant_43.tmp_0 1
I1018 08:53:57.778018 4097533 graph_helper.h:104] adj assign0x55b7d0a788c0 -> fetch0x55b7d0a36780  via assign_20.tmp_00x55b7d0c29eb0
I1018 08:53:57.778024 4097533 graph_helper.h:104] adj fill_constant0x55b7d0a7b8a0 -> assign0x55b7d0a788c0  via fill_constant_43.tmp_00x55b7d0aa7230
I1018 08:53:57.778043 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.778046 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.778054 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.778057 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.778069 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.778072 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.778160 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.778215 4097533 block_desc.cc:205] vars in desc 2
I1018 08:53:57.778218 4097533 block_desc.cc:209] Flush assign_20.tmp_0
I1018 08:53:57.778220 4097533 var_desc.cc:415] Flush  assign_20.tmp_0 1
I1018 08:53:57.778223 4097533 block_desc.cc:209] Flush fill_constant_43.tmp_0
I1018 08:53:57.778225 4097533 var_desc.cc:415] Flush  fill_constant_43.tmp_0 1
I1018 08:53:57.778008 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "assign_20.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_43.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  ops {
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "ShapeTensorList"
    }
    inputs {
      parameter: "ValueTensor"
    }
    outputs {
      parameter: "Out"
      arguments: "fill_constant_43.tmp_0"
    }
    type: "fill_constant"
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "dtype"
      type: INT
      i: 5
    }
    attrs {
      name: "force_cpu"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 117, in composite_batchnorm"
      strings: "    batch_mean = zeros(run_mean.shape, run_mean.dtype)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 1117, in zeros"
      strings: "    return fill_constant(value=0.0, shape=shape, dtype=dtype, name=name)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 977, in fill_constant"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "place_type"
      type: INT
      i: -1
    }
    attrs {
      name: "shape"
      type: LONGS
      longs: 128
    }
    attrs {
      name: "str_value"
      type: STRING
      s: "0.0"
    }
    attrs {
      name: "value"
      type: FLOAT
      f: 0
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fill_constant_43.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "assign_20.tmp_0"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 134, in composite_batchnorm"
      strings: "    batch_mean_ = assign(batch_mean)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "assign_20.tmp_0"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.778342 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.778366 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_51.tmp_0, is_only_used_internal: 1
I1018 08:53:57.778369 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_51.tmp_0
I1018 08:53:57.778371 4097533 build_cinn_pass.cc:742] Cluster Ops: (fill_constant, assign, )
I1018 08:53:57.778374 4097533 build_cinn_pass.cc:743] Cluster input vars: ()
I1018 08:53:57.778376 4097533 build_cinn_pass.cc:744] Cluster output vars: (assign_24.tmp_0, )
I1018 08:53:57.778378 4097533 build_cinn_pass.cc:745] Cluster internal vars: (fill_constant_51.tmp_0, )
I1018 08:53:57.778384 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.778421 4097533 build_cinn_pass.cc:274] Add Output Var Node: assign_24.tmp_0
I1018 08:53:57.778431 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_2[label="fill_constant_51.tmp_0
[128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_1[label="assign_24.tmp_0
[128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_0
   node_2->node_3
   node_3->node_1
   node_4->node_2
} // end G
I1018 08:53:57.778461 4097533 var_desc.cc:415] Flush  assign_24.tmp_0 1
I1018 08:53:57.778465 4097533 var_desc.cc:415] Flush  fill_constant_51.tmp_0 1
I1018 08:53:57.778468 4097533 graph_helper.h:104] adj assign0x55b7d0ac0380 -> fetch0x55b7d0a34580  via assign_24.tmp_00x55b7d0a7a380
I1018 08:53:57.778473 4097533 graph_helper.h:104] adj fill_constant0x55b7d0ac3360 -> assign0x55b7d0ac0380  via fill_constant_51.tmp_00x55b7d0aed640
I1018 08:53:57.778493 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.778496 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.778503 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.778506 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.778519 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.778522 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.778609 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.778663 4097533 block_desc.cc:205] vars in desc 2
I1018 08:53:57.778666 4097533 block_desc.cc:209] Flush assign_24.tmp_0
I1018 08:53:57.778668 4097533 var_desc.cc:415] Flush  assign_24.tmp_0 1
I1018 08:53:57.778671 4097533 block_desc.cc:209] Flush fill_constant_51.tmp_0
I1018 08:53:57.778673 4097533 var_desc.cc:415] Flush  fill_constant_51.tmp_0 1
I1018 08:53:57.778460 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "assign_24.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_51.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  ops {
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "ShapeTensorList"
    }
    inputs {
      parameter: "ValueTensor"
    }
    outputs {
      parameter: "Out"
      arguments: "fill_constant_51.tmp_0"
    }
    type: "fill_constant"
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "dtype"
      type: INT
      i: 5
    }
    attrs {
      name: "force_cpu"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 117, in composite_batchnorm"
      strings: "    batch_mean = zeros(run_mean.shape, run_mean.dtype)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 1117, in zeros"
      strings: "    return fill_constant(value=0.0, shape=shape, dtype=dtype, name=name)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 977, in fill_constant"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "place_type"
      type: INT
      i: -1
    }
    attrs {
      name: "shape"
      type: LONGS
      longs: 128
    }
    attrs {
      name: "str_value"
      type: STRING
      s: "0.0"
    }
    attrs {
      name: "value"
      type: FLOAT
      f: 0
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fill_constant_51.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "assign_24.tmp_0"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 134, in composite_batchnorm"
      strings: "    batch_mean_ = assign(batch_mean)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "assign_24.tmp_0"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.778785 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.778810 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_57.tmp_0, is_only_used_internal: 1
I1018 08:53:57.778812 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_57.tmp_0
I1018 08:53:57.778815 4097533 build_cinn_pass.cc:742] Cluster Ops: (fill_constant, assign, )
I1018 08:53:57.778816 4097533 build_cinn_pass.cc:743] Cluster input vars: ()
I1018 08:53:57.778818 4097533 build_cinn_pass.cc:744] Cluster output vars: (assign_28.tmp_0, )
I1018 08:53:57.778821 4097533 build_cinn_pass.cc:745] Cluster internal vars: (fill_constant_57.tmp_0, )
I1018 08:53:57.778827 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.778864 4097533 build_cinn_pass.cc:274] Add Output Var Node: assign_28.tmp_0
I1018 08:53:57.778874 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_2[label="fill_constant_57.tmp_0
[128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_1[label="assign_28.tmp_0
[128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_0
   node_2->node_3
   node_3->node_1
   node_4->node_2
} // end G
I1018 08:53:57.778906 4097533 var_desc.cc:415] Flush  assign_28.tmp_0 1
I1018 08:53:57.778909 4097533 var_desc.cc:415] Flush  fill_constant_57.tmp_0 1
I1018 08:53:57.778914 4097533 graph_helper.h:104] adj assign0x55b7d0b04ee0 -> fetch0x55b7d0aa5e30  via assign_28.tmp_00x55b7d0aa5c70
I1018 08:53:57.778919 4097533 graph_helper.h:104] adj fill_constant0x55b7d0b07ec0 -> assign0x55b7d0b04ee0  via fill_constant_57.tmp_00x55b7d0b321a0
I1018 08:53:57.778939 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.778941 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.778949 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.778951 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.778964 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.778966 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.779052 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.779106 4097533 block_desc.cc:205] vars in desc 2
I1018 08:53:57.779109 4097533 block_desc.cc:209] Flush assign_28.tmp_0
I1018 08:53:57.779111 4097533 var_desc.cc:415] Flush  assign_28.tmp_0 1
I1018 08:53:57.779114 4097533 block_desc.cc:209] Flush fill_constant_57.tmp_0
I1018 08:53:57.779116 4097533 var_desc.cc:415] Flush  fill_constant_57.tmp_0 1
I1018 08:53:57.778903 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "assign_28.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_57.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  ops {
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "ShapeTensorList"
    }
    inputs {
      parameter: "ValueTensor"
    }
    outputs {
      parameter: "Out"
      arguments: "fill_constant_57.tmp_0"
    }
    type: "fill_constant"
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "dtype"
      type: INT
      i: 5
    }
    attrs {
      name: "force_cpu"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 117, in composite_batchnorm"
      strings: "    batch_mean = zeros(run_mean.shape, run_mean.dtype)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 1117, in zeros"
      strings: "    return fill_constant(value=0.0, shape=shape, dtype=dtype, name=name)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 977, in fill_constant"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "place_type"
      type: INT
      i: -1
    }
    attrs {
      name: "shape"
      type: LONGS
      longs: 128
    }
    attrs {
      name: "str_value"
      type: STRING
      s: "0.0"
    }
    attrs {
      name: "value"
      type: FLOAT
      f: 0
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fill_constant_57.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "assign_28.tmp_0"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 134, in composite_batchnorm"
      strings: "    batch_mean_ = assign(batch_mean)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "assign_28.tmp_0"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.779229 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.779253 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_65.tmp_0, is_only_used_internal: 1
I1018 08:53:57.779255 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_65.tmp_0
I1018 08:53:57.779258 4097533 build_cinn_pass.cc:742] Cluster Ops: (fill_constant, assign, )
I1018 08:53:57.779260 4097533 build_cinn_pass.cc:743] Cluster input vars: ()
I1018 08:53:57.779263 4097533 build_cinn_pass.cc:744] Cluster output vars: (assign_32.tmp_0, )
I1018 08:53:57.779264 4097533 build_cinn_pass.cc:745] Cluster internal vars: (fill_constant_65.tmp_0, )
I1018 08:53:57.779270 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.779306 4097533 build_cinn_pass.cc:274] Add Output Var Node: assign_32.tmp_0
I1018 08:53:57.779316 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_2[label="fill_constant_65.tmp_0
[128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_1[label="assign_32.tmp_0
[128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_0
   node_2->node_3
   node_3->node_1
   node_4->node_2
} // end G
I1018 08:53:57.779345 4097533 var_desc.cc:415] Flush  assign_32.tmp_0 1
I1018 08:53:57.779348 4097533 var_desc.cc:415] Flush  fill_constant_65.tmp_0 1
I1018 08:53:57.779353 4097533 graph_helper.h:104] adj assign0x55b7d0b43860 -> fetch0x55b7d0e03390  via assign_32.tmp_00x55b7d0e03d40
I1018 08:53:57.779358 4097533 graph_helper.h:104] adj fill_constant0x55b7d0b46840 -> assign0x55b7d0b43860  via fill_constant_65.tmp_00x55b7d0b70b20
I1018 08:53:57.779377 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.779381 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.779387 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.779390 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.779403 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.779405 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.779492 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.779546 4097533 block_desc.cc:205] vars in desc 2
I1018 08:53:57.779548 4097533 block_desc.cc:209] Flush assign_32.tmp_0
I1018 08:53:57.779551 4097533 var_desc.cc:415] Flush  assign_32.tmp_0 1
I1018 08:53:57.779553 4097533 block_desc.cc:209] Flush fill_constant_65.tmp_0
I1018 08:53:57.779556 4097533 var_desc.cc:415] Flush  fill_constant_65.tmp_0 1
I1018 08:53:57.779343 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "assign_32.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_65.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  ops {
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "ShapeTensorList"
    }
    inputs {
      parameter: "ValueTensor"
    }
    outputs {
      parameter: "Out"
      arguments: "fill_constant_65.tmp_0"
    }
    type: "fill_constant"
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "dtype"
      type: INT
      i: 5
    }
    attrs {
      name: "force_cpu"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 117, in composite_batchnorm"
      strings: "    batch_mean = zeros(run_mean.shape, run_mean.dtype)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 1117, in zeros"
      strings: "    return fill_constant(value=0.0, shape=shape, dtype=dtype, name=name)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 977, in fill_constant"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "place_type"
      type: INT
      i: -1
    }
    attrs {
      name: "shape"
      type: LONGS
      longs: 128
    }
    attrs {
      name: "str_value"
      type: STRING
      s: "0.0"
    }
    attrs {
      name: "value"
      type: FLOAT
      f: 0
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fill_constant_65.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "assign_32.tmp_0"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 134, in composite_batchnorm"
      strings: "    batch_mean_ = assign(batch_mean)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "assign_32.tmp_0"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.779668 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.779691 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_73.tmp_0, is_only_used_internal: 1
I1018 08:53:57.779695 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_73.tmp_0
I1018 08:53:57.779696 4097533 build_cinn_pass.cc:742] Cluster Ops: (fill_constant, assign, )
I1018 08:53:57.779698 4097533 build_cinn_pass.cc:743] Cluster input vars: ()
I1018 08:53:57.779700 4097533 build_cinn_pass.cc:744] Cluster output vars: (assign_36.tmp_0, )
I1018 08:53:57.779702 4097533 build_cinn_pass.cc:745] Cluster internal vars: (fill_constant_73.tmp_0, )
I1018 08:53:57.779708 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.779745 4097533 build_cinn_pass.cc:274] Add Output Var Node: assign_36.tmp_0
I1018 08:53:57.779754 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_2[label="fill_constant_73.tmp_0
[128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_1[label="assign_36.tmp_0
[128]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_0
   node_2->node_3
   node_3->node_1
   node_4->node_2
} // end G
I1018 08:53:57.779784 4097533 var_desc.cc:415] Flush  assign_36.tmp_0 1
I1018 08:53:57.779786 4097533 var_desc.cc:415] Flush  fill_constant_73.tmp_0 1
I1018 08:53:57.779791 4097533 graph_helper.h:104] adj assign0x55b7d0b8aa60 -> fetch0x55b7d0e4a230  via assign_36.tmp_00x55b7d0e4a070
I1018 08:53:57.779800 4097533 graph_helper.h:104] adj fill_constant0x55b7d0b8da40 -> assign0x55b7d0b8aa60  via fill_constant_73.tmp_00x55b7d0bb7d00
I1018 08:53:57.779819 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.779822 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.779829 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.779832 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.779845 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.779847 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.779933 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.779987 4097533 block_desc.cc:205] vars in desc 2
I1018 08:53:57.779990 4097533 block_desc.cc:209] Flush assign_36.tmp_0
I1018 08:53:57.779992 4097533 var_desc.cc:415] Flush  assign_36.tmp_0 1
I1018 08:53:57.779994 4097533 block_desc.cc:209] Flush fill_constant_73.tmp_0
I1018 08:53:57.779997 4097533 var_desc.cc:415] Flush  fill_constant_73.tmp_0 1
I1018 08:53:57.779781 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "assign_36.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_73.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 128
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  ops {
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "ShapeTensorList"
    }
    inputs {
      parameter: "ValueTensor"
    }
    outputs {
      parameter: "Out"
      arguments: "fill_constant_73.tmp_0"
    }
    type: "fill_constant"
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "dtype"
      type: INT
      i: 5
    }
    attrs {
      name: "force_cpu"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 117, in composite_batchnorm"
      strings: "    batch_mean = zeros(run_mean.shape, run_mean.dtype)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 1117, in zeros"
      strings: "    return fill_constant(value=0.0, shape=shape, dtype=dtype, name=name)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 977, in fill_constant"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "place_type"
      type: INT
      i: -1
    }
    attrs {
      name: "shape"
      type: LONGS
      longs: 128
    }
    attrs {
      name: "str_value"
      type: STRING
      s: "0.0"
    }
    attrs {
      name: "value"
      type: FLOAT
      f: 0
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fill_constant_73.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "assign_36.tmp_0"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 134, in composite_batchnorm"
      strings: "    batch_mean_ = assign(batch_mean)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "assign_36.tmp_0"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.780109 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.780133 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_81.tmp_0, is_only_used_internal: 1
I1018 08:53:57.780135 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_81.tmp_0
I1018 08:53:57.780138 4097533 build_cinn_pass.cc:742] Cluster Ops: (fill_constant, assign, )
I1018 08:53:57.780140 4097533 build_cinn_pass.cc:743] Cluster input vars: ()
I1018 08:53:57.780143 4097533 build_cinn_pass.cc:744] Cluster output vars: (assign_40.tmp_0, )
I1018 08:53:57.780144 4097533 build_cinn_pass.cc:745] Cluster internal vars: (fill_constant_81.tmp_0, )
I1018 08:53:57.780150 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.780189 4097533 build_cinn_pass.cc:274] Add Output Var Node: assign_40.tmp_0
I1018 08:53:57.780198 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_2[label="fill_constant_81.tmp_0
[256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_1[label="assign_40.tmp_0
[256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_0
   node_2->node_3
   node_3->node_1
   node_4->node_2
} // end G
I1018 08:53:57.780227 4097533 var_desc.cc:415] Flush  assign_40.tmp_0 1
I1018 08:53:57.780230 4097533 var_desc.cc:415] Flush  fill_constant_81.tmp_0 1
I1018 08:53:57.780236 4097533 graph_helper.h:104] adj assign0x55b7d0bcf5a0 -> fetch0x55b7d0cb4290  via assign_40.tmp_00x55b7d0b07bc0
I1018 08:53:57.780241 4097533 graph_helper.h:104] adj fill_constant0x55b7d0bd2580 -> assign0x55b7d0bcf5a0  via fill_constant_81.tmp_00x55b7d0bfc860
I1018 08:53:57.780261 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.780264 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.780272 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.780275 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.780288 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.780290 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.780382 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.780439 4097533 block_desc.cc:205] vars in desc 2
I1018 08:53:57.780441 4097533 block_desc.cc:209] Flush assign_40.tmp_0
I1018 08:53:57.780444 4097533 var_desc.cc:415] Flush  assign_40.tmp_0 1
I1018 08:53:57.780447 4097533 block_desc.cc:209] Flush fill_constant_81.tmp_0
I1018 08:53:57.780448 4097533 var_desc.cc:415] Flush  fill_constant_81.tmp_0 1
I1018 08:53:57.780225 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "assign_40.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_81.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  ops {
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "ShapeTensorList"
    }
    inputs {
      parameter: "ValueTensor"
    }
    outputs {
      parameter: "Out"
      arguments: "fill_constant_81.tmp_0"
    }
    type: "fill_constant"
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "dtype"
      type: INT
      i: 5
    }
    attrs {
      name: "force_cpu"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 117, in composite_batchnorm"
      strings: "    batch_mean = zeros(run_mean.shape, run_mean.dtype)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 1117, in zeros"
      strings: "    return fill_constant(value=0.0, shape=shape, dtype=dtype, name=name)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 977, in fill_constant"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "place_type"
      type: INT
      i: -1
    }
    attrs {
      name: "shape"
      type: LONGS
      longs: 256
    }
    attrs {
      name: "str_value"
      type: STRING
      s: "0.0"
    }
    attrs {
      name: "value"
      type: FLOAT
      f: 0
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fill_constant_81.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "assign_40.tmp_0"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 134, in composite_batchnorm"
      strings: "    batch_mean_ = assign(batch_mean)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "assign_40.tmp_0"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.780560 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.780584 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_89.tmp_0, is_only_used_internal: 1
I1018 08:53:57.780586 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_89.tmp_0
I1018 08:53:57.780589 4097533 build_cinn_pass.cc:742] Cluster Ops: (fill_constant, assign, )
I1018 08:53:57.780591 4097533 build_cinn_pass.cc:743] Cluster input vars: ()
I1018 08:53:57.780593 4097533 build_cinn_pass.cc:744] Cluster output vars: (assign_44.tmp_0, )
I1018 08:53:57.780596 4097533 build_cinn_pass.cc:745] Cluster internal vars: (fill_constant_89.tmp_0, )
I1018 08:53:57.780601 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.780639 4097533 build_cinn_pass.cc:274] Add Output Var Node: assign_44.tmp_0
I1018 08:53:57.780649 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_2[label="fill_constant_89.tmp_0
[256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_1[label="assign_44.tmp_0
[256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_0
   node_2->node_3
   node_3->node_1
   node_4->node_2
} // end G
I1018 08:53:57.780678 4097533 var_desc.cc:415] Flush  assign_44.tmp_0 1
I1018 08:53:57.780681 4097533 var_desc.cc:415] Flush  fill_constant_89.tmp_0 1
I1018 08:53:57.780686 4097533 graph_helper.h:104] adj assign0x55b7d0c17c70 -> fetch0x55b7d0b8d980  via assign_44.tmp_00x55b7d0cb3d00
I1018 08:53:57.780691 4097533 graph_helper.h:104] adj fill_constant0x55b7d0c1a970 -> assign0x55b7d0c17c70  via fill_constant_89.tmp_00x55b7d0c44c20
I1018 08:53:57.780711 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.780714 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.780721 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.780730 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.780742 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.780745 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.780834 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.780889 4097533 block_desc.cc:205] vars in desc 2
I1018 08:53:57.780891 4097533 block_desc.cc:209] Flush assign_44.tmp_0
I1018 08:53:57.780894 4097533 var_desc.cc:415] Flush  assign_44.tmp_0 1
I1018 08:53:57.780896 4097533 block_desc.cc:209] Flush fill_constant_89.tmp_0
I1018 08:53:57.780898 4097533 var_desc.cc:415] Flush  fill_constant_89.tmp_0 1
I1018 08:53:57.780676 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "assign_44.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_89.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  ops {
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "ShapeTensorList"
    }
    inputs {
      parameter: "ValueTensor"
    }
    outputs {
      parameter: "Out"
      arguments: "fill_constant_89.tmp_0"
    }
    type: "fill_constant"
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "dtype"
      type: INT
      i: 5
    }
    attrs {
      name: "force_cpu"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 117, in composite_batchnorm"
      strings: "    batch_mean = zeros(run_mean.shape, run_mean.dtype)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 1117, in zeros"
      strings: "    return fill_constant(value=0.0, shape=shape, dtype=dtype, name=name)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 977, in fill_constant"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "place_type"
      type: INT
      i: -1
    }
    attrs {
      name: "shape"
      type: LONGS
      longs: 256
    }
    attrs {
      name: "str_value"
      type: STRING
      s: "0.0"
    }
    attrs {
      name: "value"
      type: FLOAT
      f: 0
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fill_constant_89.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "assign_44.tmp_0"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 134, in composite_batchnorm"
      strings: "    batch_mean_ = assign(batch_mean)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "assign_44.tmp_0"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.781009 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.781033 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_95.tmp_0, is_only_used_internal: 1
I1018 08:53:57.781035 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_95.tmp_0
I1018 08:53:57.781037 4097533 build_cinn_pass.cc:742] Cluster Ops: (fill_constant, assign, )
I1018 08:53:57.781039 4097533 build_cinn_pass.cc:743] Cluster input vars: ()
I1018 08:53:57.781044 4097533 build_cinn_pass.cc:744] Cluster output vars: (assign_48.tmp_0, )
I1018 08:53:57.781046 4097533 build_cinn_pass.cc:745] Cluster internal vars: (fill_constant_95.tmp_0, )
I1018 08:53:57.781052 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.781088 4097533 build_cinn_pass.cc:274] Add Output Var Node: assign_48.tmp_0
I1018 08:53:57.781098 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_2[label="fill_constant_95.tmp_0
[256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_1[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="assign_48.tmp_0
[256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_0->node_1
   node_2->node_3
   node_3->node_0
   node_4->node_2
} // end G
I1018 08:53:57.781127 4097533 var_desc.cc:415] Flush  assign_48.tmp_0 1
I1018 08:53:57.781131 4097533 var_desc.cc:415] Flush  fill_constant_95.tmp_0 1
I1018 08:53:57.781136 4097533 graph_helper.h:104] adj assign0x55b7d0c5c4c0 -> fetch0x55b7d0c463c0  via assign_48.tmp_00x55b7d0bb9610
I1018 08:53:57.781140 4097533 graph_helper.h:104] adj fill_constant0x55b7d0c5f4a0 -> assign0x55b7d0c5c4c0  via fill_constant_95.tmp_00x55b7d0c89780
I1018 08:53:57.781159 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.781162 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.781170 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.781173 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.781185 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.781188 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.781281 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.781337 4097533 block_desc.cc:205] vars in desc 2
I1018 08:53:57.781340 4097533 block_desc.cc:209] Flush assign_48.tmp_0
I1018 08:53:57.781342 4097533 var_desc.cc:415] Flush  assign_48.tmp_0 1
I1018 08:53:57.781344 4097533 block_desc.cc:209] Flush fill_constant_95.tmp_0
I1018 08:53:57.781347 4097533 var_desc.cc:415] Flush  fill_constant_95.tmp_0 1
I1018 08:53:57.781126 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "assign_48.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_95.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  ops {
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "ShapeTensorList"
    }
    inputs {
      parameter: "ValueTensor"
    }
    outputs {
      parameter: "Out"
      arguments: "fill_constant_95.tmp_0"
    }
    type: "fill_constant"
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "dtype"
      type: INT
      i: 5
    }
    attrs {
      name: "force_cpu"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 117, in composite_batchnorm"
      strings: "    batch_mean = zeros(run_mean.shape, run_mean.dtype)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 1117, in zeros"
      strings: "    return fill_constant(value=0.0, shape=shape, dtype=dtype, name=name)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 977, in fill_constant"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "place_type"
      type: INT
      i: -1
    }
    attrs {
      name: "shape"
      type: LONGS
      longs: 256
    }
    attrs {
      name: "str_value"
      type: STRING
      s: "0.0"
    }
    attrs {
      name: "value"
      type: FLOAT
      f: 0
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fill_constant_95.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "assign_48.tmp_0"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 134, in composite_batchnorm"
      strings: "    batch_mean_ = assign(batch_mean)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "assign_48.tmp_0"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.781461 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.781484 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_103.tmp_0, is_only_used_internal: 1
I1018 08:53:57.781487 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_103.tmp_0
I1018 08:53:57.781489 4097533 build_cinn_pass.cc:742] Cluster Ops: (fill_constant, assign, )
I1018 08:53:57.781491 4097533 build_cinn_pass.cc:743] Cluster input vars: ()
I1018 08:53:57.781493 4097533 build_cinn_pass.cc:744] Cluster output vars: (assign_52.tmp_0, )
I1018 08:53:57.781495 4097533 build_cinn_pass.cc:745] Cluster internal vars: (fill_constant_103.tmp_0, )
I1018 08:53:57.781502 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.781538 4097533 build_cinn_pass.cc:274] Add Output Var Node: assign_52.tmp_0
I1018 08:53:57.781548 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_2[label="fill_constant_103.tmp_0
[256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_1[label="assign_52.tmp_0
[256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_0
   node_2->node_3
   node_3->node_1
   node_4->node_2
} // end G
I1018 08:53:57.781577 4097533 var_desc.cc:415] Flush  assign_52.tmp_0 1
I1018 08:53:57.781580 4097533 var_desc.cc:415] Flush  fill_constant_103.tmp_0 1
I1018 08:53:57.781585 4097533 graph_helper.h:104] adj assign0x55b7d0c9ae40 -> fetch0x55b7d0cc9a10  via assign_52.tmp_00x55b7d0c44e50
I1018 08:53:57.781590 4097533 graph_helper.h:104] adj fill_constant0x55b7d0c9de20 -> assign0x55b7d0c9ae40  via fill_constant_103.tmp_00x55b7d0cc8100
I1018 08:53:57.781610 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.781612 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.781620 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.781622 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.781634 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.781637 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.781724 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.781778 4097533 block_desc.cc:205] vars in desc 2
I1018 08:53:57.781781 4097533 block_desc.cc:209] Flush assign_52.tmp_0
I1018 08:53:57.781783 4097533 var_desc.cc:415] Flush  assign_52.tmp_0 1
I1018 08:53:57.781788 4097533 block_desc.cc:209] Flush fill_constant_103.tmp_0
I1018 08:53:57.781790 4097533 var_desc.cc:415] Flush  fill_constant_103.tmp_0 1
I1018 08:53:57.781574 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "assign_52.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_103.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  ops {
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "ShapeTensorList"
    }
    inputs {
      parameter: "ValueTensor"
    }
    outputs {
      parameter: "Out"
      arguments: "fill_constant_103.tmp_0"
    }
    type: "fill_constant"
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "dtype"
      type: INT
      i: 5
    }
    attrs {
      name: "force_cpu"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 117, in composite_batchnorm"
      strings: "    batch_mean = zeros(run_mean.shape, run_mean.dtype)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 1117, in zeros"
      strings: "    return fill_constant(value=0.0, shape=shape, dtype=dtype, name=name)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 977, in fill_constant"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "place_type"
      type: INT
      i: -1
    }
    attrs {
      name: "shape"
      type: LONGS
      longs: 256
    }
    attrs {
      name: "str_value"
      type: STRING
      s: "0.0"
    }
    attrs {
      name: "value"
      type: FLOAT
      f: 0
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fill_constant_103.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "assign_52.tmp_0"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 134, in composite_batchnorm"
      strings: "    batch_mean_ = assign(batch_mean)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "assign_52.tmp_0"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.781900 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.781922 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_111.tmp_0, is_only_used_internal: 1
I1018 08:53:57.781925 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_111.tmp_0
I1018 08:53:57.781927 4097533 build_cinn_pass.cc:742] Cluster Ops: (fill_constant, assign, )
I1018 08:53:57.781929 4097533 build_cinn_pass.cc:743] Cluster input vars: ()
I1018 08:53:57.781931 4097533 build_cinn_pass.cc:744] Cluster output vars: (assign_56.tmp_0, )
I1018 08:53:57.781934 4097533 build_cinn_pass.cc:745] Cluster internal vars: (fill_constant_111.tmp_0, )
I1018 08:53:57.781939 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.781975 4097533 build_cinn_pass.cc:274] Add Output Var Node: assign_56.tmp_0
I1018 08:53:57.781984 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_2[label="fill_constant_111.tmp_0
[256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_1[label="assign_56.tmp_0
[256]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_0
   node_2->node_3
   node_3->node_1
   node_4->node_2
} // end G
I1018 08:53:57.782017 4097533 var_desc.cc:415] Flush  assign_56.tmp_0 1
I1018 08:53:57.782020 4097533 var_desc.cc:415] Flush  fill_constant_111.tmp_0 1
I1018 08:53:57.782025 4097533 graph_helper.h:104] adj assign0x55b7d0ce2040 -> fetch0x55b7d0d10900  via assign_56.tmp_00x55b7d0b1d410
I1018 08:53:57.782029 4097533 graph_helper.h:104] adj fill_constant0x55b7d0ce5020 -> assign0x55b7d0ce2040  via fill_constant_111.tmp_00x55b7d0d0f300
I1018 08:53:57.782049 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.782052 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.782059 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.782063 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.782074 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.782078 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.782163 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.782217 4097533 block_desc.cc:205] vars in desc 2
I1018 08:53:57.782220 4097533 block_desc.cc:209] Flush assign_56.tmp_0
I1018 08:53:57.782222 4097533 var_desc.cc:415] Flush  assign_56.tmp_0 1
I1018 08:53:57.782224 4097533 block_desc.cc:209] Flush fill_constant_111.tmp_0
I1018 08:53:57.782227 4097533 var_desc.cc:415] Flush  fill_constant_111.tmp_0 1
I1018 08:53:57.782016 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "assign_56.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_111.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 256
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  ops {
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "ShapeTensorList"
    }
    inputs {
      parameter: "ValueTensor"
    }
    outputs {
      parameter: "Out"
      arguments: "fill_constant_111.tmp_0"
    }
    type: "fill_constant"
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "dtype"
      type: INT
      i: 5
    }
    attrs {
      name: "force_cpu"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 117, in composite_batchnorm"
      strings: "    batch_mean = zeros(run_mean.shape, run_mean.dtype)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 1117, in zeros"
      strings: "    return fill_constant(value=0.0, shape=shape, dtype=dtype, name=name)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 977, in fill_constant"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "place_type"
      type: INT
      i: -1
    }
    attrs {
      name: "shape"
      type: LONGS
      longs: 256
    }
    attrs {
      name: "str_value"
      type: STRING
      s: "0.0"
    }
    attrs {
      name: "value"
      type: FLOAT
      f: 0
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fill_constant_111.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "assign_56.tmp_0"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 134, in composite_batchnorm"
      strings: "    batch_mean_ = assign(batch_mean)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "assign_56.tmp_0"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.782338 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.782361 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_119.tmp_0, is_only_used_internal: 1
I1018 08:53:57.782363 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_119.tmp_0
I1018 08:53:57.782366 4097533 build_cinn_pass.cc:742] Cluster Ops: (fill_constant, assign, )
I1018 08:53:57.782367 4097533 build_cinn_pass.cc:743] Cluster input vars: ()
I1018 08:53:57.782369 4097533 build_cinn_pass.cc:744] Cluster output vars: (assign_60.tmp_0, )
I1018 08:53:57.782371 4097533 build_cinn_pass.cc:745] Cluster internal vars: (fill_constant_119.tmp_0, )
I1018 08:53:57.782377 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.782413 4097533 build_cinn_pass.cc:274] Add Output Var Node: assign_60.tmp_0
I1018 08:53:57.782423 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_2[label="fill_constant_119.tmp_0
[512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_1[label="assign_60.tmp_0
[512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_0
   node_2->node_3
   node_3->node_1
   node_4->node_2
} // end G
I1018 08:53:57.782451 4097533 var_desc.cc:415] Flush  assign_60.tmp_0 1
I1018 08:53:57.782454 4097533 var_desc.cc:415] Flush  fill_constant_119.tmp_0 1
I1018 08:53:57.782459 4097533 graph_helper.h:104] adj assign0x55b7d0d26ba0 -> fetch0x55b7d0d11690  via assign_60.tmp_00x55b7d0e096f0
I1018 08:53:57.782464 4097533 graph_helper.h:104] adj fill_constant0x55b7d0d29b80 -> assign0x55b7d0d26ba0  via fill_constant_119.tmp_00x55b7d0d53e60
I1018 08:53:57.782483 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.782486 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.782493 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.782496 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.782509 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.782511 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.782599 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.782655 4097533 block_desc.cc:205] vars in desc 2
I1018 08:53:57.782657 4097533 block_desc.cc:209] Flush assign_60.tmp_0
I1018 08:53:57.782660 4097533 var_desc.cc:415] Flush  assign_60.tmp_0 1
I1018 08:53:57.782662 4097533 block_desc.cc:209] Flush fill_constant_119.tmp_0
I1018 08:53:57.782665 4097533 var_desc.cc:415] Flush  fill_constant_119.tmp_0 1
I1018 08:53:57.782449 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "assign_60.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_119.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  ops {
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "ShapeTensorList"
    }
    inputs {
      parameter: "ValueTensor"
    }
    outputs {
      parameter: "Out"
      arguments: "fill_constant_119.tmp_0"
    }
    type: "fill_constant"
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "dtype"
      type: INT
      i: 5
    }
    attrs {
      name: "force_cpu"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 117, in composite_batchnorm"
      strings: "    batch_mean = zeros(run_mean.shape, run_mean.dtype)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 1117, in zeros"
      strings: "    return fill_constant(value=0.0, shape=shape, dtype=dtype, name=name)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 977, in fill_constant"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "place_type"
      type: INT
      i: -1
    }
    attrs {
      name: "shape"
      type: LONGS
      longs: 512
    }
    attrs {
      name: "str_value"
      type: STRING
      s: "0.0"
    }
    attrs {
      name: "value"
      type: FLOAT
      f: 0
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fill_constant_119.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "assign_60.tmp_0"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 134, in composite_batchnorm"
      strings: "    batch_mean_ = assign(batch_mean)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "assign_60.tmp_0"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.782783 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.782805 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_127.tmp_0, is_only_used_internal: 1
I1018 08:53:57.782809 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_127.tmp_0
I1018 08:53:57.782810 4097533 build_cinn_pass.cc:742] Cluster Ops: (fill_constant, assign, )
I1018 08:53:57.782812 4097533 build_cinn_pass.cc:743] Cluster input vars: ()
I1018 08:53:57.782814 4097533 build_cinn_pass.cc:744] Cluster output vars: (assign_64.tmp_0, )
I1018 08:53:57.782816 4097533 build_cinn_pass.cc:745] Cluster internal vars: (fill_constant_127.tmp_0, )
I1018 08:53:57.782822 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.782861 4097533 build_cinn_pass.cc:274] Add Output Var Node: assign_64.tmp_0
I1018 08:53:57.782871 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_2[label="fill_constant_127.tmp_0
[512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_1[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_0[label="assign_64.tmp_0
[512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_0->node_1
   node_2->node_3
   node_3->node_0
   node_4->node_2
} // end G
I1018 08:53:57.782907 4097533 var_desc.cc:415] Flush  assign_64.tmp_0 1
I1018 08:53:57.782910 4097533 var_desc.cc:415] Flush  fill_constant_127.tmp_0 1
I1018 08:53:57.782915 4097533 graph_helper.h:104] adj assign0x55b7d0d6dd80 -> fetch0x55b7d0d0f6b0  via assign_64.tmp_00x55b7d0d11580
I1018 08:53:57.782920 4097533 graph_helper.h:104] adj fill_constant0x55b7d0d70d60 -> assign0x55b7d0d6dd80  via fill_constant_127.tmp_00x55b7d0d9b040
I1018 08:53:57.782939 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.782943 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.782949 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.782953 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.782965 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.782967 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.783056 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.783111 4097533 block_desc.cc:205] vars in desc 2
I1018 08:53:57.783113 4097533 block_desc.cc:209] Flush assign_64.tmp_0
I1018 08:53:57.783115 4097533 var_desc.cc:415] Flush  assign_64.tmp_0 1
I1018 08:53:57.783118 4097533 block_desc.cc:209] Flush fill_constant_127.tmp_0
I1018 08:53:57.783120 4097533 var_desc.cc:415] Flush  fill_constant_127.tmp_0 1
I1018 08:53:57.782904 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "assign_64.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_127.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  ops {
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "ShapeTensorList"
    }
    inputs {
      parameter: "ValueTensor"
    }
    outputs {
      parameter: "Out"
      arguments: "fill_constant_127.tmp_0"
    }
    type: "fill_constant"
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "dtype"
      type: INT
      i: 5
    }
    attrs {
      name: "force_cpu"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 117, in composite_batchnorm"
      strings: "    batch_mean = zeros(run_mean.shape, run_mean.dtype)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 1117, in zeros"
      strings: "    return fill_constant(value=0.0, shape=shape, dtype=dtype, name=name)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 977, in fill_constant"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "place_type"
      type: INT
      i: -1
    }
    attrs {
      name: "shape"
      type: LONGS
      longs: 512
    }
    attrs {
      name: "str_value"
      type: STRING
      s: "0.0"
    }
    attrs {
      name: "value"
      type: FLOAT
      f: 0
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fill_constant_127.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "assign_64.tmp_0"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 134, in composite_batchnorm"
      strings: "    batch_mean_ = assign(batch_mean)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "assign_64.tmp_0"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.783233 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.783257 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_133.tmp_0, is_only_used_internal: 1
I1018 08:53:57.783259 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_133.tmp_0
I1018 08:53:57.783262 4097533 build_cinn_pass.cc:742] Cluster Ops: (fill_constant, assign, )
I1018 08:53:57.783263 4097533 build_cinn_pass.cc:743] Cluster input vars: ()
I1018 08:53:57.783265 4097533 build_cinn_pass.cc:744] Cluster output vars: (assign_68.tmp_0, )
I1018 08:53:57.783267 4097533 build_cinn_pass.cc:745] Cluster internal vars: (fill_constant_133.tmp_0, )
I1018 08:53:57.783273 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.783310 4097533 build_cinn_pass.cc:274] Add Output Var Node: assign_68.tmp_0
I1018 08:53:57.783320 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_2[label="fill_constant_133.tmp_0
[512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_1[label="assign_68.tmp_0
[512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_0
   node_2->node_3
   node_3->node_1
   node_4->node_2
} // end G
I1018 08:53:57.783351 4097533 var_desc.cc:415] Flush  assign_68.tmp_0 1
I1018 08:53:57.783354 4097533 var_desc.cc:415] Flush  fill_constant_133.tmp_0 1
I1018 08:53:57.783360 4097533 graph_helper.h:104] adj assign0x55b7d0db28e0 -> fetch0x55b7d0cca640  via assign_68.tmp_00x55b7d0d6f880
I1018 08:53:57.783365 4097533 graph_helper.h:104] adj fill_constant0x55b7d0db58c0 -> assign0x55b7d0db28e0  via fill_constant_133.tmp_00x55b7d0ddfba0
I1018 08:53:57.783383 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.783385 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.783393 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.783396 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.783408 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.783411 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.783499 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.783553 4097533 block_desc.cc:205] vars in desc 2
I1018 08:53:57.783556 4097533 block_desc.cc:209] Flush assign_68.tmp_0
I1018 08:53:57.783558 4097533 var_desc.cc:415] Flush  assign_68.tmp_0 1
I1018 08:53:57.783560 4097533 block_desc.cc:209] Flush fill_constant_133.tmp_0
I1018 08:53:57.783563 4097533 var_desc.cc:415] Flush  fill_constant_133.tmp_0 1
I1018 08:53:57.783349 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "assign_68.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_133.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  ops {
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "ShapeTensorList"
    }
    inputs {
      parameter: "ValueTensor"
    }
    outputs {
      parameter: "Out"
      arguments: "fill_constant_133.tmp_0"
    }
    type: "fill_constant"
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "dtype"
      type: INT
      i: 5
    }
    attrs {
      name: "force_cpu"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 117, in composite_batchnorm"
      strings: "    batch_mean = zeros(run_mean.shape, run_mean.dtype)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 1117, in zeros"
      strings: "    return fill_constant(value=0.0, shape=shape, dtype=dtype, name=name)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 977, in fill_constant"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "place_type"
      type: INT
      i: -1
    }
    attrs {
      name: "shape"
      type: LONGS
      longs: 512
    }
    attrs {
      name: "str_value"
      type: STRING
      s: "0.0"
    }
    attrs {
      name: "value"
      type: FLOAT
      f: 0
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fill_constant_133.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "assign_68.tmp_0"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 134, in composite_batchnorm"
      strings: "    batch_mean_ = assign(batch_mean)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "assign_68.tmp_0"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.783676 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.783699 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_141.tmp_0, is_only_used_internal: 1
I1018 08:53:57.783701 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_141.tmp_0
I1018 08:53:57.783703 4097533 build_cinn_pass.cc:742] Cluster Ops: (fill_constant, assign, )
I1018 08:53:57.783705 4097533 build_cinn_pass.cc:743] Cluster input vars: ()
I1018 08:53:57.783707 4097533 build_cinn_pass.cc:744] Cluster output vars: (assign_72.tmp_0, )
I1018 08:53:57.783710 4097533 build_cinn_pass.cc:745] Cluster internal vars: (fill_constant_141.tmp_0, )
I1018 08:53:57.783715 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.783752 4097533 build_cinn_pass.cc:274] Add Output Var Node: assign_72.tmp_0
I1018 08:53:57.783761 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_2[label="assign_72.tmp_0
[512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_1[label="fill_constant_141.tmp_0
[512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_3
   node_2->node_0
   node_3->node_2
   node_4->node_1
} // end G
I1018 08:53:57.783792 4097533 var_desc.cc:415] Flush  fill_constant_141.tmp_0 1
I1018 08:53:57.783794 4097533 var_desc.cc:415] Flush  assign_72.tmp_0 1
I1018 08:53:57.783798 4097533 graph_helper.h:104] adj assign0x55b7d0df1260 -> fetch0x55b7d0e7c460  via assign_72.tmp_00x55b7d0e7c570
I1018 08:53:57.783807 4097533 graph_helper.h:104] adj fill_constant0x55b7d0df4240 -> assign0x55b7d0df1260  via fill_constant_141.tmp_00x55b7d0e1e520
I1018 08:53:57.783825 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.783828 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.783835 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.783838 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.783851 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.783854 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.783941 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.783993 4097533 block_desc.cc:205] vars in desc 2
I1018 08:53:57.783996 4097533 block_desc.cc:209] Flush fill_constant_141.tmp_0
I1018 08:53:57.783998 4097533 var_desc.cc:415] Flush  fill_constant_141.tmp_0 1
I1018 08:53:57.784001 4097533 block_desc.cc:209] Flush assign_72.tmp_0
I1018 08:53:57.784003 4097533 var_desc.cc:415] Flush  assign_72.tmp_0 1
I1018 08:53:57.783788 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "fill_constant_141.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "assign_72.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  ops {
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "ShapeTensorList"
    }
    inputs {
      parameter: "ValueTensor"
    }
    outputs {
      parameter: "Out"
      arguments: "fill_constant_141.tmp_0"
    }
    type: "fill_constant"
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "dtype"
      type: INT
      i: 5
    }
    attrs {
      name: "force_cpu"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 117, in composite_batchnorm"
      strings: "    batch_mean = zeros(run_mean.shape, run_mean.dtype)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 1117, in zeros"
      strings: "    return fill_constant(value=0.0, shape=shape, dtype=dtype, name=name)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 977, in fill_constant"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "place_type"
      type: INT
      i: -1
    }
    attrs {
      name: "shape"
      type: LONGS
      longs: 512
    }
    attrs {
      name: "str_value"
      type: STRING
      s: "0.0"
    }
    attrs {
      name: "value"
      type: FLOAT
      f: 0
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fill_constant_141.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "assign_72.tmp_0"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 134, in composite_batchnorm"
      strings: "    batch_mean_ = assign(batch_mean)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "assign_72.tmp_0"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.784116 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.784137 4097533 build_cinn_pass.cc:557] var_node->outputs[0]: fill_constant_149.tmp_0, is_only_used_internal: 1
I1018 08:53:57.784139 4097533 build_cinn_pass.cc:562] insert internal var: fill_constant_149.tmp_0
I1018 08:53:57.784142 4097533 build_cinn_pass.cc:742] Cluster Ops: (fill_constant, assign, )
I1018 08:53:57.784144 4097533 build_cinn_pass.cc:743] Cluster input vars: ()
I1018 08:53:57.784147 4097533 build_cinn_pass.cc:744] Cluster output vars: (assign_76.tmp_0, )
I1018 08:53:57.784148 4097533 build_cinn_pass.cc:745] Cluster internal vars: (fill_constant_149.tmp_0, )
I1018 08:53:57.784154 4097533 graph.cc:226] kStaleProgramOpDescs.size: 0
I1018 08:53:57.784190 4097533 build_cinn_pass.cc:274] Add Output Var Node: assign_76.tmp_0
I1018 08:53:57.784199 4097533 cinn_compiler.cc:170] -- Add a graph into CinnCompiler, which is:
digraph G {
   node_4[label="fill_constant" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_3[label="assign" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_2[label="fill_constant_149.tmp_0
[512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_1[label="assign_76.tmp_0
[512]
FP32" shape="box" style="rounded,filled,bold" color="#dddddd" fontcolor="#000000"]
   node_0[label="fetch" shape="box" style="rounded,filled,bold" color="#303A3A" fontcolor="#ffffff"]
   node_1->node_0
   node_2->node_3
   node_3->node_1
   node_4->node_2
} // end G
I1018 08:53:57.784229 4097533 var_desc.cc:415] Flush  assign_76.tmp_0 1
I1018 08:53:57.784231 4097533 var_desc.cc:415] Flush  fill_constant_149.tmp_0 1
I1018 08:53:57.784236 4097533 graph_helper.h:104] adj assign0x55b7d0e38460 -> fetch0x55b7d0e66ee0  via assign_76.tmp_00x55b7d0e66d20
I1018 08:53:57.784241 4097533 graph_helper.h:104] adj fill_constant0x55b7d0e3b440 -> assign0x55b7d0e38460  via fill_constant_149.tmp_00x55b7d0e65720
I1018 08:53:57.784260 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.784263 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.784271 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.784273 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.784286 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.784288 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.784373 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.784426 4097533 block_desc.cc:205] vars in desc 2
I1018 08:53:57.784430 4097533 block_desc.cc:209] Flush assign_76.tmp_0
I1018 08:53:57.784431 4097533 var_desc.cc:415] Flush  assign_76.tmp_0 1
I1018 08:53:57.784435 4097533 block_desc.cc:209] Flush fill_constant_149.tmp_0
I1018 08:53:57.784436 4097533 var_desc.cc:415] Flush  fill_constant_149.tmp_0 1
I1018 08:53:57.784226 4097533 build_cinn_pass.cc:758] Compilation Key:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "assign_76.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "fill_constant_149.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 512
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  ops {
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "ShapeTensorList"
    }
    inputs {
      parameter: "ValueTensor"
    }
    outputs {
      parameter: "Out"
      arguments: "fill_constant_149.tmp_0"
    }
    type: "fill_constant"
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "dtype"
      type: INT
      i: 5
    }
    attrs {
      name: "force_cpu"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 117, in composite_batchnorm"
      strings: "    batch_mean = zeros(run_mean.shape, run_mean.dtype)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 1117, in zeros"
      strings: "    return fill_constant(value=0.0, shape=shape, dtype=dtype, name=name)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 977, in fill_constant"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "place_type"
      type: INT
      i: -1
    }
    attrs {
      name: "shape"
      type: LONGS
      longs: 512
    }
    attrs {
      name: "str_value"
      type: STRING
      s: "0.0"
    }
    attrs {
      name: "value"
      type: FLOAT
      f: 0
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fill_constant_149.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "assign_76.tmp_0"
    }
    type: "assign"
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 134, in composite_batchnorm"
      strings: "    batch_mean_ = assign(batch_mean)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/creation.py\", line 2420, in assign"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "assign_76.tmp_0"
    }
    type: "fetch"
  }
}
version {
  version: 0
}
I1018 08:53:57.784549 4097533 build_cinn_pass.cc:630] Add op [cinn_launch] into graph.
I1018 08:53:57.784582 4097533 graph_helper.h:104] adj conv2d0x55b7d0e31fa0 -> cinn_launch0x55b7d0e2ff90  via conv2d_18.tmp_00x55b7d0e34e40
I1018 08:53:57.784585 4097533 graph_helper.h:104] adj conv2d0x55b7d0d206c0 -> cinn_launch0x55b7d0d1cbf0  via conv2d_14.tmp_00x55b7d0d235b0
I1018 08:53:57.784588 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0bb7460 -> cinn_launch0x55b7d0d1cbf0  via relu_10.tmp_00x55b7d0cdb9b0
I1018 08:53:57.784590 4097533 graph_helper.h:104] adj conv2d0x55b7d0cdbb80 -> cinn_launch0x55b7d0f7a4e0  via conv2d_13.tmp_00x55b7d0cdea20
I1018 08:53:57.784592 4097533 graph_helper.h:104] adj conv2d0x55b7d0bc90c0 -> cinn_launch0x55b7d0baa9c0  via conv2d_9.tmp_00x55b7d0bcbfb0
I1018 08:53:57.784595 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0ad8ab0 -> cinn_launch0x55b7d0baa9c0  via relu_6.tmp_00x55b7d0b843d0
I1018 08:53:57.784597 4097533 graph_helper.h:104] adj conv2d0x55b7d0b845a0 -> cinn_launch0x55b7d0ff9b20  via conv2d_8.tmp_00x55b7d0b87440
I1018 08:53:57.784600 4097533 graph_helper.h:104] adj conv2d0x55b7d0a2d8a0 -> cinn_launch0x55b7d09f9aa0  via conv2d_3.tmp_00x55b7d0a30740
I1018 08:53:57.784602 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c838e0 -> conv2d0x55b7d0e31fa0  via relu_14.tmp_00x55b7d0e31dd0
I1018 08:53:57.784605 4097533 graph_helper.h:104] adj conv2d0x55b7d0c102c0 -> cinn_launch0x55b7d0a71ff0  via conv2d_10.tmp_00x55b7d0c13160
I1018 08:53:57.784608 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0d1cbf0 -> conv2d0x55b7d0d678c0  via relu_12.tmp_00x55b7d0d676f0
I1018 08:53:57.784611 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f7a4e0 -> conv2d0x55b7d0d206c0  via relu_11.tmp_00x55b7d0d20510
I1018 08:53:57.784615 4097533 graph_helper.h:104] adj conv2d0x55b7d0deab80 -> cinn_launch0x55b7d0c838e0  via conv2d_17.tmp_00x55b7d0dedc30
I1018 08:53:57.784616 4097533 graph_helper.h:104] adj conv2d0x55b7d0dac400 -> cinn_launch0x55b7d0c838e0  via conv2d_16.tmp_00x55b7d0daf2f0
I1018 08:53:57.784619 4097533 graph_helper.h:104] adj conv2d0x55b7d0ab9ec0 -> cinn_launch0x55b7d0a97170  via conv2d_5.tmp_00x55b7d0abcd60
I1018 08:53:57.784622 4097533 graph_helper.h:104] adj conv2d0x55b7b85da860 -> cinn_launch0x55b7d0f710b0  via conv2d_0.tmp_00x55b7cd159bc0
I1018 08:53:57.784626 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0baa9c0 -> conv2d0x55b7d0c94760  via relu_8.tmp_00x55b7d0c100f0
I1018 08:53:57.784629 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0a71ff0 -> conv2d0x55b7d0c55fe0  via relu_9.tmp_00x55b7d0c55e30
I1018 08:53:57.784632 4097533 graph_helper.h:104] adj conv2d0x55b7d0b3d180 -> cinn_launch0x55b7d0ad8ab0  via conv2d_7.tmp_00x55b7d0b40230
I1018 08:53:57.784634 4097533 graph_helper.h:104] adj conv2d0x55b7d0afea00 -> cinn_launch0x55b7d0ad8ab0  via conv2d_6.tmp_00x55b7d0b018f0
I1018 08:53:57.784637 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0d1cbf0 -> conv2d0x55b7d0deab80  via relu_12.tmp_00x55b7d0d676f0
I1018 08:53:57.784641 4097533 graph_helper.h:104] adj cinn_launch0x55b7d09b53c0 -> conv2d0x55b7d0a2d8a0  via relu_2.tmp_00x55b7d0a2d6d0
I1018 08:53:57.784642 4097533 graph_helper.h:104] adj conv2d0x55b7d0d678c0 -> cinn_launch0x55b7d0c5af40  via conv2d_15.tmp_00x55b7d0d6a760
I1018 08:53:57.784646 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0a0c900 -> conv2d0x55b7d0ab9ec0  via relu_4.tmp_00x55b7d0ab9cf0
I1018 08:53:57.784649 4097533 graph_helper.h:104] adj conv2d0x55b7d09e5eb0 -> cinn_launch0x55b7d09b53c0  via conv2d_2.tmp_00x55b7d09e8da0
I1018 08:53:57.784651 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f710b0 -> cinn_launch0x55b7d09b53c0  via pool2d_0.tmp_00x55b7d09a1410
I1018 08:53:57.784654 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f710b0 -> conv2d0x55b7d09a1600  via pool2d_0.tmp_00x55b7d09a1410
I1018 08:53:57.784657 4097533 graph_helper.h:104] adj conv2d0x55b7d09a1600 -> cinn_launch0x55b7d0966630  via conv2d_1.tmp_00x55b7d09a4220
I1018 08:53:57.784660 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c5af40 -> conv2d0x55b7d0dac400  via relu_13.tmp_00x55b7d0dac250
I1018 08:53:57.784663 4097533 graph_helper.h:104] adj cinn_launch0x55b7d09f9aa0 -> conv2d0x55b7d0a723e0  via relu_3.tmp_00x55b7d0a72230
I1018 08:53:57.784665 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0a97170 -> conv2d0x55b7d0afea00  via relu_5.tmp_00x55b7d0afe850
I1018 08:53:57.784668 4097533 graph_helper.h:104] adj cinn_launch0x55b7d09b53c0 -> cinn_launch0x55b7d0a0c900  via relu_2.tmp_00x55b7d0a2d6d0
I1018 08:53:57.784670 4097533 graph_helper.h:104] adj conv2d0x55b7d0a723e0 -> cinn_launch0x55b7d0a0c900  via conv2d_4.tmp_00x55b7d0a752d0
I1018 08:53:57.784673 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0966630 -> conv2d0x55b7d09e5eb0  via relu_1.tmp_00x55b7d09e5d00
I1018 08:53:57.784675 4097533 graph_helper.h:104] adj conv2d0x55b7d0c94760 -> cinn_launch0x55b7d0bb7460  via conv2d_12.tmp_00x55b7d0c97810
I1018 08:53:57.784678 4097533 graph_helper.h:104] adj conv2d0x55b7d0c55fe0 -> cinn_launch0x55b7d0bb7460  via conv2d_11.tmp_00x55b7d0c58ed0
I1018 08:53:57.784680 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0ff9b20 -> conv2d0x55b7d0bc90c0  via relu_7.tmp_00x55b7d0bc8f10
I1018 08:53:57.784683 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0bb7460 -> conv2d0x55b7d0cdbb80  via relu_10.tmp_00x55b7d0cdb9b0
I1018 08:53:57.784686 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0e2ff90 -> conv2d0x55b7d0e76ae0  via relu_15.tmp_00x55b7d0e76930
I1018 08:53:57.784688 4097533 graph_helper.h:104] adj conv2d0x55b7d0e76ae0 -> cinn_launch0x55b7d0d06fb0  via conv2d_19.tmp_00x55b7d0e799d0
I1018 08:53:57.784691 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c838e0 -> cinn_launch0x55b7d0d06fb0  via relu_14.tmp_00x55b7d0e31dd0
I1018 08:53:57.784693 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0a0c900 -> conv2d0x55b7d0b3d180  via relu_4.tmp_00x55b7d0ab9cf0
I1018 08:53:57.784696 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0ad8ab0 -> conv2d0x55b7d0b845a0  via relu_6.tmp_00x55b7d0b843d0
I1018 08:53:57.784699 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0baa9c0 -> conv2d0x55b7d0c102c0  via relu_8.tmp_00x55b7d0c100f0
I1018 08:53:57.784859 4097533 build_strategy.cc:502] Finish Apply Pass build_cinn_pass
I1018 08:53:57.784861 4097533 build_strategy.cc:376] BuildStrategy::Apply pass:fuse_bn_add_act_pass
I1018 08:53:57.784868 4097533 build_strategy.cc:479] fuse_bn_add_act_pass is only supported on GPU, skipped.
I1018 08:53:57.784870 4097533 build_strategy.cc:376] BuildStrategy::Apply pass:coalesce_grad_tensor_pass
I1018 08:53:57.784873 4097533 build_strategy.cc:493] Start Apply Pass coalesce_grad_tensor_pass
I1018 08:53:57.784876 4097533 build_strategy.cc:496] Apply Pass coalesce_grad_tensor_passto SubGraph 0
I1018 08:53:57.784885 4097533 graph_helper.h:104] adj conv2d0x55b7d0e31fa0 -> cinn_launch0x55b7d0e2ff90  via conv2d_18.tmp_00x55b7d0e34e40
I1018 08:53:57.784888 4097533 graph_helper.h:104] adj conv2d0x55b7d0d206c0 -> cinn_launch0x55b7d0d1cbf0  via conv2d_14.tmp_00x55b7d0d235b0
I1018 08:53:57.784890 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0bb7460 -> cinn_launch0x55b7d0d1cbf0  via relu_10.tmp_00x55b7d0cdb9b0
I1018 08:53:57.784893 4097533 graph_helper.h:104] adj conv2d0x55b7d0cdbb80 -> cinn_launch0x55b7d0f7a4e0  via conv2d_13.tmp_00x55b7d0cdea20
I1018 08:53:57.784895 4097533 graph_helper.h:104] adj conv2d0x55b7d0bc90c0 -> cinn_launch0x55b7d0baa9c0  via conv2d_9.tmp_00x55b7d0bcbfb0
I1018 08:53:57.784897 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0ad8ab0 -> cinn_launch0x55b7d0baa9c0  via relu_6.tmp_00x55b7d0b843d0
I1018 08:53:57.784900 4097533 graph_helper.h:104] adj conv2d0x55b7d0b845a0 -> cinn_launch0x55b7d0ff9b20  via conv2d_8.tmp_00x55b7d0b87440
I1018 08:53:57.784902 4097533 graph_helper.h:104] adj conv2d0x55b7d0a2d8a0 -> cinn_launch0x55b7d09f9aa0  via conv2d_3.tmp_00x55b7d0a30740
I1018 08:53:57.784905 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c838e0 -> conv2d0x55b7d0e31fa0  via relu_14.tmp_00x55b7d0e31dd0
I1018 08:53:57.784909 4097533 graph_helper.h:104] adj conv2d0x55b7d0c102c0 -> cinn_launch0x55b7d0a71ff0  via conv2d_10.tmp_00x55b7d0c13160
I1018 08:53:57.784911 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0d1cbf0 -> conv2d0x55b7d0d678c0  via relu_12.tmp_00x55b7d0d676f0
I1018 08:53:57.784914 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f7a4e0 -> conv2d0x55b7d0d206c0  via relu_11.tmp_00x55b7d0d20510
I1018 08:53:57.784917 4097533 graph_helper.h:104] adj conv2d0x55b7d0deab80 -> cinn_launch0x55b7d0c838e0  via conv2d_17.tmp_00x55b7d0dedc30
I1018 08:53:57.784919 4097533 graph_helper.h:104] adj conv2d0x55b7d0dac400 -> cinn_launch0x55b7d0c838e0  via conv2d_16.tmp_00x55b7d0daf2f0
I1018 08:53:57.784921 4097533 graph_helper.h:104] adj conv2d0x55b7d0ab9ec0 -> cinn_launch0x55b7d0a97170  via conv2d_5.tmp_00x55b7d0abcd60
I1018 08:53:57.784924 4097533 graph_helper.h:104] adj conv2d0x55b7b85da860 -> cinn_launch0x55b7d0f710b0  via conv2d_0.tmp_00x55b7cd159bc0
I1018 08:53:57.784926 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0baa9c0 -> conv2d0x55b7d0c94760  via relu_8.tmp_00x55b7d0c100f0
I1018 08:53:57.784929 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0a71ff0 -> conv2d0x55b7d0c55fe0  via relu_9.tmp_00x55b7d0c55e30
I1018 08:53:57.784932 4097533 graph_helper.h:104] adj conv2d0x55b7d0b3d180 -> cinn_launch0x55b7d0ad8ab0  via conv2d_7.tmp_00x55b7d0b40230
I1018 08:53:57.784934 4097533 graph_helper.h:104] adj conv2d0x55b7d0afea00 -> cinn_launch0x55b7d0ad8ab0  via conv2d_6.tmp_00x55b7d0b018f0
I1018 08:53:57.784937 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0d1cbf0 -> conv2d0x55b7d0deab80  via relu_12.tmp_00x55b7d0d676f0
I1018 08:53:57.784940 4097533 graph_helper.h:104] adj cinn_launch0x55b7d09b53c0 -> conv2d0x55b7d0a2d8a0  via relu_2.tmp_00x55b7d0a2d6d0
I1018 08:53:57.784942 4097533 graph_helper.h:104] adj conv2d0x55b7d0d678c0 -> cinn_launch0x55b7d0c5af40  via conv2d_15.tmp_00x55b7d0d6a760
I1018 08:53:57.784945 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0a0c900 -> conv2d0x55b7d0ab9ec0  via relu_4.tmp_00x55b7d0ab9cf0
I1018 08:53:57.784947 4097533 graph_helper.h:104] adj conv2d0x55b7d09e5eb0 -> cinn_launch0x55b7d09b53c0  via conv2d_2.tmp_00x55b7d09e8da0
I1018 08:53:57.784950 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f710b0 -> cinn_launch0x55b7d09b53c0  via pool2d_0.tmp_00x55b7d09a1410
I1018 08:53:57.784952 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f710b0 -> conv2d0x55b7d09a1600  via pool2d_0.tmp_00x55b7d09a1410
I1018 08:53:57.784957 4097533 graph_helper.h:104] adj conv2d0x55b7d09a1600 -> cinn_launch0x55b7d0966630  via conv2d_1.tmp_00x55b7d09a4220
I1018 08:53:57.784960 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c5af40 -> conv2d0x55b7d0dac400  via relu_13.tmp_00x55b7d0dac250
I1018 08:53:57.784962 4097533 graph_helper.h:104] adj cinn_launch0x55b7d09f9aa0 -> conv2d0x55b7d0a723e0  via relu_3.tmp_00x55b7d0a72230
I1018 08:53:57.784965 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0a97170 -> conv2d0x55b7d0afea00  via relu_5.tmp_00x55b7d0afe850
I1018 08:53:57.784967 4097533 graph_helper.h:104] adj cinn_launch0x55b7d09b53c0 -> cinn_launch0x55b7d0a0c900  via relu_2.tmp_00x55b7d0a2d6d0
I1018 08:53:57.784970 4097533 graph_helper.h:104] adj conv2d0x55b7d0a723e0 -> cinn_launch0x55b7d0a0c900  via conv2d_4.tmp_00x55b7d0a752d0
I1018 08:53:57.784972 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0966630 -> conv2d0x55b7d09e5eb0  via relu_1.tmp_00x55b7d09e5d00
I1018 08:53:57.784976 4097533 graph_helper.h:104] adj conv2d0x55b7d0c94760 -> cinn_launch0x55b7d0bb7460  via conv2d_12.tmp_00x55b7d0c97810
I1018 08:53:57.784977 4097533 graph_helper.h:104] adj conv2d0x55b7d0c55fe0 -> cinn_launch0x55b7d0bb7460  via conv2d_11.tmp_00x55b7d0c58ed0
I1018 08:53:57.784981 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0ff9b20 -> conv2d0x55b7d0bc90c0  via relu_7.tmp_00x55b7d0bc8f10
I1018 08:53:57.784982 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0bb7460 -> conv2d0x55b7d0cdbb80  via relu_10.tmp_00x55b7d0cdb9b0
I1018 08:53:57.784986 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0e2ff90 -> conv2d0x55b7d0e76ae0  via relu_15.tmp_00x55b7d0e76930
I1018 08:53:57.784988 4097533 graph_helper.h:104] adj conv2d0x55b7d0e76ae0 -> cinn_launch0x55b7d0d06fb0  via conv2d_19.tmp_00x55b7d0e799d0
I1018 08:53:57.784991 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c838e0 -> cinn_launch0x55b7d0d06fb0  via relu_14.tmp_00x55b7d0e31dd0
I1018 08:53:57.784992 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0a0c900 -> conv2d0x55b7d0b3d180  via relu_4.tmp_00x55b7d0ab9cf0
I1018 08:53:57.784996 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0ad8ab0 -> conv2d0x55b7d0b845a0  via relu_6.tmp_00x55b7d0b843d0
I1018 08:53:57.784998 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0baa9c0 -> conv2d0x55b7d0c102c0  via relu_8.tmp_00x55b7d0c100f0
I1018 08:53:57.785118 4097533 build_strategy.cc:502] Finish Apply Pass coalesce_grad_tensor_pass
I1018 08:53:57.785120 4097533 build_strategy.cc:376] BuildStrategy::Apply pass:add_reader_dependency_pass
I1018 08:53:57.785123 4097533 build_strategy.cc:493] Start Apply Pass add_reader_dependency_pass
I1018 08:53:57.785125 4097533 build_strategy.cc:496] Apply Pass add_reader_dependency_passto SubGraph 0
I1018 08:53:57.785147 4097533 graph_helper.h:104] adj conv2d0x55b7d0e31fa0 -> cinn_launch0x55b7d0e2ff90  via conv2d_18.tmp_00x55b7d0e34e40
I1018 08:53:57.785151 4097533 graph_helper.h:104] adj conv2d0x55b7d0d206c0 -> cinn_launch0x55b7d0d1cbf0  via conv2d_14.tmp_00x55b7d0d235b0
I1018 08:53:57.785152 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0bb7460 -> cinn_launch0x55b7d0d1cbf0  via relu_10.tmp_00x55b7d0cdb9b0
I1018 08:53:57.785156 4097533 graph_helper.h:104] adj conv2d0x55b7d0cdbb80 -> cinn_launch0x55b7d0f7a4e0  via conv2d_13.tmp_00x55b7d0cdea20
I1018 08:53:57.785157 4097533 graph_helper.h:104] adj conv2d0x55b7d0bc90c0 -> cinn_launch0x55b7d0baa9c0  via conv2d_9.tmp_00x55b7d0bcbfb0
I1018 08:53:57.785159 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0ad8ab0 -> cinn_launch0x55b7d0baa9c0  via relu_6.tmp_00x55b7d0b843d0
I1018 08:53:57.785162 4097533 graph_helper.h:104] adj conv2d0x55b7d0b845a0 -> cinn_launch0x55b7d0ff9b20  via conv2d_8.tmp_00x55b7d0b87440
I1018 08:53:57.785164 4097533 graph_helper.h:104] adj conv2d0x55b7d0a2d8a0 -> cinn_launch0x55b7d09f9aa0  via conv2d_3.tmp_00x55b7d0a30740
I1018 08:53:57.785167 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c838e0 -> conv2d0x55b7d0e31fa0  via relu_14.tmp_00x55b7d0e31dd0
I1018 08:53:57.785169 4097533 graph_helper.h:104] adj conv2d0x55b7d0c102c0 -> cinn_launch0x55b7d0a71ff0  via conv2d_10.tmp_00x55b7d0c13160
I1018 08:53:57.785174 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0d1cbf0 -> conv2d0x55b7d0d678c0  via relu_12.tmp_00x55b7d0d676f0
I1018 08:53:57.785178 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f7a4e0 -> conv2d0x55b7d0d206c0  via relu_11.tmp_00x55b7d0d20510
I1018 08:53:57.785180 4097533 graph_helper.h:104] adj conv2d0x55b7d0deab80 -> cinn_launch0x55b7d0c838e0  via conv2d_17.tmp_00x55b7d0dedc30
I1018 08:53:57.785182 4097533 graph_helper.h:104] adj conv2d0x55b7d0dac400 -> cinn_launch0x55b7d0c838e0  via conv2d_16.tmp_00x55b7d0daf2f0
I1018 08:53:57.785185 4097533 graph_helper.h:104] adj conv2d0x55b7d0ab9ec0 -> cinn_launch0x55b7d0a97170  via conv2d_5.tmp_00x55b7d0abcd60
I1018 08:53:57.785187 4097533 graph_helper.h:104] adj conv2d0x55b7b85da860 -> cinn_launch0x55b7d0f710b0  via conv2d_0.tmp_00x55b7cd159bc0
I1018 08:53:57.785190 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0baa9c0 -> conv2d0x55b7d0c94760  via relu_8.tmp_00x55b7d0c100f0
I1018 08:53:57.785192 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0a71ff0 -> conv2d0x55b7d0c55fe0  via relu_9.tmp_00x55b7d0c55e30
I1018 08:53:57.785195 4097533 graph_helper.h:104] adj conv2d0x55b7d0b3d180 -> cinn_launch0x55b7d0ad8ab0  via conv2d_7.tmp_00x55b7d0b40230
I1018 08:53:57.785197 4097533 graph_helper.h:104] adj conv2d0x55b7d0afea00 -> cinn_launch0x55b7d0ad8ab0  via conv2d_6.tmp_00x55b7d0b018f0
I1018 08:53:57.785200 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0d1cbf0 -> conv2d0x55b7d0deab80  via relu_12.tmp_00x55b7d0d676f0
I1018 08:53:57.785202 4097533 graph_helper.h:104] adj cinn_launch0x55b7d09b53c0 -> conv2d0x55b7d0a2d8a0  via relu_2.tmp_00x55b7d0a2d6d0
I1018 08:53:57.785205 4097533 graph_helper.h:104] adj conv2d0x55b7d0d678c0 -> cinn_launch0x55b7d0c5af40  via conv2d_15.tmp_00x55b7d0d6a760
I1018 08:53:57.785221 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0a0c900 -> conv2d0x55b7d0ab9ec0  via relu_4.tmp_00x55b7d0ab9cf0
I1018 08:53:57.785224 4097533 graph_helper.h:104] adj conv2d0x55b7d09e5eb0 -> cinn_launch0x55b7d09b53c0  via conv2d_2.tmp_00x55b7d09e8da0
I1018 08:53:57.785226 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f710b0 -> cinn_launch0x55b7d09b53c0  via pool2d_0.tmp_00x55b7d09a1410
I1018 08:53:57.785228 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f710b0 -> conv2d0x55b7d09a1600  via pool2d_0.tmp_00x55b7d09a1410
I1018 08:53:57.785231 4097533 graph_helper.h:104] adj conv2d0x55b7d09a1600 -> cinn_launch0x55b7d0966630  via conv2d_1.tmp_00x55b7d09a4220
I1018 08:53:57.785234 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c5af40 -> conv2d0x55b7d0dac400  via relu_13.tmp_00x55b7d0dac250
I1018 08:53:57.785236 4097533 graph_helper.h:104] adj cinn_launch0x55b7d09f9aa0 -> conv2d0x55b7d0a723e0  via relu_3.tmp_00x55b7d0a72230
I1018 08:53:57.785239 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0a97170 -> conv2d0x55b7d0afea00  via relu_5.tmp_00x55b7d0afe850
I1018 08:53:57.785241 4097533 graph_helper.h:104] adj cinn_launch0x55b7d09b53c0 -> cinn_launch0x55b7d0a0c900  via relu_2.tmp_00x55b7d0a2d6d0
I1018 08:53:57.785244 4097533 graph_helper.h:104] adj conv2d0x55b7d0a723e0 -> cinn_launch0x55b7d0a0c900  via conv2d_4.tmp_00x55b7d0a752d0
I1018 08:53:57.785246 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0966630 -> conv2d0x55b7d09e5eb0  via relu_1.tmp_00x55b7d09e5d00
I1018 08:53:57.785249 4097533 graph_helper.h:104] adj conv2d0x55b7d0c94760 -> cinn_launch0x55b7d0bb7460  via conv2d_12.tmp_00x55b7d0c97810
I1018 08:53:57.785251 4097533 graph_helper.h:104] adj conv2d0x55b7d0c55fe0 -> cinn_launch0x55b7d0bb7460  via conv2d_11.tmp_00x55b7d0c58ed0
I1018 08:53:57.785254 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0ff9b20 -> conv2d0x55b7d0bc90c0  via relu_7.tmp_00x55b7d0bc8f10
I1018 08:53:57.785256 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0bb7460 -> conv2d0x55b7d0cdbb80  via relu_10.tmp_00x55b7d0cdb9b0
I1018 08:53:57.785259 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0e2ff90 -> conv2d0x55b7d0e76ae0  via relu_15.tmp_00x55b7d0e76930
I1018 08:53:57.785264 4097533 graph_helper.h:104] adj conv2d0x55b7d0e76ae0 -> cinn_launch0x55b7d0d06fb0  via conv2d_19.tmp_00x55b7d0e799d0
I1018 08:53:57.785265 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c838e0 -> cinn_launch0x55b7d0d06fb0  via relu_14.tmp_00x55b7d0e31dd0
I1018 08:53:57.785269 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0a0c900 -> conv2d0x55b7d0b3d180  via relu_4.tmp_00x55b7d0ab9cf0
I1018 08:53:57.785270 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0ad8ab0 -> conv2d0x55b7d0b845a0  via relu_6.tmp_00x55b7d0b843d0
I1018 08:53:57.785274 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0baa9c0 -> conv2d0x55b7d0c102c0  via relu_8.tmp_00x55b7d0c100f0
I1018 08:53:57.785389 4097533 build_strategy.cc:502] Finish Apply Pass add_reader_dependency_pass
I1018 08:53:57.785391 4097533 build_strategy.cc:376] BuildStrategy::Apply pass:all_reduce_mode_multi_devices_pass
I1018 08:53:57.785396 4097533 build_strategy.cc:493] Start Apply Pass all_reduce_mode_multi_devices_pass
I1018 08:53:57.785398 4097533 build_strategy.cc:496] Apply Pass all_reduce_mode_multi_devices_passto SubGraph 0
I1018 08:53:57.785419 4097533 graph_helper.h:104] adj conv2d0x55b7d0e31fa0 -> cinn_launch0x55b7d0e2ff90  via conv2d_18.tmp_00x55b7d0e34e40
I1018 08:53:57.785421 4097533 graph_helper.h:104] adj conv2d0x55b7d0d206c0 -> cinn_launch0x55b7d0d1cbf0  via conv2d_14.tmp_00x55b7d0d235b0
I1018 08:53:57.785424 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0bb7460 -> cinn_launch0x55b7d0d1cbf0  via relu_10.tmp_00x55b7d0cdb9b0
I1018 08:53:57.785426 4097533 graph_helper.h:104] adj conv2d0x55b7d0cdbb80 -> cinn_launch0x55b7d0f7a4e0  via conv2d_13.tmp_00x55b7d0cdea20
I1018 08:53:57.785429 4097533 graph_helper.h:104] adj conv2d0x55b7d0bc90c0 -> cinn_launch0x55b7d0baa9c0  via conv2d_9.tmp_00x55b7d0bcbfb0
I1018 08:53:57.785431 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0ad8ab0 -> cinn_launch0x55b7d0baa9c0  via relu_6.tmp_00x55b7d0b843d0
I1018 08:53:57.785434 4097533 graph_helper.h:104] adj conv2d0x55b7d0b845a0 -> cinn_launch0x55b7d0ff9b20  via conv2d_8.tmp_00x55b7d0b87440
I1018 08:53:57.785436 4097533 graph_helper.h:104] adj conv2d0x55b7d0a2d8a0 -> cinn_launch0x55b7d09f9aa0  via conv2d_3.tmp_00x55b7d0a30740
I1018 08:53:57.785439 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c838e0 -> conv2d0x55b7d0e31fa0  via relu_14.tmp_00x55b7d0e31dd0
I1018 08:53:57.785441 4097533 graph_helper.h:104] adj conv2d0x55b7d0c102c0 -> cinn_launch0x55b7d0a71ff0  via conv2d_10.tmp_00x55b7d0c13160
I1018 08:53:57.785444 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0d1cbf0 -> conv2d0x55b7d0d678c0  via relu_12.tmp_00x55b7d0d676f0
I1018 08:53:57.785446 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f7a4e0 -> conv2d0x55b7d0d206c0  via relu_11.tmp_00x55b7d0d20510
I1018 08:53:57.785449 4097533 graph_helper.h:104] adj conv2d0x55b7d0deab80 -> cinn_launch0x55b7d0c838e0  via conv2d_17.tmp_00x55b7d0dedc30
I1018 08:53:57.785451 4097533 graph_helper.h:104] adj conv2d0x55b7d0dac400 -> cinn_launch0x55b7d0c838e0  via conv2d_16.tmp_00x55b7d0daf2f0
I1018 08:53:57.785454 4097533 graph_helper.h:104] adj conv2d0x55b7d0ab9ec0 -> cinn_launch0x55b7d0a97170  via conv2d_5.tmp_00x55b7d0abcd60
I1018 08:53:57.785456 4097533 graph_helper.h:104] adj conv2d0x55b7b85da860 -> cinn_launch0x55b7d0f710b0  via conv2d_0.tmp_00x55b7cd159bc0
I1018 08:53:57.785459 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0baa9c0 -> conv2d0x55b7d0c94760  via relu_8.tmp_00x55b7d0c100f0
I1018 08:53:57.785461 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0a71ff0 -> conv2d0x55b7d0c55fe0  via relu_9.tmp_00x55b7d0c55e30
I1018 08:53:57.785465 4097533 graph_helper.h:104] adj conv2d0x55b7d0b3d180 -> cinn_launch0x55b7d0ad8ab0  via conv2d_7.tmp_00x55b7d0b40230
I1018 08:53:57.785466 4097533 graph_helper.h:104] adj conv2d0x55b7d0afea00 -> cinn_launch0x55b7d0ad8ab0  via conv2d_6.tmp_00x55b7d0b018f0
I1018 08:53:57.785470 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0d1cbf0 -> conv2d0x55b7d0deab80  via relu_12.tmp_00x55b7d0d676f0
I1018 08:53:57.785471 4097533 graph_helper.h:104] adj cinn_launch0x55b7d09b53c0 -> conv2d0x55b7d0a2d8a0  via relu_2.tmp_00x55b7d0a2d6d0
I1018 08:53:57.785476 4097533 graph_helper.h:104] adj conv2d0x55b7d0d678c0 -> cinn_launch0x55b7d0c5af40  via conv2d_15.tmp_00x55b7d0d6a760
I1018 08:53:57.785480 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0a0c900 -> conv2d0x55b7d0ab9ec0  via relu_4.tmp_00x55b7d0ab9cf0
I1018 08:53:57.785481 4097533 graph_helper.h:104] adj conv2d0x55b7d09e5eb0 -> cinn_launch0x55b7d09b53c0  via conv2d_2.tmp_00x55b7d09e8da0
I1018 08:53:57.785485 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f710b0 -> cinn_launch0x55b7d09b53c0  via pool2d_0.tmp_00x55b7d09a1410
I1018 08:53:57.785486 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f710b0 -> conv2d0x55b7d09a1600  via pool2d_0.tmp_00x55b7d09a1410
I1018 08:53:57.785490 4097533 graph_helper.h:104] adj conv2d0x55b7d09a1600 -> cinn_launch0x55b7d0966630  via conv2d_1.tmp_00x55b7d09a4220
I1018 08:53:57.785491 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c5af40 -> conv2d0x55b7d0dac400  via relu_13.tmp_00x55b7d0dac250
I1018 08:53:57.785494 4097533 graph_helper.h:104] adj cinn_launch0x55b7d09f9aa0 -> conv2d0x55b7d0a723e0  via relu_3.tmp_00x55b7d0a72230
I1018 08:53:57.785497 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0a97170 -> conv2d0x55b7d0afea00  via relu_5.tmp_00x55b7d0afe850
I1018 08:53:57.785499 4097533 graph_helper.h:104] adj cinn_launch0x55b7d09b53c0 -> cinn_launch0x55b7d0a0c900  via relu_2.tmp_00x55b7d0a2d6d0
I1018 08:53:57.785501 4097533 graph_helper.h:104] adj conv2d0x55b7d0a723e0 -> cinn_launch0x55b7d0a0c900  via conv2d_4.tmp_00x55b7d0a752d0
I1018 08:53:57.785504 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0966630 -> conv2d0x55b7d09e5eb0  via relu_1.tmp_00x55b7d09e5d00
I1018 08:53:57.785506 4097533 graph_helper.h:104] adj conv2d0x55b7d0c94760 -> cinn_launch0x55b7d0bb7460  via conv2d_12.tmp_00x55b7d0c97810
I1018 08:53:57.785508 4097533 graph_helper.h:104] adj conv2d0x55b7d0c55fe0 -> cinn_launch0x55b7d0bb7460  via conv2d_11.tmp_00x55b7d0c58ed0
I1018 08:53:57.785511 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0ff9b20 -> conv2d0x55b7d0bc90c0  via relu_7.tmp_00x55b7d0bc8f10
I1018 08:53:57.785513 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0bb7460 -> conv2d0x55b7d0cdbb80  via relu_10.tmp_00x55b7d0cdb9b0
I1018 08:53:57.785516 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0e2ff90 -> conv2d0x55b7d0e76ae0  via relu_15.tmp_00x55b7d0e76930
I1018 08:53:57.785518 4097533 graph_helper.h:104] adj conv2d0x55b7d0e76ae0 -> cinn_launch0x55b7d0d06fb0  via conv2d_19.tmp_00x55b7d0e799d0
I1018 08:53:57.785521 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c838e0 -> cinn_launch0x55b7d0d06fb0  via relu_14.tmp_00x55b7d0e31dd0
I1018 08:53:57.785523 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0a0c900 -> conv2d0x55b7d0b3d180  via relu_4.tmp_00x55b7d0ab9cf0
I1018 08:53:57.785526 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0ad8ab0 -> conv2d0x55b7d0b845a0  via relu_6.tmp_00x55b7d0b843d0
I1018 08:53:57.785528 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0baa9c0 -> conv2d0x55b7d0c102c0  via relu_8.tmp_00x55b7d0c100f0
I1018 08:53:57.786620 4097533 graph.h:183] deleting ops
I1018 08:53:57.786758 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> cinn_launch0x55b7d1002020  via __control_var@14800x55b7d1043cd0
I1018 08:53:57.786762 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> cinn_launch0x55b7d1000c10  via __control_var@14720x55b7d1042590
I1018 08:53:57.786765 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d0ffcfe0  via __control_var@14740x55b7d1042b70
I1018 08:53:57.786768 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d0ffbc30  via __control_var@14750x55b7d1042e40
I1018 08:53:57.786770 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d1068fa0  via __control_var@15090x55b7d101e2a0
I1018 08:53:57.786773 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d1067b90  via __control_var@15010x55b7d101cbc0
I1018 08:53:57.786775 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> cinn_launch0x55b7d1066780  via __control_var@14980x55b7d101c330
I1018 08:53:57.786782 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> cinn_launch0x55b7d1065370  via __control_var@14900x55b7d101ac70
I1018 08:53:57.786784 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> cinn_launch0x55b7d1063f60  via __control_var@15070x55b7d101dd40
I1018 08:53:57.786787 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> cinn_launch0x55b7d1061740  via __control_var@15080x55b7d101dfd0
I1018 08:53:57.786789 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> cinn_launch0x55b7d0fe13a0  via __control_var@14920x55b7d101b1f0
I1018 08:53:57.786792 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdeb80  via __control_var@15030x55b7d101d160
I1018 08:53:57.786794 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdd770  via __control_var@14850x55b7d1044b40
I1018 08:53:57.786796 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdc360  via __control_var@15040x55b7d101d4d0
I1018 08:53:57.786798 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> cinn_launch0x55b7d0fdaf50  via __control_var@14970x55b7d101c060
I1018 08:53:57.786801 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> cinn_launch0x55b7d0fd9b40  via __control_var@14930x55b7d101b4e0
I1018 08:53:57.786803 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> cinn_launch0x55b7d0fd8730  via __control_var@14820x55b7d1044290
I1018 08:53:57.786806 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> cinn_launch0x55b7d1104d40  via __control_var@14840x55b7d1044870
I1018 08:53:57.786808 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> cinn_launch0x55b7d1103930  via __control_var@15000x55b7d101c8f0
I1018 08:53:57.786810 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> cinn_launch0x55b7d1102520  via __control_var@14990x55b7d101c600
I1018 08:53:57.786813 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d1101110  via __control_var@14880x55b7d10453e0
I1018 08:53:57.786815 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10ffd00  via __control_var@14890x55b7d10456b0
I1018 08:53:57.786818 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10fe8f0  via __control_var@14860x55b7d1044e10
I1018 08:53:57.786820 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10fd4e0  via __control_var@14760x55b7d10431b0
I1018 08:53:57.786823 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> cinn_launch0x55b7d1113ec0  via __control_var@14790x55b7d1043a00
I1018 08:53:57.786824 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> cinn_launch0x55b7d1112ab0  via __control_var@14780x55b7d1043710
I1018 08:53:57.786827 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> cinn_launch0x55b7d11116a0  via __control_var@14830x55b7d1044580
I1018 08:53:57.786829 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> cinn_launch0x55b7d1110290  via __control_var@14940x55b7d101b7d0
I1018 08:53:57.786832 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> cinn_launch0x55b7d110ee80  via __control_var@15060x55b7d101da70
I1018 08:53:57.786834 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> cinn_launch0x55b7d110da70  via __control_var@14910x55b7d101af00
I1018 08:53:57.786836 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d110c6a0  via __control_var@14960x55b7d101bd70
I1018 08:53:57.786839 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d107f940  via __control_var@15020x55b7d101ce90
I1018 08:53:57.786841 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> cinn_launch0x55b7d107e530  via __control_var@14770x55b7d1043480
I1018 08:53:57.786844 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> cinn_launch0x55b7d107d120  via __control_var@14950x55b7d101baa0
I1018 08:53:57.786846 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d107bd10  via __control_var@14870x55b7d1045110
I1018 08:53:57.786850 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d107a890  via __control_var@14810x55b7d1043fa0
I1018 08:53:57.786854 4097533 graph_helper.h:104] adj conv2d0x55b7d106f840 -> cinn_launch0x55b7d109c290  via conv2d_19.tmp_00x55b7d109bfc0
I1018 08:53:57.786855 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d109c290  via relu_14.tmp_00x55b7d10900a0
I1018 08:53:57.786857 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> conv2d0x55b7d106f840  via relu_15.tmp_00x55b7d106f450
I1018 08:53:57.786860 4097533 graph_helper.h:104] adj conv2d0x55b7d10908e0 -> cinn_launch0x55b7d106aaf0  via conv2d_18.tmp_00x55b7d106a820
I1018 08:53:57.786863 4097533 graph_helper.h:104] adj conv2d0x55b7d0fc2680 -> cinn_launch0x55b7d1126750  via conv2d_17.tmp_00x55b7d11264a0
I1018 08:53:57.786865 4097533 graph_helper.h:104] adj conv2d0x55b7d0fb56d0 -> cinn_launch0x55b7d1126750  via conv2d_16.tmp_00x55b7d0fc23b0
I1018 08:53:57.786867 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> conv2d0x55b7d0fc2680  via relu_12.tmp_00x55b7d1122b70
I1018 08:53:57.786870 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> cinn_launch0x55b7d1062b50  via __control_var@15110x55b7d101e840
I1018 08:53:57.786872 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> conv2d0x55b7d0fb56d0  via relu_13.tmp_00x55b7d0fb52e0
I1018 08:53:57.786875 4097533 graph_helper.h:104] adj conv2d0x55b7d1122f60 -> cinn_launch0x55b7d0fd6ee0  via conv2d_15.tmp_00x55b7d0fd6c10
I1018 08:53:57.786877 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> conv2d0x55b7d1122f60  via relu_12.tmp_00x55b7d1122b70
I1018 08:53:57.786880 4097533 graph_helper.h:104] adj conv2d0x55b7d11091c0 -> cinn_launch0x55b7d0fea800  via conv2d_14.tmp_00x55b7d0fea530
I1018 08:53:57.786882 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fea800  via relu_10.tmp_00x55b7d102b4b0
I1018 08:53:57.786885 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> conv2d0x55b7d11091c0  via relu_11.tmp_00x55b7d1108dd0
I1018 08:53:57.786887 4097533 graph_helper.h:104] adj conv2d0x55b7d102bc70 -> cinn_launch0x55b7d1115560  via conv2d_13.tmp_00x55b7d11184f0
I1018 08:53:57.786890 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> conv2d0x55b7d102bc70  via relu_10.tmp_00x55b7d102b4b0
I1018 08:53:57.786892 4097533 graph_helper.h:104] adj conv2d0x55b7d0d441e0 -> cinn_launch0x55b7d1031e80  via conv2d_12.tmp_00x55b7d1031bd0
I1018 08:53:57.786895 4097533 graph_helper.h:104] adj conv2d0x55b7d0e99aa0 -> cinn_launch0x55b7d1031e80  via conv2d_11.tmp_00x55b7d0d43f10
I1018 08:53:57.786897 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> conv2d0x55b7d0d441e0  via relu_8.tmp_00x55b7d1029a90
I1018 08:53:57.786899 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> conv2d0x55b7d0e99aa0  via relu_9.tmp_00x55b7d0e996b0
I1018 08:53:57.786902 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> conv2d0x55b7d1029e80  via relu_8.tmp_00x55b7d1029a90
I1018 08:53:57.786904 4097533 graph_helper.h:104] adj conv2d0x55b7d0ec1350 -> cinn_launch0x55b7d1047f30  via conv2d_9.tmp_00x55b7d1047c60
I1018 08:53:57.786907 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d1047f30  via relu_6.tmp_00x55b7d0e88b80
I1018 08:53:57.786909 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> conv2d0x55b7d0ec1350  via relu_7.tmp_00x55b7d0e0f4e0
I1018 08:53:57.786912 4097533 graph_helper.h:104] adj conv2d0x55b7d0e89340 -> cinn_launch0x55b7d0f0e1c0  via conv2d_8.tmp_00x55b7d0f0def0
I1018 08:53:57.786914 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> conv2d0x55b7d0e89340  via relu_6.tmp_00x55b7d0e88b80
I1018 08:53:57.786917 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> cinn_launch0x55b7d0ffe3f0  via __control_var@14730x55b7d1042880
I1018 08:53:57.786919 4097533 graph_helper.h:104] adj conv2d0x55b7d1119e20 -> cinn_launch0x55b7d0add8b0  via conv2d_7.tmp_00x55b7d0add600
I1018 08:53:57.786924 4097533 graph_helper.h:104] adj conv2d0x55b7d0d89b40 -> cinn_launch0x55b7d0add8b0  via conv2d_6.tmp_00x55b7d1119b50
I1018 08:53:57.786926 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> conv2d0x55b7d1119e20  via relu_4.tmp_00x55b7d111b9f0
I1018 08:53:57.786929 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> conv2d0x55b7d0d89b40  via relu_5.tmp_00x55b7d0d89750
I1018 08:53:57.786931 4097533 graph_helper.h:104] adj conv2d0x55b7d111bde0 -> cinn_launch0x55b7d0dcf300  via conv2d_5.tmp_00x55b7d0dcf030
I1018 08:53:57.786934 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> conv2d0x55b7d111bde0  via relu_4.tmp_00x55b7d111b9f0
I1018 08:53:57.786936 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d0c640b0  via relu_2.tmp_00x55b7d0c74b50
I1018 08:53:57.786939 4097533 graph_helper.h:104] adj conv2d0x55b7d0b70160 -> cinn_launch0x55b7d0c640b0  via conv2d_4.tmp_00x55b7d0c63de0
I1018 08:53:57.786942 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> conv2d0x55b7d0b70160  via relu_3.tmp_00x55b7d0b6fd70
I1018 08:53:57.786944 4097533 graph_helper.h:104] adj conv2d0x55b7d0c74ec0 -> cinn_launch0x55b7d0c9d5e0  via conv2d_3.tmp_00x55b7d0e7f460
I1018 08:53:57.786947 4097533 graph_helper.h:104] adj conv2d0x55b7d1029e80 -> cinn_launch0x55b7d0cb5e70  via conv2d_10.tmp_00x55b7d0cb8f30
I1018 08:53:57.786949 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> conv2d0x55b7d0c74ec0  via relu_2.tmp_00x55b7d0c74b50
I1018 08:53:57.786952 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdff90  via __control_var@15050x55b7d101d7a0
I1018 08:53:57.786954 4097533 graph_helper.h:104] adj conv2d0x55b7d0edeb50 -> cinn_launch0x55b7d0cfb320  via conv2d_2.tmp_00x55b7d0cfb110
I1018 08:53:57.786957 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d0cfb320  via pool2d_0.tmp_00x55b7d0b929b0
I1018 08:53:57.786959 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> conv2d0x55b7d0edeb50  via relu_1.tmp_00x55b7d0de1f20
I1018 08:53:57.786962 4097533 graph_helper.h:104] adj conv2d0x55b7d0ddf060 -> cinn_launch0x55b7d0c892a0  via conv2d_1.tmp_00x55b7d0c89030
I1018 08:53:57.786964 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> conv2d0x55b7d0ddf060  via pool2d_0.tmp_00x55b7d0b929b0
I1018 08:53:57.786967 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> conv2d0x55b7d10908e0  via relu_14.tmp_00x55b7d10900a0
I1018 08:53:57.786969 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> cinn_launch0x55b7d0fff800  via __control_var@15100x55b7d101e570
I1018 08:53:57.786971 4097533 graph_helper.h:104] adj conv2d0x55b7d0edf1d0 -> cinn_launch0x55b7d0fac1b0  via conv2d_0.tmp_00x55b7d0eaa280
I1018 08:53:57.787168 4097533 build_strategy.cc:502] Finish Apply Pass all_reduce_mode_multi_devices_pass
I1018 08:53:57.787171 4097533 build_strategy.cc:376] BuildStrategy::Apply pass:fuse_all_reduce_op_pass
I1018 08:53:57.787176 4097533 build_strategy.cc:493] Start Apply Pass fuse_all_reduce_op_pass
I1018 08:53:57.787178 4097533 build_strategy.cc:496] Apply Pass fuse_all_reduce_op_passto SubGraph 0
I1018 08:53:57.787185 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> cinn_launch0x55b7d1002020  via __control_var@14800x55b7d1043cd0
I1018 08:53:57.787189 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> cinn_launch0x55b7d1000c10  via __control_var@14720x55b7d1042590
I1018 08:53:57.787190 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d0ffcfe0  via __control_var@14740x55b7d1042b70
I1018 08:53:57.787194 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d0ffbc30  via __control_var@14750x55b7d1042e40
I1018 08:53:57.787195 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d1068fa0  via __control_var@15090x55b7d101e2a0
I1018 08:53:57.787197 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d1067b90  via __control_var@15010x55b7d101cbc0
I1018 08:53:57.787204 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> cinn_launch0x55b7d1066780  via __control_var@14980x55b7d101c330
I1018 08:53:57.787205 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> cinn_launch0x55b7d1065370  via __control_var@14900x55b7d101ac70
I1018 08:53:57.787207 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> cinn_launch0x55b7d1063f60  via __control_var@15070x55b7d101dd40
I1018 08:53:57.787210 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> cinn_launch0x55b7d1061740  via __control_var@15080x55b7d101dfd0
I1018 08:53:57.787212 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> cinn_launch0x55b7d0fe13a0  via __control_var@14920x55b7d101b1f0
I1018 08:53:57.787215 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdeb80  via __control_var@15030x55b7d101d160
I1018 08:53:57.787217 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdd770  via __control_var@14850x55b7d1044b40
I1018 08:53:57.787220 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdc360  via __control_var@15040x55b7d101d4d0
I1018 08:53:57.787222 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> cinn_launch0x55b7d0fdaf50  via __control_var@14970x55b7d101c060
I1018 08:53:57.787225 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> cinn_launch0x55b7d0fd9b40  via __control_var@14930x55b7d101b4e0
I1018 08:53:57.787226 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> cinn_launch0x55b7d0fd8730  via __control_var@14820x55b7d1044290
I1018 08:53:57.787230 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> cinn_launch0x55b7d1104d40  via __control_var@14840x55b7d1044870
I1018 08:53:57.787231 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> cinn_launch0x55b7d1103930  via __control_var@15000x55b7d101c8f0
I1018 08:53:57.787233 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> cinn_launch0x55b7d1102520  via __control_var@14990x55b7d101c600
I1018 08:53:57.787235 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d1101110  via __control_var@14880x55b7d10453e0
I1018 08:53:57.787238 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10ffd00  via __control_var@14890x55b7d10456b0
I1018 08:53:57.787240 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10fe8f0  via __control_var@14860x55b7d1044e10
I1018 08:53:57.787242 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10fd4e0  via __control_var@14760x55b7d10431b0
I1018 08:53:57.787245 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> cinn_launch0x55b7d1113ec0  via __control_var@14790x55b7d1043a00
I1018 08:53:57.787247 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> cinn_launch0x55b7d1112ab0  via __control_var@14780x55b7d1043710
I1018 08:53:57.787249 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> cinn_launch0x55b7d11116a0  via __control_var@14830x55b7d1044580
I1018 08:53:57.787251 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> cinn_launch0x55b7d1110290  via __control_var@14940x55b7d101b7d0
I1018 08:53:57.787254 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> cinn_launch0x55b7d110ee80  via __control_var@15060x55b7d101da70
I1018 08:53:57.787256 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> cinn_launch0x55b7d110da70  via __control_var@14910x55b7d101af00
I1018 08:53:57.787259 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d110c6a0  via __control_var@14960x55b7d101bd70
I1018 08:53:57.787261 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d107f940  via __control_var@15020x55b7d101ce90
I1018 08:53:57.787263 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> cinn_launch0x55b7d107e530  via __control_var@14770x55b7d1043480
I1018 08:53:57.787266 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> cinn_launch0x55b7d107d120  via __control_var@14950x55b7d101baa0
I1018 08:53:57.787271 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d107bd10  via __control_var@14870x55b7d1045110
I1018 08:53:57.787273 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d107a890  via __control_var@14810x55b7d1043fa0
I1018 08:53:57.787276 4097533 graph_helper.h:104] adj conv2d0x55b7d106f840 -> cinn_launch0x55b7d109c290  via conv2d_19.tmp_00x55b7d109bfc0
I1018 08:53:57.787277 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d109c290  via relu_14.tmp_00x55b7d10900a0
I1018 08:53:57.787281 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> conv2d0x55b7d106f840  via relu_15.tmp_00x55b7d106f450
I1018 08:53:57.787282 4097533 graph_helper.h:104] adj conv2d0x55b7d10908e0 -> cinn_launch0x55b7d106aaf0  via conv2d_18.tmp_00x55b7d106a820
I1018 08:53:57.787285 4097533 graph_helper.h:104] adj conv2d0x55b7d0fc2680 -> cinn_launch0x55b7d1126750  via conv2d_17.tmp_00x55b7d11264a0
I1018 08:53:57.787287 4097533 graph_helper.h:104] adj conv2d0x55b7d0fb56d0 -> cinn_launch0x55b7d1126750  via conv2d_16.tmp_00x55b7d0fc23b0
I1018 08:53:57.787289 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> conv2d0x55b7d0fc2680  via relu_12.tmp_00x55b7d1122b70
I1018 08:53:57.787292 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> cinn_launch0x55b7d1062b50  via __control_var@15110x55b7d101e840
I1018 08:53:57.787294 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> conv2d0x55b7d0fb56d0  via relu_13.tmp_00x55b7d0fb52e0
I1018 08:53:57.787297 4097533 graph_helper.h:104] adj conv2d0x55b7d1122f60 -> cinn_launch0x55b7d0fd6ee0  via conv2d_15.tmp_00x55b7d0fd6c10
I1018 08:53:57.787299 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> conv2d0x55b7d1122f60  via relu_12.tmp_00x55b7d1122b70
I1018 08:53:57.787302 4097533 graph_helper.h:104] adj conv2d0x55b7d11091c0 -> cinn_launch0x55b7d0fea800  via conv2d_14.tmp_00x55b7d0fea530
I1018 08:53:57.787304 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fea800  via relu_10.tmp_00x55b7d102b4b0
I1018 08:53:57.787307 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> conv2d0x55b7d11091c0  via relu_11.tmp_00x55b7d1108dd0
I1018 08:53:57.787309 4097533 graph_helper.h:104] adj conv2d0x55b7d102bc70 -> cinn_launch0x55b7d1115560  via conv2d_13.tmp_00x55b7d11184f0
I1018 08:53:57.787312 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> conv2d0x55b7d102bc70  via relu_10.tmp_00x55b7d102b4b0
I1018 08:53:57.787313 4097533 graph_helper.h:104] adj conv2d0x55b7d0d441e0 -> cinn_launch0x55b7d1031e80  via conv2d_12.tmp_00x55b7d1031bd0
I1018 08:53:57.787317 4097533 graph_helper.h:104] adj conv2d0x55b7d0e99aa0 -> cinn_launch0x55b7d1031e80  via conv2d_11.tmp_00x55b7d0d43f10
I1018 08:53:57.787318 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> conv2d0x55b7d0d441e0  via relu_8.tmp_00x55b7d1029a90
I1018 08:53:57.787321 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> conv2d0x55b7d0e99aa0  via relu_9.tmp_00x55b7d0e996b0
I1018 08:53:57.787323 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> conv2d0x55b7d1029e80  via relu_8.tmp_00x55b7d1029a90
I1018 08:53:57.787325 4097533 graph_helper.h:104] adj conv2d0x55b7d0ec1350 -> cinn_launch0x55b7d1047f30  via conv2d_9.tmp_00x55b7d1047c60
I1018 08:53:57.787328 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d1047f30  via relu_6.tmp_00x55b7d0e88b80
I1018 08:53:57.787330 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> conv2d0x55b7d0ec1350  via relu_7.tmp_00x55b7d0e0f4e0
I1018 08:53:57.787333 4097533 graph_helper.h:104] adj conv2d0x55b7d0e89340 -> cinn_launch0x55b7d0f0e1c0  via conv2d_8.tmp_00x55b7d0f0def0
I1018 08:53:57.787335 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> conv2d0x55b7d0e89340  via relu_6.tmp_00x55b7d0e88b80
I1018 08:53:57.787338 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> cinn_launch0x55b7d0ffe3f0  via __control_var@14730x55b7d1042880
I1018 08:53:57.787343 4097533 graph_helper.h:104] adj conv2d0x55b7d1119e20 -> cinn_launch0x55b7d0add8b0  via conv2d_7.tmp_00x55b7d0add600
I1018 08:53:57.787345 4097533 graph_helper.h:104] adj conv2d0x55b7d0d89b40 -> cinn_launch0x55b7d0add8b0  via conv2d_6.tmp_00x55b7d1119b50
I1018 08:53:57.787348 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> conv2d0x55b7d1119e20  via relu_4.tmp_00x55b7d111b9f0
I1018 08:53:57.787350 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> conv2d0x55b7d0d89b40  via relu_5.tmp_00x55b7d0d89750
I1018 08:53:57.787353 4097533 graph_helper.h:104] adj conv2d0x55b7d111bde0 -> cinn_launch0x55b7d0dcf300  via conv2d_5.tmp_00x55b7d0dcf030
I1018 08:53:57.787355 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> conv2d0x55b7d111bde0  via relu_4.tmp_00x55b7d111b9f0
I1018 08:53:57.787357 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d0c640b0  via relu_2.tmp_00x55b7d0c74b50
I1018 08:53:57.787360 4097533 graph_helper.h:104] adj conv2d0x55b7d0b70160 -> cinn_launch0x55b7d0c640b0  via conv2d_4.tmp_00x55b7d0c63de0
I1018 08:53:57.787362 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> conv2d0x55b7d0b70160  via relu_3.tmp_00x55b7d0b6fd70
I1018 08:53:57.787364 4097533 graph_helper.h:104] adj conv2d0x55b7d0c74ec0 -> cinn_launch0x55b7d0c9d5e0  via conv2d_3.tmp_00x55b7d0e7f460
I1018 08:53:57.787367 4097533 graph_helper.h:104] adj conv2d0x55b7d1029e80 -> cinn_launch0x55b7d0cb5e70  via conv2d_10.tmp_00x55b7d0cb8f30
I1018 08:53:57.787369 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> conv2d0x55b7d0c74ec0  via relu_2.tmp_00x55b7d0c74b50
I1018 08:53:57.787372 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdff90  via __control_var@15050x55b7d101d7a0
I1018 08:53:57.787374 4097533 graph_helper.h:104] adj conv2d0x55b7d0edeb50 -> cinn_launch0x55b7d0cfb320  via conv2d_2.tmp_00x55b7d0cfb110
I1018 08:53:57.787376 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d0cfb320  via pool2d_0.tmp_00x55b7d0b929b0
I1018 08:53:57.787379 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> conv2d0x55b7d0edeb50  via relu_1.tmp_00x55b7d0de1f20
I1018 08:53:57.787381 4097533 graph_helper.h:104] adj conv2d0x55b7d0ddf060 -> cinn_launch0x55b7d0c892a0  via conv2d_1.tmp_00x55b7d0c89030
I1018 08:53:57.787384 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> conv2d0x55b7d0ddf060  via pool2d_0.tmp_00x55b7d0b929b0
I1018 08:53:57.787386 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> conv2d0x55b7d10908e0  via relu_14.tmp_00x55b7d10900a0
I1018 08:53:57.787389 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> cinn_launch0x55b7d0fff800  via __control_var@15100x55b7d101e570
I1018 08:53:57.787391 4097533 graph_helper.h:104] adj conv2d0x55b7d0edf1d0 -> cinn_launch0x55b7d0fac1b0  via conv2d_0.tmp_00x55b7d0eaa280
I1018 08:53:57.787513 4097533 build_strategy.cc:502] Finish Apply Pass fuse_all_reduce_op_pass
I1018 08:53:57.787515 4097533 build_strategy.cc:376] BuildStrategy::Apply pass:all_reduce_deps_pass
I1018 08:53:57.787518 4097533 build_strategy.cc:457] SeqOnlyAllReduceOps:0, num_trainers:1
I1018 08:53:57.787520 4097533 build_strategy.cc:493] Start Apply Pass all_reduce_deps_pass
I1018 08:53:57.787523 4097533 build_strategy.cc:496] Apply Pass all_reduce_deps_passto SubGraph 0
I1018 08:53:57.787565 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> cinn_launch0x55b7d1002020  via __control_var@14800x55b7d1043cd0
I1018 08:53:57.787568 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> cinn_launch0x55b7d1000c10  via __control_var@14720x55b7d1042590
I1018 08:53:57.787570 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d0ffcfe0  via __control_var@14740x55b7d1042b70
I1018 08:53:57.787573 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d0ffbc30  via __control_var@14750x55b7d1042e40
I1018 08:53:57.787575 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d1068fa0  via __control_var@15090x55b7d101e2a0
I1018 08:53:57.787580 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d1067b90  via __control_var@15010x55b7d101cbc0
I1018 08:53:57.787582 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> cinn_launch0x55b7d1066780  via __control_var@14980x55b7d101c330
I1018 08:53:57.787585 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> cinn_launch0x55b7d1065370  via __control_var@14900x55b7d101ac70
I1018 08:53:57.787586 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> cinn_launch0x55b7d1063f60  via __control_var@15070x55b7d101dd40
I1018 08:53:57.787590 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> cinn_launch0x55b7d1061740  via __control_var@15080x55b7d101dfd0
I1018 08:53:57.787591 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> cinn_launch0x55b7d0fe13a0  via __control_var@14920x55b7d101b1f0
I1018 08:53:57.787593 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdeb80  via __control_var@15030x55b7d101d160
I1018 08:53:57.787596 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdd770  via __control_var@14850x55b7d1044b40
I1018 08:53:57.787598 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdc360  via __control_var@15040x55b7d101d4d0
I1018 08:53:57.787600 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> cinn_launch0x55b7d0fdaf50  via __control_var@14970x55b7d101c060
I1018 08:53:57.787602 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> cinn_launch0x55b7d0fd9b40  via __control_var@14930x55b7d101b4e0
I1018 08:53:57.787606 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> cinn_launch0x55b7d0fd8730  via __control_var@14820x55b7d1044290
I1018 08:53:57.787607 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> cinn_launch0x55b7d1104d40  via __control_var@14840x55b7d1044870
I1018 08:53:57.787609 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> cinn_launch0x55b7d1103930  via __control_var@15000x55b7d101c8f0
I1018 08:53:57.787612 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> cinn_launch0x55b7d1102520  via __control_var@14990x55b7d101c600
I1018 08:53:57.787614 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d1101110  via __control_var@14880x55b7d10453e0
I1018 08:53:57.787616 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10ffd00  via __control_var@14890x55b7d10456b0
I1018 08:53:57.787619 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10fe8f0  via __control_var@14860x55b7d1044e10
I1018 08:53:57.787621 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10fd4e0  via __control_var@14760x55b7d10431b0
I1018 08:53:57.787623 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> cinn_launch0x55b7d1113ec0  via __control_var@14790x55b7d1043a00
I1018 08:53:57.787626 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> cinn_launch0x55b7d1112ab0  via __control_var@14780x55b7d1043710
I1018 08:53:57.787628 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> cinn_launch0x55b7d11116a0  via __control_var@14830x55b7d1044580
I1018 08:53:57.787631 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> cinn_launch0x55b7d1110290  via __control_var@14940x55b7d101b7d0
I1018 08:53:57.787632 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> cinn_launch0x55b7d110ee80  via __control_var@15060x55b7d101da70
I1018 08:53:57.787635 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> cinn_launch0x55b7d110da70  via __control_var@14910x55b7d101af00
I1018 08:53:57.787637 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d110c6a0  via __control_var@14960x55b7d101bd70
I1018 08:53:57.787639 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d107f940  via __control_var@15020x55b7d101ce90
I1018 08:53:57.787642 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> cinn_launch0x55b7d107e530  via __control_var@14770x55b7d1043480
I1018 08:53:57.787647 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> cinn_launch0x55b7d107d120  via __control_var@14950x55b7d101baa0
I1018 08:53:57.787648 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d107bd10  via __control_var@14870x55b7d1045110
I1018 08:53:57.787650 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d107a890  via __control_var@14810x55b7d1043fa0
I1018 08:53:57.787653 4097533 graph_helper.h:104] adj conv2d0x55b7d106f840 -> cinn_launch0x55b7d109c290  via conv2d_19.tmp_00x55b7d109bfc0
I1018 08:53:57.787655 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d109c290  via relu_14.tmp_00x55b7d10900a0
I1018 08:53:57.787657 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> conv2d0x55b7d106f840  via relu_15.tmp_00x55b7d106f450
I1018 08:53:57.787660 4097533 graph_helper.h:104] adj conv2d0x55b7d10908e0 -> cinn_launch0x55b7d106aaf0  via conv2d_18.tmp_00x55b7d106a820
I1018 08:53:57.787662 4097533 graph_helper.h:104] adj conv2d0x55b7d0fc2680 -> cinn_launch0x55b7d1126750  via conv2d_17.tmp_00x55b7d11264a0
I1018 08:53:57.787664 4097533 graph_helper.h:104] adj conv2d0x55b7d0fb56d0 -> cinn_launch0x55b7d1126750  via conv2d_16.tmp_00x55b7d0fc23b0
I1018 08:53:57.787667 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> conv2d0x55b7d0fc2680  via relu_12.tmp_00x55b7d1122b70
I1018 08:53:57.787669 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> cinn_launch0x55b7d1062b50  via __control_var@15110x55b7d101e840
I1018 08:53:57.787671 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> conv2d0x55b7d0fb56d0  via relu_13.tmp_00x55b7d0fb52e0
I1018 08:53:57.787674 4097533 graph_helper.h:104] adj conv2d0x55b7d1122f60 -> cinn_launch0x55b7d0fd6ee0  via conv2d_15.tmp_00x55b7d0fd6c10
I1018 08:53:57.787676 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> conv2d0x55b7d1122f60  via relu_12.tmp_00x55b7d1122b70
I1018 08:53:57.787679 4097533 graph_helper.h:104] adj conv2d0x55b7d11091c0 -> cinn_launch0x55b7d0fea800  via conv2d_14.tmp_00x55b7d0fea530
I1018 08:53:57.787681 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fea800  via relu_10.tmp_00x55b7d102b4b0
I1018 08:53:57.787683 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> conv2d0x55b7d11091c0  via relu_11.tmp_00x55b7d1108dd0
I1018 08:53:57.787685 4097533 graph_helper.h:104] adj conv2d0x55b7d102bc70 -> cinn_launch0x55b7d1115560  via conv2d_13.tmp_00x55b7d11184f0
I1018 08:53:57.787688 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> conv2d0x55b7d102bc70  via relu_10.tmp_00x55b7d102b4b0
I1018 08:53:57.787690 4097533 graph_helper.h:104] adj conv2d0x55b7d0d441e0 -> cinn_launch0x55b7d1031e80  via conv2d_12.tmp_00x55b7d1031bd0
I1018 08:53:57.787693 4097533 graph_helper.h:104] adj conv2d0x55b7d0e99aa0 -> cinn_launch0x55b7d1031e80  via conv2d_11.tmp_00x55b7d0d43f10
I1018 08:53:57.787695 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> conv2d0x55b7d0d441e0  via relu_8.tmp_00x55b7d1029a90
I1018 08:53:57.787698 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> conv2d0x55b7d0e99aa0  via relu_9.tmp_00x55b7d0e996b0
I1018 08:53:57.787699 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> conv2d0x55b7d1029e80  via relu_8.tmp_00x55b7d1029a90
I1018 08:53:57.787703 4097533 graph_helper.h:104] adj conv2d0x55b7d0ec1350 -> cinn_launch0x55b7d1047f30  via conv2d_9.tmp_00x55b7d1047c60
I1018 08:53:57.787704 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d1047f30  via relu_6.tmp_00x55b7d0e88b80
I1018 08:53:57.787706 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> conv2d0x55b7d0ec1350  via relu_7.tmp_00x55b7d0e0f4e0
I1018 08:53:57.787709 4097533 graph_helper.h:104] adj conv2d0x55b7d0e89340 -> cinn_launch0x55b7d0f0e1c0  via conv2d_8.tmp_00x55b7d0f0def0
I1018 08:53:57.787711 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> conv2d0x55b7d0e89340  via relu_6.tmp_00x55b7d0e88b80
I1018 08:53:57.787714 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> cinn_launch0x55b7d0ffe3f0  via __control_var@14730x55b7d1042880
I1018 08:53:57.787719 4097533 graph_helper.h:104] adj conv2d0x55b7d1119e20 -> cinn_launch0x55b7d0add8b0  via conv2d_7.tmp_00x55b7d0add600
I1018 08:53:57.787720 4097533 graph_helper.h:104] adj conv2d0x55b7d0d89b40 -> cinn_launch0x55b7d0add8b0  via conv2d_6.tmp_00x55b7d1119b50
I1018 08:53:57.787722 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> conv2d0x55b7d1119e20  via relu_4.tmp_00x55b7d111b9f0
I1018 08:53:57.787725 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> conv2d0x55b7d0d89b40  via relu_5.tmp_00x55b7d0d89750
I1018 08:53:57.787727 4097533 graph_helper.h:104] adj conv2d0x55b7d111bde0 -> cinn_launch0x55b7d0dcf300  via conv2d_5.tmp_00x55b7d0dcf030
I1018 08:53:57.787729 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> conv2d0x55b7d111bde0  via relu_4.tmp_00x55b7d111b9f0
I1018 08:53:57.787732 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d0c640b0  via relu_2.tmp_00x55b7d0c74b50
I1018 08:53:57.787734 4097533 graph_helper.h:104] adj conv2d0x55b7d0b70160 -> cinn_launch0x55b7d0c640b0  via conv2d_4.tmp_00x55b7d0c63de0
I1018 08:53:57.787736 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> conv2d0x55b7d0b70160  via relu_3.tmp_00x55b7d0b6fd70
I1018 08:53:57.787739 4097533 graph_helper.h:104] adj conv2d0x55b7d0c74ec0 -> cinn_launch0x55b7d0c9d5e0  via conv2d_3.tmp_00x55b7d0e7f460
I1018 08:53:57.787741 4097533 graph_helper.h:104] adj conv2d0x55b7d1029e80 -> cinn_launch0x55b7d0cb5e70  via conv2d_10.tmp_00x55b7d0cb8f30
I1018 08:53:57.787743 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> conv2d0x55b7d0c74ec0  via relu_2.tmp_00x55b7d0c74b50
I1018 08:53:57.787746 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdff90  via __control_var@15050x55b7d101d7a0
I1018 08:53:57.787748 4097533 graph_helper.h:104] adj conv2d0x55b7d0edeb50 -> cinn_launch0x55b7d0cfb320  via conv2d_2.tmp_00x55b7d0cfb110
I1018 08:53:57.787750 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d0cfb320  via pool2d_0.tmp_00x55b7d0b929b0
I1018 08:53:57.787753 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> conv2d0x55b7d0edeb50  via relu_1.tmp_00x55b7d0de1f20
I1018 08:53:57.787755 4097533 graph_helper.h:104] adj conv2d0x55b7d0ddf060 -> cinn_launch0x55b7d0c892a0  via conv2d_1.tmp_00x55b7d0c89030
I1018 08:53:57.787758 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> conv2d0x55b7d0ddf060  via pool2d_0.tmp_00x55b7d0b929b0
I1018 08:53:57.787760 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> conv2d0x55b7d10908e0  via relu_14.tmp_00x55b7d10900a0
I1018 08:53:57.787762 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> cinn_launch0x55b7d0fff800  via __control_var@15100x55b7d101e570
I1018 08:53:57.787765 4097533 graph_helper.h:104] adj conv2d0x55b7d0edf1d0 -> cinn_launch0x55b7d0fac1b0  via conv2d_0.tmp_00x55b7d0eaa280
I1018 08:53:57.787882 4097533 build_strategy.cc:502] Finish Apply Pass all_reduce_deps_pass
I1018 08:53:57.787884 4097533 build_strategy.cc:376] BuildStrategy::Apply pass:modify_op_lock_and_record_event_pass
I1018 08:53:57.787887 4097533 build_strategy.cc:493] Start Apply Pass modify_op_lock_and_record_event_pass
I1018 08:53:57.787889 4097533 build_strategy.cc:496] Apply Pass modify_op_lock_and_record_event_passto SubGraph 0
I1018 08:53:57.787937 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> cinn_launch0x55b7d1002020  via __control_var@14800x55b7d1043cd0
I1018 08:53:57.787940 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> cinn_launch0x55b7d1000c10  via __control_var@14720x55b7d1042590
I1018 08:53:57.787942 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d0ffcfe0  via __control_var@14740x55b7d1042b70
I1018 08:53:57.787945 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d0ffbc30  via __control_var@14750x55b7d1042e40
I1018 08:53:57.787947 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d1068fa0  via __control_var@15090x55b7d101e2a0
I1018 08:53:57.787951 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d1067b90  via __control_var@15010x55b7d101cbc0
I1018 08:53:57.787954 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> cinn_launch0x55b7d1066780  via __control_var@14980x55b7d101c330
I1018 08:53:57.787956 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> cinn_launch0x55b7d1065370  via __control_var@14900x55b7d101ac70
I1018 08:53:57.787958 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> cinn_launch0x55b7d1063f60  via __control_var@15070x55b7d101dd40
I1018 08:53:57.787961 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> cinn_launch0x55b7d1061740  via __control_var@15080x55b7d101dfd0
I1018 08:53:57.787963 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> cinn_launch0x55b7d0fe13a0  via __control_var@14920x55b7d101b1f0
I1018 08:53:57.787966 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdeb80  via __control_var@15030x55b7d101d160
I1018 08:53:57.787968 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdd770  via __control_var@14850x55b7d1044b40
I1018 08:53:57.787971 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdc360  via __control_var@15040x55b7d101d4d0
I1018 08:53:57.787972 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> cinn_launch0x55b7d0fdaf50  via __control_var@14970x55b7d101c060
I1018 08:53:57.787976 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> cinn_launch0x55b7d0fd9b40  via __control_var@14930x55b7d101b4e0
I1018 08:53:57.787977 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> cinn_launch0x55b7d0fd8730  via __control_var@14820x55b7d1044290
I1018 08:53:57.787979 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> cinn_launch0x55b7d1104d40  via __control_var@14840x55b7d1044870
I1018 08:53:57.787982 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> cinn_launch0x55b7d1103930  via __control_var@15000x55b7d101c8f0
I1018 08:53:57.787984 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> cinn_launch0x55b7d1102520  via __control_var@14990x55b7d101c600
I1018 08:53:57.787986 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d1101110  via __control_var@14880x55b7d10453e0
I1018 08:53:57.787989 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10ffd00  via __control_var@14890x55b7d10456b0
I1018 08:53:57.787992 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10fe8f0  via __control_var@14860x55b7d1044e10
I1018 08:53:57.787993 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10fd4e0  via __control_var@14760x55b7d10431b0
I1018 08:53:57.787997 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> cinn_launch0x55b7d1113ec0  via __control_var@14790x55b7d1043a00
I1018 08:53:57.787998 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> cinn_launch0x55b7d1112ab0  via __control_var@14780x55b7d1043710
I1018 08:53:57.788000 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> cinn_launch0x55b7d11116a0  via __control_var@14830x55b7d1044580
I1018 08:53:57.788002 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> cinn_launch0x55b7d1110290  via __control_var@14940x55b7d101b7d0
I1018 08:53:57.788005 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> cinn_launch0x55b7d110ee80  via __control_var@15060x55b7d101da70
I1018 08:53:57.788007 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> cinn_launch0x55b7d110da70  via __control_var@14910x55b7d101af00
I1018 08:53:57.788009 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d110c6a0  via __control_var@14960x55b7d101bd70
I1018 08:53:57.788012 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d107f940  via __control_var@15020x55b7d101ce90
I1018 08:53:57.788014 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> cinn_launch0x55b7d107e530  via __control_var@14770x55b7d1043480
I1018 08:53:57.788018 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> cinn_launch0x55b7d107d120  via __control_var@14950x55b7d101baa0
I1018 08:53:57.788021 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d107bd10  via __control_var@14870x55b7d1045110
I1018 08:53:57.788023 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d107a890  via __control_var@14810x55b7d1043fa0
I1018 08:53:57.788025 4097533 graph_helper.h:104] adj conv2d0x55b7d106f840 -> cinn_launch0x55b7d109c290  via conv2d_19.tmp_00x55b7d109bfc0
I1018 08:53:57.788028 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d109c290  via relu_14.tmp_00x55b7d10900a0
I1018 08:53:57.788030 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> conv2d0x55b7d106f840  via relu_15.tmp_00x55b7d106f450
I1018 08:53:57.788033 4097533 graph_helper.h:104] adj conv2d0x55b7d10908e0 -> cinn_launch0x55b7d106aaf0  via conv2d_18.tmp_00x55b7d106a820
I1018 08:53:57.788035 4097533 graph_helper.h:104] adj conv2d0x55b7d0fc2680 -> cinn_launch0x55b7d1126750  via conv2d_17.tmp_00x55b7d11264a0
I1018 08:53:57.788038 4097533 graph_helper.h:104] adj conv2d0x55b7d0fb56d0 -> cinn_launch0x55b7d1126750  via conv2d_16.tmp_00x55b7d0fc23b0
I1018 08:53:57.788039 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> conv2d0x55b7d0fc2680  via relu_12.tmp_00x55b7d1122b70
I1018 08:53:57.788043 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> cinn_launch0x55b7d1062b50  via __control_var@15110x55b7d101e840
I1018 08:53:57.788044 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> conv2d0x55b7d0fb56d0  via relu_13.tmp_00x55b7d0fb52e0
I1018 08:53:57.788046 4097533 graph_helper.h:104] adj conv2d0x55b7d1122f60 -> cinn_launch0x55b7d0fd6ee0  via conv2d_15.tmp_00x55b7d0fd6c10
I1018 08:53:57.788049 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> conv2d0x55b7d1122f60  via relu_12.tmp_00x55b7d1122b70
I1018 08:53:57.788051 4097533 graph_helper.h:104] adj conv2d0x55b7d11091c0 -> cinn_launch0x55b7d0fea800  via conv2d_14.tmp_00x55b7d0fea530
I1018 08:53:57.788053 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fea800  via relu_10.tmp_00x55b7d102b4b0
I1018 08:53:57.788056 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> conv2d0x55b7d11091c0  via relu_11.tmp_00x55b7d1108dd0
I1018 08:53:57.788058 4097533 graph_helper.h:104] adj conv2d0x55b7d102bc70 -> cinn_launch0x55b7d1115560  via conv2d_13.tmp_00x55b7d11184f0
I1018 08:53:57.788060 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> conv2d0x55b7d102bc70  via relu_10.tmp_00x55b7d102b4b0
I1018 08:53:57.788064 4097533 graph_helper.h:104] adj conv2d0x55b7d0d441e0 -> cinn_launch0x55b7d1031e80  via conv2d_12.tmp_00x55b7d1031bd0
I1018 08:53:57.788065 4097533 graph_helper.h:104] adj conv2d0x55b7d0e99aa0 -> cinn_launch0x55b7d1031e80  via conv2d_11.tmp_00x55b7d0d43f10
I1018 08:53:57.788067 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> conv2d0x55b7d0d441e0  via relu_8.tmp_00x55b7d1029a90
I1018 08:53:57.788070 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> conv2d0x55b7d0e99aa0  via relu_9.tmp_00x55b7d0e996b0
I1018 08:53:57.788072 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> conv2d0x55b7d1029e80  via relu_8.tmp_00x55b7d1029a90
I1018 08:53:57.788074 4097533 graph_helper.h:104] adj conv2d0x55b7d0ec1350 -> cinn_launch0x55b7d1047f30  via conv2d_9.tmp_00x55b7d1047c60
I1018 08:53:57.788077 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d1047f30  via relu_6.tmp_00x55b7d0e88b80
I1018 08:53:57.788079 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> conv2d0x55b7d0ec1350  via relu_7.tmp_00x55b7d0e0f4e0
I1018 08:53:57.788081 4097533 graph_helper.h:104] adj conv2d0x55b7d0e89340 -> cinn_launch0x55b7d0f0e1c0  via conv2d_8.tmp_00x55b7d0f0def0
I1018 08:53:57.788084 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> conv2d0x55b7d0e89340  via relu_6.tmp_00x55b7d0e88b80
I1018 08:53:57.788089 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> cinn_launch0x55b7d0ffe3f0  via __control_var@14730x55b7d1042880
I1018 08:53:57.788090 4097533 graph_helper.h:104] adj conv2d0x55b7d1119e20 -> cinn_launch0x55b7d0add8b0  via conv2d_7.tmp_00x55b7d0add600
I1018 08:53:57.788093 4097533 graph_helper.h:104] adj conv2d0x55b7d0d89b40 -> cinn_launch0x55b7d0add8b0  via conv2d_6.tmp_00x55b7d1119b50
I1018 08:53:57.788095 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> conv2d0x55b7d1119e20  via relu_4.tmp_00x55b7d111b9f0
I1018 08:53:57.788097 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> conv2d0x55b7d0d89b40  via relu_5.tmp_00x55b7d0d89750
I1018 08:53:57.788100 4097533 graph_helper.h:104] adj conv2d0x55b7d111bde0 -> cinn_launch0x55b7d0dcf300  via conv2d_5.tmp_00x55b7d0dcf030
I1018 08:53:57.788102 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> conv2d0x55b7d111bde0  via relu_4.tmp_00x55b7d111b9f0
I1018 08:53:57.788105 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d0c640b0  via relu_2.tmp_00x55b7d0c74b50
I1018 08:53:57.788107 4097533 graph_helper.h:104] adj conv2d0x55b7d0b70160 -> cinn_launch0x55b7d0c640b0  via conv2d_4.tmp_00x55b7d0c63de0
I1018 08:53:57.788110 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> conv2d0x55b7d0b70160  via relu_3.tmp_00x55b7d0b6fd70
I1018 08:53:57.788111 4097533 graph_helper.h:104] adj conv2d0x55b7d0c74ec0 -> cinn_launch0x55b7d0c9d5e0  via conv2d_3.tmp_00x55b7d0e7f460
I1018 08:53:57.788115 4097533 graph_helper.h:104] adj conv2d0x55b7d1029e80 -> cinn_launch0x55b7d0cb5e70  via conv2d_10.tmp_00x55b7d0cb8f30
I1018 08:53:57.788116 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> conv2d0x55b7d0c74ec0  via relu_2.tmp_00x55b7d0c74b50
I1018 08:53:57.788118 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdff90  via __control_var@15050x55b7d101d7a0
I1018 08:53:57.788121 4097533 graph_helper.h:104] adj conv2d0x55b7d0edeb50 -> cinn_launch0x55b7d0cfb320  via conv2d_2.tmp_00x55b7d0cfb110
I1018 08:53:57.788123 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d0cfb320  via pool2d_0.tmp_00x55b7d0b929b0
I1018 08:53:57.788125 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> conv2d0x55b7d0edeb50  via relu_1.tmp_00x55b7d0de1f20
I1018 08:53:57.788128 4097533 graph_helper.h:104] adj conv2d0x55b7d0ddf060 -> cinn_launch0x55b7d0c892a0  via conv2d_1.tmp_00x55b7d0c89030
I1018 08:53:57.788130 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> conv2d0x55b7d0ddf060  via pool2d_0.tmp_00x55b7d0b929b0
I1018 08:53:57.788133 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> conv2d0x55b7d10908e0  via relu_14.tmp_00x55b7d10900a0
I1018 08:53:57.788136 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> cinn_launch0x55b7d0fff800  via __control_var@15100x55b7d101e570
I1018 08:53:57.788137 4097533 graph_helper.h:104] adj conv2d0x55b7d0edf1d0 -> cinn_launch0x55b7d0fac1b0  via conv2d_0.tmp_00x55b7d0eaa280
I1018 08:53:57.788252 4097533 build_strategy.cc:502] Finish Apply Pass modify_op_lock_and_record_event_pass
I1018 08:53:57.788254 4097533 build_strategy.cc:376] BuildStrategy::Apply pass:multi_devices_check_pass
I1018 08:53:57.788257 4097533 build_strategy.cc:493] Start Apply Pass multi_devices_check_pass
I1018 08:53:57.788259 4097533 build_strategy.cc:496] Apply Pass multi_devices_check_passto SubGraph 0
I1018 08:53:57.788348 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> cinn_launch0x55b7d1002020  via __control_var@14800x55b7d1043cd0
I1018 08:53:57.788352 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> cinn_launch0x55b7d1000c10  via __control_var@14720x55b7d1042590
I1018 08:53:57.788353 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d0ffcfe0  via __control_var@14740x55b7d1042b70
I1018 08:53:57.788357 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d0ffbc30  via __control_var@14750x55b7d1042e40
I1018 08:53:57.788362 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d1068fa0  via __control_var@15090x55b7d101e2a0
I1018 08:53:57.788363 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d1067b90  via __control_var@15010x55b7d101cbc0
I1018 08:53:57.788365 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> cinn_launch0x55b7d1066780  via __control_var@14980x55b7d101c330
I1018 08:53:57.788368 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> cinn_launch0x55b7d1065370  via __control_var@14900x55b7d101ac70
I1018 08:53:57.788370 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> cinn_launch0x55b7d1063f60  via __control_var@15070x55b7d101dd40
I1018 08:53:57.788373 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> cinn_launch0x55b7d1061740  via __control_var@15080x55b7d101dfd0
I1018 08:53:57.788375 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> cinn_launch0x55b7d0fe13a0  via __control_var@14920x55b7d101b1f0
I1018 08:53:57.788378 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdeb80  via __control_var@15030x55b7d101d160
I1018 08:53:57.788379 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdd770  via __control_var@14850x55b7d1044b40
I1018 08:53:57.788383 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdc360  via __control_var@15040x55b7d101d4d0
I1018 08:53:57.788384 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> cinn_launch0x55b7d0fdaf50  via __control_var@14970x55b7d101c060
I1018 08:53:57.788386 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> cinn_launch0x55b7d0fd9b40  via __control_var@14930x55b7d101b4e0
I1018 08:53:57.788389 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> cinn_launch0x55b7d0fd8730  via __control_var@14820x55b7d1044290
I1018 08:53:57.788391 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> cinn_launch0x55b7d1104d40  via __control_var@14840x55b7d1044870
I1018 08:53:57.788393 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> cinn_launch0x55b7d1103930  via __control_var@15000x55b7d101c8f0
I1018 08:53:57.788396 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> cinn_launch0x55b7d1102520  via __control_var@14990x55b7d101c600
I1018 08:53:57.788398 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d1101110  via __control_var@14880x55b7d10453e0
I1018 08:53:57.788400 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10ffd00  via __control_var@14890x55b7d10456b0
I1018 08:53:57.788403 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10fe8f0  via __control_var@14860x55b7d1044e10
I1018 08:53:57.788405 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10fd4e0  via __control_var@14760x55b7d10431b0
I1018 08:53:57.788407 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> cinn_launch0x55b7d1113ec0  via __control_var@14790x55b7d1043a00
I1018 08:53:57.788409 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> cinn_launch0x55b7d1112ab0  via __control_var@14780x55b7d1043710
I1018 08:53:57.788411 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> cinn_launch0x55b7d11116a0  via __control_var@14830x55b7d1044580
I1018 08:53:57.788414 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> cinn_launch0x55b7d1110290  via __control_var@14940x55b7d101b7d0
I1018 08:53:57.788416 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> cinn_launch0x55b7d110ee80  via __control_var@15060x55b7d101da70
I1018 08:53:57.788419 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> cinn_launch0x55b7d110da70  via __control_var@14910x55b7d101af00
I1018 08:53:57.788420 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d110c6a0  via __control_var@14960x55b7d101bd70
I1018 08:53:57.788424 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d107f940  via __control_var@15020x55b7d101ce90
I1018 08:53:57.788427 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> cinn_launch0x55b7d107e530  via __control_var@14770x55b7d1043480
I1018 08:53:57.788429 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> cinn_launch0x55b7d107d120  via __control_var@14950x55b7d101baa0
I1018 08:53:57.788431 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d107bd10  via __control_var@14870x55b7d1045110
I1018 08:53:57.788434 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d107a890  via __control_var@14810x55b7d1043fa0
I1018 08:53:57.788436 4097533 graph_helper.h:104] adj conv2d0x55b7d106f840 -> cinn_launch0x55b7d109c290  via conv2d_19.tmp_00x55b7d109bfc0
I1018 08:53:57.788439 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d109c290  via relu_14.tmp_00x55b7d10900a0
I1018 08:53:57.788441 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> conv2d0x55b7d106f840  via relu_15.tmp_00x55b7d106f450
I1018 08:53:57.788443 4097533 graph_helper.h:104] adj conv2d0x55b7d10908e0 -> cinn_launch0x55b7d106aaf0  via conv2d_18.tmp_00x55b7d106a820
I1018 08:53:57.788445 4097533 graph_helper.h:104] adj conv2d0x55b7d0fc2680 -> cinn_launch0x55b7d1126750  via conv2d_17.tmp_00x55b7d11264a0
I1018 08:53:57.788448 4097533 graph_helper.h:104] adj conv2d0x55b7d0fb56d0 -> cinn_launch0x55b7d1126750  via conv2d_16.tmp_00x55b7d0fc23b0
I1018 08:53:57.788450 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> conv2d0x55b7d0fc2680  via relu_12.tmp_00x55b7d1122b70
I1018 08:53:57.788452 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> cinn_launch0x55b7d1062b50  via __control_var@15110x55b7d101e840
I1018 08:53:57.788455 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> conv2d0x55b7d0fb56d0  via relu_13.tmp_00x55b7d0fb52e0
I1018 08:53:57.788457 4097533 graph_helper.h:104] adj conv2d0x55b7d1122f60 -> cinn_launch0x55b7d0fd6ee0  via conv2d_15.tmp_00x55b7d0fd6c10
I1018 08:53:57.788460 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> conv2d0x55b7d1122f60  via relu_12.tmp_00x55b7d1122b70
I1018 08:53:57.788462 4097533 graph_helper.h:104] adj conv2d0x55b7d11091c0 -> cinn_launch0x55b7d0fea800  via conv2d_14.tmp_00x55b7d0fea530
I1018 08:53:57.788465 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fea800  via relu_10.tmp_00x55b7d102b4b0
I1018 08:53:57.788466 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> conv2d0x55b7d11091c0  via relu_11.tmp_00x55b7d1108dd0
I1018 08:53:57.788470 4097533 graph_helper.h:104] adj conv2d0x55b7d102bc70 -> cinn_launch0x55b7d1115560  via conv2d_13.tmp_00x55b7d11184f0
I1018 08:53:57.788471 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> conv2d0x55b7d102bc70  via relu_10.tmp_00x55b7d102b4b0
I1018 08:53:57.788473 4097533 graph_helper.h:104] adj conv2d0x55b7d0d441e0 -> cinn_launch0x55b7d1031e80  via conv2d_12.tmp_00x55b7d1031bd0
I1018 08:53:57.788475 4097533 graph_helper.h:104] adj conv2d0x55b7d0e99aa0 -> cinn_launch0x55b7d1031e80  via conv2d_11.tmp_00x55b7d0d43f10
I1018 08:53:57.788478 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> conv2d0x55b7d0d441e0  via relu_8.tmp_00x55b7d1029a90
I1018 08:53:57.788480 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> conv2d0x55b7d0e99aa0  via relu_9.tmp_00x55b7d0e996b0
I1018 08:53:57.788483 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> conv2d0x55b7d1029e80  via relu_8.tmp_00x55b7d1029a90
I1018 08:53:57.788486 4097533 graph_helper.h:104] adj conv2d0x55b7d0ec1350 -> cinn_launch0x55b7d1047f30  via conv2d_9.tmp_00x55b7d1047c60
I1018 08:53:57.788487 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d1047f30  via relu_6.tmp_00x55b7d0e88b80
I1018 08:53:57.788489 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> conv2d0x55b7d0ec1350  via relu_7.tmp_00x55b7d0e0f4e0
I1018 08:53:57.788492 4097533 graph_helper.h:104] adj conv2d0x55b7d0e89340 -> cinn_launch0x55b7d0f0e1c0  via conv2d_8.tmp_00x55b7d0f0def0
I1018 08:53:57.788496 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> conv2d0x55b7d0e89340  via relu_6.tmp_00x55b7d0e88b80
I1018 08:53:57.788498 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> cinn_launch0x55b7d0ffe3f0  via __control_var@14730x55b7d1042880
I1018 08:53:57.788501 4097533 graph_helper.h:104] adj conv2d0x55b7d1119e20 -> cinn_launch0x55b7d0add8b0  via conv2d_7.tmp_00x55b7d0add600
I1018 08:53:57.788503 4097533 graph_helper.h:104] adj conv2d0x55b7d0d89b40 -> cinn_launch0x55b7d0add8b0  via conv2d_6.tmp_00x55b7d1119b50
I1018 08:53:57.788506 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> conv2d0x55b7d1119e20  via relu_4.tmp_00x55b7d111b9f0
I1018 08:53:57.788508 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> conv2d0x55b7d0d89b40  via relu_5.tmp_00x55b7d0d89750
I1018 08:53:57.788511 4097533 graph_helper.h:104] adj conv2d0x55b7d111bde0 -> cinn_launch0x55b7d0dcf300  via conv2d_5.tmp_00x55b7d0dcf030
I1018 08:53:57.788512 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> conv2d0x55b7d111bde0  via relu_4.tmp_00x55b7d111b9f0
I1018 08:53:57.788515 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d0c640b0  via relu_2.tmp_00x55b7d0c74b50
I1018 08:53:57.788517 4097533 graph_helper.h:104] adj conv2d0x55b7d0b70160 -> cinn_launch0x55b7d0c640b0  via conv2d_4.tmp_00x55b7d0c63de0
I1018 08:53:57.788519 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> conv2d0x55b7d0b70160  via relu_3.tmp_00x55b7d0b6fd70
I1018 08:53:57.788522 4097533 graph_helper.h:104] adj conv2d0x55b7d0c74ec0 -> cinn_launch0x55b7d0c9d5e0  via conv2d_3.tmp_00x55b7d0e7f460
I1018 08:53:57.788524 4097533 graph_helper.h:104] adj conv2d0x55b7d1029e80 -> cinn_launch0x55b7d0cb5e70  via conv2d_10.tmp_00x55b7d0cb8f30
I1018 08:53:57.788527 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> conv2d0x55b7d0c74ec0  via relu_2.tmp_00x55b7d0c74b50
I1018 08:53:57.788529 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdff90  via __control_var@15050x55b7d101d7a0
I1018 08:53:57.788532 4097533 graph_helper.h:104] adj conv2d0x55b7d0edeb50 -> cinn_launch0x55b7d0cfb320  via conv2d_2.tmp_00x55b7d0cfb110
I1018 08:53:57.788533 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d0cfb320  via pool2d_0.tmp_00x55b7d0b929b0
I1018 08:53:57.788537 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> conv2d0x55b7d0edeb50  via relu_1.tmp_00x55b7d0de1f20
I1018 08:53:57.788538 4097533 graph_helper.h:104] adj conv2d0x55b7d0ddf060 -> cinn_launch0x55b7d0c892a0  via conv2d_1.tmp_00x55b7d0c89030
I1018 08:53:57.788542 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> conv2d0x55b7d0ddf060  via pool2d_0.tmp_00x55b7d0b929b0
I1018 08:53:57.788543 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> conv2d0x55b7d10908e0  via relu_14.tmp_00x55b7d10900a0
I1018 08:53:57.788545 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> cinn_launch0x55b7d0fff800  via __control_var@15100x55b7d101e570
I1018 08:53:57.788548 4097533 graph_helper.h:104] adj conv2d0x55b7d0edf1d0 -> cinn_launch0x55b7d0fac1b0  via conv2d_0.tmp_00x55b7d0eaa280
I1018 08:53:57.788666 4097533 build_strategy.cc:502] Finish Apply Pass multi_devices_check_pass
I1018 08:53:57.788669 4097533 build_strategy.cc:504] All Passes Applied
I1018 08:53:57.788738 4097533 reference_count_pass.cc:289] Place number: 1
I1018 08:53:57.788741 4097533 var_desc.cc:415] Flush  assign_76.tmp_0 1
I1018 08:53:57.788751 4097533 var_desc.cc:415] Flush  assign_72.tmp_0 1
I1018 08:53:57.788755 4097533 var_desc.cc:415] Flush  assign_64.tmp_0 1
I1018 08:53:57.788758 4097533 var_desc.cc:415] Flush  assign_56.tmp_0 1
I1018 08:53:57.788761 4097533 var_desc.cc:415] Flush  assign_48.tmp_0 1
I1018 08:53:57.788764 4097533 var_desc.cc:415] Flush  assign_44.tmp_0 1
I1018 08:53:57.788767 4097533 var_desc.cc:415] Flush  assign_36.tmp_0 1
I1018 08:53:57.788770 4097533 var_desc.cc:415] Flush  assign_32.tmp_0 1
I1018 08:53:57.788774 4097533 var_desc.cc:415] Flush  assign_28.tmp_0 1
I1018 08:53:57.788780 4097533 var_desc.cc:415] Flush  assign_16.tmp_0 1
I1018 08:53:57.788784 4097533 var_desc.cc:415] Flush  assign_12.tmp_0 1
I1018 08:53:57.788786 4097533 var_desc.cc:415] Flush  assign_0.tmp_0 1
I1018 08:53:57.788789 4097533 var_desc.cc:415] Flush  linear_0.tmp_1 1
I1018 08:53:57.788792 4097533 var_desc.cc:415] Flush  assign_77.tmp_0 1
I1018 08:53:57.788795 4097533 var_desc.cc:415] Flush  conv2d_19.tmp_0 1
I1018 08:53:57.788802 4097533 var_desc.cc:415] Flush  assign_73.tmp_0 1
I1018 08:53:57.788805 4097533 var_desc.cc:415] Flush  conv2d_18.tmp_0 1
I1018 08:53:57.788810 4097533 var_desc.cc:415] Flush  assign_65.tmp_0 1
I1018 08:53:57.788812 4097533 var_desc.cc:415] Flush  relu_14.tmp_0 1
I1018 08:53:57.788820 4097533 var_desc.cc:415] Flush  relu_15.tmp_0 1
I1018 08:53:57.788823 4097533 var_desc.cc:415] Flush  conv2d_17.tmp_0 1
I1018 08:53:57.788827 4097533 var_desc.cc:415] Flush  conv2d_10.tmp_0 1
I1018 08:53:57.788831 4097533 var_desc.cc:415] Flush  assign_60.tmp_0 1
I1018 08:53:57.788834 4097533 var_desc.cc:415] Flush  relu_13.tmp_0 1
I1018 08:53:57.788838 4097533 var_desc.cc:415] Flush  conv2d_7.tmp_0 1
I1018 08:53:57.788842 4097533 var_desc.cc:415] Flush  pool2d_0.tmp_0 1
I1018 08:53:57.788862 4097533 var_desc.cc:415] Flush  assign_41.tmp_0 1
I1018 08:53:57.788866 4097533 var_desc.cc:415] Flush  assign_20.tmp_0 1
I1018 08:53:57.788869 4097533 var_desc.cc:415] Flush  assign_24.tmp_0 1
I1018 08:53:57.788872 4097533 var_desc.cc:415] Flush  conv2d_16.tmp_0 1
I1018 08:53:57.788877 4097533 var_desc.cc:415] Flush  assign_5.tmp_0 1
I1018 08:53:57.788880 4097533 var_desc.cc:415] Flush  conv2d_14.tmp_0 1
I1018 08:53:57.788884 4097533 var_desc.cc:415] Flush  assign_13.tmp_0 1
I1018 08:53:57.788887 4097533 var_desc.cc:415] Flush  assign_40.tmp_0 1
I1018 08:53:57.788892 4097533 var_desc.cc:415] Flush  conv2d_11.tmp_0 1
I1018 08:53:57.788895 4097533 var_desc.cc:415] Flush  relu_3.tmp_0 1
I1018 08:53:57.788899 4097533 var_desc.cc:415] Flush  relu_5.tmp_0 1
I1018 08:53:57.788903 4097533 var_desc.cc:415] Flush  assign_53.tmp_0 1
I1018 08:53:57.788906 4097533 var_desc.cc:415] Flush  assign_4.tmp_0 1
I1018 08:53:57.788909 4097533 var_desc.cc:415] Flush  relu_4.tmp_0 1
I1018 08:53:57.788933 4097533 var_desc.cc:415] Flush  conv2d_0.tmp_0 1
I1018 08:53:57.788936 4097533 var_desc.cc:415] Flush  conv2d_6.tmp_0 1
I1018 08:53:57.788940 4097533 var_desc.cc:415] Flush  assign_9.tmp_0 1
I1018 08:53:57.788944 4097533 var_desc.cc:415] Flush  conv2d_1.tmp_0 1
I1018 08:53:57.788947 4097533 var_desc.cc:415] Flush  _jst.0.x.0 1
I1018 08:53:57.788950 4097533 var_desc.cc:415] Flush  conv2d_4.tmp_0 1
I1018 08:53:57.788954 4097533 var_desc.cc:415] Flush  assign_61.tmp_0 1
I1018 08:53:57.788957 4097533 var_desc.cc:415] Flush  assign_29.tmp_0 1
I1018 08:53:57.788960 4097533 var_desc.cc:415] Flush  assign_1.tmp_0 1
I1018 08:53:57.788964 4097533 var_desc.cc:415] Flush  relu_1.tmp_0 1
I1018 08:53:57.788969 4097533 var_desc.cc:415] Flush  assign_52.tmp_0 1
I1018 08:53:57.788971 4097533 var_desc.cc:415] Flush  assign_17.tmp_0 1
I1018 08:53:57.788975 4097533 var_desc.cc:415] Flush  relu_11.tmp_0 1
I1018 08:53:57.788978 4097533 var_desc.cc:415] Flush  relu_2.tmp_0 1
I1018 08:53:57.788991 4097533 var_desc.cc:415] Flush  assign_25.tmp_0 1
I1018 08:53:57.788995 4097533 var_desc.cc:415] Flush  conv2d_8.tmp_0 1
I1018 08:53:57.788998 4097533 var_desc.cc:415] Flush  assign_69.tmp_0 1
I1018 08:53:57.789001 4097533 var_desc.cc:415] Flush  relu_8.tmp_0 1
I1018 08:53:57.789016 4097533 var_desc.cc:415] Flush  assign_33.tmp_0 1
I1018 08:53:57.789021 4097533 var_desc.cc:415] Flush  relu_7.tmp_0 1
I1018 08:53:57.789026 4097533 var_desc.cc:415] Flush  conv2d_9.tmp_0 1
I1018 08:53:57.789031 4097533 var_desc.cc:415] Flush  conv2d_2.tmp_0 1
I1018 08:53:57.789034 4097533 var_desc.cc:415] Flush  assign_37.tmp_0 1
I1018 08:53:57.789037 4097533 var_desc.cc:415] Flush  relu_9.tmp_0 1
I1018 08:53:57.789042 4097533 var_desc.cc:415] Flush  conv2d_12.tmp_0 1
I1018 08:53:57.789045 4097533 var_desc.cc:415] Flush  assign_57.tmp_0 1
I1018 08:53:57.789048 4097533 var_desc.cc:415] Flush  assign_8.tmp_0 1
I1018 08:53:57.789054 4097533 var_desc.cc:415] Flush  relu_12.tmp_0 1
I1018 08:53:57.789063 4097533 var_desc.cc:415] Flush  assign_68.tmp_0 1
I1018 08:53:57.789067 4097533 var_desc.cc:415] Flush  assign_49.tmp_0 1
I1018 08:53:57.789070 4097533 var_desc.cc:415] Flush  relu_10.tmp_0 1
I1018 08:53:57.789078 4097533 var_desc.cc:415] Flush  assign_45.tmp_0 1
I1018 08:53:57.789081 4097533 var_desc.cc:415] Flush  conv2d_3.tmp_0 1
I1018 08:53:57.789085 4097533 var_desc.cc:415] Flush  conv2d_13.tmp_0 1
I1018 08:53:57.789090 4097533 var_desc.cc:415] Flush  relu_6.tmp_0 1
I1018 08:53:57.789100 4097533 var_desc.cc:415] Flush  assign_21.tmp_0 1
I1018 08:53:57.789103 4097533 var_desc.cc:415] Flush  conv2d_5.tmp_0 1
I1018 08:53:57.789108 4097533 var_desc.cc:415] Flush  conv2d_15.tmp_0 1
I1018 08:53:57.789121 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> cinn_launch0x55b7d1002020  via __control_var@14800x55b7d1043cd0
I1018 08:53:57.789124 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> cinn_launch0x55b7d1000c10  via __control_var@14720x55b7d1042590
I1018 08:53:57.789126 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d0ffcfe0  via __control_var@14740x55b7d1042b70
I1018 08:53:57.789129 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d0ffbc30  via __control_var@14750x55b7d1042e40
I1018 08:53:57.789131 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d1068fa0  via __control_var@15090x55b7d101e2a0
I1018 08:53:57.789134 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d1067b90  via __control_var@15010x55b7d101cbc0
I1018 08:53:57.789136 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> cinn_launch0x55b7d1066780  via __control_var@14980x55b7d101c330
I1018 08:53:57.789139 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> cinn_launch0x55b7d1065370  via __control_var@14900x55b7d101ac70
I1018 08:53:57.789140 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> cinn_launch0x55b7d1063f60  via __control_var@15070x55b7d101dd40
I1018 08:53:57.789144 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> cinn_launch0x55b7d1061740  via __control_var@15080x55b7d101dfd0
I1018 08:53:57.789145 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> cinn_launch0x55b7d0fe13a0  via __control_var@14920x55b7d101b1f0
I1018 08:53:57.789148 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdeb80  via __control_var@15030x55b7d101d160
I1018 08:53:57.789150 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdd770  via __control_var@14850x55b7d1044b40
I1018 08:53:57.789152 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdc360  via __control_var@15040x55b7d101d4d0
I1018 08:53:57.789155 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> cinn_launch0x55b7d0fdaf50  via __control_var@14970x55b7d101c060
I1018 08:53:57.789157 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> cinn_launch0x55b7d0fd9b40  via __control_var@14930x55b7d101b4e0
I1018 08:53:57.789160 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> cinn_launch0x55b7d0fd8730  via __control_var@14820x55b7d1044290
I1018 08:53:57.789162 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> cinn_launch0x55b7d1104d40  via __control_var@14840x55b7d1044870
I1018 08:53:57.789165 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> cinn_launch0x55b7d1103930  via __control_var@15000x55b7d101c8f0
I1018 08:53:57.789166 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> cinn_launch0x55b7d1102520  via __control_var@14990x55b7d101c600
I1018 08:53:57.789170 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d1101110  via __control_var@14880x55b7d10453e0
I1018 08:53:57.789171 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10ffd00  via __control_var@14890x55b7d10456b0
I1018 08:53:57.789173 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10fe8f0  via __control_var@14860x55b7d1044e10
I1018 08:53:57.789178 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10fd4e0  via __control_var@14760x55b7d10431b0
I1018 08:53:57.789180 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> cinn_launch0x55b7d1113ec0  via __control_var@14790x55b7d1043a00
I1018 08:53:57.789182 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> cinn_launch0x55b7d1112ab0  via __control_var@14780x55b7d1043710
I1018 08:53:57.789184 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> cinn_launch0x55b7d11116a0  via __control_var@14830x55b7d1044580
I1018 08:53:57.789187 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> cinn_launch0x55b7d1110290  via __control_var@14940x55b7d101b7d0
I1018 08:53:57.789189 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> cinn_launch0x55b7d110ee80  via __control_var@15060x55b7d101da70
I1018 08:53:57.789192 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> cinn_launch0x55b7d110da70  via __control_var@14910x55b7d101af00
I1018 08:53:57.789194 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d110c6a0  via __control_var@14960x55b7d101bd70
I1018 08:53:57.789196 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d107f940  via __control_var@15020x55b7d101ce90
I1018 08:53:57.789198 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> cinn_launch0x55b7d107e530  via __control_var@14770x55b7d1043480
I1018 08:53:57.789201 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> cinn_launch0x55b7d107d120  via __control_var@14950x55b7d101baa0
I1018 08:53:57.789203 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d107bd10  via __control_var@14870x55b7d1045110
I1018 08:53:57.789220 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d107a890  via __control_var@14810x55b7d1043fa0
I1018 08:53:57.789224 4097533 graph_helper.h:104] adj conv2d0x55b7d106f840 -> cinn_launch0x55b7d109c290  via conv2d_19.tmp_00x55b7d109bfc0
I1018 08:53:57.789227 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d109c290  via relu_14.tmp_00x55b7d10900a0
I1018 08:53:57.789229 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> conv2d0x55b7d106f840  via relu_15.tmp_00x55b7d106f450
I1018 08:53:57.789232 4097533 graph_helper.h:104] adj conv2d0x55b7d10908e0 -> cinn_launch0x55b7d106aaf0  via conv2d_18.tmp_00x55b7d106a820
I1018 08:53:57.789233 4097533 graph_helper.h:104] adj conv2d0x55b7d0fc2680 -> cinn_launch0x55b7d1126750  via conv2d_17.tmp_00x55b7d11264a0
I1018 08:53:57.789237 4097533 graph_helper.h:104] adj conv2d0x55b7d0fb56d0 -> cinn_launch0x55b7d1126750  via conv2d_16.tmp_00x55b7d0fc23b0
I1018 08:53:57.789238 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> conv2d0x55b7d0fc2680  via relu_12.tmp_00x55b7d1122b70
I1018 08:53:57.789240 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> cinn_launch0x55b7d1062b50  via __control_var@15110x55b7d101e840
I1018 08:53:57.789243 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> conv2d0x55b7d0fb56d0  via relu_13.tmp_00x55b7d0fb52e0
I1018 08:53:57.789245 4097533 graph_helper.h:104] adj conv2d0x55b7d1122f60 -> cinn_launch0x55b7d0fd6ee0  via conv2d_15.tmp_00x55b7d0fd6c10
I1018 08:53:57.789248 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> conv2d0x55b7d1122f60  via relu_12.tmp_00x55b7d1122b70
I1018 08:53:57.789250 4097533 graph_helper.h:104] adj conv2d0x55b7d11091c0 -> cinn_launch0x55b7d0fea800  via conv2d_14.tmp_00x55b7d0fea530
I1018 08:53:57.789252 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fea800  via relu_10.tmp_00x55b7d102b4b0
I1018 08:53:57.789255 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> conv2d0x55b7d11091c0  via relu_11.tmp_00x55b7d1108dd0
I1018 08:53:57.789258 4097533 graph_helper.h:104] adj conv2d0x55b7d102bc70 -> cinn_launch0x55b7d1115560  via conv2d_13.tmp_00x55b7d11184f0
I1018 08:53:57.789263 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> conv2d0x55b7d102bc70  via relu_10.tmp_00x55b7d102b4b0
I1018 08:53:57.789264 4097533 graph_helper.h:104] adj conv2d0x55b7d0d441e0 -> cinn_launch0x55b7d1031e80  via conv2d_12.tmp_00x55b7d1031bd0
I1018 08:53:57.789266 4097533 graph_helper.h:104] adj conv2d0x55b7d0e99aa0 -> cinn_launch0x55b7d1031e80  via conv2d_11.tmp_00x55b7d0d43f10
I1018 08:53:57.789269 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> conv2d0x55b7d0d441e0  via relu_8.tmp_00x55b7d1029a90
I1018 08:53:57.789271 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> conv2d0x55b7d0e99aa0  via relu_9.tmp_00x55b7d0e996b0
I1018 08:53:57.789274 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> conv2d0x55b7d1029e80  via relu_8.tmp_00x55b7d1029a90
I1018 08:53:57.789276 4097533 graph_helper.h:104] adj conv2d0x55b7d0ec1350 -> cinn_launch0x55b7d1047f30  via conv2d_9.tmp_00x55b7d1047c60
I1018 08:53:57.789278 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d1047f30  via relu_6.tmp_00x55b7d0e88b80
I1018 08:53:57.789281 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> conv2d0x55b7d0ec1350  via relu_7.tmp_00x55b7d0e0f4e0
I1018 08:53:57.789283 4097533 graph_helper.h:104] adj conv2d0x55b7d0e89340 -> cinn_launch0x55b7d0f0e1c0  via conv2d_8.tmp_00x55b7d0f0def0
I1018 08:53:57.789286 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> conv2d0x55b7d0e89340  via relu_6.tmp_00x55b7d0e88b80
I1018 08:53:57.789288 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> cinn_launch0x55b7d0ffe3f0  via __control_var@14730x55b7d1042880
I1018 08:53:57.789291 4097533 graph_helper.h:104] adj conv2d0x55b7d1119e20 -> cinn_launch0x55b7d0add8b0  via conv2d_7.tmp_00x55b7d0add600
I1018 08:53:57.789294 4097533 graph_helper.h:104] adj conv2d0x55b7d0d89b40 -> cinn_launch0x55b7d0add8b0  via conv2d_6.tmp_00x55b7d1119b50
I1018 08:53:57.789295 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> conv2d0x55b7d1119e20  via relu_4.tmp_00x55b7d111b9f0
I1018 08:53:57.789299 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> conv2d0x55b7d0d89b40  via relu_5.tmp_00x55b7d0d89750
I1018 08:53:57.789300 4097533 graph_helper.h:104] adj conv2d0x55b7d111bde0 -> cinn_launch0x55b7d0dcf300  via conv2d_5.tmp_00x55b7d0dcf030
I1018 08:53:57.789304 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> conv2d0x55b7d111bde0  via relu_4.tmp_00x55b7d111b9f0
I1018 08:53:57.789305 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d0c640b0  via relu_2.tmp_00x55b7d0c74b50
I1018 08:53:57.789307 4097533 graph_helper.h:104] adj conv2d0x55b7d0b70160 -> cinn_launch0x55b7d0c640b0  via conv2d_4.tmp_00x55b7d0c63de0
I1018 08:53:57.789310 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> conv2d0x55b7d0b70160  via relu_3.tmp_00x55b7d0b6fd70
I1018 08:53:57.789312 4097533 graph_helper.h:104] adj conv2d0x55b7d0c74ec0 -> cinn_launch0x55b7d0c9d5e0  via conv2d_3.tmp_00x55b7d0e7f460
I1018 08:53:57.789314 4097533 graph_helper.h:104] adj conv2d0x55b7d1029e80 -> cinn_launch0x55b7d0cb5e70  via conv2d_10.tmp_00x55b7d0cb8f30
I1018 08:53:57.789317 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> conv2d0x55b7d0c74ec0  via relu_2.tmp_00x55b7d0c74b50
I1018 08:53:57.789319 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdff90  via __control_var@15050x55b7d101d7a0
I1018 08:53:57.789322 4097533 graph_helper.h:104] adj conv2d0x55b7d0edeb50 -> cinn_launch0x55b7d0cfb320  via conv2d_2.tmp_00x55b7d0cfb110
I1018 08:53:57.789324 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d0cfb320  via pool2d_0.tmp_00x55b7d0b929b0
I1018 08:53:57.789327 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> conv2d0x55b7d0edeb50  via relu_1.tmp_00x55b7d0de1f20
I1018 08:53:57.789330 4097533 graph_helper.h:104] adj conv2d0x55b7d0ddf060 -> cinn_launch0x55b7d0c892a0  via conv2d_1.tmp_00x55b7d0c89030
I1018 08:53:57.789331 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> conv2d0x55b7d0ddf060  via pool2d_0.tmp_00x55b7d0b929b0
I1018 08:53:57.789336 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> conv2d0x55b7d10908e0  via relu_14.tmp_00x55b7d10900a0
I1018 08:53:57.789340 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> cinn_launch0x55b7d0fff800  via __control_var@15100x55b7d101e570
I1018 08:53:57.789341 4097533 graph_helper.h:104] adj conv2d0x55b7d0edf1d0 -> cinn_launch0x55b7d0fac1b0  via conv2d_0.tmp_00x55b7d0eaa280
I1018 08:53:57.789506 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> cinn_launch0x55b7d1002020  via __control_var@14800x55b7d1043cd0
I1018 08:53:57.789510 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> cinn_launch0x55b7d1000c10  via __control_var@14720x55b7d1042590
I1018 08:53:57.789511 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d0ffcfe0  via __control_var@14740x55b7d1042b70
I1018 08:53:57.789515 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d0ffbc30  via __control_var@14750x55b7d1042e40
I1018 08:53:57.789517 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d1068fa0  via __control_var@15090x55b7d101e2a0
I1018 08:53:57.789520 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d1067b90  via __control_var@15010x55b7d101cbc0
I1018 08:53:57.789521 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> cinn_launch0x55b7d1066780  via __control_var@14980x55b7d101c330
I1018 08:53:57.789523 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> cinn_launch0x55b7d1065370  via __control_var@14900x55b7d101ac70
I1018 08:53:57.789526 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> cinn_launch0x55b7d1063f60  via __control_var@15070x55b7d101dd40
I1018 08:53:57.789528 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> cinn_launch0x55b7d1061740  via __control_var@15080x55b7d101dfd0
I1018 08:53:57.789530 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> cinn_launch0x55b7d0fe13a0  via __control_var@14920x55b7d101b1f0
I1018 08:53:57.789533 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdeb80  via __control_var@15030x55b7d101d160
I1018 08:53:57.789535 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdd770  via __control_var@14850x55b7d1044b40
I1018 08:53:57.789538 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdc360  via __control_var@15040x55b7d101d4d0
I1018 08:53:57.789541 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> cinn_launch0x55b7d0fdaf50  via __control_var@14970x55b7d101c060
I1018 08:53:57.789542 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> cinn_launch0x55b7d0fd9b40  via __control_var@14930x55b7d101b4e0
I1018 08:53:57.789544 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> cinn_launch0x55b7d0fd8730  via __control_var@14820x55b7d1044290
I1018 08:53:57.789547 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> cinn_launch0x55b7d1104d40  via __control_var@14840x55b7d1044870
I1018 08:53:57.789549 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> cinn_launch0x55b7d1103930  via __control_var@15000x55b7d101c8f0
I1018 08:53:57.789551 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> cinn_launch0x55b7d1102520  via __control_var@14990x55b7d101c600
I1018 08:53:57.789554 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d1101110  via __control_var@14880x55b7d10453e0
I1018 08:53:57.789556 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10ffd00  via __control_var@14890x55b7d10456b0
I1018 08:53:57.789558 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10fe8f0  via __control_var@14860x55b7d1044e10
I1018 08:53:57.789561 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10fd4e0  via __control_var@14760x55b7d10431b0
I1018 08:53:57.789563 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> cinn_launch0x55b7d1113ec0  via __control_var@14790x55b7d1043a00
I1018 08:53:57.789568 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> cinn_launch0x55b7d1112ab0  via __control_var@14780x55b7d1043710
I1018 08:53:57.789570 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> cinn_launch0x55b7d11116a0  via __control_var@14830x55b7d1044580
I1018 08:53:57.789572 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> cinn_launch0x55b7d1110290  via __control_var@14940x55b7d101b7d0
I1018 08:53:57.789575 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> cinn_launch0x55b7d110ee80  via __control_var@15060x55b7d101da70
I1018 08:53:57.789577 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> cinn_launch0x55b7d110da70  via __control_var@14910x55b7d101af00
I1018 08:53:57.789579 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d110c6a0  via __control_var@14960x55b7d101bd70
I1018 08:53:57.789582 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d107f940  via __control_var@15020x55b7d101ce90
I1018 08:53:57.789584 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> cinn_launch0x55b7d107e530  via __control_var@14770x55b7d1043480
I1018 08:53:57.789587 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> cinn_launch0x55b7d107d120  via __control_var@14950x55b7d101baa0
I1018 08:53:57.789588 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d107bd10  via __control_var@14870x55b7d1045110
I1018 08:53:57.789592 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d107a890  via __control_var@14810x55b7d1043fa0
I1018 08:53:57.789593 4097533 graph_helper.h:104] adj conv2d0x55b7d106f840 -> cinn_launch0x55b7d109c290  via conv2d_19.tmp_00x55b7d109bfc0
I1018 08:53:57.789597 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d109c290  via relu_14.tmp_00x55b7d10900a0
I1018 08:53:57.789598 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> conv2d0x55b7d106f840  via relu_15.tmp_00x55b7d106f450
I1018 08:53:57.789600 4097533 graph_helper.h:104] adj conv2d0x55b7d10908e0 -> cinn_launch0x55b7d106aaf0  via conv2d_18.tmp_00x55b7d106a820
I1018 08:53:57.789603 4097533 graph_helper.h:104] adj conv2d0x55b7d0fc2680 -> cinn_launch0x55b7d1126750  via conv2d_17.tmp_00x55b7d11264a0
I1018 08:53:57.789605 4097533 graph_helper.h:104] adj conv2d0x55b7d0fb56d0 -> cinn_launch0x55b7d1126750  via conv2d_16.tmp_00x55b7d0fc23b0
I1018 08:53:57.789608 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> conv2d0x55b7d0fc2680  via relu_12.tmp_00x55b7d1122b70
I1018 08:53:57.789610 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> cinn_launch0x55b7d1062b50  via __control_var@15110x55b7d101e840
I1018 08:53:57.789613 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> conv2d0x55b7d0fb56d0  via relu_13.tmp_00x55b7d0fb52e0
I1018 08:53:57.789615 4097533 graph_helper.h:104] adj conv2d0x55b7d1122f60 -> cinn_launch0x55b7d0fd6ee0  via conv2d_15.tmp_00x55b7d0fd6c10
I1018 08:53:57.789618 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> conv2d0x55b7d1122f60  via relu_12.tmp_00x55b7d1122b70
I1018 08:53:57.789619 4097533 graph_helper.h:104] adj conv2d0x55b7d11091c0 -> cinn_launch0x55b7d0fea800  via conv2d_14.tmp_00x55b7d0fea530
I1018 08:53:57.789623 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fea800  via relu_10.tmp_00x55b7d102b4b0
I1018 08:53:57.789624 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> conv2d0x55b7d11091c0  via relu_11.tmp_00x55b7d1108dd0
I1018 08:53:57.789626 4097533 graph_helper.h:104] adj conv2d0x55b7d102bc70 -> cinn_launch0x55b7d1115560  via conv2d_13.tmp_00x55b7d11184f0
I1018 08:53:57.789629 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> conv2d0x55b7d102bc70  via relu_10.tmp_00x55b7d102b4b0
I1018 08:53:57.789631 4097533 graph_helper.h:104] adj conv2d0x55b7d0d441e0 -> cinn_launch0x55b7d1031e80  via conv2d_12.tmp_00x55b7d1031bd0
I1018 08:53:57.789634 4097533 graph_helper.h:104] adj conv2d0x55b7d0e99aa0 -> cinn_launch0x55b7d1031e80  via conv2d_11.tmp_00x55b7d0d43f10
I1018 08:53:57.789638 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> conv2d0x55b7d0d441e0  via relu_8.tmp_00x55b7d1029a90
I1018 08:53:57.789640 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> conv2d0x55b7d0e99aa0  via relu_9.tmp_00x55b7d0e996b0
I1018 08:53:57.789644 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> conv2d0x55b7d1029e80  via relu_8.tmp_00x55b7d1029a90
I1018 08:53:57.789645 4097533 graph_helper.h:104] adj conv2d0x55b7d0ec1350 -> cinn_launch0x55b7d1047f30  via conv2d_9.tmp_00x55b7d1047c60
I1018 08:53:57.789647 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d1047f30  via relu_6.tmp_00x55b7d0e88b80
I1018 08:53:57.789650 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> conv2d0x55b7d0ec1350  via relu_7.tmp_00x55b7d0e0f4e0
I1018 08:53:57.789652 4097533 graph_helper.h:104] adj conv2d0x55b7d0e89340 -> cinn_launch0x55b7d0f0e1c0  via conv2d_8.tmp_00x55b7d0f0def0
I1018 08:53:57.789654 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> conv2d0x55b7d0e89340  via relu_6.tmp_00x55b7d0e88b80
I1018 08:53:57.789657 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> cinn_launch0x55b7d0ffe3f0  via __control_var@14730x55b7d1042880
I1018 08:53:57.789659 4097533 graph_helper.h:104] adj conv2d0x55b7d1119e20 -> cinn_launch0x55b7d0add8b0  via conv2d_7.tmp_00x55b7d0add600
I1018 08:53:57.789661 4097533 graph_helper.h:104] adj conv2d0x55b7d0d89b40 -> cinn_launch0x55b7d0add8b0  via conv2d_6.tmp_00x55b7d1119b50
I1018 08:53:57.789664 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> conv2d0x55b7d1119e20  via relu_4.tmp_00x55b7d111b9f0
I1018 08:53:57.789666 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> conv2d0x55b7d0d89b40  via relu_5.tmp_00x55b7d0d89750
I1018 08:53:57.789669 4097533 graph_helper.h:104] adj conv2d0x55b7d111bde0 -> cinn_launch0x55b7d0dcf300  via conv2d_5.tmp_00x55b7d0dcf030
I1018 08:53:57.789671 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> conv2d0x55b7d111bde0  via relu_4.tmp_00x55b7d111b9f0
I1018 08:53:57.789673 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d0c640b0  via relu_2.tmp_00x55b7d0c74b50
I1018 08:53:57.789676 4097533 graph_helper.h:104] adj conv2d0x55b7d0b70160 -> cinn_launch0x55b7d0c640b0  via conv2d_4.tmp_00x55b7d0c63de0
I1018 08:53:57.789678 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> conv2d0x55b7d0b70160  via relu_3.tmp_00x55b7d0b6fd70
I1018 08:53:57.789681 4097533 graph_helper.h:104] adj conv2d0x55b7d0c74ec0 -> cinn_launch0x55b7d0c9d5e0  via conv2d_3.tmp_00x55b7d0e7f460
I1018 08:53:57.789683 4097533 graph_helper.h:104] adj conv2d0x55b7d1029e80 -> cinn_launch0x55b7d0cb5e70  via conv2d_10.tmp_00x55b7d0cb8f30
I1018 08:53:57.789685 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> conv2d0x55b7d0c74ec0  via relu_2.tmp_00x55b7d0c74b50
I1018 08:53:57.789688 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdff90  via __control_var@15050x55b7d101d7a0
I1018 08:53:57.789690 4097533 graph_helper.h:104] adj conv2d0x55b7d0edeb50 -> cinn_launch0x55b7d0cfb320  via conv2d_2.tmp_00x55b7d0cfb110
I1018 08:53:57.789692 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d0cfb320  via pool2d_0.tmp_00x55b7d0b929b0
I1018 08:53:57.789695 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> conv2d0x55b7d0edeb50  via relu_1.tmp_00x55b7d0de1f20
I1018 08:53:57.789697 4097533 graph_helper.h:104] adj conv2d0x55b7d0ddf060 -> cinn_launch0x55b7d0c892a0  via conv2d_1.tmp_00x55b7d0c89030
I1018 08:53:57.789700 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> conv2d0x55b7d0ddf060  via pool2d_0.tmp_00x55b7d0b929b0
I1018 08:53:57.789702 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> conv2d0x55b7d10908e0  via relu_14.tmp_00x55b7d10900a0
I1018 08:53:57.789705 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> cinn_launch0x55b7d0fff800  via __control_var@15100x55b7d101e570
I1018 08:53:57.789709 4097533 graph_helper.h:104] adj conv2d0x55b7d0edf1d0 -> cinn_launch0x55b7d0fac1b0  via conv2d_0.tmp_00x55b7d0eaa280
I1018 08:53:57.789829 4097533 parallel_executor.cc:482] Inplace strategy is enabled, when build_strategy.enable_inplace = True
I1018 08:53:57.789868 4097533 var_desc.cc:415] Flush  relu_8.tmp_0 0
I1018 08:53:57.789871 4097533 var_desc.cc:415] Flush  assign_33.tmp_0 0
I1018 08:53:57.789873 4097533 var_desc.cc:415] Flush  conv2d_8.tmp_0 0
I1018 08:53:57.789876 4097533 var_desc.cc:415] Flush  assign_68.tmp_0 0
I1018 08:53:57.789878 4097533 var_desc.cc:415] Flush  _jst.0.x.0 0
I1018 08:53:57.789880 4097533 var_desc.cc:415] Flush  assign_24.tmp_0 0
I1018 08:53:57.789883 4097533 var_desc.cc:415] Flush  relu_3.tmp_0 0
I1018 08:53:57.789885 4097533 var_desc.cc:415] Flush  assign_20.tmp_0 0
I1018 08:53:57.789887 4097533 var_desc.cc:415] Flush  assign_32.tmp_0 0
I1018 08:53:57.789891 4097533 var_desc.cc:415] Flush  relu_1.tmp_0 0
I1018 08:53:57.789892 4097533 var_desc.cc:415] Flush  assign_0.tmp_0 0
I1018 08:53:57.789894 4097533 var_desc.cc:415] Flush  assign_4.tmp_0 0
I1018 08:53:57.789897 4097533 var_desc.cc:415] Flush  assign_28.tmp_0 0
I1018 08:53:57.789899 4097533 var_desc.cc:415] Flush  assign_36.tmp_0 0
I1018 08:53:57.789902 4097533 var_desc.cc:415] Flush  assign_52.tmp_0 0
I1018 08:53:57.789904 4097533 var_desc.cc:415] Flush  conv2d_4.tmp_0 0
I1018 08:53:57.789906 4097533 var_desc.cc:415] Flush  assign_17.tmp_0 0
I1018 08:53:57.789909 4097533 var_desc.cc:415] Flush  relu_2.tmp_0 0
I1018 08:53:57.789911 4097533 var_desc.cc:415] Flush  assign_48.tmp_0 0
I1018 08:53:57.789913 4097533 var_desc.cc:415] Flush  assign_16.tmp_0 0
I1018 08:53:57.789916 4097533 var_desc.cc:415] Flush  relu_4.tmp_0 0
I1018 08:53:57.789918 4097533 var_desc.cc:415] Flush  relu_13.tmp_0 0
I1018 08:53:57.789920 4097533 var_desc.cc:415] Flush  conv2d_1.tmp_0 0
I1018 08:53:57.789923 4097533 var_desc.cc:415] Flush  assign_5.tmp_0 0
I1018 08:53:57.789925 4097533 var_desc.cc:415] Flush  relu_15.tmp_0 0
I1018 08:53:57.789927 4097533 var_desc.cc:415] Flush  assign_53.tmp_0 0
I1018 08:53:57.789930 4097533 var_desc.cc:415] Flush  conv2d_13.tmp_0 0
I1018 08:53:57.789932 4097533 var_desc.cc:415] Flush  assign_1.tmp_0 0
I1018 08:53:57.789934 4097533 var_desc.cc:415] Flush  conv2d_0.tmp_0 0
I1018 08:53:57.789937 4097533 var_desc.cc:415] Flush  assign_77.tmp_0 0
I1018 08:53:57.789939 4097533 var_desc.cc:415] Flush  conv2d_19.tmp_0 0
I1018 08:53:57.789942 4097533 var_desc.cc:415] Flush  relu_14.tmp_0 0
I1018 08:53:57.789943 4097533 var_desc.cc:415] Flush  linear_0.tmp_1 0
I1018 08:53:57.789947 4097533 var_desc.cc:415] Flush  relu_6.tmp_0 0
I1018 08:53:57.789948 4097533 var_desc.cc:415] Flush  conv2d_9.tmp_0 0
I1018 08:53:57.789950 4097533 var_desc.cc:415] Flush  assign_37.tmp_0 0
I1018 08:53:57.789952 4097533 var_desc.cc:415] Flush  assign_41.tmp_0 0
I1018 08:53:57.789955 4097533 var_desc.cc:415] Flush  conv2d_10.tmp_0 0
I1018 08:53:57.789958 4097533 var_desc.cc:415] Flush  assign_56.tmp_0 0
I1018 08:53:57.789960 4097533 var_desc.cc:415] Flush  relu_12.tmp_0 0
I1018 08:53:57.789963 4097533 var_desc.cc:415] Flush  assign_61.tmp_0 0
I1018 08:53:57.789964 4097533 var_desc.cc:415] Flush  conv2d_15.tmp_0 0
I1018 08:53:57.789966 4097533 var_desc.cc:415] Flush  relu_12.tmp_0 0
I1018 08:53:57.789969 4097533 var_desc.cc:415] Flush  conv2d_14.tmp_0 0
I1018 08:53:57.789971 4097533 var_desc.cc:415] Flush  assign_57.tmp_0 0
I1018 08:53:57.789974 4097533 var_desc.cc:415] Flush  relu_10.tmp_0 0
I1018 08:53:57.789976 4097533 var_desc.cc:415] Flush  relu_9.tmp_0 0
I1018 08:53:57.789979 4097533 var_desc.cc:415] Flush  conv2d_11.tmp_0 0
I1018 08:53:57.789981 4097533 var_desc.cc:415] Flush  conv2d_12.tmp_0 0
I1018 08:53:57.789983 4097533 var_desc.cc:415] Flush  assign_49.tmp_0 0
I1018 08:53:57.789985 4097533 var_desc.cc:415] Flush  assign_45.tmp_0 0
I1018 08:53:57.789988 4097533 var_desc.cc:415] Flush  relu_11.tmp_0 0
I1018 08:53:57.789990 4097533 var_desc.cc:415] Flush  relu_4.tmp_0 0
I1018 08:53:57.789992 4097533 var_desc.cc:415] Flush  relu_5.tmp_0 0
I1018 08:53:57.789997 4097533 var_desc.cc:415] Flush  relu_7.tmp_0 0
I1018 08:53:57.789999 4097533 var_desc.cc:415] Flush  assign_8.tmp_0 0
I1018 08:53:57.790001 4097533 var_desc.cc:415] Flush  assign_73.tmp_0 0
I1018 08:53:57.790004 4097533 var_desc.cc:415] Flush  conv2d_18.tmp_0 0
I1018 08:53:57.790006 4097533 var_desc.cc:415] Flush  assign_60.tmp_0 0
I1018 08:53:57.790009 4097533 var_desc.cc:415] Flush  assign_21.tmp_0 0
I1018 08:53:57.790011 4097533 var_desc.cc:415] Flush  conv2d_5.tmp_0 0
I1018 08:53:57.790014 4097533 var_desc.cc:415] Flush  assign_72.tmp_0 0
I1018 08:53:57.790015 4097533 var_desc.cc:415] Flush  assign_13.tmp_0 0
I1018 08:53:57.790019 4097533 var_desc.cc:415] Flush  conv2d_3.tmp_0 0
I1018 08:53:57.790020 4097533 var_desc.cc:415] Flush  assign_40.tmp_0 0
I1018 08:53:57.790022 4097533 var_desc.cc:415] Flush  assign_9.tmp_0 0
I1018 08:53:57.790024 4097533 var_desc.cc:415] Flush  conv2d_2.tmp_0 0
I1018 08:53:57.790026 4097533 var_desc.cc:415] Flush  pool2d_0.tmp_0 0
I1018 08:53:57.790030 4097533 var_desc.cc:415] Flush  relu_8.tmp_0 0
I1018 08:53:57.790031 4097533 var_desc.cc:415] Flush  assign_12.tmp_0 0
I1018 08:53:57.790035 4097533 var_desc.cc:415] Flush  assign_76.tmp_0 0
I1018 08:53:57.790036 4097533 var_desc.cc:415] Flush  assign_69.tmp_0 0
I1018 08:53:57.790038 4097533 var_desc.cc:415] Flush  conv2d_16.tmp_0 0
I1018 08:53:57.790040 4097533 var_desc.cc:415] Flush  conv2d_17.tmp_0 0
I1018 08:53:57.790042 4097533 var_desc.cc:415] Flush  assign_65.tmp_0 0
I1018 08:53:57.790045 4097533 var_desc.cc:415] Flush  assign_44.tmp_0 0
I1018 08:53:57.790047 4097533 var_desc.cc:415] Flush  conv2d_6.tmp_0 0
I1018 08:53:57.790050 4097533 var_desc.cc:415] Flush  assign_25.tmp_0 0
I1018 08:53:57.790052 4097533 var_desc.cc:415] Flush  assign_29.tmp_0 0
I1018 08:53:57.790055 4097533 var_desc.cc:415] Flush  conv2d_7.tmp_0 0
I1018 08:53:57.790056 4097533 var_desc.cc:415] Flush  assign_64.tmp_0 0
I1018 08:53:57.790277 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103d9e0 -> eager_deletion0x55b7d0bf8d60  via __control_var@16490x55b7d0be5660
I1018 08:53:57.790279 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> eager_deletion0x55b7d0c2da20  via __control_var@14760x55b7d10431b0
I1018 08:53:57.790282 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> eager_deletion0x55b7d0c7f270  via __control_var@14740x55b7d1042b70
I1018 08:53:57.790284 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> eager_deletion0x55b7d0d36320  via __control_var@14960x55b7d101bd70
I1018 08:53:57.790287 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1013850 -> eager_deletion0x55b7d0d50360  via __control_var@16280x55b7d0d50420
I1018 08:53:57.790289 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> eager_deletion0x55b7d0d83e40  via __control_var@14910x55b7d101af00
I1018 08:53:57.790292 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103ffa0 -> eager_deletion0x55b7d0da05e0  via __control_var@16230x55b7d0da06a0
I1018 08:53:57.790294 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> eager_deletion0x55b7d0dd5690  via __control_var@14780x55b7d1043710
I1018 08:53:57.790297 4097533 graph_helper.h:104] adj conv2d0x55b7d1029e80 -> eager_deletion0x55b7d0d04df0  via __control_var@16330x55b7d0d04eb0
I1018 08:53:57.790299 4097533 graph_helper.h:104] adj cinn_launch0x55b7d10059f0 -> eager_deletion0x55b7d0ec5bb0  via __control_var@16130x55b7d0c239b0
I1018 08:53:57.790302 4097533 graph_helper.h:104] adj conv2d0x55b7d11091c0 -> eager_deletion0x55b7d0e31dd0  via __control_var@16010x55b7d0e680a0
I1018 08:53:57.790304 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> eager_deletion0x55b7d0dac250  via __control_var@15070x55b7d101dd40
I1018 08:53:57.790306 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> eager_deletion0x55b7d0d20510  via __control_var@14900x55b7d101ac70
I1018 08:53:57.790308 4097533 graph_helper.h:104] adj conv2d0x55b7d1122f60 -> eager_deletion0x55b7d0d2e9e0  via __control_var@15860x55b7d0d11c80
I1018 08:53:57.790311 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> eager_deletion0x55b7d0ccaa80  via __control_var@14930x55b7d101b4e0
I1018 08:53:57.790318 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> eager_deletion0x55b7d0c8c100  via __control_var@14820x55b7d1044290
I1018 08:53:57.790320 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> eager_deletion0x55b7d0c82340  via __control_var@14730x55b7d1042880
I1018 08:53:57.790323 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> eager_deletion0x55b7d0be1970  via __control_var@14720x55b7d1042590
I1018 08:53:57.790325 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> eager_deletion0x55b7d0e7c680  via __control_var@14810x55b7d1043fa0
I1018 08:53:57.790328 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> eager_deletion0x55b7d0c100f0  via __control_var@14920x55b7d101b1f0
I1018 08:53:57.790329 4097533 graph_helper.h:104] adj conv2d0x55b7d106f840 -> eager_deletion0x55b7d0fc1590  via __control_var@15700x55b7d0bff1e0
I1018 08:53:57.790333 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> eager_deletion0x55b7d0c74830  via __control_var@14770x55b7d1043480
I1018 08:53:57.790334 4097533 graph_helper.h:104] adj conv2d0x55b7d0fb56d0 -> eager_deletion0x55b7d0ebbb40  via __control_var@15650x55b7d0a9d670
I1018 08:53:57.790336 4097533 graph_helper.h:104] adj conv2d0x55b7d111bde0 -> eager_deletion0x55b7d1106060  via __control_var@15620x55b7d0b734a0
I1018 08:53:57.790339 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100c710 -> eager_deletion0x55b7d0aeffc0  via __control_var@15590x55b7d0afe850
I1018 08:53:57.790342 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1038e60 -> eager_deletion0x55b7d0aa9140  via __control_var@15560x55b7d0e966c0
I1018 08:53:57.790344 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> eager_deletion0x55b7d0a72230  via __control_var@14830x55b7d1044580
I1018 08:53:57.790346 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103a140 -> eager_deletion0x55b7d0a639a0  via __control_var@15510x55b7d0dbb500
I1018 08:53:57.790349 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1012570 -> eager_deletion0x55b7d0a1f3e0  via __control_var@15480x55b7d0995060
I1018 08:53:57.790351 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1004710 -> eager_deletion0x55b7d09a1410  via __control_var@15420x55b7d09da090
I1018 08:53:57.790354 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1003430 -> eager_deletion0x55b7d0abb300  via __control_var@15390x55b7d0990130
I1018 08:53:57.790356 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100d9f0 -> eager_deletion0x55b7d0c57440  via __control_var@15300x55b7d0affe60
I1018 08:53:57.790359 4097533 graph_helper.h:104] adj conv2d0x55b7d0b70160 -> eager_deletion0x55b7d0a73840  via __control_var@15270x55b7d0e77f40
I1018 08:53:57.790361 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100ecd0 -> eager_deletion0x55b7d0c96340  via __control_var@15240x55b7d1033be0
I1018 08:53:57.790364 4097533 graph_helper.h:104] adj conv2d0x55b7d0edf1d0 -> eager_deletion0x55b7d0c57d60  via __control_var@15210x55b7d0b86300
I1018 08:53:57.790366 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> eager_deletion0x55b7cd159bc0  via __control_var@14990x55b7d101c600
I1018 08:53:57.790369 4097533 graph_helper.h:104] adj conv2d0x55b7d0d441e0 -> eager_deletion0x55b7d09765a0  via __control_var@15130x55b7d096fc60
I1018 08:53:57.790370 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103ecc0 -> eager_deletion0x55b7b78d2db0  via __control_var@15180x55b7b7024640
I1018 08:53:57.790374 4097533 graph_helper.h:104] adj conv2d0x55b7d0edeb50 -> eager_deletion0x55b7d0cdcfc0  via __control_var@15360x55b7d0dad860
I1018 08:53:57.790377 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> cinn_launch0x55b7d1002020  via __control_var@14800x55b7d1043cd0
I1018 08:53:57.790380 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> cinn_launch0x55b7d1000c10  via __control_var@14720x55b7d1042590
I1018 08:53:57.790383 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d0ffcfe0  via __control_var@14740x55b7d1042b70
I1018 08:53:57.790385 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d0ffbc30  via __control_var@14750x55b7d1042e40
I1018 08:53:57.790390 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d1068fa0  via __control_var@15090x55b7d101e2a0
I1018 08:53:57.790392 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d1067b90  via __control_var@15010x55b7d101cbc0
I1018 08:53:57.790395 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> cinn_launch0x55b7d1066780  via __control_var@14980x55b7d101c330
I1018 08:53:57.790396 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> cinn_launch0x55b7d1065370  via __control_var@14900x55b7d101ac70
I1018 08:53:57.790400 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> cinn_launch0x55b7d1063f60  via __control_var@15070x55b7d101dd40
I1018 08:53:57.790401 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> cinn_launch0x55b7d1061740  via __control_var@15080x55b7d101dfd0
I1018 08:53:57.790405 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> cinn_launch0x55b7d0fe13a0  via __control_var@14920x55b7d101b1f0
I1018 08:53:57.790406 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdeb80  via __control_var@15030x55b7d101d160
I1018 08:53:57.790409 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdd770  via __control_var@14850x55b7d1044b40
I1018 08:53:57.790411 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdc360  via __control_var@15040x55b7d101d4d0
I1018 08:53:57.790413 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> cinn_launch0x55b7d0fdaf50  via __control_var@14970x55b7d101c060
I1018 08:53:57.790416 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> cinn_launch0x55b7d0fd9b40  via __control_var@14930x55b7d101b4e0
I1018 08:53:57.790418 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> cinn_launch0x55b7d0fd8730  via __control_var@14820x55b7d1044290
I1018 08:53:57.790421 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> cinn_launch0x55b7d1104d40  via __control_var@14840x55b7d1044870
I1018 08:53:57.790423 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> cinn_launch0x55b7d1103930  via __control_var@15000x55b7d101c8f0
I1018 08:53:57.790426 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> cinn_launch0x55b7d1102520  via __control_var@14990x55b7d101c600
I1018 08:53:57.790427 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d1101110  via __control_var@14880x55b7d10453e0
I1018 08:53:57.790431 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10ffd00  via __control_var@14890x55b7d10456b0
I1018 08:53:57.790432 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10fe8f0  via __control_var@14860x55b7d1044e10
I1018 08:53:57.790434 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10fd4e0  via __control_var@14760x55b7d10431b0
I1018 08:53:57.790437 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> cinn_launch0x55b7d1113ec0  via __control_var@14790x55b7d1043a00
I1018 08:53:57.790439 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> cinn_launch0x55b7d1112ab0  via __control_var@14780x55b7d1043710
I1018 08:53:57.790442 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> cinn_launch0x55b7d11116a0  via __control_var@14830x55b7d1044580
I1018 08:53:57.790444 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> cinn_launch0x55b7d1110290  via __control_var@14940x55b7d101b7d0
I1018 08:53:57.790446 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> cinn_launch0x55b7d110ee80  via __control_var@15060x55b7d101da70
I1018 08:53:57.790448 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> cinn_launch0x55b7d110da70  via __control_var@14910x55b7d101af00
I1018 08:53:57.790452 4097533 graph_helper.h:104] adj conv2d0x55b7d1119e20 -> eager_deletion0x55b7d0e76930  via __control_var@16040x55b7d0e8a420
I1018 08:53:57.790455 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d110c6a0  via __control_var@14960x55b7d101bd70
I1018 08:53:57.790458 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d107f940  via __control_var@15020x55b7d101ce90
I1018 08:53:57.790460 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> cinn_launch0x55b7d107e530  via __control_var@14770x55b7d1043480
I1018 08:53:57.790463 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> cinn_launch0x55b7d107d120  via __control_var@14950x55b7d101baa0
I1018 08:53:57.790465 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d107bd10  via __control_var@14870x55b7d1045110
I1018 08:53:57.790467 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d107a890  via __control_var@14810x55b7d1043fa0
I1018 08:53:57.790470 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1011290 -> eager_deletion0x55b7d0bca520  via __control_var@15330x55b7d0c11700
I1018 08:53:57.790472 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100b430 -> eager_deletion0x55b7d0ccd6a0  via __control_var@16360x55b7d0cbdbf0
I1018 08:53:57.790475 4097533 graph_helper.h:104] adj conv2d0x55b7d106f840 -> cinn_launch0x55b7d109c290  via conv2d_19.tmp_00x55b7d109bfc0
I1018 08:53:57.790477 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d109c290  via relu_14.tmp_00x55b7d10900a0
I1018 08:53:57.790480 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> conv2d0x55b7d106f840  via relu_15.tmp_00x55b7d106f450
I1018 08:53:57.790482 4097533 graph_helper.h:104] adj conv2d0x55b7d10908e0 -> cinn_launch0x55b7d106aaf0  via conv2d_18.tmp_00x55b7d106a820
I1018 08:53:57.790485 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103c700 -> eager_deletion0x55b7d0e11740  via __control_var@16180x55b7d0de5140
I1018 08:53:57.790488 4097533 graph_helper.h:104] adj conv2d0x55b7d0fc2680 -> cinn_launch0x55b7d1126750  via conv2d_17.tmp_00x55b7d11264a0
I1018 08:53:57.790490 4097533 graph_helper.h:104] adj conv2d0x55b7d0fb56d0 -> cinn_launch0x55b7d1126750  via conv2d_16.tmp_00x55b7d0fc23b0
I1018 08:53:57.790493 4097533 graph_helper.h:104] adj conv2d0x55b7d0fc2680 -> eager_deletion0x55b7d0a126c0  via __control_var@15910x55b7d0d676f0
I1018 08:53:57.790495 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> conv2d0x55b7d0fc2680  via relu_12.tmp_00x55b7d1122b70
I1018 08:53:57.790498 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> eager_deletion0x55b7d0e04390  via __control_var@14850x55b7d1044b40
I1018 08:53:57.790500 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> cinn_launch0x55b7d1062b50  via __control_var@15110x55b7d101e840
I1018 08:53:57.790503 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> conv2d0x55b7d0fb56d0  via relu_13.tmp_00x55b7d0fb52e0
I1018 08:53:57.790505 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1014b30 -> eager_deletion0x55b7d0c4a1c0  via __control_var@16440x55b7d0c4a280
I1018 08:53:57.790508 4097533 graph_helper.h:104] adj conv2d0x55b7d1122f60 -> cinn_launch0x55b7d0fd6ee0  via conv2d_15.tmp_00x55b7d0fd6c10
I1018 08:53:57.790510 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> conv2d0x55b7d1122f60  via relu_12.tmp_00x55b7d1122b70
I1018 08:53:57.790513 4097533 graph_helper.h:104] adj conv2d0x55b7d0d89b40 -> eager_deletion0x55b7d0ebf660  via __control_var@16070x55b7d0dd21c0
I1018 08:53:57.790515 4097533 graph_helper.h:104] adj conv2d0x55b7d11091c0 -> cinn_launch0x55b7d0fea800  via conv2d_14.tmp_00x55b7d0fea530
I1018 08:53:57.790517 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fea800  via relu_10.tmp_00x55b7d102b4b0
I1018 08:53:57.790520 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> conv2d0x55b7d11091c0  via relu_11.tmp_00x55b7d1108dd0
I1018 08:53:57.790522 4097533 graph_helper.h:104] adj conv2d0x55b7d102bc70 -> cinn_launch0x55b7d1115560  via conv2d_13.tmp_00x55b7d11184f0
I1018 08:53:57.790525 4097533 graph_helper.h:104] adj conv2d0x55b7d0ec1350 -> eager_deletion0x55b7d0c30050  via __control_var@16100x55b7d0eacc00
I1018 08:53:57.790529 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> conv2d0x55b7d102bc70  via relu_10.tmp_00x55b7d102b4b0
I1018 08:53:57.790532 4097533 graph_helper.h:104] adj conv2d0x55b7d0d441e0 -> cinn_launch0x55b7d1031e80  via conv2d_12.tmp_00x55b7d1031bd0
I1018 08:53:57.790534 4097533 graph_helper.h:104] adj conv2d0x55b7d0e99aa0 -> cinn_launch0x55b7d1031e80  via conv2d_11.tmp_00x55b7d0d43f10
I1018 08:53:57.790537 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> conv2d0x55b7d0d441e0  via relu_8.tmp_00x55b7d1029a90
I1018 08:53:57.790539 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> conv2d0x55b7d0e99aa0  via relu_9.tmp_00x55b7d0e996b0
I1018 08:53:57.790542 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103b420 -> eager_deletion0x55b7d0cdb9b0  via __control_var@15830x55b7d0e5ec50
I1018 08:53:57.790544 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> conv2d0x55b7d1029e80  via relu_8.tmp_00x55b7d1029a90
I1018 08:53:57.790546 4097533 graph_helper.h:104] adj conv2d0x55b7d0ec1350 -> cinn_launch0x55b7d1047f30  via conv2d_9.tmp_00x55b7d1047c60
I1018 08:53:57.790549 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d1047f30  via relu_6.tmp_00x55b7d0e88b80
I1018 08:53:57.790551 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> conv2d0x55b7d0ec1350  via relu_7.tmp_00x55b7d0e0f4e0
I1018 08:53:57.790555 4097533 graph_helper.h:104] adj conv2d0x55b7d0e89340 -> cinn_launch0x55b7d0f0e1c0  via conv2d_8.tmp_00x55b7d0f0def0
I1018 08:53:57.790556 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> conv2d0x55b7d0e89340  via relu_6.tmp_00x55b7d0e88b80
I1018 08:53:57.790560 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> cinn_launch0x55b7d0ffe3f0  via __control_var@14730x55b7d1042880
I1018 08:53:57.790561 4097533 graph_helper.h:104] adj conv2d0x55b7d1119e20 -> cinn_launch0x55b7d0add8b0  via conv2d_7.tmp_00x55b7d0add600
I1018 08:53:57.790563 4097533 graph_helper.h:104] adj conv2d0x55b7d0d89b40 -> cinn_launch0x55b7d0add8b0  via conv2d_6.tmp_00x55b7d1119b50
I1018 08:53:57.790566 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> conv2d0x55b7d1119e20  via relu_4.tmp_00x55b7d111b9f0
I1018 08:53:57.790568 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> conv2d0x55b7d0d89b40  via relu_5.tmp_00x55b7d0d89750
I1018 08:53:57.790571 4097533 graph_helper.h:104] adj conv2d0x55b7d0e99aa0 -> eager_deletion0x55b7d0faad50  via __control_var@15960x55b7d0d48cf0
I1018 08:53:57.790573 4097533 graph_helper.h:104] adj conv2d0x55b7d111bde0 -> cinn_launch0x55b7d0dcf300  via conv2d_5.tmp_00x55b7d0dcf030
I1018 08:53:57.790576 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> conv2d0x55b7d111bde0  via relu_4.tmp_00x55b7d111b9f0
I1018 08:53:57.790578 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d0c640b0  via relu_2.tmp_00x55b7d0c74b50
I1018 08:53:57.790580 4097533 graph_helper.h:104] adj conv2d0x55b7d0b70160 -> cinn_launch0x55b7d0c640b0  via conv2d_4.tmp_00x55b7d0c63de0
I1018 08:53:57.790583 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> conv2d0x55b7d0b70160  via relu_3.tmp_00x55b7d0b6fd70
I1018 08:53:57.790585 4097533 graph_helper.h:104] adj conv2d0x55b7d0c74ec0 -> cinn_launch0x55b7d0c9d5e0  via conv2d_3.tmp_00x55b7d0e7f460
I1018 08:53:57.790588 4097533 graph_helper.h:104] adj conv2d0x55b7d1029e80 -> cinn_launch0x55b7d0cb5e70  via conv2d_10.tmp_00x55b7d0cb8f30
I1018 08:53:57.790591 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> conv2d0x55b7d0c74ec0  via relu_2.tmp_00x55b7d0c74b50
I1018 08:53:57.790593 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1041280 -> eager_deletion0x55b7d0caa5c0  via __control_var@16390x55b7d0c8ed20
I1018 08:53:57.790596 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdff90  via __control_var@15050x55b7d101d7a0
I1018 08:53:57.790598 4097533 graph_helper.h:104] adj conv2d0x55b7d0edeb50 -> cinn_launch0x55b7d0cfb320  via conv2d_2.tmp_00x55b7d0cfb110
I1018 08:53:57.790602 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d0cfb320  via pool2d_0.tmp_00x55b7d0b929b0
I1018 08:53:57.790606 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> conv2d0x55b7d0edeb50  via relu_1.tmp_00x55b7d0de1f20
I1018 08:53:57.790607 4097533 graph_helper.h:104] adj conv2d0x55b7d0ddf060 -> cinn_launch0x55b7d0c892a0  via conv2d_1.tmp_00x55b7d0c89030
I1018 08:53:57.790611 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100ffb0 -> eager_deletion0x55b7d0f91490  via __control_var@15450x55b7d0dbab10
I1018 08:53:57.790612 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> conv2d0x55b7d0ddf060  via pool2d_0.tmp_00x55b7d0b929b0
I1018 08:53:57.790616 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> conv2d0x55b7d10908e0  via relu_14.tmp_00x55b7d10900a0
I1018 08:53:57.790618 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> cinn_launch0x55b7d0fff800  via __control_var@15100x55b7d101e570
I1018 08:53:57.790620 4097533 graph_helper.h:104] adj conv2d0x55b7d0edf1d0 -> cinn_launch0x55b7d0fac1b0  via conv2d_0.tmp_00x55b7d0eaa280
I1018 08:53:57.790805 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103d9e0 -> eager_deletion0x55b7d0bf8d60  via __control_var@16490x55b7d0be5660
I1018 08:53:57.790808 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> eager_deletion0x55b7d0c2da20  via __control_var@14760x55b7d10431b0
I1018 08:53:57.790810 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> eager_deletion0x55b7d0c7f270  via __control_var@14740x55b7d1042b70
I1018 08:53:57.790812 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> eager_deletion0x55b7d0d36320  via __control_var@14960x55b7d101bd70
I1018 08:53:57.790815 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1013850 -> eager_deletion0x55b7d0d50360  via __control_var@16280x55b7d0d50420
I1018 08:53:57.790817 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> eager_deletion0x55b7d0d83e40  via __control_var@14910x55b7d101af00
I1018 08:53:57.790820 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103ffa0 -> eager_deletion0x55b7d0da05e0  via __control_var@16230x55b7d0da06a0
I1018 08:53:57.790822 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> eager_deletion0x55b7d0dd5690  via __control_var@14780x55b7d1043710
I1018 08:53:57.790825 4097533 graph_helper.h:104] adj conv2d0x55b7d1029e80 -> eager_deletion0x55b7d0d04df0  via __control_var@16330x55b7d0d04eb0
I1018 08:53:57.790827 4097533 graph_helper.h:104] adj cinn_launch0x55b7d10059f0 -> eager_deletion0x55b7d0ec5bb0  via __control_var@16130x55b7d0c239b0
I1018 08:53:57.790829 4097533 graph_helper.h:104] adj conv2d0x55b7d11091c0 -> eager_deletion0x55b7d0e31dd0  via __control_var@16010x55b7d0e680a0
I1018 08:53:57.790832 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> eager_deletion0x55b7d0dac250  via __control_var@15070x55b7d101dd40
I1018 08:53:57.790834 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> eager_deletion0x55b7d0d20510  via __control_var@14900x55b7d101ac70
I1018 08:53:57.790836 4097533 graph_helper.h:104] adj conv2d0x55b7d1122f60 -> eager_deletion0x55b7d0d2e9e0  via __control_var@15860x55b7d0d11c80
I1018 08:53:57.790839 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> eager_deletion0x55b7d0ccaa80  via __control_var@14930x55b7d101b4e0
I1018 08:53:57.790841 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> eager_deletion0x55b7d0c8c100  via __control_var@14820x55b7d1044290
I1018 08:53:57.790844 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> eager_deletion0x55b7d0c82340  via __control_var@14730x55b7d1042880
I1018 08:53:57.790846 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> eager_deletion0x55b7d0be1970  via __control_var@14720x55b7d1042590
I1018 08:53:57.790848 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> eager_deletion0x55b7d0e7c680  via __control_var@14810x55b7d1043fa0
I1018 08:53:57.790851 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> eager_deletion0x55b7d0c100f0  via __control_var@14920x55b7d101b1f0
I1018 08:53:57.790855 4097533 graph_helper.h:104] adj conv2d0x55b7d106f840 -> eager_deletion0x55b7d0fc1590  via __control_var@15700x55b7d0bff1e0
I1018 08:53:57.790858 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> eager_deletion0x55b7d0c74830  via __control_var@14770x55b7d1043480
I1018 08:53:57.790860 4097533 graph_helper.h:104] adj conv2d0x55b7d0fb56d0 -> eager_deletion0x55b7d0ebbb40  via __control_var@15650x55b7d0a9d670
I1018 08:53:57.790863 4097533 graph_helper.h:104] adj conv2d0x55b7d111bde0 -> eager_deletion0x55b7d1106060  via __control_var@15620x55b7d0b734a0
I1018 08:53:57.790865 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100c710 -> eager_deletion0x55b7d0aeffc0  via __control_var@15590x55b7d0afe850
I1018 08:53:57.790867 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1038e60 -> eager_deletion0x55b7d0aa9140  via __control_var@15560x55b7d0e966c0
I1018 08:53:57.790869 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> eager_deletion0x55b7d0a72230  via __control_var@14830x55b7d1044580
I1018 08:53:57.790872 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103a140 -> eager_deletion0x55b7d0a639a0  via __control_var@15510x55b7d0dbb500
I1018 08:53:57.790874 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1012570 -> eager_deletion0x55b7d0a1f3e0  via __control_var@15480x55b7d0995060
I1018 08:53:57.790876 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1004710 -> eager_deletion0x55b7d09a1410  via __control_var@15420x55b7d09da090
I1018 08:53:57.790879 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1003430 -> eager_deletion0x55b7d0abb300  via __control_var@15390x55b7d0990130
I1018 08:53:57.790881 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100d9f0 -> eager_deletion0x55b7d0c57440  via __control_var@15300x55b7d0affe60
I1018 08:53:57.790884 4097533 graph_helper.h:104] adj conv2d0x55b7d0b70160 -> eager_deletion0x55b7d0a73840  via __control_var@15270x55b7d0e77f40
I1018 08:53:57.790886 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100ecd0 -> eager_deletion0x55b7d0c96340  via __control_var@15240x55b7d1033be0
I1018 08:53:57.790889 4097533 graph_helper.h:104] adj conv2d0x55b7d0edf1d0 -> eager_deletion0x55b7d0c57d60  via __control_var@15210x55b7d0b86300
I1018 08:53:57.790891 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> eager_deletion0x55b7cd159bc0  via __control_var@14990x55b7d101c600
I1018 08:53:57.790894 4097533 graph_helper.h:104] adj conv2d0x55b7d0d441e0 -> eager_deletion0x55b7d09765a0  via __control_var@15130x55b7d096fc60
I1018 08:53:57.790895 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103ecc0 -> eager_deletion0x55b7b78d2db0  via __control_var@15180x55b7b7024640
I1018 08:53:57.790899 4097533 graph_helper.h:104] adj conv2d0x55b7d0edeb50 -> eager_deletion0x55b7d0cdcfc0  via __control_var@15360x55b7d0dad860
I1018 08:53:57.790903 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> cinn_launch0x55b7d1002020  via __control_var@14800x55b7d1043cd0
I1018 08:53:57.790905 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> cinn_launch0x55b7d1000c10  via __control_var@14720x55b7d1042590
I1018 08:53:57.790908 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d0ffcfe0  via __control_var@14740x55b7d1042b70
I1018 08:53:57.790910 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d0ffbc30  via __control_var@14750x55b7d1042e40
I1018 08:53:57.790912 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d1068fa0  via __control_var@15090x55b7d101e2a0
I1018 08:53:57.790915 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d1067b90  via __control_var@15010x55b7d101cbc0
I1018 08:53:57.790917 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> cinn_launch0x55b7d1066780  via __control_var@14980x55b7d101c330
I1018 08:53:57.790920 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> cinn_launch0x55b7d1065370  via __control_var@14900x55b7d101ac70
I1018 08:53:57.790922 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> cinn_launch0x55b7d1063f60  via __control_var@15070x55b7d101dd40
I1018 08:53:57.790926 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> cinn_launch0x55b7d1061740  via __control_var@15080x55b7d101dfd0
I1018 08:53:57.790930 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> cinn_launch0x55b7d0fe13a0  via __control_var@14920x55b7d101b1f0
I1018 08:53:57.790931 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdeb80  via __control_var@15030x55b7d101d160
I1018 08:53:57.790933 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdd770  via __control_var@14850x55b7d1044b40
I1018 08:53:57.790936 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdc360  via __control_var@15040x55b7d101d4d0
I1018 08:53:57.790938 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> cinn_launch0x55b7d0fdaf50  via __control_var@14970x55b7d101c060
I1018 08:53:57.790940 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> cinn_launch0x55b7d0fd9b40  via __control_var@14930x55b7d101b4e0
I1018 08:53:57.790943 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> cinn_launch0x55b7d0fd8730  via __control_var@14820x55b7d1044290
I1018 08:53:57.790946 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> cinn_launch0x55b7d1104d40  via __control_var@14840x55b7d1044870
I1018 08:53:57.790947 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> cinn_launch0x55b7d1103930  via __control_var@15000x55b7d101c8f0
I1018 08:53:57.790951 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> cinn_launch0x55b7d1102520  via __control_var@14990x55b7d101c600
I1018 08:53:57.790952 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d1101110  via __control_var@14880x55b7d10453e0
I1018 08:53:57.790954 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10ffd00  via __control_var@14890x55b7d10456b0
I1018 08:53:57.790957 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10fe8f0  via __control_var@14860x55b7d1044e10
I1018 08:53:57.790959 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10fd4e0  via __control_var@14760x55b7d10431b0
I1018 08:53:57.790961 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> cinn_launch0x55b7d1113ec0  via __control_var@14790x55b7d1043a00
I1018 08:53:57.790964 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> cinn_launch0x55b7d1112ab0  via __control_var@14780x55b7d1043710
I1018 08:53:57.790966 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> cinn_launch0x55b7d11116a0  via __control_var@14830x55b7d1044580
I1018 08:53:57.790968 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> cinn_launch0x55b7d1110290  via __control_var@14940x55b7d101b7d0
I1018 08:53:57.790971 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> cinn_launch0x55b7d110ee80  via __control_var@15060x55b7d101da70
I1018 08:53:57.790973 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> cinn_launch0x55b7d110da70  via __control_var@14910x55b7d101af00
I1018 08:53:57.790975 4097533 graph_helper.h:104] adj conv2d0x55b7d1119e20 -> eager_deletion0x55b7d0e76930  via __control_var@16040x55b7d0e8a420
I1018 08:53:57.790978 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d110c6a0  via __control_var@14960x55b7d101bd70
I1018 08:53:57.790980 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d107f940  via __control_var@15020x55b7d101ce90
I1018 08:53:57.790982 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> cinn_launch0x55b7d107e530  via __control_var@14770x55b7d1043480
I1018 08:53:57.790985 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> cinn_launch0x55b7d107d120  via __control_var@14950x55b7d101baa0
I1018 08:53:57.790987 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d107bd10  via __control_var@14870x55b7d1045110
I1018 08:53:57.790989 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d107a890  via __control_var@14810x55b7d1043fa0
I1018 08:53:57.790994 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1011290 -> eager_deletion0x55b7d0bca520  via __control_var@15330x55b7d0c11700
I1018 08:53:57.790997 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100b430 -> eager_deletion0x55b7d0ccd6a0  via __control_var@16360x55b7d0cbdbf0
I1018 08:53:57.790999 4097533 graph_helper.h:104] adj conv2d0x55b7d106f840 -> cinn_launch0x55b7d109c290  via conv2d_19.tmp_00x55b7d109bfc0
I1018 08:53:57.791002 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d109c290  via relu_14.tmp_00x55b7d10900a0
I1018 08:53:57.791003 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> conv2d0x55b7d106f840  via relu_15.tmp_00x55b7d106f450
I1018 08:53:57.791006 4097533 graph_helper.h:104] adj conv2d0x55b7d10908e0 -> cinn_launch0x55b7d106aaf0  via conv2d_18.tmp_00x55b7d106a820
I1018 08:53:57.791008 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103c700 -> eager_deletion0x55b7d0e11740  via __control_var@16180x55b7d0de5140
I1018 08:53:57.791011 4097533 graph_helper.h:104] adj conv2d0x55b7d0fc2680 -> cinn_launch0x55b7d1126750  via conv2d_17.tmp_00x55b7d11264a0
I1018 08:53:57.791013 4097533 graph_helper.h:104] adj conv2d0x55b7d0fb56d0 -> cinn_launch0x55b7d1126750  via conv2d_16.tmp_00x55b7d0fc23b0
I1018 08:53:57.791016 4097533 graph_helper.h:104] adj conv2d0x55b7d0fc2680 -> eager_deletion0x55b7d0a126c0  via __control_var@15910x55b7d0d676f0
I1018 08:53:57.791018 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> conv2d0x55b7d0fc2680  via relu_12.tmp_00x55b7d1122b70
I1018 08:53:57.791021 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> eager_deletion0x55b7d0e04390  via __control_var@14850x55b7d1044b40
I1018 08:53:57.791023 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> cinn_launch0x55b7d1062b50  via __control_var@15110x55b7d101e840
I1018 08:53:57.791026 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> conv2d0x55b7d0fb56d0  via relu_13.tmp_00x55b7d0fb52e0
I1018 08:53:57.791028 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1014b30 -> eager_deletion0x55b7d0c4a1c0  via __control_var@16440x55b7d0c4a280
I1018 08:53:57.791031 4097533 graph_helper.h:104] adj conv2d0x55b7d1122f60 -> cinn_launch0x55b7d0fd6ee0  via conv2d_15.tmp_00x55b7d0fd6c10
I1018 08:53:57.791033 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> conv2d0x55b7d1122f60  via relu_12.tmp_00x55b7d1122b70
I1018 08:53:57.791036 4097533 graph_helper.h:104] adj conv2d0x55b7d0d89b40 -> eager_deletion0x55b7d0ebf660  via __control_var@16070x55b7d0dd21c0
I1018 08:53:57.791038 4097533 graph_helper.h:104] adj conv2d0x55b7d11091c0 -> cinn_launch0x55b7d0fea800  via conv2d_14.tmp_00x55b7d0fea530
I1018 08:53:57.791040 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fea800  via relu_10.tmp_00x55b7d102b4b0
I1018 08:53:57.791043 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> conv2d0x55b7d11091c0  via relu_11.tmp_00x55b7d1108dd0
I1018 08:53:57.791045 4097533 graph_helper.h:104] adj conv2d0x55b7d102bc70 -> cinn_launch0x55b7d1115560  via conv2d_13.tmp_00x55b7d11184f0
I1018 08:53:57.791049 4097533 graph_helper.h:104] adj conv2d0x55b7d0ec1350 -> eager_deletion0x55b7d0c30050  via __control_var@16100x55b7d0eacc00
I1018 08:53:57.791050 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> conv2d0x55b7d102bc70  via relu_10.tmp_00x55b7d102b4b0
I1018 08:53:57.791052 4097533 graph_helper.h:104] adj conv2d0x55b7d0d441e0 -> cinn_launch0x55b7d1031e80  via conv2d_12.tmp_00x55b7d1031bd0
I1018 08:53:57.791055 4097533 graph_helper.h:104] adj conv2d0x55b7d0e99aa0 -> cinn_launch0x55b7d1031e80  via conv2d_11.tmp_00x55b7d0d43f10
I1018 08:53:57.791057 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> conv2d0x55b7d0d441e0  via relu_8.tmp_00x55b7d1029a90
I1018 08:53:57.791060 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> conv2d0x55b7d0e99aa0  via relu_9.tmp_00x55b7d0e996b0
I1018 08:53:57.791062 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103b420 -> eager_deletion0x55b7d0cdb9b0  via __control_var@15830x55b7d0e5ec50
I1018 08:53:57.791067 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> conv2d0x55b7d1029e80  via relu_8.tmp_00x55b7d1029a90
I1018 08:53:57.791069 4097533 graph_helper.h:104] adj conv2d0x55b7d0ec1350 -> cinn_launch0x55b7d1047f30  via conv2d_9.tmp_00x55b7d1047c60
I1018 08:53:57.791071 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d1047f30  via relu_6.tmp_00x55b7d0e88b80
I1018 08:53:57.791074 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> conv2d0x55b7d0ec1350  via relu_7.tmp_00x55b7d0e0f4e0
I1018 08:53:57.791076 4097533 graph_helper.h:104] adj conv2d0x55b7d0e89340 -> cinn_launch0x55b7d0f0e1c0  via conv2d_8.tmp_00x55b7d0f0def0
I1018 08:53:57.791078 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> conv2d0x55b7d0e89340  via relu_6.tmp_00x55b7d0e88b80
I1018 08:53:57.791081 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> cinn_launch0x55b7d0ffe3f0  via __control_var@14730x55b7d1042880
I1018 08:53:57.791085 4097533 graph_helper.h:104] adj conv2d0x55b7d1119e20 -> cinn_launch0x55b7d0add8b0  via conv2d_7.tmp_00x55b7d0add600
I1018 08:53:57.791086 4097533 graph_helper.h:104] adj conv2d0x55b7d0d89b40 -> cinn_launch0x55b7d0add8b0  via conv2d_6.tmp_00x55b7d1119b50
I1018 08:53:57.791088 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> conv2d0x55b7d1119e20  via relu_4.tmp_00x55b7d111b9f0
I1018 08:53:57.791091 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> conv2d0x55b7d0d89b40  via relu_5.tmp_00x55b7d0d89750
I1018 08:53:57.791093 4097533 graph_helper.h:104] adj conv2d0x55b7d0e99aa0 -> eager_deletion0x55b7d0faad50  via __control_var@15960x55b7d0d48cf0
I1018 08:53:57.791096 4097533 graph_helper.h:104] adj conv2d0x55b7d111bde0 -> cinn_launch0x55b7d0dcf300  via conv2d_5.tmp_00x55b7d0dcf030
I1018 08:53:57.791098 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> conv2d0x55b7d111bde0  via relu_4.tmp_00x55b7d111b9f0
I1018 08:53:57.791100 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d0c640b0  via relu_2.tmp_00x55b7d0c74b50
I1018 08:53:57.791103 4097533 graph_helper.h:104] adj conv2d0x55b7d0b70160 -> cinn_launch0x55b7d0c640b0  via conv2d_4.tmp_00x55b7d0c63de0
I1018 08:53:57.791105 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> conv2d0x55b7d0b70160  via relu_3.tmp_00x55b7d0b6fd70
I1018 08:53:57.791108 4097533 graph_helper.h:104] adj conv2d0x55b7d0c74ec0 -> cinn_launch0x55b7d0c9d5e0  via conv2d_3.tmp_00x55b7d0e7f460
I1018 08:53:57.791110 4097533 graph_helper.h:104] adj conv2d0x55b7d1029e80 -> cinn_launch0x55b7d0cb5e70  via conv2d_10.tmp_00x55b7d0cb8f30
I1018 08:53:57.791112 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> conv2d0x55b7d0c74ec0  via relu_2.tmp_00x55b7d0c74b50
I1018 08:53:57.791115 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1041280 -> eager_deletion0x55b7d0caa5c0  via __control_var@16390x55b7d0c8ed20
I1018 08:53:57.791118 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdff90  via __control_var@15050x55b7d101d7a0
I1018 08:53:57.791121 4097533 graph_helper.h:104] adj conv2d0x55b7d0edeb50 -> cinn_launch0x55b7d0cfb320  via conv2d_2.tmp_00x55b7d0cfb110
I1018 08:53:57.791122 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d0cfb320  via pool2d_0.tmp_00x55b7d0b929b0
I1018 08:53:57.791126 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> conv2d0x55b7d0edeb50  via relu_1.tmp_00x55b7d0de1f20
I1018 08:53:57.791127 4097533 graph_helper.h:104] adj conv2d0x55b7d0ddf060 -> cinn_launch0x55b7d0c892a0  via conv2d_1.tmp_00x55b7d0c89030
I1018 08:53:57.791131 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100ffb0 -> eager_deletion0x55b7d0f91490  via __control_var@15450x55b7d0dbab10
I1018 08:53:57.791132 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> conv2d0x55b7d0ddf060  via pool2d_0.tmp_00x55b7d0b929b0
I1018 08:53:57.791136 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> conv2d0x55b7d10908e0  via relu_14.tmp_00x55b7d10900a0
I1018 08:53:57.791139 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> cinn_launch0x55b7d0fff800  via __control_var@15100x55b7d101e570
I1018 08:53:57.791142 4097533 graph_helper.h:104] adj conv2d0x55b7d0edf1d0 -> cinn_launch0x55b7d0fac1b0  via conv2d_0.tmp_00x55b7d0eaa280
I1018 08:53:57.791322 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103d9e0 -> eager_deletion0x55b7d0bf8d60  via __control_var@16490x55b7d0be5660
I1018 08:53:57.791325 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> eager_deletion0x55b7d0c2da20  via __control_var@14760x55b7d10431b0
I1018 08:53:57.791328 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> eager_deletion0x55b7d0c7f270  via __control_var@14740x55b7d1042b70
I1018 08:53:57.791330 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> eager_deletion0x55b7d0d36320  via __control_var@14960x55b7d101bd70
I1018 08:53:57.791332 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1013850 -> eager_deletion0x55b7d0d50360  via __control_var@16280x55b7d0d50420
I1018 08:53:57.791335 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> eager_deletion0x55b7d0d83e40  via __control_var@14910x55b7d101af00
I1018 08:53:57.791337 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103ffa0 -> eager_deletion0x55b7d0da05e0  via __control_var@16230x55b7d0da06a0
I1018 08:53:57.791340 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> eager_deletion0x55b7d0dd5690  via __control_var@14780x55b7d1043710
I1018 08:53:57.791342 4097533 graph_helper.h:104] adj conv2d0x55b7d1029e80 -> eager_deletion0x55b7d0d04df0  via __control_var@16330x55b7d0d04eb0
I1018 08:53:57.791344 4097533 graph_helper.h:104] adj cinn_launch0x55b7d10059f0 -> eager_deletion0x55b7d0ec5bb0  via __control_var@16130x55b7d0c239b0
I1018 08:53:57.791347 4097533 graph_helper.h:104] adj conv2d0x55b7d11091c0 -> eager_deletion0x55b7d0e31dd0  via __control_var@16010x55b7d0e680a0
I1018 08:53:57.791349 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> eager_deletion0x55b7d0dac250  via __control_var@15070x55b7d101dd40
I1018 08:53:57.791352 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> eager_deletion0x55b7d0d20510  via __control_var@14900x55b7d101ac70
I1018 08:53:57.791354 4097533 graph_helper.h:104] adj conv2d0x55b7d1122f60 -> eager_deletion0x55b7d0d2e9e0  via __control_var@15860x55b7d0d11c80
I1018 08:53:57.791357 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> eager_deletion0x55b7d0ccaa80  via __control_var@14930x55b7d101b4e0
I1018 08:53:57.791359 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> eager_deletion0x55b7d0c8c100  via __control_var@14820x55b7d1044290
I1018 08:53:57.791361 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> eager_deletion0x55b7d0c82340  via __control_var@14730x55b7d1042880
I1018 08:53:57.791363 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> eager_deletion0x55b7d0be1970  via __control_var@14720x55b7d1042590
I1018 08:53:57.791366 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> eager_deletion0x55b7d0e7c680  via __control_var@14810x55b7d1043fa0
I1018 08:53:57.791368 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> eager_deletion0x55b7d0c100f0  via __control_var@14920x55b7d101b1f0
I1018 08:53:57.791370 4097533 graph_helper.h:104] adj conv2d0x55b7d106f840 -> eager_deletion0x55b7d0fc1590  via __control_var@15700x55b7d0bff1e0
I1018 08:53:57.791373 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> eager_deletion0x55b7d0c74830  via __control_var@14770x55b7d1043480
I1018 08:53:57.791375 4097533 graph_helper.h:104] adj conv2d0x55b7d0fb56d0 -> eager_deletion0x55b7d0ebbb40  via __control_var@15650x55b7d0a9d670
I1018 08:53:57.791378 4097533 graph_helper.h:104] adj conv2d0x55b7d111bde0 -> eager_deletion0x55b7d1106060  via __control_var@15620x55b7d0b734a0
I1018 08:53:57.791380 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100c710 -> eager_deletion0x55b7d0aeffc0  via __control_var@15590x55b7d0afe850
I1018 08:53:57.791383 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1038e60 -> eager_deletion0x55b7d0aa9140  via __control_var@15560x55b7d0e966c0
I1018 08:53:57.791388 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> eager_deletion0x55b7d0a72230  via __control_var@14830x55b7d1044580
I1018 08:53:57.791389 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103a140 -> eager_deletion0x55b7d0a639a0  via __control_var@15510x55b7d0dbb500
I1018 08:53:57.791391 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1012570 -> eager_deletion0x55b7d0a1f3e0  via __control_var@15480x55b7d0995060
I1018 08:53:57.791394 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1004710 -> eager_deletion0x55b7d09a1410  via __control_var@15420x55b7d09da090
I1018 08:53:57.791396 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1003430 -> eager_deletion0x55b7d0abb300  via __control_var@15390x55b7d0990130
I1018 08:53:57.791399 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100d9f0 -> eager_deletion0x55b7d0c57440  via __control_var@15300x55b7d0affe60
I1018 08:53:57.791401 4097533 graph_helper.h:104] adj conv2d0x55b7d0b70160 -> eager_deletion0x55b7d0a73840  via __control_var@15270x55b7d0e77f40
I1018 08:53:57.791404 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100ecd0 -> eager_deletion0x55b7d0c96340  via __control_var@15240x55b7d1033be0
I1018 08:53:57.791406 4097533 graph_helper.h:104] adj conv2d0x55b7d0edf1d0 -> eager_deletion0x55b7d0c57d60  via __control_var@15210x55b7d0b86300
I1018 08:53:57.791409 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> eager_deletion0x55b7cd159bc0  via __control_var@14990x55b7d101c600
I1018 08:53:57.791410 4097533 graph_helper.h:104] adj conv2d0x55b7d0d441e0 -> eager_deletion0x55b7d09765a0  via __control_var@15130x55b7d096fc60
I1018 08:53:57.791414 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103ecc0 -> eager_deletion0x55b7b78d2db0  via __control_var@15180x55b7b7024640
I1018 08:53:57.791415 4097533 graph_helper.h:104] adj conv2d0x55b7d0edeb50 -> eager_deletion0x55b7d0cdcfc0  via __control_var@15360x55b7d0dad860
I1018 08:53:57.791420 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> cinn_launch0x55b7d1002020  via __control_var@14800x55b7d1043cd0
I1018 08:53:57.791422 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> cinn_launch0x55b7d1000c10  via __control_var@14720x55b7d1042590
I1018 08:53:57.791424 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d0ffcfe0  via __control_var@14740x55b7d1042b70
I1018 08:53:57.791427 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d0ffbc30  via __control_var@14750x55b7d1042e40
I1018 08:53:57.791429 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d1068fa0  via __control_var@15090x55b7d101e2a0
I1018 08:53:57.791432 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d1067b90  via __control_var@15010x55b7d101cbc0
I1018 08:53:57.791435 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> cinn_launch0x55b7d1066780  via __control_var@14980x55b7d101c330
I1018 08:53:57.791436 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> cinn_launch0x55b7d1065370  via __control_var@14900x55b7d101ac70
I1018 08:53:57.791440 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> cinn_launch0x55b7d1063f60  via __control_var@15070x55b7d101dd40
I1018 08:53:57.791441 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> cinn_launch0x55b7d1061740  via __control_var@15080x55b7d101dfd0
I1018 08:53:57.791443 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> cinn_launch0x55b7d0fe13a0  via __control_var@14920x55b7d101b1f0
I1018 08:53:57.791446 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdeb80  via __control_var@15030x55b7d101d160
I1018 08:53:57.791448 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdd770  via __control_var@14850x55b7d1044b40
I1018 08:53:57.791451 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdc360  via __control_var@15040x55b7d101d4d0
I1018 08:53:57.791455 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> cinn_launch0x55b7d0fdaf50  via __control_var@14970x55b7d101c060
I1018 08:53:57.791457 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> cinn_launch0x55b7d0fd9b40  via __control_var@14930x55b7d101b4e0
I1018 08:53:57.791460 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> cinn_launch0x55b7d0fd8730  via __control_var@14820x55b7d1044290
I1018 08:53:57.791462 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> cinn_launch0x55b7d1104d40  via __control_var@14840x55b7d1044870
I1018 08:53:57.791464 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> cinn_launch0x55b7d1103930  via __control_var@15000x55b7d101c8f0
I1018 08:53:57.791467 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> cinn_launch0x55b7d1102520  via __control_var@14990x55b7d101c600
I1018 08:53:57.791469 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d1101110  via __control_var@14880x55b7d10453e0
I1018 08:53:57.791471 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10ffd00  via __control_var@14890x55b7d10456b0
I1018 08:53:57.791474 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10fe8f0  via __control_var@14860x55b7d1044e10
I1018 08:53:57.791476 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10fd4e0  via __control_var@14760x55b7d10431b0
I1018 08:53:57.791478 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> cinn_launch0x55b7d1113ec0  via __control_var@14790x55b7d1043a00
I1018 08:53:57.791481 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> cinn_launch0x55b7d1112ab0  via __control_var@14780x55b7d1043710
I1018 08:53:57.791483 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> cinn_launch0x55b7d11116a0  via __control_var@14830x55b7d1044580
I1018 08:53:57.791486 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> cinn_launch0x55b7d1110290  via __control_var@14940x55b7d101b7d0
I1018 08:53:57.791487 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> cinn_launch0x55b7d110ee80  via __control_var@15060x55b7d101da70
I1018 08:53:57.791491 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> cinn_launch0x55b7d110da70  via __control_var@14910x55b7d101af00
I1018 08:53:57.791492 4097533 graph_helper.h:104] adj conv2d0x55b7d1119e20 -> eager_deletion0x55b7d0e76930  via __control_var@16040x55b7d0e8a420
I1018 08:53:57.791494 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d110c6a0  via __control_var@14960x55b7d101bd70
I1018 08:53:57.791497 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d107f940  via __control_var@15020x55b7d101ce90
I1018 08:53:57.791499 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> cinn_launch0x55b7d107e530  via __control_var@14770x55b7d1043480
I1018 08:53:57.791501 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> cinn_launch0x55b7d107d120  via __control_var@14950x55b7d101baa0
I1018 08:53:57.791504 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d107bd10  via __control_var@14870x55b7d1045110
I1018 08:53:57.791507 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d107a890  via __control_var@14810x55b7d1043fa0
I1018 08:53:57.791508 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1011290 -> eager_deletion0x55b7d0bca520  via __control_var@15330x55b7d0c11700
I1018 08:53:57.791512 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100b430 -> eager_deletion0x55b7d0ccd6a0  via __control_var@16360x55b7d0cbdbf0
I1018 08:53:57.791513 4097533 graph_helper.h:104] adj conv2d0x55b7d106f840 -> cinn_launch0x55b7d109c290  via conv2d_19.tmp_00x55b7d109bfc0
I1018 08:53:57.791517 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d109c290  via relu_14.tmp_00x55b7d10900a0
I1018 08:53:57.791518 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> conv2d0x55b7d106f840  via relu_15.tmp_00x55b7d106f450
I1018 08:53:57.791522 4097533 graph_helper.h:104] adj conv2d0x55b7d10908e0 -> cinn_launch0x55b7d106aaf0  via conv2d_18.tmp_00x55b7d106a820
I1018 08:53:57.791525 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103c700 -> eager_deletion0x55b7d0e11740  via __control_var@16180x55b7d0de5140
I1018 08:53:57.791527 4097533 graph_helper.h:104] adj conv2d0x55b7d0fc2680 -> cinn_launch0x55b7d1126750  via conv2d_17.tmp_00x55b7d11264a0
I1018 08:53:57.791529 4097533 graph_helper.h:104] adj conv2d0x55b7d0fb56d0 -> cinn_launch0x55b7d1126750  via conv2d_16.tmp_00x55b7d0fc23b0
I1018 08:53:57.791532 4097533 graph_helper.h:104] adj conv2d0x55b7d0fc2680 -> eager_deletion0x55b7d0a126c0  via __control_var@15910x55b7d0d676f0
I1018 08:53:57.791534 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> conv2d0x55b7d0fc2680  via relu_12.tmp_00x55b7d1122b70
I1018 08:53:57.791538 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> eager_deletion0x55b7d0e04390  via __control_var@14850x55b7d1044b40
I1018 08:53:57.791539 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> cinn_launch0x55b7d1062b50  via __control_var@15110x55b7d101e840
I1018 08:53:57.791543 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> conv2d0x55b7d0fb56d0  via relu_13.tmp_00x55b7d0fb52e0
I1018 08:53:57.791544 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1014b30 -> eager_deletion0x55b7d0c4a1c0  via __control_var@16440x55b7d0c4a280
I1018 08:53:57.791548 4097533 graph_helper.h:104] adj conv2d0x55b7d1122f60 -> cinn_launch0x55b7d0fd6ee0  via conv2d_15.tmp_00x55b7d0fd6c10
I1018 08:53:57.791549 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> conv2d0x55b7d1122f60  via relu_12.tmp_00x55b7d1122b70
I1018 08:53:57.791551 4097533 graph_helper.h:104] adj conv2d0x55b7d0d89b40 -> eager_deletion0x55b7d0ebf660  via __control_var@16070x55b7d0dd21c0
I1018 08:53:57.791554 4097533 graph_helper.h:104] adj conv2d0x55b7d11091c0 -> cinn_launch0x55b7d0fea800  via conv2d_14.tmp_00x55b7d0fea530
I1018 08:53:57.791556 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fea800  via relu_10.tmp_00x55b7d102b4b0
I1018 08:53:57.791558 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> conv2d0x55b7d11091c0  via relu_11.tmp_00x55b7d1108dd0
I1018 08:53:57.791561 4097533 graph_helper.h:104] adj conv2d0x55b7d102bc70 -> cinn_launch0x55b7d1115560  via conv2d_13.tmp_00x55b7d11184f0
I1018 08:53:57.791563 4097533 graph_helper.h:104] adj conv2d0x55b7d0ec1350 -> eager_deletion0x55b7d0c30050  via __control_var@16100x55b7d0eacc00
I1018 08:53:57.791566 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> conv2d0x55b7d102bc70  via relu_10.tmp_00x55b7d102b4b0
I1018 08:53:57.791568 4097533 graph_helper.h:104] adj conv2d0x55b7d0d441e0 -> cinn_launch0x55b7d1031e80  via conv2d_12.tmp_00x55b7d1031bd0
I1018 08:53:57.791570 4097533 graph_helper.h:104] adj conv2d0x55b7d0e99aa0 -> cinn_launch0x55b7d1031e80  via conv2d_11.tmp_00x55b7d0d43f10
I1018 08:53:57.791574 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> conv2d0x55b7d0d441e0  via relu_8.tmp_00x55b7d1029a90
I1018 08:53:57.791575 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> conv2d0x55b7d0e99aa0  via relu_9.tmp_00x55b7d0e996b0
I1018 08:53:57.791579 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103b420 -> eager_deletion0x55b7d0cdb9b0  via __control_var@15830x55b7d0e5ec50
I1018 08:53:57.791580 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> conv2d0x55b7d1029e80  via relu_8.tmp_00x55b7d1029a90
I1018 08:53:57.791582 4097533 graph_helper.h:104] adj conv2d0x55b7d0ec1350 -> cinn_launch0x55b7d1047f30  via conv2d_9.tmp_00x55b7d1047c60
I1018 08:53:57.791585 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d1047f30  via relu_6.tmp_00x55b7d0e88b80
I1018 08:53:57.791587 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> conv2d0x55b7d0ec1350  via relu_7.tmp_00x55b7d0e0f4e0
I1018 08:53:57.791589 4097533 graph_helper.h:104] adj conv2d0x55b7d0e89340 -> cinn_launch0x55b7d0f0e1c0  via conv2d_8.tmp_00x55b7d0f0def0
I1018 08:53:57.791592 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> conv2d0x55b7d0e89340  via relu_6.tmp_00x55b7d0e88b80
I1018 08:53:57.791597 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> cinn_launch0x55b7d0ffe3f0  via __control_var@14730x55b7d1042880
I1018 08:53:57.791600 4097533 graph_helper.h:104] adj conv2d0x55b7d1119e20 -> cinn_launch0x55b7d0add8b0  via conv2d_7.tmp_00x55b7d0add600
I1018 08:53:57.791602 4097533 graph_helper.h:104] adj conv2d0x55b7d0d89b40 -> cinn_launch0x55b7d0add8b0  via conv2d_6.tmp_00x55b7d1119b50
I1018 08:53:57.791605 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> conv2d0x55b7d1119e20  via relu_4.tmp_00x55b7d111b9f0
I1018 08:53:57.791607 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> conv2d0x55b7d0d89b40  via relu_5.tmp_00x55b7d0d89750
I1018 08:53:57.791610 4097533 graph_helper.h:104] adj conv2d0x55b7d0e99aa0 -> eager_deletion0x55b7d0faad50  via __control_var@15960x55b7d0d48cf0
I1018 08:53:57.791612 4097533 graph_helper.h:104] adj conv2d0x55b7d111bde0 -> cinn_launch0x55b7d0dcf300  via conv2d_5.tmp_00x55b7d0dcf030
I1018 08:53:57.791615 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> conv2d0x55b7d111bde0  via relu_4.tmp_00x55b7d111b9f0
I1018 08:53:57.791617 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d0c640b0  via relu_2.tmp_00x55b7d0c74b50
I1018 08:53:57.791620 4097533 graph_helper.h:104] adj conv2d0x55b7d0b70160 -> cinn_launch0x55b7d0c640b0  via conv2d_4.tmp_00x55b7d0c63de0
I1018 08:53:57.791621 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> conv2d0x55b7d0b70160  via relu_3.tmp_00x55b7d0b6fd70
I1018 08:53:57.791625 4097533 graph_helper.h:104] adj conv2d0x55b7d0c74ec0 -> cinn_launch0x55b7d0c9d5e0  via conv2d_3.tmp_00x55b7d0e7f460
I1018 08:53:57.791626 4097533 graph_helper.h:104] adj conv2d0x55b7d1029e80 -> cinn_launch0x55b7d0cb5e70  via conv2d_10.tmp_00x55b7d0cb8f30
I1018 08:53:57.791630 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> conv2d0x55b7d0c74ec0  via relu_2.tmp_00x55b7d0c74b50
I1018 08:53:57.791631 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1041280 -> eager_deletion0x55b7d0caa5c0  via __control_var@16390x55b7d0c8ed20
I1018 08:53:57.791635 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdff90  via __control_var@15050x55b7d101d7a0
I1018 08:53:57.791636 4097533 graph_helper.h:104] adj conv2d0x55b7d0edeb50 -> cinn_launch0x55b7d0cfb320  via conv2d_2.tmp_00x55b7d0cfb110
I1018 08:53:57.791638 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d0cfb320  via pool2d_0.tmp_00x55b7d0b929b0
I1018 08:53:57.791641 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> conv2d0x55b7d0edeb50  via relu_1.tmp_00x55b7d0de1f20
I1018 08:53:57.791643 4097533 graph_helper.h:104] adj conv2d0x55b7d0ddf060 -> cinn_launch0x55b7d0c892a0  via conv2d_1.tmp_00x55b7d0c89030
I1018 08:53:57.791646 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100ffb0 -> eager_deletion0x55b7d0f91490  via __control_var@15450x55b7d0dbab10
I1018 08:53:57.791648 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> conv2d0x55b7d0ddf060  via pool2d_0.tmp_00x55b7d0b929b0
I1018 08:53:57.791651 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> conv2d0x55b7d10908e0  via relu_14.tmp_00x55b7d10900a0
I1018 08:53:57.791653 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> cinn_launch0x55b7d0fff800  via __control_var@15100x55b7d101e570
I1018 08:53:57.791656 4097533 graph_helper.h:104] adj conv2d0x55b7d0edf1d0 -> cinn_launch0x55b7d0fac1b0  via conv2d_0.tmp_00x55b7d0eaa280
I1018 08:53:57.791831 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103d9e0 -> eager_deletion0x55b7d0bf8d60  via __control_var@16490x55b7d0be5660
I1018 08:53:57.791834 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> eager_deletion0x55b7d0c2da20  via __control_var@14760x55b7d10431b0
I1018 08:53:57.791836 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> eager_deletion0x55b7d0c7f270  via __control_var@14740x55b7d1042b70
I1018 08:53:57.791838 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> eager_deletion0x55b7d0d36320  via __control_var@14960x55b7d101bd70
I1018 08:53:57.791843 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1013850 -> eager_deletion0x55b7d0d50360  via __control_var@16280x55b7d0d50420
I1018 08:53:57.791846 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> eager_deletion0x55b7d0d83e40  via __control_var@14910x55b7d101af00
I1018 08:53:57.791847 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103ffa0 -> eager_deletion0x55b7d0da05e0  via __control_var@16230x55b7d0da06a0
I1018 08:53:57.791850 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> eager_deletion0x55b7d0dd5690  via __control_var@14780x55b7d1043710
I1018 08:53:57.791852 4097533 graph_helper.h:104] adj conv2d0x55b7d1029e80 -> eager_deletion0x55b7d0d04df0  via __control_var@16330x55b7d0d04eb0
I1018 08:53:57.791854 4097533 graph_helper.h:104] adj cinn_launch0x55b7d10059f0 -> eager_deletion0x55b7d0ec5bb0  via __control_var@16130x55b7d0c239b0
I1018 08:53:57.791857 4097533 graph_helper.h:104] adj conv2d0x55b7d11091c0 -> eager_deletion0x55b7d0e31dd0  via __control_var@16010x55b7d0e680a0
I1018 08:53:57.791859 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> eager_deletion0x55b7d0dac250  via __control_var@15070x55b7d101dd40
I1018 08:53:57.791862 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> eager_deletion0x55b7d0d20510  via __control_var@14900x55b7d101ac70
I1018 08:53:57.791864 4097533 graph_helper.h:104] adj conv2d0x55b7d1122f60 -> eager_deletion0x55b7d0d2e9e0  via __control_var@15860x55b7d0d11c80
I1018 08:53:57.791867 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> eager_deletion0x55b7d0ccaa80  via __control_var@14930x55b7d101b4e0
I1018 08:53:57.791869 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> eager_deletion0x55b7d0c8c100  via __control_var@14820x55b7d1044290
I1018 08:53:57.791872 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> eager_deletion0x55b7d0c82340  via __control_var@14730x55b7d1042880
I1018 08:53:57.791873 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> eager_deletion0x55b7d0be1970  via __control_var@14720x55b7d1042590
I1018 08:53:57.791877 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> eager_deletion0x55b7d0e7c680  via __control_var@14810x55b7d1043fa0
I1018 08:53:57.791878 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> eager_deletion0x55b7d0c100f0  via __control_var@14920x55b7d101b1f0
I1018 08:53:57.791880 4097533 graph_helper.h:104] adj conv2d0x55b7d106f840 -> eager_deletion0x55b7d0fc1590  via __control_var@15700x55b7d0bff1e0
I1018 08:53:57.791883 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> eager_deletion0x55b7d0c74830  via __control_var@14770x55b7d1043480
I1018 08:53:57.791885 4097533 graph_helper.h:104] adj conv2d0x55b7d0fb56d0 -> eager_deletion0x55b7d0ebbb40  via __control_var@15650x55b7d0a9d670
I1018 08:53:57.791887 4097533 graph_helper.h:104] adj conv2d0x55b7d111bde0 -> eager_deletion0x55b7d1106060  via __control_var@15620x55b7d0b734a0
I1018 08:53:57.791890 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100c710 -> eager_deletion0x55b7d0aeffc0  via __control_var@15590x55b7d0afe850
I1018 08:53:57.791893 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1038e60 -> eager_deletion0x55b7d0aa9140  via __control_var@15560x55b7d0e966c0
I1018 08:53:57.791894 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> eager_deletion0x55b7d0a72230  via __control_var@14830x55b7d1044580
I1018 08:53:57.791898 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103a140 -> eager_deletion0x55b7d0a639a0  via __control_var@15510x55b7d0dbb500
I1018 08:53:57.791899 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1012570 -> eager_deletion0x55b7d0a1f3e0  via __control_var@15480x55b7d0995060
I1018 08:53:57.791901 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1004710 -> eager_deletion0x55b7d09a1410  via __control_var@15420x55b7d09da090
I1018 08:53:57.791903 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1003430 -> eager_deletion0x55b7d0abb300  via __control_var@15390x55b7d0990130
I1018 08:53:57.791908 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100d9f0 -> eager_deletion0x55b7d0c57440  via __control_var@15300x55b7d0affe60
I1018 08:53:57.791910 4097533 graph_helper.h:104] adj conv2d0x55b7d0b70160 -> eager_deletion0x55b7d0a73840  via __control_var@15270x55b7d0e77f40
I1018 08:53:57.791913 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100ecd0 -> eager_deletion0x55b7d0c96340  via __control_var@15240x55b7d1033be0
I1018 08:53:57.791915 4097533 graph_helper.h:104] adj conv2d0x55b7d0edf1d0 -> eager_deletion0x55b7d0c57d60  via __control_var@15210x55b7d0b86300
I1018 08:53:57.791918 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> eager_deletion0x55b7cd159bc0  via __control_var@14990x55b7d101c600
I1018 08:53:57.791919 4097533 graph_helper.h:104] adj conv2d0x55b7d0d441e0 -> eager_deletion0x55b7d09765a0  via __control_var@15130x55b7d096fc60
I1018 08:53:57.791922 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103ecc0 -> eager_deletion0x55b7b78d2db0  via __control_var@15180x55b7b7024640
I1018 08:53:57.791924 4097533 graph_helper.h:104] adj conv2d0x55b7d0edeb50 -> eager_deletion0x55b7d0cdcfc0  via __control_var@15360x55b7d0dad860
I1018 08:53:57.791929 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> cinn_launch0x55b7d1002020  via __control_var@14800x55b7d1043cd0
I1018 08:53:57.791931 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> cinn_launch0x55b7d1000c10  via __control_var@14720x55b7d1042590
I1018 08:53:57.791934 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d0ffcfe0  via __control_var@14740x55b7d1042b70
I1018 08:53:57.791936 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d0ffbc30  via __control_var@14750x55b7d1042e40
I1018 08:53:57.791939 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d1068fa0  via __control_var@15090x55b7d101e2a0
I1018 08:53:57.791940 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d1067b90  via __control_var@15010x55b7d101cbc0
I1018 08:53:57.791944 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> cinn_launch0x55b7d1066780  via __control_var@14980x55b7d101c330
I1018 08:53:57.791945 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> cinn_launch0x55b7d1065370  via __control_var@14900x55b7d101ac70
I1018 08:53:57.791947 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> cinn_launch0x55b7d1063f60  via __control_var@15070x55b7d101dd40
I1018 08:53:57.791950 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> cinn_launch0x55b7d1061740  via __control_var@15080x55b7d101dfd0
I1018 08:53:57.791952 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> cinn_launch0x55b7d0fe13a0  via __control_var@14920x55b7d101b1f0
I1018 08:53:57.791955 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdeb80  via __control_var@15030x55b7d101d160
I1018 08:53:57.791957 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdd770  via __control_var@14850x55b7d1044b40
I1018 08:53:57.791960 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdc360  via __control_var@15040x55b7d101d4d0
I1018 08:53:57.791962 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> cinn_launch0x55b7d0fdaf50  via __control_var@14970x55b7d101c060
I1018 08:53:57.791965 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> cinn_launch0x55b7d0fd9b40  via __control_var@14930x55b7d101b4e0
I1018 08:53:57.791966 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> cinn_launch0x55b7d0fd8730  via __control_var@14820x55b7d1044290
I1018 08:53:57.791970 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> cinn_launch0x55b7d1104d40  via __control_var@14840x55b7d1044870
I1018 08:53:57.791971 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> cinn_launch0x55b7d1103930  via __control_var@15000x55b7d101c8f0
I1018 08:53:57.791973 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> cinn_launch0x55b7d1102520  via __control_var@14990x55b7d101c600
I1018 08:53:57.791977 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d1101110  via __control_var@14880x55b7d10453e0
I1018 08:53:57.791980 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10ffd00  via __control_var@14890x55b7d10456b0
I1018 08:53:57.791982 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10fe8f0  via __control_var@14860x55b7d1044e10
I1018 08:53:57.791985 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10fd4e0  via __control_var@14760x55b7d10431b0
I1018 08:53:57.791987 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> cinn_launch0x55b7d1113ec0  via __control_var@14790x55b7d1043a00
I1018 08:53:57.791989 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> cinn_launch0x55b7d1112ab0  via __control_var@14780x55b7d1043710
I1018 08:53:57.791991 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> cinn_launch0x55b7d11116a0  via __control_var@14830x55b7d1044580
I1018 08:53:57.791993 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> cinn_launch0x55b7d1110290  via __control_var@14940x55b7d101b7d0
I1018 08:53:57.791996 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> cinn_launch0x55b7d110ee80  via __control_var@15060x55b7d101da70
I1018 08:53:57.791998 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> cinn_launch0x55b7d110da70  via __control_var@14910x55b7d101af00
I1018 08:53:57.792001 4097533 graph_helper.h:104] adj conv2d0x55b7d1119e20 -> eager_deletion0x55b7d0e76930  via __control_var@16040x55b7d0e8a420
I1018 08:53:57.792003 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d110c6a0  via __control_var@14960x55b7d101bd70
I1018 08:53:57.792006 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d107f940  via __control_var@15020x55b7d101ce90
I1018 08:53:57.792007 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> cinn_launch0x55b7d107e530  via __control_var@14770x55b7d1043480
I1018 08:53:57.792011 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> cinn_launch0x55b7d107d120  via __control_var@14950x55b7d101baa0
I1018 08:53:57.792012 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d107bd10  via __control_var@14870x55b7d1045110
I1018 08:53:57.792014 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d107a890  via __control_var@14810x55b7d1043fa0
I1018 08:53:57.792017 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1011290 -> eager_deletion0x55b7d0bca520  via __control_var@15330x55b7d0c11700
I1018 08:53:57.792019 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100b430 -> eager_deletion0x55b7d0ccd6a0  via __control_var@16360x55b7d0cbdbf0
I1018 08:53:57.792022 4097533 graph_helper.h:104] adj conv2d0x55b7d106f840 -> cinn_launch0x55b7d109c290  via conv2d_19.tmp_00x55b7d109bfc0
I1018 08:53:57.792024 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d109c290  via relu_14.tmp_00x55b7d10900a0
I1018 08:53:57.792026 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> conv2d0x55b7d106f840  via relu_15.tmp_00x55b7d106f450
I1018 08:53:57.792029 4097533 graph_helper.h:104] adj conv2d0x55b7d10908e0 -> cinn_launch0x55b7d106aaf0  via conv2d_18.tmp_00x55b7d106a820
I1018 08:53:57.792032 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103c700 -> eager_deletion0x55b7d0e11740  via __control_var@16180x55b7d0de5140
I1018 08:53:57.792034 4097533 graph_helper.h:104] adj conv2d0x55b7d0fc2680 -> cinn_launch0x55b7d1126750  via conv2d_17.tmp_00x55b7d11264a0
I1018 08:53:57.792037 4097533 graph_helper.h:104] adj conv2d0x55b7d0fb56d0 -> cinn_launch0x55b7d1126750  via conv2d_16.tmp_00x55b7d0fc23b0
I1018 08:53:57.792038 4097533 graph_helper.h:104] adj conv2d0x55b7d0fc2680 -> eager_deletion0x55b7d0a126c0  via __control_var@15910x55b7d0d676f0
I1018 08:53:57.792042 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> conv2d0x55b7d0fc2680  via relu_12.tmp_00x55b7d1122b70
I1018 08:53:57.792045 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> eager_deletion0x55b7d0e04390  via __control_var@14850x55b7d1044b40
I1018 08:53:57.792047 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> cinn_launch0x55b7d1062b50  via __control_var@15110x55b7d101e840
I1018 08:53:57.792050 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> conv2d0x55b7d0fb56d0  via relu_13.tmp_00x55b7d0fb52e0
I1018 08:53:57.792052 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1014b30 -> eager_deletion0x55b7d0c4a1c0  via __control_var@16440x55b7d0c4a280
I1018 08:53:57.792055 4097533 graph_helper.h:104] adj conv2d0x55b7d1122f60 -> cinn_launch0x55b7d0fd6ee0  via conv2d_15.tmp_00x55b7d0fd6c10
I1018 08:53:57.792057 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> conv2d0x55b7d1122f60  via relu_12.tmp_00x55b7d1122b70
I1018 08:53:57.792059 4097533 graph_helper.h:104] adj conv2d0x55b7d0d89b40 -> eager_deletion0x55b7d0ebf660  via __control_var@16070x55b7d0dd21c0
I1018 08:53:57.792062 4097533 graph_helper.h:104] adj conv2d0x55b7d11091c0 -> cinn_launch0x55b7d0fea800  via conv2d_14.tmp_00x55b7d0fea530
I1018 08:53:57.792064 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fea800  via relu_10.tmp_00x55b7d102b4b0
I1018 08:53:57.792066 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> conv2d0x55b7d11091c0  via relu_11.tmp_00x55b7d1108dd0
I1018 08:53:57.792069 4097533 graph_helper.h:104] adj conv2d0x55b7d102bc70 -> cinn_launch0x55b7d1115560  via conv2d_13.tmp_00x55b7d11184f0
I1018 08:53:57.792071 4097533 graph_helper.h:104] adj conv2d0x55b7d0ec1350 -> eager_deletion0x55b7d0c30050  via __control_var@16100x55b7d0eacc00
I1018 08:53:57.792074 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> conv2d0x55b7d102bc70  via relu_10.tmp_00x55b7d102b4b0
I1018 08:53:57.792076 4097533 graph_helper.h:104] adj conv2d0x55b7d0d441e0 -> cinn_launch0x55b7d1031e80  via conv2d_12.tmp_00x55b7d1031bd0
I1018 08:53:57.792078 4097533 graph_helper.h:104] adj conv2d0x55b7d0e99aa0 -> cinn_launch0x55b7d1031e80  via conv2d_11.tmp_00x55b7d0d43f10
I1018 08:53:57.792080 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> conv2d0x55b7d0d441e0  via relu_8.tmp_00x55b7d1029a90
I1018 08:53:57.792083 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> conv2d0x55b7d0e99aa0  via relu_9.tmp_00x55b7d0e996b0
I1018 08:53:57.792085 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103b420 -> eager_deletion0x55b7d0cdb9b0  via __control_var@15830x55b7d0e5ec50
I1018 08:53:57.792088 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> conv2d0x55b7d1029e80  via relu_8.tmp_00x55b7d1029a90
I1018 08:53:57.792090 4097533 graph_helper.h:104] adj conv2d0x55b7d0ec1350 -> cinn_launch0x55b7d1047f30  via conv2d_9.tmp_00x55b7d1047c60
I1018 08:53:57.792093 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d1047f30  via relu_6.tmp_00x55b7d0e88b80
I1018 08:53:57.792095 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> conv2d0x55b7d0ec1350  via relu_7.tmp_00x55b7d0e0f4e0
I1018 08:53:57.792097 4097533 graph_helper.h:104] adj conv2d0x55b7d0e89340 -> cinn_launch0x55b7d0f0e1c0  via conv2d_8.tmp_00x55b7d0f0def0
I1018 08:53:57.792099 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> conv2d0x55b7d0e89340  via relu_6.tmp_00x55b7d0e88b80
I1018 08:53:57.792102 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> cinn_launch0x55b7d0ffe3f0  via __control_var@14730x55b7d1042880
I1018 08:53:57.792104 4097533 graph_helper.h:104] adj conv2d0x55b7d1119e20 -> cinn_launch0x55b7d0add8b0  via conv2d_7.tmp_00x55b7d0add600
I1018 08:53:57.792107 4097533 graph_helper.h:104] adj conv2d0x55b7d0d89b40 -> cinn_launch0x55b7d0add8b0  via conv2d_6.tmp_00x55b7d1119b50
I1018 08:53:57.792109 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> conv2d0x55b7d1119e20  via relu_4.tmp_00x55b7d111b9f0
I1018 08:53:57.792111 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> conv2d0x55b7d0d89b40  via relu_5.tmp_00x55b7d0d89750
I1018 08:53:57.792116 4097533 graph_helper.h:104] adj conv2d0x55b7d0e99aa0 -> eager_deletion0x55b7d0faad50  via __control_var@15960x55b7d0d48cf0
I1018 08:53:57.792119 4097533 graph_helper.h:104] adj conv2d0x55b7d111bde0 -> cinn_launch0x55b7d0dcf300  via conv2d_5.tmp_00x55b7d0dcf030
I1018 08:53:57.792121 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> conv2d0x55b7d111bde0  via relu_4.tmp_00x55b7d111b9f0
I1018 08:53:57.792124 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d0c640b0  via relu_2.tmp_00x55b7d0c74b50
I1018 08:53:57.792125 4097533 graph_helper.h:104] adj conv2d0x55b7d0b70160 -> cinn_launch0x55b7d0c640b0  via conv2d_4.tmp_00x55b7d0c63de0
I1018 08:53:57.792129 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> conv2d0x55b7d0b70160  via relu_3.tmp_00x55b7d0b6fd70
I1018 08:53:57.792130 4097533 graph_helper.h:104] adj conv2d0x55b7d0c74ec0 -> cinn_launch0x55b7d0c9d5e0  via conv2d_3.tmp_00x55b7d0e7f460
I1018 08:53:57.792132 4097533 graph_helper.h:104] adj conv2d0x55b7d1029e80 -> cinn_launch0x55b7d0cb5e70  via conv2d_10.tmp_00x55b7d0cb8f30
I1018 08:53:57.792135 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> conv2d0x55b7d0c74ec0  via relu_2.tmp_00x55b7d0c74b50
I1018 08:53:57.792137 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1041280 -> eager_deletion0x55b7d0caa5c0  via __control_var@16390x55b7d0c8ed20
I1018 08:53:57.792140 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdff90  via __control_var@15050x55b7d101d7a0
I1018 08:53:57.792142 4097533 graph_helper.h:104] adj conv2d0x55b7d0edeb50 -> cinn_launch0x55b7d0cfb320  via conv2d_2.tmp_00x55b7d0cfb110
I1018 08:53:57.792145 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d0cfb320  via pool2d_0.tmp_00x55b7d0b929b0
I1018 08:53:57.792147 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> conv2d0x55b7d0edeb50  via relu_1.tmp_00x55b7d0de1f20
I1018 08:53:57.792150 4097533 graph_helper.h:104] adj conv2d0x55b7d0ddf060 -> cinn_launch0x55b7d0c892a0  via conv2d_1.tmp_00x55b7d0c89030
I1018 08:53:57.792152 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100ffb0 -> eager_deletion0x55b7d0f91490  via __control_var@15450x55b7d0dbab10
I1018 08:53:57.792155 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> conv2d0x55b7d0ddf060  via pool2d_0.tmp_00x55b7d0b929b0
I1018 08:53:57.792157 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> conv2d0x55b7d10908e0  via relu_14.tmp_00x55b7d10900a0
I1018 08:53:57.792160 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> cinn_launch0x55b7d0fff800  via __control_var@15100x55b7d101e570
I1018 08:53:57.792162 4097533 graph_helper.h:104] adj conv2d0x55b7d0edf1d0 -> cinn_launch0x55b7d0fac1b0  via conv2d_0.tmp_00x55b7d0eaa280
I1018 08:53:57.792318 4097533 share_varinfo_into_cinn_pass.cc:65] Variables would be deleted by the eager_deletion_op following the cinn_launch:assign_76.tmp_0
I1018 08:53:57.792325 4097533 share_varinfo_into_cinn_pass.cc:65] Variables would be deleted by the eager_deletion_op following the cinn_launch:assign_72.tmp_0
I1018 08:53:57.792330 4097533 share_varinfo_into_cinn_pass.cc:65] Variables would be deleted by the eager_deletion_op following the cinn_launch:assign_64.tmp_0
I1018 08:53:57.792333 4097533 share_varinfo_into_cinn_pass.cc:65] Variables would be deleted by the eager_deletion_op following the cinn_launch:assign_60.tmp_0
I1018 08:53:57.792337 4097533 share_varinfo_into_cinn_pass.cc:65] Variables would be deleted by the eager_deletion_op following the cinn_launch:assign_56.tmp_0
I1018 08:53:57.792340 4097533 share_varinfo_into_cinn_pass.cc:65] Variables would be deleted by the eager_deletion_op following the cinn_launch:assign_52.tmp_0
I1018 08:53:57.792343 4097533 share_varinfo_into_cinn_pass.cc:65] Variables would be deleted by the eager_deletion_op following the cinn_launch:assign_48.tmp_0
I1018 08:53:57.792347 4097533 share_varinfo_into_cinn_pass.cc:65] Variables would be deleted by the eager_deletion_op following the cinn_launch:assign_44.tmp_0
I1018 08:53:57.792352 4097533 share_varinfo_into_cinn_pass.cc:65] Variables would be deleted by the eager_deletion_op following the cinn_launch:assign_40.tmp_0
I1018 08:53:57.792356 4097533 share_varinfo_into_cinn_pass.cc:65] Variables would be deleted by the eager_deletion_op following the cinn_launch:assign_36.tmp_0
I1018 08:53:57.792359 4097533 share_varinfo_into_cinn_pass.cc:65] Variables would be deleted by the eager_deletion_op following the cinn_launch:assign_32.tmp_0
I1018 08:53:57.792362 4097533 share_varinfo_into_cinn_pass.cc:65] Variables would be deleted by the eager_deletion_op following the cinn_launch:assign_24.tmp_0
I1018 08:53:57.792366 4097533 share_varinfo_into_cinn_pass.cc:65] Variables would be deleted by the eager_deletion_op following the cinn_launch:assign_20.tmp_0
I1018 08:53:57.792369 4097533 share_varinfo_into_cinn_pass.cc:65] Variables would be deleted by the eager_deletion_op following the cinn_launch:assign_16.tmp_0
I1018 08:53:57.792372 4097533 share_varinfo_into_cinn_pass.cc:65] Variables would be deleted by the eager_deletion_op following the cinn_launch:assign_12.tmp_0
I1018 08:53:57.792376 4097533 share_varinfo_into_cinn_pass.cc:65] Variables would be deleted by the eager_deletion_op following the cinn_launch:assign_8.tmp_0
I1018 08:53:57.792379 4097533 share_varinfo_into_cinn_pass.cc:65] Variables would be deleted by the eager_deletion_op following the cinn_launch:assign_4.tmp_0
I1018 08:53:57.792382 4097533 share_varinfo_into_cinn_pass.cc:65] Variables would be deleted by the eager_deletion_op following the cinn_launch:assign_0.tmp_0
I1018 08:53:57.792387 4097533 share_varinfo_into_cinn_pass.cc:65] Variables would be deleted by the eager_deletion_op following the cinn_launch:assign_28.tmp_0
I1018 08:53:57.792389 4097533 share_varinfo_into_cinn_pass.cc:55] No eager_deletion op found after this cinn_launch op
I1018 08:53:57.792392 4097533 share_varinfo_into_cinn_pass.cc:55] No eager_deletion op found after this cinn_launch op
I1018 08:53:57.792393 4097533 share_varinfo_into_cinn_pass.cc:55] No eager_deletion op found after this cinn_launch op
I1018 08:53:57.792397 4097533 share_varinfo_into_cinn_pass.cc:55] No eager_deletion op found after this cinn_launch op
I1018 08:53:57.792398 4097533 share_varinfo_into_cinn_pass.cc:55] No eager_deletion op found after this cinn_launch op
I1018 08:53:57.792400 4097533 share_varinfo_into_cinn_pass.cc:55] No eager_deletion op found after this cinn_launch op
I1018 08:53:57.792402 4097533 share_varinfo_into_cinn_pass.cc:55] No eager_deletion op found after this cinn_launch op
I1018 08:53:57.792404 4097533 share_varinfo_into_cinn_pass.cc:55] No eager_deletion op found after this cinn_launch op
I1018 08:53:57.792407 4097533 share_varinfo_into_cinn_pass.cc:55] No eager_deletion op found after this cinn_launch op
I1018 08:53:57.792408 4097533 share_varinfo_into_cinn_pass.cc:65] Variables would be deleted by the eager_deletion_op following the cinn_launch:assign_68.tmp_0
I1018 08:53:57.792411 4097533 share_varinfo_into_cinn_pass.cc:55] No eager_deletion op found after this cinn_launch op
I1018 08:53:57.792413 4097533 share_varinfo_into_cinn_pass.cc:55] No eager_deletion op found after this cinn_launch op
I1018 08:53:57.792415 4097533 share_varinfo_into_cinn_pass.cc:55] No eager_deletion op found after this cinn_launch op
I1018 08:53:57.792418 4097533 share_varinfo_into_cinn_pass.cc:55] No eager_deletion op found after this cinn_launch op
I1018 08:53:57.792419 4097533 share_varinfo_into_cinn_pass.cc:55] No eager_deletion op found after this cinn_launch op
I1018 08:53:57.792421 4097533 share_varinfo_into_cinn_pass.cc:55] No eager_deletion op found after this cinn_launch op
I1018 08:53:57.792423 4097533 share_varinfo_into_cinn_pass.cc:55] No eager_deletion op found after this cinn_launch op
I1018 08:53:57.792425 4097533 share_varinfo_into_cinn_pass.cc:55] No eager_deletion op found after this cinn_launch op
I1018 08:53:57.792428 4097533 share_varinfo_into_cinn_pass.cc:55] No eager_deletion op found after this cinn_launch op
I1018 08:53:57.792429 4097533 share_varinfo_into_cinn_pass.cc:55] No eager_deletion op found after this cinn_launch op
I1018 08:53:57.792433 4097533 share_varinfo_into_cinn_pass.cc:55] No eager_deletion op found after this cinn_launch op
I1018 08:53:57.792435 4097533 share_varinfo_into_cinn_pass.cc:55] No eager_deletion op found after this cinn_launch op
I1018 08:53:57.792438 4097533 share_varinfo_into_cinn_pass.cc:55] No eager_deletion op found after this cinn_launch op
I1018 08:53:57.792440 4097533 share_varinfo_into_cinn_pass.cc:55] No eager_deletion op found after this cinn_launch op
I1018 08:53:57.792443 4097533 share_varinfo_into_cinn_pass.cc:55] No eager_deletion op found after this cinn_launch op
I1018 08:53:57.792444 4097533 share_varinfo_into_cinn_pass.cc:55] No eager_deletion op found after this cinn_launch op
I1018 08:53:57.792446 4097533 share_varinfo_into_cinn_pass.cc:55] No eager_deletion op found after this cinn_launch op
I1018 08:53:57.792448 4097533 share_varinfo_into_cinn_pass.cc:55] No eager_deletion op found after this cinn_launch op
I1018 08:53:57.792450 4097533 share_varinfo_into_cinn_pass.cc:55] No eager_deletion op found after this cinn_launch op
I1018 08:53:57.792452 4097533 share_varinfo_into_cinn_pass.cc:55] No eager_deletion op found after this cinn_launch op
I1018 08:53:57.792454 4097533 share_varinfo_into_cinn_pass.cc:55] No eager_deletion op found after this cinn_launch op
I1018 08:53:57.792456 4097533 share_varinfo_into_cinn_pass.cc:55] No eager_deletion op found after this cinn_launch op
I1018 08:53:57.792459 4097533 share_varinfo_into_cinn_pass.cc:55] No eager_deletion op found after this cinn_launch op
I1018 08:53:57.792460 4097533 share_varinfo_into_cinn_pass.cc:55] No eager_deletion op found after this cinn_launch op
I1018 08:53:57.792462 4097533 share_varinfo_into_cinn_pass.cc:55] No eager_deletion op found after this cinn_launch op
I1018 08:53:57.792464 4097533 share_varinfo_into_cinn_pass.cc:55] No eager_deletion op found after this cinn_launch op
I1018 08:53:57.792466 4097533 share_varinfo_into_cinn_pass.cc:55] No eager_deletion op found after this cinn_launch op
I1018 08:53:57.792469 4097533 share_varinfo_into_cinn_pass.cc:65] Variables would be deleted by the eager_deletion_op following the cinn_launch:assign_77.tmp_0,conv2d_19.tmp_0,linear_0.tmp_1,relu_14.tmp_0
I1018 08:53:57.792474 4097533 share_varinfo_into_cinn_pass.cc:65] Variables would be deleted by the eager_deletion_op following the cinn_launch:assign_73.tmp_0,conv2d_18.tmp_0
I1018 08:53:57.792479 4097533 share_varinfo_into_cinn_pass.cc:65] Variables would be deleted by the eager_deletion_op following the cinn_launch:assign_65.tmp_0,assign_69.tmp_0,conv2d_16.tmp_0,conv2d_17.tmp_0
I1018 08:53:57.792484 4097533 share_varinfo_into_cinn_pass.cc:55] No eager_deletion op found after this cinn_launch op
I1018 08:53:57.792486 4097533 share_varinfo_into_cinn_pass.cc:65] Variables would be deleted by the eager_deletion_op following the cinn_launch:assign_61.tmp_0,conv2d_15.tmp_0
I1018 08:53:57.792490 4097533 share_varinfo_into_cinn_pass.cc:65] Variables would be deleted by the eager_deletion_op following the cinn_launch:assign_57.tmp_0,conv2d_14.tmp_0,relu_10.tmp_0
I1018 08:53:57.792495 4097533 share_varinfo_into_cinn_pass.cc:65] Variables would be deleted by the eager_deletion_op following the cinn_launch:assign_53.tmp_0,conv2d_13.tmp_0
I1018 08:53:57.792500 4097533 share_varinfo_into_cinn_pass.cc:65] Variables would be deleted by the eager_deletion_op following the cinn_launch:assign_45.tmp_0,assign_49.tmp_0,conv2d_11.tmp_0,conv2d_12.tmp_0
I1018 08:53:57.792505 4097533 share_varinfo_into_cinn_pass.cc:65] Variables would be deleted by the eager_deletion_op following the cinn_launch:assign_37.tmp_0,conv2d_9.tmp_0,relu_6.tmp_0
I1018 08:53:57.792510 4097533 share_varinfo_into_cinn_pass.cc:65] Variables would be deleted by the eager_deletion_op following the cinn_launch:assign_33.tmp_0,conv2d_8.tmp_0
I1018 08:53:57.792512 4097533 share_varinfo_into_cinn_pass.cc:55] No eager_deletion op found after this cinn_launch op
I1018 08:53:57.792515 4097533 share_varinfo_into_cinn_pass.cc:65] Variables would be deleted by the eager_deletion_op following the cinn_launch:assign_25.tmp_0,assign_29.tmp_0,conv2d_6.tmp_0,conv2d_7.tmp_0
I1018 08:53:57.792522 4097533 share_varinfo_into_cinn_pass.cc:65] Variables would be deleted by the eager_deletion_op following the cinn_launch:assign_21.tmp_0,conv2d_5.tmp_0
I1018 08:53:57.792526 4097533 share_varinfo_into_cinn_pass.cc:65] Variables would be deleted by the eager_deletion_op following the cinn_launch:assign_17.tmp_0,conv2d_4.tmp_0,relu_2.tmp_0
I1018 08:53:57.792531 4097533 share_varinfo_into_cinn_pass.cc:65] Variables would be deleted by the eager_deletion_op following the cinn_launch:assign_13.tmp_0,conv2d_3.tmp_0
I1018 08:53:57.792536 4097533 share_varinfo_into_cinn_pass.cc:65] Variables would be deleted by the eager_deletion_op following the cinn_launch:assign_41.tmp_0,conv2d_10.tmp_0
I1018 08:53:57.792539 4097533 share_varinfo_into_cinn_pass.cc:55] No eager_deletion op found after this cinn_launch op
I1018 08:53:57.792542 4097533 share_varinfo_into_cinn_pass.cc:65] Variables would be deleted by the eager_deletion_op following the cinn_launch:assign_9.tmp_0,conv2d_2.tmp_0,pool2d_0.tmp_0
I1018 08:53:57.792546 4097533 share_varinfo_into_cinn_pass.cc:65] Variables would be deleted by the eager_deletion_op following the cinn_launch:assign_5.tmp_0,conv2d_1.tmp_0
I1018 08:53:57.792551 4097533 share_varinfo_into_cinn_pass.cc:55] No eager_deletion op found after this cinn_launch op
I1018 08:53:57.792553 4097533 share_varinfo_into_cinn_pass.cc:65] Variables would be deleted by the eager_deletion_op following the cinn_launch:assign_1.tmp_0,conv2d_0.tmp_0
I1018 08:53:57.792557 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103d9e0 -> eager_deletion0x55b7d0bf8d60  via __control_var@16490x55b7d0be5660
I1018 08:53:57.792559 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> eager_deletion0x55b7d0c2da20  via __control_var@14760x55b7d10431b0
I1018 08:53:57.792562 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> eager_deletion0x55b7d0c7f270  via __control_var@14740x55b7d1042b70
I1018 08:53:57.792564 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> eager_deletion0x55b7d0d36320  via __control_var@14960x55b7d101bd70
I1018 08:53:57.792567 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1013850 -> eager_deletion0x55b7d0d50360  via __control_var@16280x55b7d0d50420
I1018 08:53:57.792569 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> eager_deletion0x55b7d0d83e40  via __control_var@14910x55b7d101af00
I1018 08:53:57.792572 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103ffa0 -> eager_deletion0x55b7d0da05e0  via __control_var@16230x55b7d0da06a0
I1018 08:53:57.792574 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> eager_deletion0x55b7d0dd5690  via __control_var@14780x55b7d1043710
I1018 08:53:57.792577 4097533 graph_helper.h:104] adj conv2d0x55b7d1029e80 -> eager_deletion0x55b7d0d04df0  via __control_var@16330x55b7d0d04eb0
I1018 08:53:57.792578 4097533 graph_helper.h:104] adj cinn_launch0x55b7d10059f0 -> eager_deletion0x55b7d0ec5bb0  via __control_var@16130x55b7d0c239b0
I1018 08:53:57.792582 4097533 graph_helper.h:104] adj conv2d0x55b7d11091c0 -> eager_deletion0x55b7d0e31dd0  via __control_var@16010x55b7d0e680a0
I1018 08:53:57.792583 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> eager_deletion0x55b7d0dac250  via __control_var@15070x55b7d101dd40
I1018 08:53:57.792585 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> eager_deletion0x55b7d0d20510  via __control_var@14900x55b7d101ac70
I1018 08:53:57.792588 4097533 graph_helper.h:104] adj conv2d0x55b7d1122f60 -> eager_deletion0x55b7d0d2e9e0  via __control_var@15860x55b7d0d11c80
I1018 08:53:57.792590 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> eager_deletion0x55b7d0ccaa80  via __control_var@14930x55b7d101b4e0
I1018 08:53:57.792593 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> eager_deletion0x55b7d0c8c100  via __control_var@14820x55b7d1044290
I1018 08:53:57.792595 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> eager_deletion0x55b7d0c82340  via __control_var@14730x55b7d1042880
I1018 08:53:57.792599 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> eager_deletion0x55b7d0be1970  via __control_var@14720x55b7d1042590
I1018 08:53:57.792603 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> eager_deletion0x55b7d0e7c680  via __control_var@14810x55b7d1043fa0
I1018 08:53:57.792604 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> eager_deletion0x55b7d0c100f0  via __control_var@14920x55b7d101b1f0
I1018 08:53:57.792608 4097533 graph_helper.h:104] adj conv2d0x55b7d106f840 -> eager_deletion0x55b7d0fc1590  via __control_var@15700x55b7d0bff1e0
I1018 08:53:57.792609 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> eager_deletion0x55b7d0c74830  via __control_var@14770x55b7d1043480
I1018 08:53:57.792613 4097533 graph_helper.h:104] adj conv2d0x55b7d0fb56d0 -> eager_deletion0x55b7d0ebbb40  via __control_var@15650x55b7d0a9d670
I1018 08:53:57.792614 4097533 graph_helper.h:104] adj conv2d0x55b7d111bde0 -> eager_deletion0x55b7d1106060  via __control_var@15620x55b7d0b734a0
I1018 08:53:57.792616 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100c710 -> eager_deletion0x55b7d0aeffc0  via __control_var@15590x55b7d0afe850
I1018 08:53:57.792619 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1038e60 -> eager_deletion0x55b7d0aa9140  via __control_var@15560x55b7d0e966c0
I1018 08:53:57.792621 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> eager_deletion0x55b7d0a72230  via __control_var@14830x55b7d1044580
I1018 08:53:57.792623 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103a140 -> eager_deletion0x55b7d0a639a0  via __control_var@15510x55b7d0dbb500
I1018 08:53:57.792626 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1012570 -> eager_deletion0x55b7d0a1f3e0  via __control_var@15480x55b7d0995060
I1018 08:53:57.792629 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1004710 -> eager_deletion0x55b7d09a1410  via __control_var@15420x55b7d09da090
I1018 08:53:57.792631 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1003430 -> eager_deletion0x55b7d0abb300  via __control_var@15390x55b7d0990130
I1018 08:53:57.792634 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100d9f0 -> eager_deletion0x55b7d0c57440  via __control_var@15300x55b7d0affe60
I1018 08:53:57.792635 4097533 graph_helper.h:104] adj conv2d0x55b7d0b70160 -> eager_deletion0x55b7d0a73840  via __control_var@15270x55b7d0e77f40
I1018 08:53:57.792639 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100ecd0 -> eager_deletion0x55b7d0c96340  via __control_var@15240x55b7d1033be0
I1018 08:53:57.792640 4097533 graph_helper.h:104] adj conv2d0x55b7d0edf1d0 -> eager_deletion0x55b7d0c57d60  via __control_var@15210x55b7d0b86300
I1018 08:53:57.792642 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> eager_deletion0x55b7cd159bc0  via __control_var@14990x55b7d101c600
I1018 08:53:57.792645 4097533 graph_helper.h:104] adj conv2d0x55b7d0d441e0 -> eager_deletion0x55b7d09765a0  via __control_var@15130x55b7d096fc60
I1018 08:53:57.792647 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103ecc0 -> eager_deletion0x55b7b78d2db0  via __control_var@15180x55b7b7024640
I1018 08:53:57.792650 4097533 graph_helper.h:104] adj conv2d0x55b7d0edeb50 -> eager_deletion0x55b7d0cdcfc0  via __control_var@15360x55b7d0dad860
I1018 08:53:57.792654 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> cinn_launch0x55b7d1002020  via __control_var@14800x55b7d1043cd0
I1018 08:53:57.792656 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> cinn_launch0x55b7d1000c10  via __control_var@14720x55b7d1042590
I1018 08:53:57.792659 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d0ffcfe0  via __control_var@14740x55b7d1042b70
I1018 08:53:57.792661 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d0ffbc30  via __control_var@14750x55b7d1042e40
I1018 08:53:57.792665 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d1068fa0  via __control_var@15090x55b7d101e2a0
I1018 08:53:57.792668 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d1067b90  via __control_var@15010x55b7d101cbc0
I1018 08:53:57.792670 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> cinn_launch0x55b7d1066780  via __control_var@14980x55b7d101c330
I1018 08:53:57.792673 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> cinn_launch0x55b7d1065370  via __control_var@14900x55b7d101ac70
I1018 08:53:57.792675 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> cinn_launch0x55b7d1063f60  via __control_var@15070x55b7d101dd40
I1018 08:53:57.792677 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> cinn_launch0x55b7d1061740  via __control_var@15080x55b7d101dfd0
I1018 08:53:57.792680 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> cinn_launch0x55b7d0fe13a0  via __control_var@14920x55b7d101b1f0
I1018 08:53:57.792682 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdeb80  via __control_var@15030x55b7d101d160
I1018 08:53:57.792685 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdd770  via __control_var@14850x55b7d1044b40
I1018 08:53:57.792687 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdc360  via __control_var@15040x55b7d101d4d0
I1018 08:53:57.792690 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> cinn_launch0x55b7d0fdaf50  via __control_var@14970x55b7d101c060
I1018 08:53:57.792692 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> cinn_launch0x55b7d0fd9b40  via __control_var@14930x55b7d101b4e0
I1018 08:53:57.792694 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> cinn_launch0x55b7d0fd8730  via __control_var@14820x55b7d1044290
I1018 08:53:57.792696 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> cinn_launch0x55b7d1104d40  via __control_var@14840x55b7d1044870
I1018 08:53:57.792699 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> cinn_launch0x55b7d1103930  via __control_var@15000x55b7d101c8f0
I1018 08:53:57.792701 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> cinn_launch0x55b7d1102520  via __control_var@14990x55b7d101c600
I1018 08:53:57.792703 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d1101110  via __control_var@14880x55b7d10453e0
I1018 08:53:57.792706 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10ffd00  via __control_var@14890x55b7d10456b0
I1018 08:53:57.792708 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10fe8f0  via __control_var@14860x55b7d1044e10
I1018 08:53:57.792711 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10fd4e0  via __control_var@14760x55b7d10431b0
I1018 08:53:57.792713 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> cinn_launch0x55b7d1113ec0  via __control_var@14790x55b7d1043a00
I1018 08:53:57.792716 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> cinn_launch0x55b7d1112ab0  via __control_var@14780x55b7d1043710
I1018 08:53:57.792717 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> cinn_launch0x55b7d11116a0  via __control_var@14830x55b7d1044580
I1018 08:53:57.792719 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> cinn_launch0x55b7d1110290  via __control_var@14940x55b7d101b7d0
I1018 08:53:57.792722 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> cinn_launch0x55b7d110ee80  via __control_var@15060x55b7d101da70
I1018 08:53:57.792724 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> cinn_launch0x55b7d110da70  via __control_var@14910x55b7d101af00
I1018 08:53:57.792727 4097533 graph_helper.h:104] adj conv2d0x55b7d1119e20 -> eager_deletion0x55b7d0e76930  via __control_var@16040x55b7d0e8a420
I1018 08:53:57.792729 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d110c6a0  via __control_var@14960x55b7d101bd70
I1018 08:53:57.792732 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d107f940  via __control_var@15020x55b7d101ce90
I1018 08:53:57.792737 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> cinn_launch0x55b7d107e530  via __control_var@14770x55b7d1043480
I1018 08:53:57.792739 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> cinn_launch0x55b7d107d120  via __control_var@14950x55b7d101baa0
I1018 08:53:57.792742 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d107bd10  via __control_var@14870x55b7d1045110
I1018 08:53:57.792743 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d107a890  via __control_var@14810x55b7d1043fa0
I1018 08:53:57.792747 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1011290 -> eager_deletion0x55b7d0bca520  via __control_var@15330x55b7d0c11700
I1018 08:53:57.792749 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100b430 -> eager_deletion0x55b7d0ccd6a0  via __control_var@16360x55b7d0cbdbf0
I1018 08:53:57.792752 4097533 graph_helper.h:104] adj conv2d0x55b7d106f840 -> cinn_launch0x55b7d109c290  via conv2d_19.tmp_00x55b7d109bfc0
I1018 08:53:57.792753 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d109c290  via relu_14.tmp_00x55b7d10900a0
I1018 08:53:57.792757 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> conv2d0x55b7d106f840  via relu_15.tmp_00x55b7d106f450
I1018 08:53:57.792758 4097533 graph_helper.h:104] adj conv2d0x55b7d10908e0 -> cinn_launch0x55b7d106aaf0  via conv2d_18.tmp_00x55b7d106a820
I1018 08:53:57.792761 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103c700 -> eager_deletion0x55b7d0e11740  via __control_var@16180x55b7d0de5140
I1018 08:53:57.792763 4097533 graph_helper.h:104] adj conv2d0x55b7d0fc2680 -> cinn_launch0x55b7d1126750  via conv2d_17.tmp_00x55b7d11264a0
I1018 08:53:57.792765 4097533 graph_helper.h:104] adj conv2d0x55b7d0fb56d0 -> cinn_launch0x55b7d1126750  via conv2d_16.tmp_00x55b7d0fc23b0
I1018 08:53:57.792768 4097533 graph_helper.h:104] adj conv2d0x55b7d0fc2680 -> eager_deletion0x55b7d0a126c0  via __control_var@15910x55b7d0d676f0
I1018 08:53:57.792770 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> conv2d0x55b7d0fc2680  via relu_12.tmp_00x55b7d1122b70
I1018 08:53:57.792773 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> eager_deletion0x55b7d0e04390  via __control_var@14850x55b7d1044b40
I1018 08:53:57.792775 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> cinn_launch0x55b7d1062b50  via __control_var@15110x55b7d101e840
I1018 08:53:57.792778 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> conv2d0x55b7d0fb56d0  via relu_13.tmp_00x55b7d0fb52e0
I1018 08:53:57.792780 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1014b30 -> eager_deletion0x55b7d0c4a1c0  via __control_var@16440x55b7d0c4a280
I1018 08:53:57.792783 4097533 graph_helper.h:104] adj conv2d0x55b7d1122f60 -> cinn_launch0x55b7d0fd6ee0  via conv2d_15.tmp_00x55b7d0fd6c10
I1018 08:53:57.792785 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> conv2d0x55b7d1122f60  via relu_12.tmp_00x55b7d1122b70
I1018 08:53:57.792788 4097533 graph_helper.h:104] adj conv2d0x55b7d0d89b40 -> eager_deletion0x55b7d0ebf660  via __control_var@16070x55b7d0dd21c0
I1018 08:53:57.792790 4097533 graph_helper.h:104] adj conv2d0x55b7d11091c0 -> cinn_launch0x55b7d0fea800  via conv2d_14.tmp_00x55b7d0fea530
I1018 08:53:57.792793 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fea800  via relu_10.tmp_00x55b7d102b4b0
I1018 08:53:57.792794 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> conv2d0x55b7d11091c0  via relu_11.tmp_00x55b7d1108dd0
I1018 08:53:57.792796 4097533 graph_helper.h:104] adj conv2d0x55b7d102bc70 -> cinn_launch0x55b7d1115560  via conv2d_13.tmp_00x55b7d11184f0
I1018 08:53:57.792799 4097533 graph_helper.h:104] adj conv2d0x55b7d0ec1350 -> eager_deletion0x55b7d0c30050  via __control_var@16100x55b7d0eacc00
I1018 08:53:57.792801 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> conv2d0x55b7d102bc70  via relu_10.tmp_00x55b7d102b4b0
I1018 08:53:57.792804 4097533 graph_helper.h:104] adj conv2d0x55b7d0d441e0 -> cinn_launch0x55b7d1031e80  via conv2d_12.tmp_00x55b7d1031bd0
I1018 08:53:57.792809 4097533 graph_helper.h:104] adj conv2d0x55b7d0e99aa0 -> cinn_launch0x55b7d1031e80  via conv2d_11.tmp_00x55b7d0d43f10
I1018 08:53:57.792810 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> conv2d0x55b7d0d441e0  via relu_8.tmp_00x55b7d1029a90
I1018 08:53:57.792814 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> conv2d0x55b7d0e99aa0  via relu_9.tmp_00x55b7d0e996b0
I1018 08:53:57.792815 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103b420 -> eager_deletion0x55b7d0cdb9b0  via __control_var@15830x55b7d0e5ec50
I1018 08:53:57.792819 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> conv2d0x55b7d1029e80  via relu_8.tmp_00x55b7d1029a90
I1018 08:53:57.792820 4097533 graph_helper.h:104] adj conv2d0x55b7d0ec1350 -> cinn_launch0x55b7d1047f30  via conv2d_9.tmp_00x55b7d1047c60
I1018 08:53:57.792824 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d1047f30  via relu_6.tmp_00x55b7d0e88b80
I1018 08:53:57.792825 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> conv2d0x55b7d0ec1350  via relu_7.tmp_00x55b7d0e0f4e0
I1018 08:53:57.792829 4097533 graph_helper.h:104] adj conv2d0x55b7d0e89340 -> cinn_launch0x55b7d0f0e1c0  via conv2d_8.tmp_00x55b7d0f0def0
I1018 08:53:57.792830 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> conv2d0x55b7d0e89340  via relu_6.tmp_00x55b7d0e88b80
I1018 08:53:57.792832 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> cinn_launch0x55b7d0ffe3f0  via __control_var@14730x55b7d1042880
I1018 08:53:57.792835 4097533 graph_helper.h:104] adj conv2d0x55b7d1119e20 -> cinn_launch0x55b7d0add8b0  via conv2d_7.tmp_00x55b7d0add600
I1018 08:53:57.792837 4097533 graph_helper.h:104] adj conv2d0x55b7d0d89b40 -> cinn_launch0x55b7d0add8b0  via conv2d_6.tmp_00x55b7d1119b50
I1018 08:53:57.792840 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> conv2d0x55b7d1119e20  via relu_4.tmp_00x55b7d111b9f0
I1018 08:53:57.792842 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> conv2d0x55b7d0d89b40  via relu_5.tmp_00x55b7d0d89750
I1018 08:53:57.792845 4097533 graph_helper.h:104] adj conv2d0x55b7d0e99aa0 -> eager_deletion0x55b7d0faad50  via __control_var@15960x55b7d0d48cf0
I1018 08:53:57.792847 4097533 graph_helper.h:104] adj conv2d0x55b7d111bde0 -> cinn_launch0x55b7d0dcf300  via conv2d_5.tmp_00x55b7d0dcf030
I1018 08:53:57.792850 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> conv2d0x55b7d111bde0  via relu_4.tmp_00x55b7d111b9f0
I1018 08:53:57.792852 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d0c640b0  via relu_2.tmp_00x55b7d0c74b50
I1018 08:53:57.792855 4097533 graph_helper.h:104] adj conv2d0x55b7d0b70160 -> cinn_launch0x55b7d0c640b0  via conv2d_4.tmp_00x55b7d0c63de0
I1018 08:53:57.792857 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> conv2d0x55b7d0b70160  via relu_3.tmp_00x55b7d0b6fd70
I1018 08:53:57.792860 4097533 graph_helper.h:104] adj conv2d0x55b7d0c74ec0 -> cinn_launch0x55b7d0c9d5e0  via conv2d_3.tmp_00x55b7d0e7f460
I1018 08:53:57.792861 4097533 graph_helper.h:104] adj conv2d0x55b7d1029e80 -> cinn_launch0x55b7d0cb5e70  via conv2d_10.tmp_00x55b7d0cb8f30
I1018 08:53:57.792865 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> conv2d0x55b7d0c74ec0  via relu_2.tmp_00x55b7d0c74b50
I1018 08:53:57.792866 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1041280 -> eager_deletion0x55b7d0caa5c0  via __control_var@16390x55b7d0c8ed20
I1018 08:53:57.792869 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdff90  via __control_var@15050x55b7d101d7a0
I1018 08:53:57.792871 4097533 graph_helper.h:104] adj conv2d0x55b7d0edeb50 -> cinn_launch0x55b7d0cfb320  via conv2d_2.tmp_00x55b7d0cfb110
I1018 08:53:57.792874 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d0cfb320  via pool2d_0.tmp_00x55b7d0b929b0
I1018 08:53:57.792876 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> conv2d0x55b7d0edeb50  via relu_1.tmp_00x55b7d0de1f20
I1018 08:53:57.792881 4097533 graph_helper.h:104] adj conv2d0x55b7d0ddf060 -> cinn_launch0x55b7d0c892a0  via conv2d_1.tmp_00x55b7d0c89030
I1018 08:53:57.792883 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100ffb0 -> eager_deletion0x55b7d0f91490  via __control_var@15450x55b7d0dbab10
I1018 08:53:57.792886 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> conv2d0x55b7d0ddf060  via pool2d_0.tmp_00x55b7d0b929b0
I1018 08:53:57.792888 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> conv2d0x55b7d10908e0  via relu_14.tmp_00x55b7d10900a0
I1018 08:53:57.792891 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> cinn_launch0x55b7d0fff800  via __control_var@15100x55b7d101e570
I1018 08:53:57.792893 4097533 graph_helper.h:104] adj conv2d0x55b7d0edf1d0 -> cinn_launch0x55b7d0fac1b0  via conv2d_0.tmp_00x55b7d0eaa280
I1018 08:53:57.793038 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103d9e0 -> eager_deletion0x55b7d0bf8d60  via __control_var@16490x55b7d0be5660
I1018 08:53:57.793041 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> eager_deletion0x55b7d0c2da20  via __control_var@14760x55b7d10431b0
I1018 08:53:57.793043 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> eager_deletion0x55b7d0c7f270  via __control_var@14740x55b7d1042b70
I1018 08:53:57.793046 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> eager_deletion0x55b7d0d36320  via __control_var@14960x55b7d101bd70
I1018 08:53:57.793048 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1013850 -> eager_deletion0x55b7d0d50360  via __control_var@16280x55b7d0d50420
I1018 08:53:57.793051 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> eager_deletion0x55b7d0d83e40  via __control_var@14910x55b7d101af00
I1018 08:53:57.793052 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103ffa0 -> eager_deletion0x55b7d0da05e0  via __control_var@16230x55b7d0da06a0
I1018 08:53:57.793056 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> eager_deletion0x55b7d0dd5690  via __control_var@14780x55b7d1043710
I1018 08:53:57.793057 4097533 graph_helper.h:104] adj conv2d0x55b7d1029e80 -> eager_deletion0x55b7d0d04df0  via __control_var@16330x55b7d0d04eb0
I1018 08:53:57.793061 4097533 graph_helper.h:104] adj cinn_launch0x55b7d10059f0 -> eager_deletion0x55b7d0ec5bb0  via __control_var@16130x55b7d0c239b0
I1018 08:53:57.793062 4097533 graph_helper.h:104] adj conv2d0x55b7d11091c0 -> eager_deletion0x55b7d0e31dd0  via __control_var@16010x55b7d0e680a0
I1018 08:53:57.793064 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> eager_deletion0x55b7d0dac250  via __control_var@15070x55b7d101dd40
I1018 08:53:57.793067 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> eager_deletion0x55b7d0d20510  via __control_var@14900x55b7d101ac70
I1018 08:53:57.793069 4097533 graph_helper.h:104] adj conv2d0x55b7d1122f60 -> eager_deletion0x55b7d0d2e9e0  via __control_var@15860x55b7d0d11c80
I1018 08:53:57.793071 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> eager_deletion0x55b7d0ccaa80  via __control_var@14930x55b7d101b4e0
I1018 08:53:57.793074 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> eager_deletion0x55b7d0c8c100  via __control_var@14820x55b7d1044290
I1018 08:53:57.793076 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> eager_deletion0x55b7d0c82340  via __control_var@14730x55b7d1042880
I1018 08:53:57.793079 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> eager_deletion0x55b7d0be1970  via __control_var@14720x55b7d1042590
I1018 08:53:57.793081 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> eager_deletion0x55b7d0e7c680  via __control_var@14810x55b7d1043fa0
I1018 08:53:57.793083 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> eager_deletion0x55b7d0c100f0  via __control_var@14920x55b7d101b1f0
I1018 08:53:57.793085 4097533 graph_helper.h:104] adj conv2d0x55b7d106f840 -> eager_deletion0x55b7d0fc1590  via __control_var@15700x55b7d0bff1e0
I1018 08:53:57.793088 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> eager_deletion0x55b7d0c74830  via __control_var@14770x55b7d1043480
I1018 08:53:57.793093 4097533 graph_helper.h:104] adj conv2d0x55b7d0fb56d0 -> eager_deletion0x55b7d0ebbb40  via __control_var@15650x55b7d0a9d670
I1018 08:53:57.793097 4097533 graph_helper.h:104] adj conv2d0x55b7d111bde0 -> eager_deletion0x55b7d1106060  via __control_var@15620x55b7d0b734a0
I1018 08:53:57.793098 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100c710 -> eager_deletion0x55b7d0aeffc0  via __control_var@15590x55b7d0afe850
I1018 08:53:57.793102 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1038e60 -> eager_deletion0x55b7d0aa9140  via __control_var@15560x55b7d0e966c0
I1018 08:53:57.793103 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> eager_deletion0x55b7d0a72230  via __control_var@14830x55b7d1044580
I1018 08:53:57.793105 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103a140 -> eager_deletion0x55b7d0a639a0  via __control_var@15510x55b7d0dbb500
I1018 08:53:57.793108 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1012570 -> eager_deletion0x55b7d0a1f3e0  via __control_var@15480x55b7d0995060
I1018 08:53:57.793110 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1004710 -> eager_deletion0x55b7d09a1410  via __control_var@15420x55b7d09da090
I1018 08:53:57.793112 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1003430 -> eager_deletion0x55b7d0abb300  via __control_var@15390x55b7d0990130
I1018 08:53:57.793115 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100d9f0 -> eager_deletion0x55b7d0c57440  via __control_var@15300x55b7d0affe60
I1018 08:53:57.793118 4097533 graph_helper.h:104] adj conv2d0x55b7d0b70160 -> eager_deletion0x55b7d0a73840  via __control_var@15270x55b7d0e77f40
I1018 08:53:57.793120 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100ecd0 -> eager_deletion0x55b7d0c96340  via __control_var@15240x55b7d1033be0
I1018 08:53:57.793123 4097533 graph_helper.h:104] adj conv2d0x55b7d0edf1d0 -> eager_deletion0x55b7d0c57d60  via __control_var@15210x55b7d0b86300
I1018 08:53:57.793124 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> eager_deletion0x55b7cd159bc0  via __control_var@14990x55b7d101c600
I1018 08:53:57.793126 4097533 graph_helper.h:104] adj conv2d0x55b7d0d441e0 -> eager_deletion0x55b7d09765a0  via __control_var@15130x55b7d096fc60
I1018 08:53:57.793129 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103ecc0 -> eager_deletion0x55b7b78d2db0  via __control_var@15180x55b7b7024640
I1018 08:53:57.793131 4097533 graph_helper.h:104] adj conv2d0x55b7d0edeb50 -> eager_deletion0x55b7d0cdcfc0  via __control_var@15360x55b7d0dad860
I1018 08:53:57.793135 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> cinn_launch0x55b7d1002020  via __control_var@14800x55b7d1043cd0
I1018 08:53:57.793138 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> cinn_launch0x55b7d1000c10  via __control_var@14720x55b7d1042590
I1018 08:53:57.793140 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d0ffcfe0  via __control_var@14740x55b7d1042b70
I1018 08:53:57.793143 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d0ffbc30  via __control_var@14750x55b7d1042e40
I1018 08:53:57.793145 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d1068fa0  via __control_var@15090x55b7d101e2a0
I1018 08:53:57.793148 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d1067b90  via __control_var@15010x55b7d101cbc0
I1018 08:53:57.793150 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> cinn_launch0x55b7d1066780  via __control_var@14980x55b7d101c330
I1018 08:53:57.793152 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> cinn_launch0x55b7d1065370  via __control_var@14900x55b7d101ac70
I1018 08:53:57.793154 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> cinn_launch0x55b7d1063f60  via __control_var@15070x55b7d101dd40
I1018 08:53:57.793157 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> cinn_launch0x55b7d1061740  via __control_var@15080x55b7d101dfd0
I1018 08:53:57.793159 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> cinn_launch0x55b7d0fe13a0  via __control_var@14920x55b7d101b1f0
I1018 08:53:57.793164 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdeb80  via __control_var@15030x55b7d101d160
I1018 08:53:57.793166 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdd770  via __control_var@14850x55b7d1044b40
I1018 08:53:57.793169 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdc360  via __control_var@15040x55b7d101d4d0
I1018 08:53:57.793171 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> cinn_launch0x55b7d0fdaf50  via __control_var@14970x55b7d101c060
I1018 08:53:57.793174 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> cinn_launch0x55b7d0fd9b40  via __control_var@14930x55b7d101b4e0
I1018 08:53:57.793175 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> cinn_launch0x55b7d0fd8730  via __control_var@14820x55b7d1044290
I1018 08:53:57.793179 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> cinn_launch0x55b7d1104d40  via __control_var@14840x55b7d1044870
I1018 08:53:57.793180 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> cinn_launch0x55b7d1103930  via __control_var@15000x55b7d101c8f0
I1018 08:53:57.793182 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> cinn_launch0x55b7d1102520  via __control_var@14990x55b7d101c600
I1018 08:53:57.793185 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d1101110  via __control_var@14880x55b7d10453e0
I1018 08:53:57.793187 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10ffd00  via __control_var@14890x55b7d10456b0
I1018 08:53:57.793190 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10fe8f0  via __control_var@14860x55b7d1044e10
I1018 08:53:57.793191 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10fd4e0  via __control_var@14760x55b7d10431b0
I1018 08:53:57.793195 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> cinn_launch0x55b7d1113ec0  via __control_var@14790x55b7d1043a00
I1018 08:53:57.793196 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> cinn_launch0x55b7d1112ab0  via __control_var@14780x55b7d1043710
I1018 08:53:57.793200 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> cinn_launch0x55b7d11116a0  via __control_var@14830x55b7d1044580
I1018 08:53:57.793201 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> cinn_launch0x55b7d1110290  via __control_var@14940x55b7d101b7d0
I1018 08:53:57.793203 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> cinn_launch0x55b7d110ee80  via __control_var@15060x55b7d101da70
I1018 08:53:57.793211 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> cinn_launch0x55b7d110da70  via __control_var@14910x55b7d101af00
I1018 08:53:57.793215 4097533 graph_helper.h:104] adj conv2d0x55b7d1119e20 -> eager_deletion0x55b7d0e76930  via __control_var@16040x55b7d0e8a420
I1018 08:53:57.793216 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d110c6a0  via __control_var@14960x55b7d101bd70
I1018 08:53:57.793218 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d107f940  via __control_var@15020x55b7d101ce90
I1018 08:53:57.793221 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> cinn_launch0x55b7d107e530  via __control_var@14770x55b7d1043480
I1018 08:53:57.793223 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> cinn_launch0x55b7d107d120  via __control_var@14950x55b7d101baa0
I1018 08:53:57.793226 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d107bd10  via __control_var@14870x55b7d1045110
I1018 08:53:57.793228 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d107a890  via __control_var@14810x55b7d1043fa0
I1018 08:53:57.793231 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1011290 -> eager_deletion0x55b7d0bca520  via __control_var@15330x55b7d0c11700
I1018 08:53:57.793233 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100b430 -> eager_deletion0x55b7d0ccd6a0  via __control_var@16360x55b7d0cbdbf0
I1018 08:53:57.793237 4097533 graph_helper.h:104] adj conv2d0x55b7d106f840 -> cinn_launch0x55b7d109c290  via conv2d_19.tmp_00x55b7d109bfc0
I1018 08:53:57.793239 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d109c290  via relu_14.tmp_00x55b7d10900a0
I1018 08:53:57.793242 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> conv2d0x55b7d106f840  via relu_15.tmp_00x55b7d106f450
I1018 08:53:57.793244 4097533 graph_helper.h:104] adj conv2d0x55b7d10908e0 -> cinn_launch0x55b7d106aaf0  via conv2d_18.tmp_00x55b7d106a820
I1018 08:53:57.793246 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103c700 -> eager_deletion0x55b7d0e11740  via __control_var@16180x55b7d0de5140
I1018 08:53:57.793249 4097533 graph_helper.h:104] adj conv2d0x55b7d0fc2680 -> cinn_launch0x55b7d1126750  via conv2d_17.tmp_00x55b7d11264a0
I1018 08:53:57.793251 4097533 graph_helper.h:104] adj conv2d0x55b7d0fb56d0 -> cinn_launch0x55b7d1126750  via conv2d_16.tmp_00x55b7d0fc23b0
I1018 08:53:57.793254 4097533 graph_helper.h:104] adj conv2d0x55b7d0fc2680 -> eager_deletion0x55b7d0a126c0  via __control_var@15910x55b7d0d676f0
I1018 08:53:57.793256 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> conv2d0x55b7d0fc2680  via relu_12.tmp_00x55b7d1122b70
I1018 08:53:57.793259 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> eager_deletion0x55b7d0e04390  via __control_var@14850x55b7d1044b40
I1018 08:53:57.793262 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> cinn_launch0x55b7d1062b50  via __control_var@15110x55b7d101e840
I1018 08:53:57.793264 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> conv2d0x55b7d0fb56d0  via relu_13.tmp_00x55b7d0fb52e0
I1018 08:53:57.793267 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1014b30 -> eager_deletion0x55b7d0c4a1c0  via __control_var@16440x55b7d0c4a280
I1018 08:53:57.793268 4097533 graph_helper.h:104] adj conv2d0x55b7d1122f60 -> cinn_launch0x55b7d0fd6ee0  via conv2d_15.tmp_00x55b7d0fd6c10
I1018 08:53:57.793272 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> conv2d0x55b7d1122f60  via relu_12.tmp_00x55b7d1122b70
I1018 08:53:57.793273 4097533 graph_helper.h:104] adj conv2d0x55b7d0d89b40 -> eager_deletion0x55b7d0ebf660  via __control_var@16070x55b7d0dd21c0
I1018 08:53:57.793277 4097533 graph_helper.h:104] adj conv2d0x55b7d11091c0 -> cinn_launch0x55b7d0fea800  via conv2d_14.tmp_00x55b7d0fea530
I1018 08:53:57.793278 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fea800  via relu_10.tmp_00x55b7d102b4b0
I1018 08:53:57.793280 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> conv2d0x55b7d11091c0  via relu_11.tmp_00x55b7d1108dd0
I1018 08:53:57.793283 4097533 graph_helper.h:104] adj conv2d0x55b7d102bc70 -> cinn_launch0x55b7d1115560  via conv2d_13.tmp_00x55b7d11184f0
I1018 08:53:57.793285 4097533 graph_helper.h:104] adj conv2d0x55b7d0ec1350 -> eager_deletion0x55b7d0c30050  via __control_var@16100x55b7d0eacc00
I1018 08:53:57.793287 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> conv2d0x55b7d102bc70  via relu_10.tmp_00x55b7d102b4b0
I1018 08:53:57.793290 4097533 graph_helper.h:104] adj conv2d0x55b7d0d441e0 -> cinn_launch0x55b7d1031e80  via conv2d_12.tmp_00x55b7d1031bd0
I1018 08:53:57.793292 4097533 graph_helper.h:104] adj conv2d0x55b7d0e99aa0 -> cinn_launch0x55b7d1031e80  via conv2d_11.tmp_00x55b7d0d43f10
I1018 08:53:57.793295 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> conv2d0x55b7d0d441e0  via relu_8.tmp_00x55b7d1029a90
I1018 08:53:57.793298 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> conv2d0x55b7d0e99aa0  via relu_9.tmp_00x55b7d0e996b0
I1018 08:53:57.793299 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103b420 -> eager_deletion0x55b7d0cdb9b0  via __control_var@15830x55b7d0e5ec50
I1018 08:53:57.793303 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> conv2d0x55b7d1029e80  via relu_8.tmp_00x55b7d1029a90
I1018 08:53:57.793304 4097533 graph_helper.h:104] adj conv2d0x55b7d0ec1350 -> cinn_launch0x55b7d1047f30  via conv2d_9.tmp_00x55b7d1047c60
I1018 08:53:57.793311 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d1047f30  via relu_6.tmp_00x55b7d0e88b80
I1018 08:53:57.793313 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> conv2d0x55b7d0ec1350  via relu_7.tmp_00x55b7d0e0f4e0
I1018 08:53:57.793316 4097533 graph_helper.h:104] adj conv2d0x55b7d0e89340 -> cinn_launch0x55b7d0f0e1c0  via conv2d_8.tmp_00x55b7d0f0def0
I1018 08:53:57.793318 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> conv2d0x55b7d0e89340  via relu_6.tmp_00x55b7d0e88b80
I1018 08:53:57.793320 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> cinn_launch0x55b7d0ffe3f0  via __control_var@14730x55b7d1042880
I1018 08:53:57.793323 4097533 graph_helper.h:104] adj conv2d0x55b7d1119e20 -> cinn_launch0x55b7d0add8b0  via conv2d_7.tmp_00x55b7d0add600
I1018 08:53:57.793325 4097533 graph_helper.h:104] adj conv2d0x55b7d0d89b40 -> cinn_launch0x55b7d0add8b0  via conv2d_6.tmp_00x55b7d1119b50
I1018 08:53:57.793327 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> conv2d0x55b7d1119e20  via relu_4.tmp_00x55b7d111b9f0
I1018 08:53:57.793330 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> conv2d0x55b7d0d89b40  via relu_5.tmp_00x55b7d0d89750
I1018 08:53:57.793332 4097533 graph_helper.h:104] adj conv2d0x55b7d0e99aa0 -> eager_deletion0x55b7d0faad50  via __control_var@15960x55b7d0d48cf0
I1018 08:53:57.793335 4097533 graph_helper.h:104] adj conv2d0x55b7d111bde0 -> cinn_launch0x55b7d0dcf300  via conv2d_5.tmp_00x55b7d0dcf030
I1018 08:53:57.793337 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> conv2d0x55b7d111bde0  via relu_4.tmp_00x55b7d111b9f0
I1018 08:53:57.793339 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d0c640b0  via relu_2.tmp_00x55b7d0c74b50
I1018 08:53:57.793342 4097533 graph_helper.h:104] adj conv2d0x55b7d0b70160 -> cinn_launch0x55b7d0c640b0  via conv2d_4.tmp_00x55b7d0c63de0
I1018 08:53:57.793344 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> conv2d0x55b7d0b70160  via relu_3.tmp_00x55b7d0b6fd70
I1018 08:53:57.793347 4097533 graph_helper.h:104] adj conv2d0x55b7d0c74ec0 -> cinn_launch0x55b7d0c9d5e0  via conv2d_3.tmp_00x55b7d0e7f460
I1018 08:53:57.793349 4097533 graph_helper.h:104] adj conv2d0x55b7d1029e80 -> cinn_launch0x55b7d0cb5e70  via conv2d_10.tmp_00x55b7d0cb8f30
I1018 08:53:57.793351 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> conv2d0x55b7d0c74ec0  via relu_2.tmp_00x55b7d0c74b50
I1018 08:53:57.793354 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1041280 -> eager_deletion0x55b7d0caa5c0  via __control_var@16390x55b7d0c8ed20
I1018 08:53:57.793356 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdff90  via __control_var@15050x55b7d101d7a0
I1018 08:53:57.793359 4097533 graph_helper.h:104] adj conv2d0x55b7d0edeb50 -> cinn_launch0x55b7d0cfb320  via conv2d_2.tmp_00x55b7d0cfb110
I1018 08:53:57.793361 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d0cfb320  via pool2d_0.tmp_00x55b7d0b929b0
I1018 08:53:57.793363 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> conv2d0x55b7d0edeb50  via relu_1.tmp_00x55b7d0de1f20
I1018 08:53:57.793366 4097533 graph_helper.h:104] adj conv2d0x55b7d0ddf060 -> cinn_launch0x55b7d0c892a0  via conv2d_1.tmp_00x55b7d0c89030
I1018 08:53:57.793368 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100ffb0 -> eager_deletion0x55b7d0f91490  via __control_var@15450x55b7d0dbab10
I1018 08:53:57.793371 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> conv2d0x55b7d0ddf060  via pool2d_0.tmp_00x55b7d0b929b0
I1018 08:53:57.793373 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> conv2d0x55b7d10908e0  via relu_14.tmp_00x55b7d10900a0
I1018 08:53:57.793375 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> cinn_launch0x55b7d0fff800  via __control_var@15100x55b7d101e570
I1018 08:53:57.793378 4097533 graph_helper.h:104] adj conv2d0x55b7d0edf1d0 -> cinn_launch0x55b7d0fac1b0  via conv2d_0.tmp_00x55b7d0eaa280
I1018 08:53:57.793522 4097533 parallel_executor.cc:583] Garbage collection strategy is enabled, when FLAGS_eager_delete_tensor_gb = 0
I1018 08:53:57.793689 4097533 parallel_executor.cc:1741] use FastThreadedSSAGraphExecutor
I1018 08:53:57.793766 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103d9e0 -> eager_deletion0x55b7d0bf8d60  via __control_var@16490x55b7d0be5660
I1018 08:53:57.793771 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> eager_deletion0x55b7d0c2da20  via __control_var@14760x55b7d10431b0
I1018 08:53:57.793772 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> eager_deletion0x55b7d0c7f270  via __control_var@14740x55b7d1042b70
I1018 08:53:57.793776 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> eager_deletion0x55b7d0d36320  via __control_var@14960x55b7d101bd70
I1018 08:53:57.793777 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1013850 -> eager_deletion0x55b7d0d50360  via __control_var@16280x55b7d0d50420
I1018 08:53:57.793779 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> eager_deletion0x55b7d0d83e40  via __control_var@14910x55b7d101af00
I1018 08:53:57.793782 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103ffa0 -> eager_deletion0x55b7d0da05e0  via __control_var@16230x55b7d0da06a0
I1018 08:53:57.793784 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> eager_deletion0x55b7d0dd5690  via __control_var@14780x55b7d1043710
I1018 08:53:57.793787 4097533 graph_helper.h:104] adj conv2d0x55b7d1029e80 -> eager_deletion0x55b7d0d04df0  via __control_var@16330x55b7d0d04eb0
I1018 08:53:57.793789 4097533 graph_helper.h:104] adj cinn_launch0x55b7d10059f0 -> eager_deletion0x55b7d0ec5bb0  via __control_var@16130x55b7d0c239b0
I1018 08:53:57.793792 4097533 graph_helper.h:104] adj conv2d0x55b7d11091c0 -> eager_deletion0x55b7d0e31dd0  via __control_var@16010x55b7d0e680a0
I1018 08:53:57.793794 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> eager_deletion0x55b7d0dac250  via __control_var@15070x55b7d101dd40
I1018 08:53:57.793797 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> eager_deletion0x55b7d0d20510  via __control_var@14900x55b7d101ac70
I1018 08:53:57.793799 4097533 graph_helper.h:104] adj conv2d0x55b7d1122f60 -> eager_deletion0x55b7d0d2e9e0  via __control_var@15860x55b7d0d11c80
I1018 08:53:57.793802 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> eager_deletion0x55b7d0ccaa80  via __control_var@14930x55b7d101b4e0
I1018 08:53:57.793803 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> eager_deletion0x55b7d0c8c100  via __control_var@14820x55b7d1044290
I1018 08:53:57.793807 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> eager_deletion0x55b7d0c82340  via __control_var@14730x55b7d1042880
I1018 08:53:57.793808 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> eager_deletion0x55b7d0be1970  via __control_var@14720x55b7d1042590
I1018 08:53:57.793810 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> eager_deletion0x55b7d0e7c680  via __control_var@14810x55b7d1043fa0
I1018 08:53:57.793813 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> eager_deletion0x55b7d0c100f0  via __control_var@14920x55b7d101b1f0
I1018 08:53:57.793815 4097533 graph_helper.h:104] adj conv2d0x55b7d106f840 -> eager_deletion0x55b7d0fc1590  via __control_var@15700x55b7d0bff1e0
I1018 08:53:57.793817 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> eager_deletion0x55b7d0c74830  via __control_var@14770x55b7d1043480
I1018 08:53:57.793820 4097533 graph_helper.h:104] adj conv2d0x55b7d0fb56d0 -> eager_deletion0x55b7d0ebbb40  via __control_var@15650x55b7d0a9d670
I1018 08:53:57.793823 4097533 graph_helper.h:104] adj conv2d0x55b7d111bde0 -> eager_deletion0x55b7d1106060  via __control_var@15620x55b7d0b734a0
I1018 08:53:57.793825 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100c710 -> eager_deletion0x55b7d0aeffc0  via __control_var@15590x55b7d0afe850
I1018 08:53:57.793828 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1038e60 -> eager_deletion0x55b7d0aa9140  via __control_var@15560x55b7d0e966c0
I1018 08:53:57.793833 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> eager_deletion0x55b7d0a72230  via __control_var@14830x55b7d1044580
I1018 08:53:57.793835 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103a140 -> eager_deletion0x55b7d0a639a0  via __control_var@15510x55b7d0dbb500
I1018 08:53:57.793838 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1012570 -> eager_deletion0x55b7d0a1f3e0  via __control_var@15480x55b7d0995060
I1018 08:53:57.793840 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1004710 -> eager_deletion0x55b7d09a1410  via __control_var@15420x55b7d09da090
I1018 08:53:57.793843 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1003430 -> eager_deletion0x55b7d0abb300  via __control_var@15390x55b7d0990130
I1018 08:53:57.793845 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100d9f0 -> eager_deletion0x55b7d0c57440  via __control_var@15300x55b7d0affe60
I1018 08:53:57.793848 4097533 graph_helper.h:104] adj conv2d0x55b7d0b70160 -> eager_deletion0x55b7d0a73840  via __control_var@15270x55b7d0e77f40
I1018 08:53:57.793849 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100ecd0 -> eager_deletion0x55b7d0c96340  via __control_var@15240x55b7d1033be0
I1018 08:53:57.793852 4097533 graph_helper.h:104] adj conv2d0x55b7d0edf1d0 -> eager_deletion0x55b7d0c57d60  via __control_var@15210x55b7d0b86300
I1018 08:53:57.793854 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> eager_deletion0x55b7cd159bc0  via __control_var@14990x55b7d101c600
I1018 08:53:57.793857 4097533 graph_helper.h:104] adj conv2d0x55b7d0d441e0 -> eager_deletion0x55b7d09765a0  via __control_var@15130x55b7d096fc60
I1018 08:53:57.793859 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103ecc0 -> eager_deletion0x55b7b78d2db0  via __control_var@15180x55b7b7024640
I1018 08:53:57.793861 4097533 graph_helper.h:104] adj conv2d0x55b7d0edeb50 -> eager_deletion0x55b7d0cdcfc0  via __control_var@15360x55b7d0dad860
I1018 08:53:57.793866 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> cinn_launch0x55b7d1002020  via __control_var@14800x55b7d1043cd0
I1018 08:53:57.793869 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> cinn_launch0x55b7d1000c10  via __control_var@14720x55b7d1042590
I1018 08:53:57.793871 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d0ffcfe0  via __control_var@14740x55b7d1042b70
I1018 08:53:57.793874 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d0ffbc30  via __control_var@14750x55b7d1042e40
I1018 08:53:57.793876 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d1068fa0  via __control_var@15090x55b7d101e2a0
I1018 08:53:57.793879 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d1067b90  via __control_var@15010x55b7d101cbc0
I1018 08:53:57.793880 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> cinn_launch0x55b7d1066780  via __control_var@14980x55b7d101c330
I1018 08:53:57.793884 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> cinn_launch0x55b7d1065370  via __control_var@14900x55b7d101ac70
I1018 08:53:57.793885 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> cinn_launch0x55b7d1063f60  via __control_var@15070x55b7d101dd40
I1018 08:53:57.793888 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> cinn_launch0x55b7d1061740  via __control_var@15080x55b7d101dfd0
I1018 08:53:57.793890 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> cinn_launch0x55b7d0fe13a0  via __control_var@14920x55b7d101b1f0
I1018 08:53:57.793893 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdeb80  via __control_var@15030x55b7d101d160
I1018 08:53:57.793895 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdd770  via __control_var@14850x55b7d1044b40
I1018 08:53:57.793897 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdc360  via __control_var@15040x55b7d101d4d0
I1018 08:53:57.793900 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> cinn_launch0x55b7d0fdaf50  via __control_var@14970x55b7d101c060
I1018 08:53:57.793905 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> cinn_launch0x55b7d0fd9b40  via __control_var@14930x55b7d101b4e0
I1018 08:53:57.793907 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> cinn_launch0x55b7d0fd8730  via __control_var@14820x55b7d1044290
I1018 08:53:57.793910 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> cinn_launch0x55b7d1104d40  via __control_var@14840x55b7d1044870
I1018 08:53:57.793911 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> cinn_launch0x55b7d1103930  via __control_var@15000x55b7d101c8f0
I1018 08:53:57.793913 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> cinn_launch0x55b7d1102520  via __control_var@14990x55b7d101c600
I1018 08:53:57.793916 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d1101110  via __control_var@14880x55b7d10453e0
I1018 08:53:57.793918 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10ffd00  via __control_var@14890x55b7d10456b0
I1018 08:53:57.793920 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10fe8f0  via __control_var@14860x55b7d1044e10
I1018 08:53:57.793923 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10fd4e0  via __control_var@14760x55b7d10431b0
I1018 08:53:57.793926 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> cinn_launch0x55b7d1113ec0  via __control_var@14790x55b7d1043a00
I1018 08:53:57.793927 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> cinn_launch0x55b7d1112ab0  via __control_var@14780x55b7d1043710
I1018 08:53:57.793931 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> cinn_launch0x55b7d11116a0  via __control_var@14830x55b7d1044580
I1018 08:53:57.793932 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> cinn_launch0x55b7d1110290  via __control_var@14940x55b7d101b7d0
I1018 08:53:57.793934 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> cinn_launch0x55b7d110ee80  via __control_var@15060x55b7d101da70
I1018 08:53:57.793937 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> cinn_launch0x55b7d110da70  via __control_var@14910x55b7d101af00
I1018 08:53:57.793939 4097533 graph_helper.h:104] adj conv2d0x55b7d1119e20 -> eager_deletion0x55b7d0e76930  via __control_var@16040x55b7d0e8a420
I1018 08:53:57.793941 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d110c6a0  via __control_var@14960x55b7d101bd70
I1018 08:53:57.793944 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d107f940  via __control_var@15020x55b7d101ce90
I1018 08:53:57.793946 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> cinn_launch0x55b7d107e530  via __control_var@14770x55b7d1043480
I1018 08:53:57.793948 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> cinn_launch0x55b7d107d120  via __control_var@14950x55b7d101baa0
I1018 08:53:57.793951 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d107bd10  via __control_var@14870x55b7d1045110
I1018 08:53:57.793953 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d107a890  via __control_var@14810x55b7d1043fa0
I1018 08:53:57.793955 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1011290 -> eager_deletion0x55b7d0bca520  via __control_var@15330x55b7d0c11700
I1018 08:53:57.793958 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100b430 -> eager_deletion0x55b7d0ccd6a0  via __control_var@16360x55b7d0cbdbf0
I1018 08:53:57.793960 4097533 graph_helper.h:104] adj conv2d0x55b7d106f840 -> cinn_launch0x55b7d109c290  via conv2d_19.tmp_00x55b7d109bfc0
I1018 08:53:57.793963 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d109c290  via relu_14.tmp_00x55b7d10900a0
I1018 08:53:57.793965 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> conv2d0x55b7d106f840  via relu_15.tmp_00x55b7d106f450
I1018 08:53:57.793967 4097533 graph_helper.h:104] adj conv2d0x55b7d10908e0 -> cinn_launch0x55b7d106aaf0  via conv2d_18.tmp_00x55b7d106a820
I1018 08:53:57.793972 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103c700 -> eager_deletion0x55b7d0e11740  via __control_var@16180x55b7d0de5140
I1018 08:53:57.793974 4097533 graph_helper.h:104] adj conv2d0x55b7d0fc2680 -> cinn_launch0x55b7d1126750  via conv2d_17.tmp_00x55b7d11264a0
I1018 08:53:57.793977 4097533 graph_helper.h:104] adj conv2d0x55b7d0fb56d0 -> cinn_launch0x55b7d1126750  via conv2d_16.tmp_00x55b7d0fc23b0
I1018 08:53:57.793979 4097533 graph_helper.h:104] adj conv2d0x55b7d0fc2680 -> eager_deletion0x55b7d0a126c0  via __control_var@15910x55b7d0d676f0
I1018 08:53:57.793982 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> conv2d0x55b7d0fc2680  via relu_12.tmp_00x55b7d1122b70
I1018 08:53:57.793984 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> eager_deletion0x55b7d0e04390  via __control_var@14850x55b7d1044b40
I1018 08:53:57.793987 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> cinn_launch0x55b7d1062b50  via __control_var@15110x55b7d101e840
I1018 08:53:57.793989 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> conv2d0x55b7d0fb56d0  via relu_13.tmp_00x55b7d0fb52e0
I1018 08:53:57.793991 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1014b30 -> eager_deletion0x55b7d0c4a1c0  via __control_var@16440x55b7d0c4a280
I1018 08:53:57.793994 4097533 graph_helper.h:104] adj conv2d0x55b7d1122f60 -> cinn_launch0x55b7d0fd6ee0  via conv2d_15.tmp_00x55b7d0fd6c10
I1018 08:53:57.793996 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> conv2d0x55b7d1122f60  via relu_12.tmp_00x55b7d1122b70
I1018 08:53:57.793998 4097533 graph_helper.h:104] adj conv2d0x55b7d0d89b40 -> eager_deletion0x55b7d0ebf660  via __control_var@16070x55b7d0dd21c0
I1018 08:53:57.794001 4097533 graph_helper.h:104] adj conv2d0x55b7d11091c0 -> cinn_launch0x55b7d0fea800  via conv2d_14.tmp_00x55b7d0fea530
I1018 08:53:57.794003 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fea800  via relu_10.tmp_00x55b7d102b4b0
I1018 08:53:57.794006 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> conv2d0x55b7d11091c0  via relu_11.tmp_00x55b7d1108dd0
I1018 08:53:57.794008 4097533 graph_helper.h:104] adj conv2d0x55b7d102bc70 -> cinn_launch0x55b7d1115560  via conv2d_13.tmp_00x55b7d11184f0
I1018 08:53:57.794010 4097533 graph_helper.h:104] adj conv2d0x55b7d0ec1350 -> eager_deletion0x55b7d0c30050  via __control_var@16100x55b7d0eacc00
I1018 08:53:57.794013 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> conv2d0x55b7d102bc70  via relu_10.tmp_00x55b7d102b4b0
I1018 08:53:57.794015 4097533 graph_helper.h:104] adj conv2d0x55b7d0d441e0 -> cinn_launch0x55b7d1031e80  via conv2d_12.tmp_00x55b7d1031bd0
I1018 08:53:57.794018 4097533 graph_helper.h:104] adj conv2d0x55b7d0e99aa0 -> cinn_launch0x55b7d1031e80  via conv2d_11.tmp_00x55b7d0d43f10
I1018 08:53:57.794020 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> conv2d0x55b7d0d441e0  via relu_8.tmp_00x55b7d1029a90
I1018 08:53:57.794023 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> conv2d0x55b7d0e99aa0  via relu_9.tmp_00x55b7d0e996b0
I1018 08:53:57.794025 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103b420 -> eager_deletion0x55b7d0cdb9b0  via __control_var@15830x55b7d0e5ec50
I1018 08:53:57.794027 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> conv2d0x55b7d1029e80  via relu_8.tmp_00x55b7d1029a90
I1018 08:53:57.794029 4097533 graph_helper.h:104] adj conv2d0x55b7d0ec1350 -> cinn_launch0x55b7d1047f30  via conv2d_9.tmp_00x55b7d1047c60
I1018 08:53:57.794031 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d1047f30  via relu_6.tmp_00x55b7d0e88b80
I1018 08:53:57.794034 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> conv2d0x55b7d0ec1350  via relu_7.tmp_00x55b7d0e0f4e0
I1018 08:53:57.794036 4097533 graph_helper.h:104] adj conv2d0x55b7d0e89340 -> cinn_launch0x55b7d0f0e1c0  via conv2d_8.tmp_00x55b7d0f0def0
I1018 08:53:57.794039 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> conv2d0x55b7d0e89340  via relu_6.tmp_00x55b7d0e88b80
I1018 08:53:57.794044 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> cinn_launch0x55b7d0ffe3f0  via __control_var@14730x55b7d1042880
I1018 08:53:57.794046 4097533 graph_helper.h:104] adj conv2d0x55b7d1119e20 -> cinn_launch0x55b7d0add8b0  via conv2d_7.tmp_00x55b7d0add600
I1018 08:53:57.794049 4097533 graph_helper.h:104] adj conv2d0x55b7d0d89b40 -> cinn_launch0x55b7d0add8b0  via conv2d_6.tmp_00x55b7d1119b50
I1018 08:53:57.794050 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> conv2d0x55b7d1119e20  via relu_4.tmp_00x55b7d111b9f0
I1018 08:53:57.794052 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> conv2d0x55b7d0d89b40  via relu_5.tmp_00x55b7d0d89750
I1018 08:53:57.794055 4097533 graph_helper.h:104] adj conv2d0x55b7d0e99aa0 -> eager_deletion0x55b7d0faad50  via __control_var@15960x55b7d0d48cf0
I1018 08:53:57.794057 4097533 graph_helper.h:104] adj conv2d0x55b7d111bde0 -> cinn_launch0x55b7d0dcf300  via conv2d_5.tmp_00x55b7d0dcf030
I1018 08:53:57.794060 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> conv2d0x55b7d111bde0  via relu_4.tmp_00x55b7d111b9f0
I1018 08:53:57.794062 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d0c640b0  via relu_2.tmp_00x55b7d0c74b50
I1018 08:53:57.794064 4097533 graph_helper.h:104] adj conv2d0x55b7d0b70160 -> cinn_launch0x55b7d0c640b0  via conv2d_4.tmp_00x55b7d0c63de0
I1018 08:53:57.794067 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> conv2d0x55b7d0b70160  via relu_3.tmp_00x55b7d0b6fd70
I1018 08:53:57.794070 4097533 graph_helper.h:104] adj conv2d0x55b7d0c74ec0 -> cinn_launch0x55b7d0c9d5e0  via conv2d_3.tmp_00x55b7d0e7f460
I1018 08:53:57.794071 4097533 graph_helper.h:104] adj conv2d0x55b7d1029e80 -> cinn_launch0x55b7d0cb5e70  via conv2d_10.tmp_00x55b7d0cb8f30
I1018 08:53:57.794075 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> conv2d0x55b7d0c74ec0  via relu_2.tmp_00x55b7d0c74b50
I1018 08:53:57.794076 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1041280 -> eager_deletion0x55b7d0caa5c0  via __control_var@16390x55b7d0c8ed20
I1018 08:53:57.794080 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdff90  via __control_var@15050x55b7d101d7a0
I1018 08:53:57.794081 4097533 graph_helper.h:104] adj conv2d0x55b7d0edeb50 -> cinn_launch0x55b7d0cfb320  via conv2d_2.tmp_00x55b7d0cfb110
I1018 08:53:57.794083 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d0cfb320  via pool2d_0.tmp_00x55b7d0b929b0
I1018 08:53:57.794086 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> conv2d0x55b7d0edeb50  via relu_1.tmp_00x55b7d0de1f20
I1018 08:53:57.794088 4097533 graph_helper.h:104] adj conv2d0x55b7d0ddf060 -> cinn_launch0x55b7d0c892a0  via conv2d_1.tmp_00x55b7d0c89030
I1018 08:53:57.794091 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100ffb0 -> eager_deletion0x55b7d0f91490  via __control_var@15450x55b7d0dbab10
I1018 08:53:57.794093 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> conv2d0x55b7d0ddf060  via pool2d_0.tmp_00x55b7d0b929b0
I1018 08:53:57.794095 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> conv2d0x55b7d10908e0  via relu_14.tmp_00x55b7d10900a0
I1018 08:53:57.794098 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> cinn_launch0x55b7d0fff800  via __control_var@15100x55b7d101e570
I1018 08:53:57.794100 4097533 graph_helper.h:104] adj conv2d0x55b7d0edf1d0 -> cinn_launch0x55b7d0fac1b0  via conv2d_0.tmp_00x55b7d0eaa280
I1018 08:53:57.794271 4097533 parallel_executor.cc:729] use ScopeBufferedSSAGraphExecutor
I1018 08:53:57.794281 4097533 scope.cc:203] Create variable assign_72.tmp_0
I1018 08:53:57.794284 4097533 scope.cc:203] Create variable assign_68.tmp_0
I1018 08:53:57.794287 4097533 scope.cc:203] Create variable assign_64.tmp_0
I1018 08:53:57.794289 4097533 scope.cc:203] Create variable assign_60.tmp_0
I1018 08:53:57.794291 4097533 scope.cc:203] Create variable assign_52.tmp_0
I1018 08:53:57.794293 4097533 scope.cc:203] Create variable assign_44.tmp_0
I1018 08:53:57.794298 4097533 scope.cc:203] Create variable assign_40.tmp_0
I1018 08:53:57.794301 4097533 scope.cc:203] Create variable assign_36.tmp_0
I1018 08:53:57.794303 4097533 scope.cc:203] Create variable assign_32.tmp_0
I1018 08:53:57.794306 4097533 scope.cc:203] Create variable assign_28.tmp_0
I1018 08:53:57.794307 4097533 scope.cc:203] Create variable assign_24.tmp_0
I1018 08:53:57.794310 4097533 scope.cc:203] Create variable assign_20.tmp_0
I1018 08:53:57.794312 4097533 scope.cc:203] Create variable assign_16.tmp_0
I1018 08:53:57.794315 4097533 scope.cc:203] Create variable assign_12.tmp_0
I1018 08:53:57.794317 4097533 scope.cc:203] Create variable assign_8.tmp_0
I1018 08:53:57.794322 4097533 scope.cc:203] Create variable batch_norm2d_19.w_2
I1018 08:53:57.794325 4097533 scope.cc:203] Create variable batch_norm2d_18.w_2
I1018 08:53:57.794327 4097533 scope.cc:203] Create variable batch_norm2d_18.w_1
I1018 08:53:57.794330 4097533 scope.cc:203] Create variable batch_norm2d_15.w_2
I1018 08:53:57.794333 4097533 scope.cc:203] Create variable batch_norm2d_15.w_1
I1018 08:53:57.794337 4097533 scope.cc:203] Create variable batch_norm2d_17.w_2
I1018 08:53:57.794339 4097533 scope.cc:203] Create variable batch_norm2d_17.w_1
I1018 08:53:57.794341 4097533 scope.cc:203] Create variable batch_norm2d_16.w_2
I1018 08:53:57.794344 4097533 scope.cc:203] Create variable batch_norm2d_16.w_1
I1018 08:53:57.794346 4097533 scope.cc:203] Create variable batch_norm2d_14.w_2
I1018 08:53:57.794349 4097533 scope.cc:203] Create variable batch_norm2d_14.w_1
I1018 08:53:57.794351 4097533 scope.cc:203] Create variable batch_norm2d_13.w_2
I1018 08:53:57.794354 4097533 scope.cc:203] Create variable batch_norm2d_13.w_1
I1018 08:53:57.794358 4097533 scope.cc:203] Create variable batch_norm2d_10.w_2
I1018 08:53:57.794359 4097533 scope.cc:203] Create variable batch_norm2d_12.w_2
I1018 08:53:57.794363 4097533 scope.cc:203] Create variable batch_norm2d_12.w_1
I1018 08:53:57.794365 4097533 scope.cc:203] Create variable batch_norm2d_9.w_2
I1018 08:53:57.794368 4097533 scope.cc:203] Create variable batch_norm2d_9.w_1
I1018 08:53:57.794370 4097533 scope.cc:203] Create variable batch_norm2d_8.w_2
I1018 08:53:57.794373 4097533 scope.cc:203] Create variable batch_norm2d_8.w_1
I1018 08:53:57.794375 4097533 scope.cc:203] Create variable batch_norm2d_5.w_2
I1018 08:53:57.794377 4097533 scope.cc:203] Create variable batch_norm2d_5.w_1
I1018 08:53:57.794380 4097533 scope.cc:203] Create variable batch_norm2d_7.w_2
I1018 08:53:57.794384 4097533 scope.cc:203] Create variable batch_norm2d_7.w_1
I1018 08:53:57.794385 4097533 scope.cc:203] Create variable batch_norm2d_6.w_2
I1018 08:53:57.794389 4097533 scope.cc:203] Create variable batch_norm2d_6.w_1
I1018 08:53:57.794390 4097533 scope.cc:203] Create variable batch_norm2d_4.w_1
I1018 08:53:57.794394 4097533 scope.cc:203] Create variable batch_norm2d_3.w_2
I1018 08:53:57.794395 4097533 scope.cc:203] Create variable assign_76.tmp_0
I1018 08:53:57.794399 4097533 scope.cc:203] Create variable batch_norm2d_2.w_2
I1018 08:53:57.794401 4097533 scope.cc:203] Create variable batch_norm2d_2.w_1
I1018 08:53:57.794404 4097533 scope.cc:203] Create variable batch_norm2d_1.w_2
I1018 08:53:57.794406 4097533 scope.cc:203] Create variable batch_norm2d_1.w_1
I1018 08:53:57.794409 4097533 scope.cc:203] Create variable batch_norm2d_0.w_2
I1018 08:53:57.794411 4097533 scope.cc:203] Create variable batch_norm2d_0.w_1
I1018 08:53:57.794413 4097533 scope.cc:203] Create variable linear_0.tmp_1
I1018 08:53:57.794416 4097533 scope.cc:203] Create variable assign_77.tmp_0
I1018 08:53:57.794418 4097533 scope.cc:203] Create variable batch_norm2d_19.w_0
I1018 08:53:57.794421 4097533 scope.cc:203] Create variable batch_norm2d_19.w_1
I1018 08:53:57.794423 4097533 scope.cc:203] Create variable linear_0.w_0
I1018 08:53:57.794426 4097533 scope.cc:203] Create variable conv2d_19.tmp_0
I1018 08:53:57.794428 4097533 scope.cc:203] Create variable conv2d_19.w_0
I1018 08:53:57.794431 4097533 scope.cc:203] Create variable relu_15.tmp_0
I1018 08:53:57.794433 4097533 scope.cc:203] Create variable assign_73.tmp_0
I1018 08:53:57.794438 4097533 scope.cc:203] Create variable batch_norm2d_18.w_0
I1018 08:53:57.794440 4097533 scope.cc:203] Create variable batch_norm2d_11.w_1
I1018 08:53:57.794443 4097533 scope_buffered_ssa_graph_executor.cc:207] batch_norm2d_18.w_2 has been initialized beforehand in global scope, skipped
I1018 08:53:57.794446 4097533 scope.cc:203] Create variable conv2d_18.tmp_0
I1018 08:53:57.794448 4097533 scope.cc:203] Create variable conv2d_18.w_0
I1018 08:53:57.794451 4097533 scope.cc:203] Create variable assign_65.tmp_0
I1018 08:53:57.794453 4097533 scope.cc:203] Create variable relu_14.tmp_0
I1018 08:53:57.794456 4097533 scope.cc:203] Create variable assign_69.tmp_0
I1018 08:53:57.794458 4097533 scope.cc:203] Create variable batch_norm2d_17.b_0
I1018 08:53:57.794461 4097533 scope.cc:203] Create variable batch_norm2d_17.w_0
I1018 08:53:57.794463 4097533 scope.cc:203] Create variable batch_norm2d_15.w_0
I1018 08:53:57.794466 4097533 scope_buffered_ssa_graph_executor.cc:207] batch_norm2d_17.w_2 has been initialized beforehand in global scope, skipped
I1018 08:53:57.794468 4097533 scope.cc:203] Create variable linear_0.b_0
I1018 08:53:57.794471 4097533 scope.cc:203] Create variable batch_norm2d_15.b_0
I1018 08:53:57.794472 4097533 scope.cc:203] Create variable conv2d_17.tmp_0
I1018 08:53:57.794476 4097533 scope.cc:203] Create variable conv2d_15.w_0
I1018 08:53:57.794477 4097533 scope.cc:203] Create variable conv2d_16.tmp_0
I1018 08:53:57.794481 4097533 scope.cc:203] Create variable conv2d_17.w_0
I1018 08:53:57.794482 4097533 scope.cc:203] Create variable relu_13.tmp_0
I1018 08:53:57.794485 4097533 scope.cc:203] Create variable batch_norm2d_16.b_0
I1018 08:53:57.794487 4097533 scope_buffered_ssa_graph_executor.cc:207] batch_norm2d_16.w_1 has been initialized beforehand in global scope, skipped
I1018 08:53:57.794489 4097533 scope_buffered_ssa_graph_executor.cc:207] batch_norm2d_16.w_2 has been initialized beforehand in global scope, skipped
I1018 08:53:57.794492 4097533 scope.cc:203] Create variable conv2d_15.tmp_0
I1018 08:53:57.794494 4097533 scope.cc:203] Create variable conv2d_16.w_0
I1018 08:53:57.794497 4097533 scope.cc:203] Create variable relu_12.tmp_0
I1018 08:53:57.794499 4097533 scope_buffered_ssa_graph_executor.cc:207] batch_norm2d_15.w_2 has been initialized beforehand in global scope, skipped
I1018 08:53:57.794502 4097533 scope.cc:203] Create variable assign_57.tmp_0
I1018 08:53:57.794504 4097533 scope_buffered_ssa_graph_executor.cc:207] batch_norm2d_14.w_1 has been initialized beforehand in global scope, skipped
I1018 08:53:57.794507 4097533 scope.cc:203] Create variable batch_norm2d_14.b_0
I1018 08:53:57.794509 4097533 scope.cc:203] Create variable batch_norm2d_14.w_0
I1018 08:53:57.794512 4097533 scope_buffered_ssa_graph_executor.cc:207] batch_norm2d_14.w_2 has been initialized beforehand in global scope, skipped
I1018 08:53:57.794513 4097533 scope.cc:203] Create variable conv2d_14.tmp_0
I1018 08:53:57.794517 4097533 scope.cc:203] Create variable conv2d_14.w_0
I1018 08:53:57.794518 4097533 scope.cc:203] Create variable relu_11.tmp_0
I1018 08:53:57.794520 4097533 scope.cc:203] Create variable assign_53.tmp_0
I1018 08:53:57.794523 4097533 scope.cc:203] Create variable batch_norm2d_13.b_0
I1018 08:53:57.794525 4097533 scope.cc:203] Create variable assign_0.tmp_0
I1018 08:53:57.794528 4097533 scope_buffered_ssa_graph_executor.cc:207] batch_norm2d_19.w_2 has been initialized beforehand in global scope, skipped
I1018 08:53:57.794530 4097533 scope.cc:203] Create variable batch_norm2d_13.w_0
I1018 08:53:57.794533 4097533 scope_buffered_ssa_graph_executor.cc:207] batch_norm2d_17.w_1 has been initialized beforehand in global scope, skipped
I1018 08:53:57.794534 4097533 scope_buffered_ssa_graph_executor.cc:207] batch_norm2d_13.w_1 has been initialized beforehand in global scope, skipped
I1018 08:53:57.794536 4097533 scope_buffered_ssa_graph_executor.cc:207] batch_norm2d_15.w_1 has been initialized beforehand in global scope, skipped
I1018 08:53:57.794539 4097533 scope_buffered_ssa_graph_executor.cc:207] batch_norm2d_13.w_2 has been initialized beforehand in global scope, skipped
I1018 08:53:57.794543 4097533 scope.cc:203] Create variable conv2d_13.tmp_0
I1018 08:53:57.794545 4097533 scope.cc:203] Create variable conv2d_13.w_0
I1018 08:53:57.794548 4097533 scope.cc:203] Create variable relu_10.tmp_0
I1018 08:53:57.794550 4097533 scope.cc:203] Create variable assign_49.tmp_0
I1018 08:53:57.794552 4097533 scope.cc:203] Create variable batch_norm2d_12.b_0
I1018 08:53:57.794555 4097533 scope.cc:203] Create variable batch_norm2d_3.w_1
I1018 08:53:57.794559 4097533 scope.cc:203] Create variable batch_norm2d_12.w_0
I1018 08:53:57.794561 4097533 scope.cc:203] Create variable batch_norm2d_10.w_0
I1018 08:53:57.794564 4097533 scope_buffered_ssa_graph_executor.cc:207] batch_norm2d_12.w_2 has been initialized beforehand in global scope, skipped
I1018 08:53:57.794566 4097533 scope_buffered_ssa_graph_executor.cc:207] batch_norm2d_12.w_1 has been initialized beforehand in global scope, skipped
I1018 08:53:57.794569 4097533 scope.cc:203] Create variable batch_norm2d_10.w_1
I1018 08:53:57.794570 4097533 scope.cc:203] Create variable batch_norm2d_10.b_0
I1018 08:53:57.794574 4097533 scope.cc:203] Create variable conv2d_12.tmp_0
I1018 08:53:57.794575 4097533 scope.cc:203] Create variable conv2d_10.w_0
I1018 08:53:57.794579 4097533 scope_buffered_ssa_graph_executor.cc:207] batch_norm2d_18.w_1 has been initialized beforehand in global scope, skipped
I1018 08:53:57.794580 4097533 scope_buffered_ssa_graph_executor.cc:207] batch_norm2d_19.w_1 has been initialized beforehand in global scope, skipped
I1018 08:53:57.794582 4097533 scope.cc:203] Create variable conv2d_11.tmp_0
I1018 08:53:57.794585 4097533 scope.cc:203] Create variable conv2d_12.w_0
I1018 08:53:57.794587 4097533 scope.cc:203] Create variable relu_9.tmp_0
I1018 08:53:57.794589 4097533 scope.cc:203] Create variable assign_41.tmp_0
I1018 08:53:57.794592 4097533 scope.cc:203] Create variable batch_norm2d_19.b_0
I1018 08:53:57.794595 4097533 scope.cc:203] Create variable batch_norm2d_11.w_0
I1018 08:53:57.794597 4097533 scope.cc:203] Create variable assign_45.tmp_0
I1018 08:53:57.794600 4097533 scope.cc:203] Create variable batch_norm2d_11.b_0
I1018 08:53:57.794602 4097533 scope.cc:203] Create variable batch_norm2d_11.w_2
I1018 08:53:57.794605 4097533 scope_buffered_ssa_graph_executor.cc:207] batch_norm2d_11.w_1 has been initialized beforehand in global scope, skipped
I1018 08:53:57.794607 4097533 scope.cc:203] Create variable conv2d_10.tmp_0
I1018 08:53:57.794610 4097533 scope.cc:203] Create variable conv2d_11.w_0
I1018 08:53:57.794612 4097533 scope.cc:203] Create variable relu_8.tmp_0
I1018 08:53:57.794615 4097533 scope.cc:203] Create variable batch_norm2d_18.b_0
I1018 08:53:57.794616 4097533 scope.cc:203] Create variable assign_37.tmp_0
I1018 08:53:57.794620 4097533 scope_buffered_ssa_graph_executor.cc:207] batch_norm2d_9.w_1 has been initialized beforehand in global scope, skipped
I1018 08:53:57.794621 4097533 scope.cc:203] Create variable batch_norm2d_9.b_0
I1018 08:53:57.794624 4097533 scope.cc:203] Create variable batch_norm2d_9.w_0
I1018 08:53:57.794626 4097533 scope_buffered_ssa_graph_executor.cc:207] batch_norm2d_9.w_2 has been initialized beforehand in global scope, skipped
I1018 08:53:57.794628 4097533 scope.cc:203] Create variable conv2d_9.tmp_0
I1018 08:53:57.794631 4097533 scope.cc:203] Create variable conv2d_9.w_0
I1018 08:53:57.794633 4097533 scope.cc:203] Create variable batch_norm2d_8.b_0
I1018 08:53:57.794636 4097533 scope.cc:203] Create variable batch_norm2d_8.w_0
I1018 08:53:57.794638 4097533 scope_buffered_ssa_graph_executor.cc:207] batch_norm2d_8.w_1 has been initialized beforehand in global scope, skipped
I1018 08:53:57.794641 4097533 scope_buffered_ssa_graph_executor.cc:207] batch_norm2d_8.w_2 has been initialized beforehand in global scope, skipped
I1018 08:53:57.794643 4097533 scope.cc:203] Create variable conv2d_8.tmp_0
I1018 08:53:57.794646 4097533 scope.cc:203] Create variable conv2d_8.w_0
I1018 08:53:57.794648 4097533 scope.cc:203] Create variable assign_25.tmp_0
I1018 08:53:57.794652 4097533 scope.cc:203] Create variable relu_6.tmp_0
I1018 08:53:57.794656 4097533 scope.cc:203] Create variable assign_29.tmp_0
I1018 08:53:57.794657 4097533 scope.cc:203] Create variable batch_norm2d_7.b_0
I1018 08:53:57.794660 4097533 scope_buffered_ssa_graph_executor.cc:207] batch_norm2d_5.w_2 has been initialized beforehand in global scope, skipped
I1018 08:53:57.794662 4097533 scope.cc:203] Create variable batch_norm2d_7.w_0
I1018 08:53:57.794665 4097533 scope.cc:203] Create variable batch_norm2d_5.w_0
I1018 08:53:57.794667 4097533 scope_buffered_ssa_graph_executor.cc:207] batch_norm2d_7.w_2 has been initialized beforehand in global scope, skipped
I1018 08:53:57.794669 4097533 scope_buffered_ssa_graph_executor.cc:207] batch_norm2d_7.w_1 has been initialized beforehand in global scope, skipped
I1018 08:53:57.794672 4097533 scope_buffered_ssa_graph_executor.cc:207] batch_norm2d_5.w_1 has been initialized beforehand in global scope, skipped
I1018 08:53:57.794674 4097533 scope.cc:203] Create variable batch_norm2d_5.b_0
I1018 08:53:57.794677 4097533 scope.cc:203] Create variable conv2d_7.tmp_0
I1018 08:53:57.794679 4097533 scope.cc:203] Create variable assign_4.tmp_0
I1018 08:53:57.794682 4097533 scope.cc:203] Create variable conv2d_5.w_0
I1018 08:53:57.794683 4097533 scope.cc:203] Create variable conv2d_6.tmp_0
I1018 08:53:57.794687 4097533 scope.cc:203] Create variable conv2d_7.w_0
I1018 08:53:57.794688 4097533 scope.cc:203] Create variable relu_5.tmp_0
I1018 08:53:57.794692 4097533 scope.cc:203] Create variable assign_21.tmp_0
I1018 08:53:57.794693 4097533 scope.cc:203] Create variable batch_norm2d_6.b_0
I1018 08:53:57.794695 4097533 scope_buffered_ssa_graph_executor.cc:207] batch_norm2d_6.w_1 has been initialized beforehand in global scope, skipped
I1018 08:53:57.794698 4097533 scope.cc:203] Create variable batch_norm2d_6.w_0
I1018 08:53:57.794700 4097533 scope.cc:203] Create variable assign_48.tmp_0
I1018 08:53:57.794703 4097533 scope.cc:203] Create variable batch_norm2d_4.w_2
I1018 08:53:57.794705 4097533 scope_buffered_ssa_graph_executor.cc:207] batch_norm2d_6.w_2 has been initialized beforehand in global scope, skipped
I1018 08:53:57.794708 4097533 scope_buffered_ssa_graph_executor.cc:207] batch_norm2d_10.w_1 has been initialized beforehand in global scope, skipped
I1018 08:53:57.794710 4097533 scope.cc:203] Create variable batch_norm2d_16.w_0
I1018 08:53:57.794713 4097533 scope_buffered_ssa_graph_executor.cc:207] batch_norm2d_10.w_2 has been initialized beforehand in global scope, skipped
I1018 08:53:57.794714 4097533 scope.cc:203] Create variable conv2d_5.tmp_0
I1018 08:53:57.794718 4097533 scope.cc:203] Create variable conv2d_6.w_0
I1018 08:53:57.794719 4097533 scope.cc:203] Create variable relu_4.tmp_0
I1018 08:53:57.794723 4097533 scope.cc:203] Create variable assign_17.tmp_0
I1018 08:53:57.794726 4097533 scope.cc:203] Create variable batch_norm2d_4.b_0
I1018 08:53:57.794729 4097533 scope_buffered_ssa_graph_executor.cc:207] batch_norm2d_4.w_1 has been initialized beforehand in global scope, skipped
I1018 08:53:57.794731 4097533 scope.cc:203] Create variable batch_norm2d_4.w_0
I1018 08:53:57.794734 4097533 scope_buffered_ssa_graph_executor.cc:207] batch_norm2d_4.w_2 has been initialized beforehand in global scope, skipped
I1018 08:53:57.794735 4097533 scope_buffered_ssa_graph_executor.cc:207] batch_norm2d_11.w_2 has been initialized beforehand in global scope, skipped
I1018 08:53:57.794739 4097533 scope.cc:203] Create variable assign_61.tmp_0
I1018 08:53:57.794740 4097533 scope.cc:203] Create variable conv2d_4.tmp_0
I1018 08:53:57.794742 4097533 scope.cc:203] Create variable conv2d_4.w_0
I1018 08:53:57.794745 4097533 scope.cc:203] Create variable relu_7.tmp_0
I1018 08:53:57.794747 4097533 scope.cc:203] Create variable relu_3.tmp_0
I1018 08:53:57.794750 4097533 scope.cc:203] Create variable assign_33.tmp_0
I1018 08:53:57.794752 4097533 scope.cc:203] Create variable assign_13.tmp_0
I1018 08:53:57.794755 4097533 scope.cc:203] Create variable batch_norm2d_3.b_0
I1018 08:53:57.794759 4097533 scope.cc:203] Create variable batch_norm2d_3.w_0
I1018 08:53:57.794761 4097533 scope_buffered_ssa_graph_executor.cc:207] batch_norm2d_3.w_1 has been initialized beforehand in global scope, skipped
I1018 08:53:57.794764 4097533 scope_buffered_ssa_graph_executor.cc:207] batch_norm2d_3.w_2 has been initialized beforehand in global scope, skipped
I1018 08:53:57.794766 4097533 scope.cc:203] Create variable conv2d_3.tmp_0
I1018 08:53:57.794768 4097533 scope.cc:203] Create variable conv2d_3.w_0
I1018 08:53:57.794771 4097533 scope.cc:203] Create variable relu_2.tmp_0
I1018 08:53:57.794773 4097533 scope.cc:203] Create variable assign_9.tmp_0
I1018 08:53:57.794775 4097533 scope_buffered_ssa_graph_executor.cc:207] batch_norm2d_2.w_1 has been initialized beforehand in global scope, skipped
I1018 08:53:57.794778 4097533 scope.cc:203] Create variable batch_norm2d_2.b_0
I1018 08:53:57.794780 4097533 scope.cc:203] Create variable batch_norm2d_2.w_0
I1018 08:53:57.794782 4097533 scope_buffered_ssa_graph_executor.cc:207] batch_norm2d_2.w_2 has been initialized beforehand in global scope, skipped
I1018 08:53:57.794785 4097533 scope.cc:203] Create variable conv2d_2.tmp_0
I1018 08:53:57.794787 4097533 scope.cc:203] Create variable conv2d_2.w_0
I1018 08:53:57.794790 4097533 scope.cc:203] Create variable relu_1.tmp_0
I1018 08:53:57.794792 4097533 scope.cc:203] Create variable assign_5.tmp_0
I1018 08:53:57.794795 4097533 scope_buffered_ssa_graph_executor.cc:207] batch_norm2d_1.w_2 has been initialized beforehand in global scope, skipped
I1018 08:53:57.794797 4097533 scope_buffered_ssa_graph_executor.cc:207] batch_norm2d_1.w_1 has been initialized beforehand in global scope, skipped
I1018 08:53:57.794799 4097533 scope.cc:203] Create variable batch_norm2d_1.b_0
I1018 08:53:57.794802 4097533 scope.cc:203] Create variable batch_norm2d_1.w_0
I1018 08:53:57.794804 4097533 scope.cc:203] Create variable conv2d_1.tmp_0
I1018 08:53:57.794806 4097533 scope.cc:203] Create variable conv2d_1.w_0
I1018 08:53:57.794809 4097533 scope.cc:203] Create variable pool2d_0.tmp_0
I1018 08:53:57.794811 4097533 scope.cc:203] Create variable assign_56.tmp_0
I1018 08:53:57.794814 4097533 scope.cc:203] Create variable assign_1.tmp_0
I1018 08:53:57.794816 4097533 scope_buffered_ssa_graph_executor.cc:207] batch_norm2d_0.w_2 has been initialized beforehand in global scope, skipped
I1018 08:53:57.794818 4097533 scope_buffered_ssa_graph_executor.cc:207] batch_norm2d_0.w_1 has been initialized beforehand in global scope, skipped
I1018 08:53:57.794821 4097533 scope.cc:203] Create variable batch_norm2d_0.b_0
I1018 08:53:57.794823 4097533 scope.cc:203] Create variable batch_norm2d_0.w_0
I1018 08:53:57.794826 4097533 scope.cc:203] Create variable conv2d_0.tmp_0
I1018 08:53:57.794828 4097533 scope.cc:203] Create variable _jst.0.x.0
I1018 08:53:57.794831 4097533 scope.cc:203] Create variable conv2d_0.w_0
I1018 08:53:57.794929 4097533 var_desc.cc:415] Flush  assign_72.tmp_0 0
I1018 08:53:57.794934 4097533 var_desc.cc:415] Flush  assign_68.tmp_0 0
I1018 08:53:57.794937 4097533 var_desc.cc:415] Flush  assign_64.tmp_0 0
I1018 08:53:57.794940 4097533 var_desc.cc:415] Flush  assign_60.tmp_0 0
I1018 08:53:57.794943 4097533 var_desc.cc:415] Flush  assign_52.tmp_0 0
I1018 08:53:57.794946 4097533 var_desc.cc:415] Flush  assign_44.tmp_0 0
I1018 08:53:57.794948 4097533 var_desc.cc:415] Flush  assign_40.tmp_0 0
I1018 08:53:57.794951 4097533 var_desc.cc:415] Flush  assign_36.tmp_0 0
I1018 08:53:57.794953 4097533 var_desc.cc:415] Flush  assign_32.tmp_0 0
I1018 08:53:57.794956 4097533 var_desc.cc:415] Flush  assign_28.tmp_0 0
I1018 08:53:57.794958 4097533 var_desc.cc:415] Flush  assign_24.tmp_0 0
I1018 08:53:57.794961 4097533 var_desc.cc:415] Flush  assign_20.tmp_0 0
I1018 08:53:57.794963 4097533 var_desc.cc:415] Flush  assign_16.tmp_0 0
I1018 08:53:57.794965 4097533 var_desc.cc:415] Flush  assign_12.tmp_0 0
I1018 08:53:57.794968 4097533 var_desc.cc:415] Flush  assign_8.tmp_0 0
I1018 08:53:57.794970 4097533 var_desc.cc:415] Flush  batch_norm2d_19.w_2 1
I1018 08:53:57.794976 4097533 var_desc.cc:415] Flush  batch_norm2d_18.w_2 1
I1018 08:53:57.794979 4097533 var_desc.cc:415] Flush  batch_norm2d_18.w_1 1
I1018 08:53:57.794981 4097533 var_desc.cc:415] Flush  batch_norm2d_15.w_2 1
I1018 08:53:57.794984 4097533 var_desc.cc:415] Flush  batch_norm2d_15.w_1 1
I1018 08:53:57.794986 4097533 var_desc.cc:415] Flush  batch_norm2d_17.w_2 1
I1018 08:53:57.794989 4097533 var_desc.cc:415] Flush  batch_norm2d_17.w_1 1
I1018 08:53:57.794991 4097533 var_desc.cc:415] Flush  batch_norm2d_16.w_2 1
I1018 08:53:57.794994 4097533 var_desc.cc:415] Flush  batch_norm2d_16.w_1 1
I1018 08:53:57.794996 4097533 var_desc.cc:415] Flush  batch_norm2d_14.w_2 1
I1018 08:53:57.794998 4097533 var_desc.cc:415] Flush  batch_norm2d_14.w_1 1
I1018 08:53:57.795001 4097533 var_desc.cc:415] Flush  batch_norm2d_13.w_2 1
I1018 08:53:57.795003 4097533 var_desc.cc:415] Flush  batch_norm2d_13.w_1 1
I1018 08:53:57.795006 4097533 var_desc.cc:415] Flush  batch_norm2d_10.w_2 1
I1018 08:53:57.795008 4097533 var_desc.cc:415] Flush  batch_norm2d_12.w_2 1
I1018 08:53:57.795010 4097533 var_desc.cc:415] Flush  batch_norm2d_12.w_1 1
I1018 08:53:57.795013 4097533 var_desc.cc:415] Flush  batch_norm2d_9.w_2 1
I1018 08:53:57.795017 4097533 var_desc.cc:415] Flush  batch_norm2d_9.w_1 1
I1018 08:53:57.795019 4097533 var_desc.cc:415] Flush  batch_norm2d_8.w_2 1
I1018 08:53:57.795022 4097533 var_desc.cc:415] Flush  batch_norm2d_8.w_1 1
I1018 08:53:57.795024 4097533 var_desc.cc:415] Flush  batch_norm2d_5.w_2 1
I1018 08:53:57.795027 4097533 var_desc.cc:415] Flush  batch_norm2d_5.w_1 1
I1018 08:53:57.795028 4097533 var_desc.cc:415] Flush  batch_norm2d_7.w_2 1
I1018 08:53:57.795032 4097533 var_desc.cc:415] Flush  batch_norm2d_7.w_1 1
I1018 08:53:57.795033 4097533 var_desc.cc:415] Flush  batch_norm2d_6.w_2 1
I1018 08:53:57.795037 4097533 var_desc.cc:415] Flush  batch_norm2d_6.w_1 1
I1018 08:53:57.795038 4097533 var_desc.cc:415] Flush  batch_norm2d_4.w_1 1
I1018 08:53:57.795040 4097533 var_desc.cc:415] Flush  batch_norm2d_3.w_2 1
I1018 08:53:57.795043 4097533 var_desc.cc:415] Flush  assign_76.tmp_0 0
I1018 08:53:57.795045 4097533 var_desc.cc:415] Flush  batch_norm2d_2.w_2 1
I1018 08:53:57.795048 4097533 var_desc.cc:415] Flush  batch_norm2d_2.w_1 1
I1018 08:53:57.795050 4097533 var_desc.cc:415] Flush  batch_norm2d_1.w_2 1
I1018 08:53:57.795053 4097533 var_desc.cc:415] Flush  batch_norm2d_1.w_1 1
I1018 08:53:57.795055 4097533 var_desc.cc:415] Flush  batch_norm2d_0.w_2 1
I1018 08:53:57.795058 4097533 var_desc.cc:415] Flush  batch_norm2d_0.w_1 1
I1018 08:53:57.795060 4097533 var_desc.cc:415] Flush  linear_0.tmp_1 0
I1018 08:53:57.795063 4097533 var_desc.cc:415] Flush  assign_77.tmp_0 0
I1018 08:53:57.795065 4097533 var_desc.cc:415] Flush  batch_norm2d_19.w_0 1
I1018 08:53:57.795068 4097533 var_desc.cc:415] Flush  batch_norm2d_19.w_1 1
I1018 08:53:57.795070 4097533 var_desc.cc:415] Flush  linear_0.w_0 1
I1018 08:53:57.795073 4097533 var_desc.cc:415] Flush  conv2d_19.tmp_0 0
I1018 08:53:57.795075 4097533 var_desc.cc:415] Flush  conv2d_19.w_0 1
I1018 08:53:57.795078 4097533 var_desc.cc:415] Flush  relu_15.tmp_0 0
I1018 08:53:57.795080 4097533 var_desc.cc:415] Flush  assign_73.tmp_0 0
I1018 08:53:57.795083 4097533 var_desc.cc:415] Flush  batch_norm2d_18.w_0 1
I1018 08:53:57.795085 4097533 var_desc.cc:415] Flush  batch_norm2d_11.w_1 1
I1018 08:53:57.795087 4097533 var_desc.cc:415] Flush  batch_norm2d_18.w_2 1
I1018 08:53:57.795090 4097533 var_desc.cc:415] Flush  conv2d_18.tmp_0 0
I1018 08:53:57.795094 4097533 var_desc.cc:415] Flush  conv2d_18.w_0 1
I1018 08:53:57.795095 4097533 var_desc.cc:415] Flush  assign_65.tmp_0 0
I1018 08:53:57.795099 4097533 var_desc.cc:415] Flush  relu_14.tmp_0 0
I1018 08:53:57.795101 4097533 var_desc.cc:415] Flush  assign_69.tmp_0 0
I1018 08:53:57.795104 4097533 var_desc.cc:415] Flush  batch_norm2d_17.b_0 1
I1018 08:53:57.795106 4097533 var_desc.cc:415] Flush  batch_norm2d_17.w_0 1
I1018 08:53:57.795109 4097533 var_desc.cc:415] Flush  batch_norm2d_15.w_0 1
I1018 08:53:57.795111 4097533 var_desc.cc:415] Flush  batch_norm2d_17.w_2 1
I1018 08:53:57.795116 4097533 var_desc.cc:415] Flush  linear_0.b_0 1
I1018 08:53:57.795120 4097533 var_desc.cc:415] Flush  batch_norm2d_15.b_0 1
I1018 08:53:57.795121 4097533 var_desc.cc:415] Flush  conv2d_17.tmp_0 0
I1018 08:53:57.795125 4097533 var_desc.cc:415] Flush  conv2d_15.w_0 1
I1018 08:53:57.795126 4097533 var_desc.cc:415] Flush  conv2d_16.tmp_0 0
I1018 08:53:57.795130 4097533 var_desc.cc:415] Flush  conv2d_17.w_0 1
I1018 08:53:57.795131 4097533 var_desc.cc:415] Flush  relu_13.tmp_0 0
I1018 08:53:57.795135 4097533 var_desc.cc:415] Flush  batch_norm2d_16.b_0 1
I1018 08:53:57.795136 4097533 var_desc.cc:415] Flush  batch_norm2d_16.w_1 1
I1018 08:53:57.795140 4097533 var_desc.cc:415] Flush  batch_norm2d_16.w_2 1
I1018 08:53:57.795142 4097533 var_desc.cc:415] Flush  conv2d_15.tmp_0 0
I1018 08:53:57.795145 4097533 var_desc.cc:415] Flush  conv2d_16.w_0 1
I1018 08:53:57.795147 4097533 var_desc.cc:415] Flush  relu_12.tmp_0 0
I1018 08:53:57.795149 4097533 var_desc.cc:415] Flush  batch_norm2d_15.w_2 1
I1018 08:53:57.795151 4097533 var_desc.cc:415] Flush  assign_57.tmp_0 0
I1018 08:53:57.795154 4097533 var_desc.cc:415] Flush  batch_norm2d_14.w_1 1
I1018 08:53:57.795157 4097533 var_desc.cc:415] Flush  batch_norm2d_14.b_0 1
I1018 08:53:57.795159 4097533 var_desc.cc:415] Flush  batch_norm2d_14.w_0 1
I1018 08:53:57.795161 4097533 var_desc.cc:415] Flush  batch_norm2d_14.w_2 1
I1018 08:53:57.795164 4097533 var_desc.cc:415] Flush  conv2d_14.tmp_0 0
I1018 08:53:57.795166 4097533 var_desc.cc:415] Flush  conv2d_14.w_0 1
I1018 08:53:57.795169 4097533 var_desc.cc:415] Flush  relu_11.tmp_0 0
I1018 08:53:57.795171 4097533 var_desc.cc:415] Flush  assign_53.tmp_0 0
I1018 08:53:57.795174 4097533 var_desc.cc:415] Flush  batch_norm2d_13.b_0 1
I1018 08:53:57.795176 4097533 var_desc.cc:415] Flush  assign_0.tmp_0 0
I1018 08:53:57.795179 4097533 var_desc.cc:415] Flush  batch_norm2d_19.w_2 1
I1018 08:53:57.795181 4097533 var_desc.cc:415] Flush  batch_norm2d_13.w_0 1
I1018 08:53:57.795184 4097533 var_desc.cc:415] Flush  batch_norm2d_17.w_1 1
I1018 08:53:57.795186 4097533 var_desc.cc:415] Flush  batch_norm2d_13.w_1 1
I1018 08:53:57.795188 4097533 var_desc.cc:415] Flush  batch_norm2d_15.w_1 1
I1018 08:53:57.795192 4097533 var_desc.cc:415] Flush  batch_norm2d_13.w_2 1
I1018 08:53:57.795193 4097533 var_desc.cc:415] Flush  conv2d_13.tmp_0 0
I1018 08:53:57.795197 4097533 var_desc.cc:415] Flush  conv2d_13.w_0 1
I1018 08:53:57.795199 4097533 var_desc.cc:415] Flush  relu_10.tmp_0 0
I1018 08:53:57.795202 4097533 var_desc.cc:415] Flush  assign_49.tmp_0 0
I1018 08:53:57.795204 4097533 var_desc.cc:415] Flush  batch_norm2d_12.b_0 1
I1018 08:53:57.795207 4097533 var_desc.cc:415] Flush  batch_norm2d_3.w_1 1
I1018 08:53:57.795209 4097533 var_desc.cc:415] Flush  batch_norm2d_12.w_0 1
I1018 08:53:57.795212 4097533 var_desc.cc:415] Flush  batch_norm2d_10.w_0 1
I1018 08:53:57.795213 4097533 var_desc.cc:415] Flush  batch_norm2d_12.w_2 1
I1018 08:53:57.795217 4097533 var_desc.cc:415] Flush  batch_norm2d_12.w_1 1
I1018 08:53:57.795218 4097533 var_desc.cc:415] Flush  batch_norm2d_10.w_1 1
I1018 08:53:57.795221 4097533 var_desc.cc:415] Flush  batch_norm2d_10.b_0 1
I1018 08:53:57.795223 4097533 var_desc.cc:415] Flush  conv2d_12.tmp_0 0
I1018 08:53:57.795226 4097533 var_desc.cc:415] Flush  conv2d_10.w_0 1
I1018 08:53:57.795228 4097533 var_desc.cc:415] Flush  batch_norm2d_18.w_1 1
I1018 08:53:57.795231 4097533 var_desc.cc:415] Flush  batch_norm2d_19.w_1 1
I1018 08:53:57.795233 4097533 var_desc.cc:415] Flush  conv2d_11.tmp_0 0
I1018 08:53:57.795236 4097533 var_desc.cc:415] Flush  conv2d_12.w_0 1
I1018 08:53:57.795238 4097533 var_desc.cc:415] Flush  relu_9.tmp_0 0
I1018 08:53:57.795241 4097533 var_desc.cc:415] Flush  assign_41.tmp_0 0
I1018 08:53:57.795243 4097533 var_desc.cc:415] Flush  batch_norm2d_19.b_0 1
I1018 08:53:57.795246 4097533 var_desc.cc:415] Flush  batch_norm2d_11.w_0 1
I1018 08:53:57.795248 4097533 var_desc.cc:415] Flush  assign_45.tmp_0 0
I1018 08:53:57.795251 4097533 var_desc.cc:415] Flush  batch_norm2d_11.b_0 1
I1018 08:53:57.795253 4097533 var_desc.cc:415] Flush  batch_norm2d_11.w_2 1
I1018 08:53:57.795258 4097533 var_desc.cc:415] Flush  batch_norm2d_11.w_1 1
I1018 08:53:57.795260 4097533 var_desc.cc:415] Flush  conv2d_10.tmp_0 0
I1018 08:53:57.795265 4097533 var_desc.cc:415] Flush  conv2d_11.w_0 1
I1018 08:53:57.795267 4097533 var_desc.cc:415] Flush  relu_8.tmp_0 0
I1018 08:53:57.795270 4097533 var_desc.cc:415] Flush  batch_norm2d_18.b_0 1
I1018 08:53:57.795272 4097533 var_desc.cc:415] Flush  assign_37.tmp_0 0
I1018 08:53:57.795275 4097533 var_desc.cc:415] Flush  batch_norm2d_9.w_1 1
I1018 08:53:57.795277 4097533 var_desc.cc:415] Flush  batch_norm2d_9.b_0 1
I1018 08:53:57.795280 4097533 var_desc.cc:415] Flush  batch_norm2d_9.w_0 1
I1018 08:53:57.795282 4097533 var_desc.cc:415] Flush  batch_norm2d_9.w_2 1
I1018 08:53:57.795285 4097533 var_desc.cc:415] Flush  conv2d_9.tmp_0 0
I1018 08:53:57.795289 4097533 var_desc.cc:415] Flush  conv2d_9.w_0 1
I1018 08:53:57.795290 4097533 var_desc.cc:415] Flush  batch_norm2d_8.b_0 1
I1018 08:53:57.795293 4097533 var_desc.cc:415] Flush  batch_norm2d_8.w_0 1
I1018 08:53:57.795295 4097533 var_desc.cc:415] Flush  batch_norm2d_8.w_1 1
I1018 08:53:57.795298 4097533 var_desc.cc:415] Flush  batch_norm2d_8.w_2 1
I1018 08:53:57.795300 4097533 var_desc.cc:415] Flush  conv2d_8.tmp_0 0
I1018 08:53:57.795303 4097533 var_desc.cc:415] Flush  conv2d_8.w_0 1
I1018 08:53:57.795305 4097533 var_desc.cc:415] Flush  assign_25.tmp_0 0
I1018 08:53:57.795308 4097533 var_desc.cc:415] Flush  relu_6.tmp_0 0
I1018 08:53:57.795310 4097533 var_desc.cc:415] Flush  assign_29.tmp_0 0
I1018 08:53:57.795313 4097533 var_desc.cc:415] Flush  batch_norm2d_7.b_0 1
I1018 08:53:57.795315 4097533 var_desc.cc:415] Flush  batch_norm2d_5.w_2 1
I1018 08:53:57.795318 4097533 var_desc.cc:415] Flush  batch_norm2d_7.w_0 1
I1018 08:53:57.795320 4097533 var_desc.cc:415] Flush  batch_norm2d_5.w_0 1
I1018 08:53:57.795323 4097533 var_desc.cc:415] Flush  batch_norm2d_7.w_2 1
I1018 08:53:57.795325 4097533 var_desc.cc:415] Flush  batch_norm2d_7.w_1 1
I1018 08:53:57.795328 4097533 var_desc.cc:415] Flush  batch_norm2d_5.w_1 1
I1018 08:53:57.795331 4097533 var_desc.cc:415] Flush  batch_norm2d_5.b_0 1
I1018 08:53:57.795333 4097533 var_desc.cc:415] Flush  conv2d_7.tmp_0 0
I1018 08:53:57.795336 4097533 var_desc.cc:415] Flush  assign_4.tmp_0 0
I1018 08:53:57.795338 4097533 var_desc.cc:415] Flush  conv2d_5.w_0 1
I1018 08:53:57.795341 4097533 var_desc.cc:415] Flush  conv2d_6.tmp_0 0
I1018 08:53:57.795343 4097533 var_desc.cc:415] Flush  conv2d_7.w_0 1
I1018 08:53:57.795346 4097533 var_desc.cc:415] Flush  relu_5.tmp_0 0
I1018 08:53:57.795348 4097533 var_desc.cc:415] Flush  assign_21.tmp_0 0
I1018 08:53:57.795351 4097533 var_desc.cc:415] Flush  batch_norm2d_6.b_0 1
I1018 08:53:57.795353 4097533 var_desc.cc:415] Flush  batch_norm2d_6.w_1 1
I1018 08:53:57.795356 4097533 var_desc.cc:415] Flush  batch_norm2d_6.w_0 1
I1018 08:53:57.795358 4097533 var_desc.cc:415] Flush  assign_48.tmp_0 0
I1018 08:53:57.795360 4097533 var_desc.cc:415] Flush  batch_norm2d_4.w_2 1
I1018 08:53:57.795362 4097533 var_desc.cc:415] Flush  batch_norm2d_6.w_2 1
I1018 08:53:57.795365 4097533 var_desc.cc:415] Flush  batch_norm2d_10.w_1 1
I1018 08:53:57.795367 4097533 var_desc.cc:415] Flush  batch_norm2d_16.w_0 1
I1018 08:53:57.795370 4097533 var_desc.cc:415] Flush  batch_norm2d_10.w_2 1
I1018 08:53:57.795372 4097533 var_desc.cc:415] Flush  conv2d_5.tmp_0 0
I1018 08:53:57.795375 4097533 var_desc.cc:415] Flush  conv2d_6.w_0 1
I1018 08:53:57.795377 4097533 var_desc.cc:415] Flush  relu_4.tmp_0 0
I1018 08:53:57.795379 4097533 var_desc.cc:415] Flush  assign_17.tmp_0 0
I1018 08:53:57.795382 4097533 var_desc.cc:415] Flush  batch_norm2d_4.b_0 1
I1018 08:53:57.795384 4097533 var_desc.cc:415] Flush  batch_norm2d_4.w_1 1
I1018 08:53:57.795387 4097533 var_desc.cc:415] Flush  batch_norm2d_4.w_0 1
I1018 08:53:57.795389 4097533 var_desc.cc:415] Flush  batch_norm2d_4.w_2 1
I1018 08:53:57.795392 4097533 var_desc.cc:415] Flush  batch_norm2d_11.w_2 1
I1018 08:53:57.795394 4097533 var_desc.cc:415] Flush  assign_61.tmp_0 0
I1018 08:53:57.795396 4097533 var_desc.cc:415] Flush  conv2d_4.tmp_0 0
I1018 08:53:57.795401 4097533 var_desc.cc:415] Flush  conv2d_4.w_0 1
I1018 08:53:57.795403 4097533 var_desc.cc:415] Flush  relu_7.tmp_0 0
I1018 08:53:57.795406 4097533 var_desc.cc:415] Flush  relu_3.tmp_0 0
I1018 08:53:57.795408 4097533 var_desc.cc:415] Flush  assign_33.tmp_0 0
I1018 08:53:57.795410 4097533 var_desc.cc:415] Flush  assign_13.tmp_0 0
I1018 08:53:57.795413 4097533 var_desc.cc:415] Flush  batch_norm2d_3.b_0 1
I1018 08:53:57.795415 4097533 var_desc.cc:415] Flush  batch_norm2d_3.w_0 1
I1018 08:53:57.795418 4097533 var_desc.cc:415] Flush  batch_norm2d_3.w_1 1
I1018 08:53:57.795420 4097533 var_desc.cc:415] Flush  batch_norm2d_3.w_2 1
I1018 08:53:57.795423 4097533 var_desc.cc:415] Flush  conv2d_3.tmp_0 0
I1018 08:53:57.795425 4097533 var_desc.cc:415] Flush  conv2d_3.w_0 1
I1018 08:53:57.795428 4097533 var_desc.cc:415] Flush  relu_2.tmp_0 0
I1018 08:53:57.795430 4097533 var_desc.cc:415] Flush  assign_9.tmp_0 0
I1018 08:53:57.795432 4097533 var_desc.cc:415] Flush  batch_norm2d_2.w_1 1
I1018 08:53:57.795435 4097533 var_desc.cc:415] Flush  batch_norm2d_2.b_0 1
I1018 08:53:57.795437 4097533 var_desc.cc:415] Flush  batch_norm2d_2.w_0 1
I1018 08:53:57.795440 4097533 var_desc.cc:415] Flush  batch_norm2d_2.w_2 1
I1018 08:53:57.795442 4097533 var_desc.cc:415] Flush  conv2d_2.tmp_0 0
I1018 08:53:57.795445 4097533 var_desc.cc:415] Flush  conv2d_2.w_0 1
I1018 08:53:57.795447 4097533 var_desc.cc:415] Flush  relu_1.tmp_0 0
I1018 08:53:57.795450 4097533 var_desc.cc:415] Flush  assign_5.tmp_0 0
I1018 08:53:57.795452 4097533 var_desc.cc:415] Flush  batch_norm2d_1.w_2 1
I1018 08:53:57.795454 4097533 var_desc.cc:415] Flush  batch_norm2d_1.w_1 1
I1018 08:53:57.795457 4097533 var_desc.cc:415] Flush  batch_norm2d_1.b_0 1
I1018 08:53:57.795459 4097533 var_desc.cc:415] Flush  batch_norm2d_1.w_0 1
I1018 08:53:57.795462 4097533 var_desc.cc:415] Flush  conv2d_1.tmp_0 0
I1018 08:53:57.795464 4097533 var_desc.cc:415] Flush  conv2d_1.w_0 1
I1018 08:53:57.795467 4097533 var_desc.cc:415] Flush  pool2d_0.tmp_0 0
I1018 08:53:57.795469 4097533 var_desc.cc:415] Flush  assign_56.tmp_0 0
I1018 08:53:57.795472 4097533 var_desc.cc:415] Flush  assign_1.tmp_0 0
I1018 08:53:57.795475 4097533 var_desc.cc:415] Flush  batch_norm2d_0.w_2 1
I1018 08:53:57.795477 4097533 var_desc.cc:415] Flush  batch_norm2d_0.w_1 1
I1018 08:53:57.795480 4097533 var_desc.cc:415] Flush  batch_norm2d_0.b_0 1
I1018 08:53:57.795481 4097533 var_desc.cc:415] Flush  batch_norm2d_0.w_0 1
I1018 08:53:57.795485 4097533 var_desc.cc:415] Flush  conv2d_0.tmp_0 0
I1018 08:53:57.795486 4097533 var_desc.cc:415] Flush  _jst.0.x.0 0
I1018 08:53:57.795490 4097533 var_desc.cc:415] Flush  conv2d_0.w_0 1
I1018 08:53:57.795573 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103d9e0 -> eager_deletion0x55b7d0bf8d60  via __control_var@16490x55b7d0be5660
I1018 08:53:57.795583 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> eager_deletion0x55b7d0c2da20  via __control_var@14760x55b7d10431b0
I1018 08:53:57.795588 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> eager_deletion0x55b7d0c7f270  via __control_var@14740x55b7d1042b70
I1018 08:53:57.795593 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> eager_deletion0x55b7d0d36320  via __control_var@14960x55b7d101bd70
I1018 08:53:57.795596 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1013850 -> eager_deletion0x55b7d0d50360  via __control_var@16280x55b7d0d50420
I1018 08:53:57.795600 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> eager_deletion0x55b7d0d83e40  via __control_var@14910x55b7d101af00
I1018 08:53:57.795604 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103ffa0 -> eager_deletion0x55b7d0da05e0  via __control_var@16230x55b7d0da06a0
I1018 08:53:57.795609 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> eager_deletion0x55b7d0dd5690  via __control_var@14780x55b7d1043710
I1018 08:53:57.795614 4097533 graph_helper.h:104] adj conv2d0x55b7d1029e80 -> eager_deletion0x55b7d0d04df0  via __control_var@16330x55b7d0d04eb0
I1018 08:53:57.795617 4097533 graph_helper.h:104] adj cinn_launch0x55b7d10059f0 -> eager_deletion0x55b7d0ec5bb0  via __control_var@16130x55b7d0c239b0
I1018 08:53:57.795624 4097533 graph_helper.h:104] adj conv2d0x55b7d11091c0 -> eager_deletion0x55b7d0e31dd0  via __control_var@16010x55b7d0e680a0
I1018 08:53:57.795629 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> eager_deletion0x55b7d0dac250  via __control_var@15070x55b7d101dd40
I1018 08:53:57.795632 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> eager_deletion0x55b7d0d20510  via __control_var@14900x55b7d101ac70
I1018 08:53:57.795637 4097533 graph_helper.h:104] adj conv2d0x55b7d1122f60 -> eager_deletion0x55b7d0d2e9e0  via __control_var@15860x55b7d0d11c80
I1018 08:53:57.795641 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> eager_deletion0x55b7d0ccaa80  via __control_var@14930x55b7d101b4e0
I1018 08:53:57.795645 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> eager_deletion0x55b7d0c8c100  via __control_var@14820x55b7d1044290
I1018 08:53:57.795650 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> eager_deletion0x55b7d0c82340  via __control_var@14730x55b7d1042880
I1018 08:53:57.795655 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> eager_deletion0x55b7d0be1970  via __control_var@14720x55b7d1042590
I1018 08:53:57.795658 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> eager_deletion0x55b7d0e7c680  via __control_var@14810x55b7d1043fa0
I1018 08:53:57.795662 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> eager_deletion0x55b7d0c100f0  via __control_var@14920x55b7d101b1f0
I1018 08:53:57.795666 4097533 graph_helper.h:104] adj conv2d0x55b7d106f840 -> eager_deletion0x55b7d0fc1590  via __control_var@15700x55b7d0bff1e0
I1018 08:53:57.795670 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> eager_deletion0x55b7d0c74830  via __control_var@14770x55b7d1043480
I1018 08:53:57.795675 4097533 graph_helper.h:104] adj conv2d0x55b7d0fb56d0 -> eager_deletion0x55b7d0ebbb40  via __control_var@15650x55b7d0a9d670
I1018 08:53:57.795679 4097533 graph_helper.h:104] adj conv2d0x55b7d111bde0 -> eager_deletion0x55b7d1106060  via __control_var@15620x55b7d0b734a0
I1018 08:53:57.795683 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100c710 -> eager_deletion0x55b7d0aeffc0  via __control_var@15590x55b7d0afe850
I1018 08:53:57.795687 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1038e60 -> eager_deletion0x55b7d0aa9140  via __control_var@15560x55b7d0e966c0
I1018 08:53:57.795692 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> eager_deletion0x55b7d0a72230  via __control_var@14830x55b7d1044580
I1018 08:53:57.795696 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103a140 -> eager_deletion0x55b7d0a639a0  via __control_var@15510x55b7d0dbb500
I1018 08:53:57.795701 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1012570 -> eager_deletion0x55b7d0a1f3e0  via __control_var@15480x55b7d0995060
I1018 08:53:57.795706 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1004710 -> eager_deletion0x55b7d09a1410  via __control_var@15420x55b7d09da090
I1018 08:53:57.795709 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1003430 -> eager_deletion0x55b7d0abb300  via __control_var@15390x55b7d0990130
I1018 08:53:57.795713 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100d9f0 -> eager_deletion0x55b7d0c57440  via __control_var@15300x55b7d0affe60
I1018 08:53:57.795717 4097533 graph_helper.h:104] adj conv2d0x55b7d0b70160 -> eager_deletion0x55b7d0a73840  via __control_var@15270x55b7d0e77f40
I1018 08:53:57.795722 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100ecd0 -> eager_deletion0x55b7d0c96340  via __control_var@15240x55b7d1033be0
I1018 08:53:57.795727 4097533 graph_helper.h:104] adj conv2d0x55b7d0edf1d0 -> eager_deletion0x55b7d0c57d60  via __control_var@15210x55b7d0b86300
I1018 08:53:57.795730 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> eager_deletion0x55b7cd159bc0  via __control_var@14990x55b7d101c600
I1018 08:53:57.795734 4097533 graph_helper.h:104] adj conv2d0x55b7d0d441e0 -> eager_deletion0x55b7d09765a0  via __control_var@15130x55b7d096fc60
I1018 08:53:57.795742 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103ecc0 -> eager_deletion0x55b7b78d2db0  via __control_var@15180x55b7b7024640
I1018 08:53:57.795747 4097533 graph_helper.h:104] adj conv2d0x55b7d0edeb50 -> eager_deletion0x55b7d0cdcfc0  via __control_var@15360x55b7d0dad860
I1018 08:53:57.795754 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> cinn_launch0x55b7d1002020  via __control_var@14800x55b7d1043cd0
I1018 08:53:57.795763 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> cinn_launch0x55b7d1000c10  via __control_var@14720x55b7d1042590
I1018 08:53:57.795768 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d0ffcfe0  via __control_var@14740x55b7d1042b70
I1018 08:53:57.795773 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d0ffbc30  via __control_var@14750x55b7d1042e40
I1018 08:53:57.795778 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d1068fa0  via __control_var@15090x55b7d101e2a0
I1018 08:53:57.795784 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d1067b90  via __control_var@15010x55b7d101cbc0
I1018 08:53:57.795789 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> cinn_launch0x55b7d1066780  via __control_var@14980x55b7d101c330
I1018 08:53:57.795794 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> cinn_launch0x55b7d1065370  via __control_var@14900x55b7d101ac70
I1018 08:53:57.795799 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> cinn_launch0x55b7d1063f60  via __control_var@15070x55b7d101dd40
I1018 08:53:57.795805 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> cinn_launch0x55b7d1061740  via __control_var@15080x55b7d101dfd0
I1018 08:53:57.795810 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> cinn_launch0x55b7d0fe13a0  via __control_var@14920x55b7d101b1f0
I1018 08:53:57.795815 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdeb80  via __control_var@15030x55b7d101d160
I1018 08:53:57.795820 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdd770  via __control_var@14850x55b7d1044b40
I1018 08:53:57.795825 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdc360  via __control_var@15040x55b7d101d4d0
I1018 08:53:57.795831 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> cinn_launch0x55b7d0fdaf50  via __control_var@14970x55b7d101c060
I1018 08:53:57.795836 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> cinn_launch0x55b7d0fd9b40  via __control_var@14930x55b7d101b4e0
I1018 08:53:57.795841 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> cinn_launch0x55b7d0fd8730  via __control_var@14820x55b7d1044290
I1018 08:53:57.795846 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> cinn_launch0x55b7d1104d40  via __control_var@14840x55b7d1044870
I1018 08:53:57.795851 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> cinn_launch0x55b7d1103930  via __control_var@15000x55b7d101c8f0
I1018 08:53:57.795856 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> cinn_launch0x55b7d1102520  via __control_var@14990x55b7d101c600
I1018 08:53:57.795862 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d1101110  via __control_var@14880x55b7d10453e0
I1018 08:53:57.795867 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10ffd00  via __control_var@14890x55b7d10456b0
I1018 08:53:57.795872 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10fe8f0  via __control_var@14860x55b7d1044e10
I1018 08:53:57.795877 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10fd4e0  via __control_var@14760x55b7d10431b0
I1018 08:53:57.795882 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> cinn_launch0x55b7d1113ec0  via __control_var@14790x55b7d1043a00
I1018 08:53:57.795887 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> cinn_launch0x55b7d1112ab0  via __control_var@14780x55b7d1043710
I1018 08:53:57.795895 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> cinn_launch0x55b7d11116a0  via __control_var@14830x55b7d1044580
I1018 08:53:57.795900 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> cinn_launch0x55b7d1110290  via __control_var@14940x55b7d101b7d0
I1018 08:53:57.795905 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> cinn_launch0x55b7d110ee80  via __control_var@15060x55b7d101da70
I1018 08:53:57.795912 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> cinn_launch0x55b7d110da70  via __control_var@14910x55b7d101af00
I1018 08:53:57.795917 4097533 graph_helper.h:104] adj conv2d0x55b7d1119e20 -> eager_deletion0x55b7d0e76930  via __control_var@16040x55b7d0e8a420
I1018 08:53:57.795920 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d110c6a0  via __control_var@14960x55b7d101bd70
I1018 08:53:57.795926 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d107f940  via __control_var@15020x55b7d101ce90
I1018 08:53:57.795931 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> cinn_launch0x55b7d107e530  via __control_var@14770x55b7d1043480
I1018 08:53:57.795936 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> cinn_launch0x55b7d107d120  via __control_var@14950x55b7d101baa0
I1018 08:53:57.795941 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d107bd10  via __control_var@14870x55b7d1045110
I1018 08:53:57.795948 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d107a890  via __control_var@14810x55b7d1043fa0
I1018 08:53:57.795953 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1011290 -> eager_deletion0x55b7d0bca520  via __control_var@15330x55b7d0c11700
I1018 08:53:57.795957 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100b430 -> eager_deletion0x55b7d0ccd6a0  via __control_var@16360x55b7d0cbdbf0
I1018 08:53:57.795961 4097533 graph_helper.h:104] adj conv2d0x55b7d106f840 -> cinn_launch0x55b7d109c290  via conv2d_19.tmp_00x55b7d109bfc0
I1018 08:53:57.795969 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d109c290  via relu_14.tmp_00x55b7d10900a0
I1018 08:53:57.795975 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> conv2d0x55b7d106f840  via relu_15.tmp_00x55b7d106f450
I1018 08:53:57.795981 4097533 graph_helper.h:104] adj conv2d0x55b7d10908e0 -> cinn_launch0x55b7d106aaf0  via conv2d_18.tmp_00x55b7d106a820
I1018 08:53:57.795989 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103c700 -> eager_deletion0x55b7d0e11740  via __control_var@16180x55b7d0de5140
I1018 08:53:57.795993 4097533 graph_helper.h:104] adj conv2d0x55b7d0fc2680 -> cinn_launch0x55b7d1126750  via conv2d_17.tmp_00x55b7d11264a0
I1018 08:53:57.796000 4097533 graph_helper.h:104] adj conv2d0x55b7d0fb56d0 -> cinn_launch0x55b7d1126750  via conv2d_16.tmp_00x55b7d0fc23b0
I1018 08:53:57.796008 4097533 graph_helper.h:104] adj conv2d0x55b7d0fc2680 -> eager_deletion0x55b7d0a126c0  via __control_var@15910x55b7d0d676f0
I1018 08:53:57.796012 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> conv2d0x55b7d0fc2680  via relu_12.tmp_00x55b7d1122b70
I1018 08:53:57.796017 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> eager_deletion0x55b7d0e04390  via __control_var@14850x55b7d1044b40
I1018 08:53:57.796022 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> cinn_launch0x55b7d1062b50  via __control_var@15110x55b7d101e840
I1018 08:53:57.796027 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> conv2d0x55b7d0fb56d0  via relu_13.tmp_00x55b7d0fb52e0
I1018 08:53:57.796033 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1014b30 -> eager_deletion0x55b7d0c4a1c0  via __control_var@16440x55b7d0c4a280
I1018 08:53:57.796037 4097533 graph_helper.h:104] adj conv2d0x55b7d1122f60 -> cinn_launch0x55b7d0fd6ee0  via conv2d_15.tmp_00x55b7d0fd6c10
I1018 08:53:57.796044 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> conv2d0x55b7d1122f60  via relu_12.tmp_00x55b7d1122b70
I1018 08:53:57.796049 4097533 graph_helper.h:104] adj conv2d0x55b7d0d89b40 -> eager_deletion0x55b7d0ebf660  via __control_var@16070x55b7d0dd21c0
I1018 08:53:57.796056 4097533 graph_helper.h:104] adj conv2d0x55b7d11091c0 -> cinn_launch0x55b7d0fea800  via conv2d_14.tmp_00x55b7d0fea530
I1018 08:53:57.796061 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fea800  via relu_10.tmp_00x55b7d102b4b0
I1018 08:53:57.796068 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> conv2d0x55b7d11091c0  via relu_11.tmp_00x55b7d1108dd0
I1018 08:53:57.796073 4097533 graph_helper.h:104] adj conv2d0x55b7d102bc70 -> cinn_launch0x55b7d1115560  via conv2d_13.tmp_00x55b7d11184f0
I1018 08:53:57.796079 4097533 graph_helper.h:104] adj conv2d0x55b7d0ec1350 -> eager_deletion0x55b7d0c30050  via __control_var@16100x55b7d0eacc00
I1018 08:53:57.796084 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> conv2d0x55b7d102bc70  via relu_10.tmp_00x55b7d102b4b0
I1018 08:53:57.796089 4097533 graph_helper.h:104] adj conv2d0x55b7d0d441e0 -> cinn_launch0x55b7d1031e80  via conv2d_12.tmp_00x55b7d1031bd0
I1018 08:53:57.796097 4097533 graph_helper.h:104] adj conv2d0x55b7d0e99aa0 -> cinn_launch0x55b7d1031e80  via conv2d_11.tmp_00x55b7d0d43f10
I1018 08:53:57.796104 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> conv2d0x55b7d0d441e0  via relu_8.tmp_00x55b7d1029a90
I1018 08:53:57.796109 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> conv2d0x55b7d0e99aa0  via relu_9.tmp_00x55b7d0e996b0
I1018 08:53:57.796114 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103b420 -> eager_deletion0x55b7d0cdb9b0  via __control_var@15830x55b7d0e5ec50
I1018 08:53:57.796119 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> conv2d0x55b7d1029e80  via relu_8.tmp_00x55b7d1029a90
I1018 08:53:57.796124 4097533 graph_helper.h:104] adj conv2d0x55b7d0ec1350 -> cinn_launch0x55b7d1047f30  via conv2d_9.tmp_00x55b7d1047c60
I1018 08:53:57.796130 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d1047f30  via relu_6.tmp_00x55b7d0e88b80
I1018 08:53:57.796137 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> conv2d0x55b7d0ec1350  via relu_7.tmp_00x55b7d0e0f4e0
I1018 08:53:57.796142 4097533 graph_helper.h:104] adj conv2d0x55b7d0e89340 -> cinn_launch0x55b7d0f0e1c0  via conv2d_8.tmp_00x55b7d0f0def0
I1018 08:53:57.796149 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> conv2d0x55b7d0e89340  via relu_6.tmp_00x55b7d0e88b80
I1018 08:53:57.796154 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> cinn_launch0x55b7d0ffe3f0  via __control_var@14730x55b7d1042880
I1018 08:53:57.796159 4097533 graph_helper.h:104] adj conv2d0x55b7d1119e20 -> cinn_launch0x55b7d0add8b0  via conv2d_7.tmp_00x55b7d0add600
I1018 08:53:57.796166 4097533 graph_helper.h:104] adj conv2d0x55b7d0d89b40 -> cinn_launch0x55b7d0add8b0  via conv2d_6.tmp_00x55b7d1119b50
I1018 08:53:57.796175 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> conv2d0x55b7d1119e20  via relu_4.tmp_00x55b7d111b9f0
I1018 08:53:57.796180 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> conv2d0x55b7d0d89b40  via relu_5.tmp_00x55b7d0d89750
I1018 08:53:57.796185 4097533 graph_helper.h:104] adj conv2d0x55b7d0e99aa0 -> eager_deletion0x55b7d0faad50  via __control_var@15960x55b7d0d48cf0
I1018 08:53:57.796190 4097533 graph_helper.h:104] adj conv2d0x55b7d111bde0 -> cinn_launch0x55b7d0dcf300  via conv2d_5.tmp_00x55b7d0dcf030
I1018 08:53:57.796195 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> conv2d0x55b7d111bde0  via relu_4.tmp_00x55b7d111b9f0
I1018 08:53:57.796200 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d0c640b0  via relu_2.tmp_00x55b7d0c74b50
I1018 08:53:57.796207 4097533 graph_helper.h:104] adj conv2d0x55b7d0b70160 -> cinn_launch0x55b7d0c640b0  via conv2d_4.tmp_00x55b7d0c63de0
I1018 08:53:57.796213 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> conv2d0x55b7d0b70160  via relu_3.tmp_00x55b7d0b6fd70
I1018 08:53:57.796218 4097533 graph_helper.h:104] adj conv2d0x55b7d0c74ec0 -> cinn_launch0x55b7d0c9d5e0  via conv2d_3.tmp_00x55b7d0e7f460
I1018 08:53:57.796227 4097533 graph_helper.h:104] adj conv2d0x55b7d1029e80 -> cinn_launch0x55b7d0cb5e70  via conv2d_10.tmp_00x55b7d0cb8f30
I1018 08:53:57.796233 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> conv2d0x55b7d0c74ec0  via relu_2.tmp_00x55b7d0c74b50
I1018 08:53:57.796239 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1041280 -> eager_deletion0x55b7d0caa5c0  via __control_var@16390x55b7d0c8ed20
I1018 08:53:57.796244 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdff90  via __control_var@15050x55b7d101d7a0
I1018 08:53:57.796249 4097533 graph_helper.h:104] adj conv2d0x55b7d0edeb50 -> cinn_launch0x55b7d0cfb320  via conv2d_2.tmp_00x55b7d0cfb110
I1018 08:53:57.796255 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d0cfb320  via pool2d_0.tmp_00x55b7d0b929b0
I1018 08:53:57.796262 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> conv2d0x55b7d0edeb50  via relu_1.tmp_00x55b7d0de1f20
I1018 08:53:57.796267 4097533 graph_helper.h:104] adj conv2d0x55b7d0ddf060 -> cinn_launch0x55b7d0c892a0  via conv2d_1.tmp_00x55b7d0c89030
I1018 08:53:57.796274 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100ffb0 -> eager_deletion0x55b7d0f91490  via __control_var@15450x55b7d0dbab10
I1018 08:53:57.796278 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> conv2d0x55b7d0ddf060  via pool2d_0.tmp_00x55b7d0b929b0
I1018 08:53:57.796284 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> conv2d0x55b7d10908e0  via relu_14.tmp_00x55b7d10900a0
I1018 08:53:57.796289 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> cinn_launch0x55b7d0fff800  via __control_var@15100x55b7d101e570
I1018 08:53:57.796295 4097533 graph_helper.h:104] adj conv2d0x55b7d0edf1d0 -> cinn_launch0x55b7d0fac1b0  via conv2d_0.tmp_00x55b7d0eaa280
I1018 08:53:57.797015 4097533 graph_helper.cc:698] convert op node to desc conv2d
I1018 08:53:57.797019 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797034 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797037 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797048 4097533 graph_helper.cc:698] convert op node to desc conv2d
I1018 08:53:57.797051 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797066 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797070 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797073 4097533 graph_helper.cc:698] convert op node to desc conv2d
I1018 08:53:57.797076 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797097 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797101 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797104 4097533 graph_helper.cc:698] convert op node to desc conv2d
I1018 08:53:57.797106 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797115 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797118 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797122 4097533 graph_helper.cc:698] convert op node to desc conv2d
I1018 08:53:57.797124 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797156 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797159 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797163 4097533 graph_helper.cc:698] convert op node to desc conv2d
I1018 08:53:57.797165 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797176 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797179 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797183 4097533 graph_helper.cc:698] convert op node to desc conv2d
I1018 08:53:57.797185 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797194 4097533 graph_helper.cc:698] convert op node to desc conv2d
I1018 08:53:57.797195 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797204 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797219 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797224 4097533 graph_helper.cc:698] convert op node to desc conv2d
I1018 08:53:57.797227 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797236 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797240 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797289 4097533 graph_helper.cc:698] convert op node to desc conv2d
I1018 08:53:57.797291 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797308 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797312 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797314 4097533 graph_helper.cc:698] convert op node to desc conv2d
I1018 08:53:57.797317 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797324 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797327 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797330 4097533 graph_helper.cc:698] convert op node to desc conv2d
I1018 08:53:57.797333 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797339 4097533 graph_helper.cc:698] convert op node to desc conv2d
I1018 08:53:57.797343 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797349 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797353 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797356 4097533 graph_helper.cc:698] convert op node to desc conv2d
I1018 08:53:57.797358 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797365 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797369 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797371 4097533 graph_helper.cc:698] convert op node to desc conv2d
I1018 08:53:57.797374 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797380 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797384 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797386 4097533 graph_helper.cc:698] convert op node to desc conv2d
I1018 08:53:57.797389 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797395 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797398 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797401 4097533 graph_helper.cc:698] convert op node to desc conv2d
I1018 08:53:57.797405 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797410 4097533 graph_helper.cc:698] convert op node to desc conv2d
I1018 08:53:57.797412 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797420 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797422 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797513 4097533 graph_helper.cc:698] convert op node to desc conv2d
I1018 08:53:57.797515 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797539 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797542 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797545 4097533 graph_helper.cc:698] convert op node to desc conv2d
I1018 08:53:57.797547 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797555 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797559 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797561 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797564 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797567 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797570 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797573 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797575 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797580 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797581 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797585 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797587 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797590 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797593 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797596 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797598 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797602 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797605 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797610 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797612 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797616 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797618 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797621 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797623 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797627 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797629 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797632 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797636 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797638 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797641 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797644 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797646 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797649 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797652 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797654 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797657 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797660 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797662 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797667 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797668 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797672 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797673 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797677 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797679 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797683 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797684 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797688 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797690 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797693 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797696 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797699 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797701 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797704 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797708 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797710 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797713 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797716 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797719 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797837 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797838 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797842 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797844 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797847 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797850 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797853 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797855 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797859 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797861 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797864 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797868 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797870 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797873 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797876 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797878 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797881 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797884 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797889 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797892 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797895 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797897 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797900 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797904 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797906 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797909 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797912 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797914 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797917 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797920 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797923 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797925 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797928 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797930 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797935 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797936 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797940 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797941 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797945 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797946 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797950 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797952 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797955 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797957 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797960 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797962 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797966 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797968 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797971 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797973 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797977 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797979 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797982 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797984 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797987 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797989 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797993 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.797995 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.797998 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.798000 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.798003 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.798007 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.798009 4097533 graph_helper.cc:698] convert op node to desc cinn_launch
I1018 08:53:57.798012 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.798982 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.799858 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103d9e0 -> eager_deletion0x55b7d0bf8d60  via __control_var@16490x55b7d0be5660
I1018 08:53:57.799865 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> eager_deletion0x55b7d0c2da20  via __control_var@14760x55b7d10431b0
I1018 08:53:57.799867 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> eager_deletion0x55b7d0c7f270  via __control_var@14740x55b7d1042b70
I1018 08:53:57.799870 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> eager_deletion0x55b7d0d36320  via __control_var@14960x55b7d101bd70
I1018 08:53:57.799872 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1013850 -> eager_deletion0x55b7d0d50360  via __control_var@16280x55b7d0d50420
I1018 08:53:57.799878 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> eager_deletion0x55b7d0d83e40  via __control_var@14910x55b7d101af00
I1018 08:53:57.799881 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103ffa0 -> eager_deletion0x55b7d0da05e0  via __control_var@16230x55b7d0da06a0
I1018 08:53:57.799883 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> eager_deletion0x55b7d0dd5690  via __control_var@14780x55b7d1043710
I1018 08:53:57.799886 4097533 graph_helper.h:104] adj conv2d0x55b7d1029e80 -> eager_deletion0x55b7d0d04df0  via __control_var@16330x55b7d0d04eb0
I1018 08:53:57.799888 4097533 graph_helper.h:104] adj cinn_launch0x55b7d10059f0 -> eager_deletion0x55b7d0ec5bb0  via __control_var@16130x55b7d0c239b0
I1018 08:53:57.799891 4097533 graph_helper.h:104] adj conv2d0x55b7d11091c0 -> eager_deletion0x55b7d0e31dd0  via __control_var@16010x55b7d0e680a0
I1018 08:53:57.799894 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> eager_deletion0x55b7d0dac250  via __control_var@15070x55b7d101dd40
I1018 08:53:57.799896 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> eager_deletion0x55b7d0d20510  via __control_var@14900x55b7d101ac70
I1018 08:53:57.799899 4097533 graph_helper.h:104] adj conv2d0x55b7d1122f60 -> eager_deletion0x55b7d0d2e9e0  via __control_var@15860x55b7d0d11c80
I1018 08:53:57.799901 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> eager_deletion0x55b7d0ccaa80  via __control_var@14930x55b7d101b4e0
I1018 08:53:57.799904 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> eager_deletion0x55b7d0c8c100  via __control_var@14820x55b7d1044290
I1018 08:53:57.799906 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> eager_deletion0x55b7d0c82340  via __control_var@14730x55b7d1042880
I1018 08:53:57.799909 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> eager_deletion0x55b7d0be1970  via __control_var@14720x55b7d1042590
I1018 08:53:57.799911 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> eager_deletion0x55b7d0e7c680  via __control_var@14810x55b7d1043fa0
I1018 08:53:57.799914 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> eager_deletion0x55b7d0c100f0  via __control_var@14920x55b7d101b1f0
I1018 08:53:57.799916 4097533 graph_helper.h:104] adj conv2d0x55b7d106f840 -> eager_deletion0x55b7d0fc1590  via __control_var@15700x55b7d0bff1e0
I1018 08:53:57.799919 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> eager_deletion0x55b7d0c74830  via __control_var@14770x55b7d1043480
I1018 08:53:57.799921 4097533 graph_helper.h:104] adj conv2d0x55b7d0fb56d0 -> eager_deletion0x55b7d0ebbb40  via __control_var@15650x55b7d0a9d670
I1018 08:53:57.799924 4097533 graph_helper.h:104] adj conv2d0x55b7d111bde0 -> eager_deletion0x55b7d1106060  via __control_var@15620x55b7d0b734a0
I1018 08:53:57.799927 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100c710 -> eager_deletion0x55b7d0aeffc0  via __control_var@15590x55b7d0afe850
I1018 08:53:57.799929 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1038e60 -> eager_deletion0x55b7d0aa9140  via __control_var@15560x55b7d0e966c0
I1018 08:53:57.799932 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> eager_deletion0x55b7d0a72230  via __control_var@14830x55b7d1044580
I1018 08:53:57.799934 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103a140 -> eager_deletion0x55b7d0a639a0  via __control_var@15510x55b7d0dbb500
I1018 08:53:57.799937 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1012570 -> eager_deletion0x55b7d0a1f3e0  via __control_var@15480x55b7d0995060
I1018 08:53:57.799939 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1004710 -> eager_deletion0x55b7d09a1410  via __control_var@15420x55b7d09da090
I1018 08:53:57.799942 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1003430 -> eager_deletion0x55b7d0abb300  via __control_var@15390x55b7d0990130
I1018 08:53:57.799944 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100d9f0 -> eager_deletion0x55b7d0c57440  via __control_var@15300x55b7d0affe60
I1018 08:53:57.799947 4097533 graph_helper.h:104] adj conv2d0x55b7d0b70160 -> eager_deletion0x55b7d0a73840  via __control_var@15270x55b7d0e77f40
I1018 08:53:57.799952 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100ecd0 -> eager_deletion0x55b7d0c96340  via __control_var@15240x55b7d1033be0
I1018 08:53:57.799953 4097533 graph_helper.h:104] adj conv2d0x55b7d0edf1d0 -> eager_deletion0x55b7d0c57d60  via __control_var@15210x55b7d0b86300
I1018 08:53:57.799957 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> eager_deletion0x55b7cd159bc0  via __control_var@14990x55b7d101c600
I1018 08:53:57.799958 4097533 graph_helper.h:104] adj conv2d0x55b7d0d441e0 -> eager_deletion0x55b7d09765a0  via __control_var@15130x55b7d096fc60
I1018 08:53:57.799961 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103ecc0 -> eager_deletion0x55b7b78d2db0  via __control_var@15180x55b7b7024640
I1018 08:53:57.799964 4097533 graph_helper.h:104] adj conv2d0x55b7d0edeb50 -> eager_deletion0x55b7d0cdcfc0  via __control_var@15360x55b7d0dad860
I1018 08:53:57.799969 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> cinn_launch0x55b7d1002020  via __control_var@14800x55b7d1043cd0
I1018 08:53:57.799973 4097533 graph_helper.h:104] adj cinn_launch0x55b7d109c290 -> cinn_launch0x55b7d1000c10  via __control_var@14720x55b7d1042590
I1018 08:53:57.799974 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d0ffcfe0  via __control_var@14740x55b7d1042b70
I1018 08:53:57.799978 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d0ffbc30  via __control_var@14750x55b7d1042e40
I1018 08:53:57.799979 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d1068fa0  via __control_var@15090x55b7d101e2a0
I1018 08:53:57.799981 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d1067b90  via __control_var@15010x55b7d101cbc0
I1018 08:53:57.799984 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> cinn_launch0x55b7d1066780  via __control_var@14980x55b7d101c330
I1018 08:53:57.799986 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> cinn_launch0x55b7d1065370  via __control_var@14900x55b7d101ac70
I1018 08:53:57.799989 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> cinn_launch0x55b7d1063f60  via __control_var@15070x55b7d101dd40
I1018 08:53:57.799991 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> cinn_launch0x55b7d1061740  via __control_var@15080x55b7d101dfd0
I1018 08:53:57.799994 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> cinn_launch0x55b7d0fe13a0  via __control_var@14920x55b7d101b1f0
I1018 08:53:57.799996 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdeb80  via __control_var@15030x55b7d101d160
I1018 08:53:57.799999 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdd770  via __control_var@14850x55b7d1044b40
I1018 08:53:57.800001 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdc360  via __control_var@15040x55b7d101d4d0
I1018 08:53:57.800004 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> cinn_launch0x55b7d0fdaf50  via __control_var@14970x55b7d101c060
I1018 08:53:57.800006 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> cinn_launch0x55b7d0fd9b40  via __control_var@14930x55b7d101b4e0
I1018 08:53:57.800009 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> cinn_launch0x55b7d0fd8730  via __control_var@14820x55b7d1044290
I1018 08:53:57.800011 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> cinn_launch0x55b7d1104d40  via __control_var@14840x55b7d1044870
I1018 08:53:57.800014 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> cinn_launch0x55b7d1103930  via __control_var@15000x55b7d101c8f0
I1018 08:53:57.800015 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> cinn_launch0x55b7d1102520  via __control_var@14990x55b7d101c600
I1018 08:53:57.800019 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d1101110  via __control_var@14880x55b7d10453e0
I1018 08:53:57.800020 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10ffd00  via __control_var@14890x55b7d10456b0
I1018 08:53:57.800024 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10fe8f0  via __control_var@14860x55b7d1044e10
I1018 08:53:57.800027 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d10fd4e0  via __control_var@14760x55b7d10431b0
I1018 08:53:57.800029 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> cinn_launch0x55b7d1113ec0  via __control_var@14790x55b7d1043a00
I1018 08:53:57.800031 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> cinn_launch0x55b7d1112ab0  via __control_var@14780x55b7d1043710
I1018 08:53:57.800034 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> cinn_launch0x55b7d11116a0  via __control_var@14830x55b7d1044580
I1018 08:53:57.800040 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> cinn_launch0x55b7d1110290  via __control_var@14940x55b7d101b7d0
I1018 08:53:57.800041 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> cinn_launch0x55b7d110ee80  via __control_var@15060x55b7d101da70
I1018 08:53:57.800045 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> cinn_launch0x55b7d110da70  via __control_var@14910x55b7d101af00
I1018 08:53:57.800046 4097533 graph_helper.h:104] adj conv2d0x55b7d1119e20 -> eager_deletion0x55b7d0e76930  via __control_var@16040x55b7d0e8a420
I1018 08:53:57.800050 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d110c6a0  via __control_var@14960x55b7d101bd70
I1018 08:53:57.800051 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d107f940  via __control_var@15020x55b7d101ce90
I1018 08:53:57.800055 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> cinn_launch0x55b7d107e530  via __control_var@14770x55b7d1043480
I1018 08:53:57.800056 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> cinn_launch0x55b7d107d120  via __control_var@14950x55b7d101baa0
I1018 08:53:57.800060 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d107bd10  via __control_var@14870x55b7d1045110
I1018 08:53:57.800061 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d107a890  via __control_var@14810x55b7d1043fa0
I1018 08:53:57.800065 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1011290 -> eager_deletion0x55b7d0bca520  via __control_var@15330x55b7d0c11700
I1018 08:53:57.800066 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100b430 -> eager_deletion0x55b7d0ccd6a0  via __control_var@16360x55b7d0cbdbf0
I1018 08:53:57.800069 4097533 graph_helper.h:104] adj conv2d0x55b7d106f840 -> cinn_launch0x55b7d109c290  via conv2d_19.tmp_00x55b7d109bfc0
I1018 08:53:57.800071 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> cinn_launch0x55b7d109c290  via relu_14.tmp_00x55b7d10900a0
I1018 08:53:57.800074 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> conv2d0x55b7d106f840  via relu_15.tmp_00x55b7d106f450
I1018 08:53:57.800077 4097533 graph_helper.h:104] adj conv2d0x55b7d10908e0 -> cinn_launch0x55b7d106aaf0  via conv2d_18.tmp_00x55b7d106a820
I1018 08:53:57.800079 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103c700 -> eager_deletion0x55b7d0e11740  via __control_var@16180x55b7d0de5140
I1018 08:53:57.800082 4097533 graph_helper.h:104] adj conv2d0x55b7d0fc2680 -> cinn_launch0x55b7d1126750  via conv2d_17.tmp_00x55b7d11264a0
I1018 08:53:57.800084 4097533 graph_helper.h:104] adj conv2d0x55b7d0fb56d0 -> cinn_launch0x55b7d1126750  via conv2d_16.tmp_00x55b7d0fc23b0
I1018 08:53:57.800087 4097533 graph_helper.h:104] adj conv2d0x55b7d0fc2680 -> eager_deletion0x55b7d0a126c0  via __control_var@15910x55b7d0d676f0
I1018 08:53:57.800091 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> conv2d0x55b7d0fc2680  via relu_12.tmp_00x55b7d1122b70
I1018 08:53:57.800092 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> eager_deletion0x55b7d0e04390  via __control_var@14850x55b7d1044b40
I1018 08:53:57.800096 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> cinn_launch0x55b7d1062b50  via __control_var@15110x55b7d101e840
I1018 08:53:57.800099 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fd6ee0 -> conv2d0x55b7d0fb56d0  via relu_13.tmp_00x55b7d0fb52e0
I1018 08:53:57.800102 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1014b30 -> eager_deletion0x55b7d0c4a1c0  via __control_var@16440x55b7d0c4a280
I1018 08:53:57.800104 4097533 graph_helper.h:104] adj conv2d0x55b7d1122f60 -> cinn_launch0x55b7d0fd6ee0  via conv2d_15.tmp_00x55b7d0fd6c10
I1018 08:53:57.800107 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fea800 -> conv2d0x55b7d1122f60  via relu_12.tmp_00x55b7d1122b70
I1018 08:53:57.800110 4097533 graph_helper.h:104] adj conv2d0x55b7d0d89b40 -> eager_deletion0x55b7d0ebf660  via __control_var@16070x55b7d0dd21c0
I1018 08:53:57.800112 4097533 graph_helper.h:104] adj conv2d0x55b7d11091c0 -> cinn_launch0x55b7d0fea800  via conv2d_14.tmp_00x55b7d0fea530
I1018 08:53:57.800115 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fea800  via relu_10.tmp_00x55b7d102b4b0
I1018 08:53:57.800117 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1115560 -> conv2d0x55b7d11091c0  via relu_11.tmp_00x55b7d1108dd0
I1018 08:53:57.800120 4097533 graph_helper.h:104] adj conv2d0x55b7d102bc70 -> cinn_launch0x55b7d1115560  via conv2d_13.tmp_00x55b7d11184f0
I1018 08:53:57.800123 4097533 graph_helper.h:104] adj conv2d0x55b7d0ec1350 -> eager_deletion0x55b7d0c30050  via __control_var@16100x55b7d0eacc00
I1018 08:53:57.800125 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> conv2d0x55b7d102bc70  via relu_10.tmp_00x55b7d102b4b0
I1018 08:53:57.800128 4097533 graph_helper.h:104] adj conv2d0x55b7d0d441e0 -> cinn_launch0x55b7d1031e80  via conv2d_12.tmp_00x55b7d1031bd0
I1018 08:53:57.800130 4097533 graph_helper.h:104] adj conv2d0x55b7d0e99aa0 -> cinn_launch0x55b7d1031e80  via conv2d_11.tmp_00x55b7d0d43f10
I1018 08:53:57.800133 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> conv2d0x55b7d0d441e0  via relu_8.tmp_00x55b7d1029a90
I1018 08:53:57.800135 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cb5e70 -> conv2d0x55b7d0e99aa0  via relu_9.tmp_00x55b7d0e996b0
I1018 08:53:57.800138 4097533 graph_helper.h:104] adj cinn_launch0x55b7d103b420 -> eager_deletion0x55b7d0cdb9b0  via __control_var@15830x55b7d0e5ec50
I1018 08:53:57.800140 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1047f30 -> conv2d0x55b7d1029e80  via relu_8.tmp_00x55b7d1029a90
I1018 08:53:57.800143 4097533 graph_helper.h:104] adj conv2d0x55b7d0ec1350 -> cinn_launch0x55b7d1047f30  via conv2d_9.tmp_00x55b7d1047c60
I1018 08:53:57.800145 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> cinn_launch0x55b7d1047f30  via relu_6.tmp_00x55b7d0e88b80
I1018 08:53:57.800148 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0f0e1c0 -> conv2d0x55b7d0ec1350  via relu_7.tmp_00x55b7d0e0f4e0
I1018 08:53:57.800150 4097533 graph_helper.h:104] adj conv2d0x55b7d0e89340 -> cinn_launch0x55b7d0f0e1c0  via conv2d_8.tmp_00x55b7d0f0def0
I1018 08:53:57.800153 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0add8b0 -> conv2d0x55b7d0e89340  via relu_6.tmp_00x55b7d0e88b80
I1018 08:53:57.800156 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> cinn_launch0x55b7d0ffe3f0  via __control_var@14730x55b7d1042880
I1018 08:53:57.800158 4097533 graph_helper.h:104] adj conv2d0x55b7d1119e20 -> cinn_launch0x55b7d0add8b0  via conv2d_7.tmp_00x55b7d0add600
I1018 08:53:57.800161 4097533 graph_helper.h:104] adj conv2d0x55b7d0d89b40 -> cinn_launch0x55b7d0add8b0  via conv2d_6.tmp_00x55b7d1119b50
I1018 08:53:57.800163 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> conv2d0x55b7d1119e20  via relu_4.tmp_00x55b7d111b9f0
I1018 08:53:57.800166 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0dcf300 -> conv2d0x55b7d0d89b40  via relu_5.tmp_00x55b7d0d89750
I1018 08:53:57.800168 4097533 graph_helper.h:104] adj conv2d0x55b7d0e99aa0 -> eager_deletion0x55b7d0faad50  via __control_var@15960x55b7d0d48cf0
I1018 08:53:57.800171 4097533 graph_helper.h:104] adj conv2d0x55b7d111bde0 -> cinn_launch0x55b7d0dcf300  via conv2d_5.tmp_00x55b7d0dcf030
I1018 08:53:57.800177 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c640b0 -> conv2d0x55b7d111bde0  via relu_4.tmp_00x55b7d111b9f0
I1018 08:53:57.800180 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> cinn_launch0x55b7d0c640b0  via relu_2.tmp_00x55b7d0c74b50
I1018 08:53:57.800182 4097533 graph_helper.h:104] adj conv2d0x55b7d0b70160 -> cinn_launch0x55b7d0c640b0  via conv2d_4.tmp_00x55b7d0c63de0
I1018 08:53:57.800185 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c9d5e0 -> conv2d0x55b7d0b70160  via relu_3.tmp_00x55b7d0b6fd70
I1018 08:53:57.800187 4097533 graph_helper.h:104] adj conv2d0x55b7d0c74ec0 -> cinn_launch0x55b7d0c9d5e0  via conv2d_3.tmp_00x55b7d0e7f460
I1018 08:53:57.800190 4097533 graph_helper.h:104] adj conv2d0x55b7d1029e80 -> cinn_launch0x55b7d0cb5e70  via conv2d_10.tmp_00x55b7d0cb8f30
I1018 08:53:57.800192 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0cfb320 -> conv2d0x55b7d0c74ec0  via relu_2.tmp_00x55b7d0c74b50
I1018 08:53:57.800195 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1041280 -> eager_deletion0x55b7d0caa5c0  via __control_var@16390x55b7d0c8ed20
I1018 08:53:57.800199 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1031e80 -> cinn_launch0x55b7d0fdff90  via __control_var@15050x55b7d101d7a0
I1018 08:53:57.800201 4097533 graph_helper.h:104] adj conv2d0x55b7d0edeb50 -> cinn_launch0x55b7d0cfb320  via conv2d_2.tmp_00x55b7d0cfb110
I1018 08:53:57.800204 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> cinn_launch0x55b7d0cfb320  via pool2d_0.tmp_00x55b7d0b929b0
I1018 08:53:57.800205 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0c892a0 -> conv2d0x55b7d0edeb50  via relu_1.tmp_00x55b7d0de1f20
I1018 08:53:57.800208 4097533 graph_helper.h:104] adj conv2d0x55b7d0ddf060 -> cinn_launch0x55b7d0c892a0  via conv2d_1.tmp_00x55b7d0c89030
I1018 08:53:57.800211 4097533 graph_helper.h:104] adj cinn_launch0x55b7d100ffb0 -> eager_deletion0x55b7d0f91490  via __control_var@15450x55b7d0dbab10
I1018 08:53:57.800213 4097533 graph_helper.h:104] adj cinn_launch0x55b7d0fac1b0 -> conv2d0x55b7d0ddf060  via pool2d_0.tmp_00x55b7d0b929b0
I1018 08:53:57.800216 4097533 graph_helper.h:104] adj cinn_launch0x55b7d1126750 -> conv2d0x55b7d10908e0  via relu_14.tmp_00x55b7d10900a0
I1018 08:53:57.800220 4097533 graph_helper.h:104] adj cinn_launch0x55b7d106aaf0 -> cinn_launch0x55b7d0fff800  via __control_var@15100x55b7d101e570
I1018 08:53:57.800221 4097533 graph_helper.h:104] adj conv2d0x55b7d0edf1d0 -> cinn_launch0x55b7d0fac1b0  via conv2d_0.tmp_00x55b7d0eaa280
I1018 08:53:57.804885 4097533 graph.h:183] deleting dep_vars
I1018 08:53:57.804898 4097533 graph.h:183] deleting pass_recorder
I1018 08:53:57.804901 4097533 graph.h:183] deleting skip_gc_vars
I1018 08:53:57.804904 4097533 graph.h:183] deleting stale_program_op_descs
I1018 08:53:57.804908 4097533 graph.h:183] deleting vars
I1018 08:53:57.804934 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_17.w_1, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.804944 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_15.w_1, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.804948 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_15.b_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.804957 4097533 var_handle.cc:23] deleting var handle name:conv2d_17.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.804961 4097533 var_handle.cc:23] deleting var handle name:conv2d_15.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.804970 4097533 var_handle.cc:23] deleting var handle name:relu_12.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.804975 4097533 var_handle.cc:23] deleting var handle name:assign_57.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.804978 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_14.w_1, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.804982 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_14.b_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.804991 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_14.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.804994 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_14.w_2, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805002 4097533 var_handle.cc:23] deleting var handle name:relu_4.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805006 4097533 var_handle.cc:23] deleting var handle name:assign_17.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805011 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_4.b_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805014 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_4.w_1, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805018 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_4.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805025 4097533 var_handle.cc:23] deleting var handle name:conv2d_6.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805029 4097533 var_handle.cc:23] deleting var handle name:conv2d_13.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805033 4097533 var_handle.cc:23] deleting var handle name:conv2d_13.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805040 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_6.w_1, place:Place(mlu:0), version:1, scope_idx:0
I1018 08:53:57.805044 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_4.w_2, place:Place(mlu:0), version:1, scope_idx:0
I1018 08:53:57.805049 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_4.w_1, place:Place(mlu:0), version:1, scope_idx:0
I1018 08:53:57.805054 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_3.w_2, place:Place(mlu:0), version:1, scope_idx:0
I1018 08:53:57.805059 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_3.w_1, place:Place(mlu:0), version:1, scope_idx:0
I1018 08:53:57.805063 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_2.w_2, place:Place(mlu:0), version:1, scope_idx:0
I1018 08:53:57.805073 4097533 var_handle.cc:23] deleting var handle name:relu_11.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805076 4097533 var_handle.cc:23] deleting var handle name:assign_53.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805079 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_13.b_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805083 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_13.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805087 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_13.w_1, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805091 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_13.w_2, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805097 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_8.w_2, place:Place(mlu:0), version:1, scope_idx:0
I1018 08:53:57.805102 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_8.w_1, place:Place(mlu:0), version:1, scope_idx:0
I1018 08:53:57.805106 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_5.w_2, place:Place(mlu:0), version:1, scope_idx:0
I1018 08:53:57.805111 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_5.w_1, place:Place(mlu:0), version:1, scope_idx:0
I1018 08:53:57.805116 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_7.w_2, place:Place(mlu:0), version:1, scope_idx:0
I1018 08:53:57.805120 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_7.w_1, place:Place(mlu:0), version:1, scope_idx:0
I1018 08:53:57.805125 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_6.w_2, place:Place(mlu:0), version:1, scope_idx:0
I1018 08:53:57.805130 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_12.b_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805133 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_10.w_2, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805140 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_12.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805142 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_10.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805146 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_12.w_2, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805150 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_12.w_1, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805155 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_10.w_1, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805158 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_10.b_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805162 4097533 var_handle.cc:23] deleting var handle name:linear_0.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805166 4097533 var_handle.cc:23] deleting var handle name:linear_0.b_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805171 4097533 var_handle.cc:23] deleting var handle name:conv2d_19.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805176 4097533 var_handle.cc:23] deleting var handle name:conv2d_19.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805182 4097533 var_handle.cc:23] deleting var handle name:assign_65.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805186 4097533 var_handle.cc:23] deleting var handle name:relu_14.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805191 4097533 var_handle.cc:23] deleting var handle name:assign_69.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805193 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_17.b_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805197 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_15.w_2, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805202 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_17.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805219 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_15.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805224 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_17.w_2, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805228 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_2.w_1, place:Place(mlu:0), version:1, scope_idx:0
I1018 08:53:57.805233 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_1.w_2, place:Place(mlu:0), version:1, scope_idx:0
I1018 08:53:57.805238 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_1.w_1, place:Place(mlu:0), version:1, scope_idx:0
I1018 08:53:57.805243 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_0.w_2, place:Place(mlu:0), version:1, scope_idx:0
I1018 08:53:57.805248 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_0.w_1, place:Place(mlu:0), version:1, scope_idx:0
I1018 08:53:57.805253 4097533 var_handle.cc:23] deleting var handle name:linear_0.tmp_1, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805256 4097533 var_handle.cc:23] deleting var handle name:assign_77.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805259 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_19.b_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805263 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_19.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805267 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_19.w_2, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805271 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_19.w_1, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805279 4097533 var_handle.cc:23] deleting var handle name:relu_15.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805285 4097533 var_handle.cc:23] deleting var handle name:assign_73.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805289 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_18.b_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805294 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_18.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805296 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_18.w_1, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805300 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_18.w_2, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805305 4097533 var_handle.cc:23] deleting var handle name:conv2d_18.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805310 4097533 var_handle.cc:23] deleting var handle name:conv2d_18.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805313 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_17.w_2, place:Place(mlu:0), version:1, scope_idx:0
I1018 08:53:57.805318 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_17.w_1, place:Place(mlu:0), version:1, scope_idx:0
I1018 08:53:57.805323 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_16.w_2, place:Place(mlu:0), version:1, scope_idx:0
I1018 08:53:57.805327 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_16.w_1, place:Place(mlu:0), version:1, scope_idx:0
I1018 08:53:57.805332 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_14.w_2, place:Place(mlu:0), version:1, scope_idx:0
I1018 08:53:57.805337 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_14.w_1, place:Place(mlu:0), version:1, scope_idx:0
I1018 08:53:57.805341 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_13.w_2, place:Place(mlu:0), version:1, scope_idx:0
I1018 08:53:57.805346 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_13.w_1, place:Place(mlu:0), version:1, scope_idx:0
I1018 08:53:57.805351 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1566
I1018 08:53:57.805354 4097533 var_handle.cc:23] deleting var handle name:conv2d_9.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805358 4097533 var_handle.cc:23] deleting var handle name:conv2d_9.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805362 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1489
I1018 08:53:57.805364 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1488
I1018 08:53:57.805366 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1487
I1018 08:53:57.805369 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1486
I1018 08:53:57.805371 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1485
I1018 08:53:57.805373 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1484
I1018 08:53:57.805377 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1483
I1018 08:53:57.805378 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1482
I1018 08:53:57.805380 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1481
I1018 08:53:57.805382 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1480
I1018 08:53:57.805385 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1479
I1018 08:53:57.805387 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1478
I1018 08:53:57.805389 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1477
I1018 08:53:57.805392 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1476
I1018 08:53:57.805394 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1475
I1018 08:53:57.805397 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1474
I1018 08:53:57.805398 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1473
I1018 08:53:57.805402 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1472
I1018 08:53:57.805405 4097533 var_handle.cc:23] deleting var handle name:assign_76.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805410 4097533 var_handle.cc:23] deleting var handle name:assign_72.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805414 4097533 var_handle.cc:23] deleting var handle name:assign_68.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805419 4097533 var_handle.cc:23] deleting var handle name:assign_64.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805424 4097533 var_handle.cc:23] deleting var handle name:assign_60.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805428 4097533 var_handle.cc:23] deleting var handle name:assign_56.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805433 4097533 var_handle.cc:23] deleting var handle name:assign_52.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805437 4097533 var_handle.cc:23] deleting var handle name:assign_48.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805442 4097533 var_handle.cc:23] deleting var handle name:assign_44.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805446 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1524
I1018 08:53:57.805449 4097533 var_handle.cc:23] deleting var handle name:conv2d_12.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805454 4097533 var_handle.cc:23] deleting var handle name:conv2d_10.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805460 4097533 var_handle.cc:23] deleting var handle name:assign_45.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805464 4097533 var_handle.cc:23] deleting var handle name:relu_10.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805469 4097533 var_handle.cc:23] deleting var handle name:assign_49.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805475 4097533 var_handle.cc:23] deleting var handle name:relu_8.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805480 4097533 var_handle.cc:23] deleting var handle name:assign_37.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805483 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_9.w_1, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805487 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_9.b_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805491 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_9.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805495 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_9.w_2, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805498 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1511
I1018 08:53:57.805501 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1510
I1018 08:53:57.805503 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1509
I1018 08:53:57.805505 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1508
I1018 08:53:57.805508 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1507
I1018 08:53:57.805510 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1506
I1018 08:53:57.805512 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1505
I1018 08:53:57.805514 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1504
I1018 08:53:57.805517 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1503
I1018 08:53:57.805519 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1502
I1018 08:53:57.805521 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1501
I1018 08:53:57.805524 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1500
I1018 08:53:57.805526 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1499
I1018 08:53:57.805528 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1498
I1018 08:53:57.805532 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1497
I1018 08:53:57.805534 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1496
I1018 08:53:57.805537 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1495
I1018 08:53:57.805539 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1494
I1018 08:53:57.805541 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1493
I1018 08:53:57.805544 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1492
I1018 08:53:57.805546 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1491
I1018 08:53:57.805548 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1490
I1018 08:53:57.805552 4097533 var_handle.cc:23] deleting var handle name:assign_40.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805557 4097533 var_handle.cc:23] deleting var handle name:assign_36.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805562 4097533 var_handle.cc:23] deleting var handle name:assign_32.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805565 4097533 var_handle.cc:23] deleting var handle name:assign_28.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805570 4097533 var_handle.cc:23] deleting var handle name:assign_24.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805574 4097533 var_handle.cc:23] deleting var handle name:assign_20.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805579 4097533 var_handle.cc:23] deleting var handle name:assign_16.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805583 4097533 var_handle.cc:23] deleting var handle name:assign_12.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805588 4097533 var_handle.cc:23] deleting var handle name:assign_8.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805593 4097533 var_handle.cc:23] deleting var handle name:assign_4.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805596 4097533 var_handle.cc:23] deleting var handle name:assign_0.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805601 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_19.w_2, place:Place(mlu:0), version:1, scope_idx:0
I1018 08:53:57.805606 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_19.w_1, place:Place(mlu:0), version:1, scope_idx:0
I1018 08:53:57.805611 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_18.w_2, place:Place(mlu:0), version:1, scope_idx:0
I1018 08:53:57.805616 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_18.w_1, place:Place(mlu:0), version:1, scope_idx:0
I1018 08:53:57.805620 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_15.w_2, place:Place(mlu:0), version:1, scope_idx:0
I1018 08:53:57.805625 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_15.w_1, place:Place(mlu:0), version:1, scope_idx:0
I1018 08:53:57.805630 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1557
I1018 08:53:57.805634 4097533 var_handle.cc:23] deleting var handle name:conv2d_14.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805637 4097533 var_handle.cc:23] deleting var handle name:conv2d_14.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805641 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1599
I1018 08:53:57.805644 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_10.w_2, place:Place(mlu:0), version:1, scope_idx:0
I1018 08:53:57.805649 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_10.w_1, place:Place(mlu:0), version:1, scope_idx:0
I1018 08:53:57.805653 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_12.w_2, place:Place(mlu:0), version:1, scope_idx:0
I1018 08:53:57.805658 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_12.w_1, place:Place(mlu:0), version:1, scope_idx:0
I1018 08:53:57.805663 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_11.w_2, place:Place(mlu:0), version:1, scope_idx:0
I1018 08:53:57.805670 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_11.w_1, place:Place(mlu:0), version:1, scope_idx:0
I1018 08:53:57.805673 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_9.w_2, place:Place(mlu:0), version:1, scope_idx:0
I1018 08:53:57.805678 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_9.w_1, place:Place(mlu:0), version:1, scope_idx:0
I1018 08:53:57.805683 4097533 var_handle.cc:23] deleting var handle name:conv2d_15.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805687 4097533 var_handle.cc:23] deleting var handle name:conv2d_16.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805694 4097533 var_handle.cc:23] deleting var handle name:conv2d_16.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805698 4097533 var_handle.cc:23] deleting var handle name:conv2d_17.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805706 4097533 var_handle.cc:23] deleting var handle name:relu_13.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805711 4097533 var_handle.cc:23] deleting var handle name:assign_61.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805713 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_16.b_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805717 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_16.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805721 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_16.w_1, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805725 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_16.w_2, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805732 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_0.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805737 4097533 var_handle.cc:23] deleting var handle name:conv2d_8.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805740 4097533 var_handle.cc:23] deleting var handle name:conv2d_8.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805744 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1614
I1018 08:53:57.805759 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1579
I1018 08:53:57.805763 4097533 var_handle.cc:23] deleting var handle name:conv2d_6.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805766 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1610
I1018 08:53:57.805768 4097533 var_handle.cc:23] deleting var handle name:conv2d_2.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805773 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_0.w_1, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805778 4097533 var_handle.cc:23] deleting var handle name:conv2d_0.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805781 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1571
I1018 08:53:57.805786 4097533 var_handle.cc:23] deleting var handle name:relu_9.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805790 4097533 var_handle.cc:23] deleting var handle name:assign_41.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805794 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_11.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805799 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_11.b_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805802 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_11.w_2, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805806 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_11.w_1, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805810 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1556
I1018 08:53:57.805814 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1563
I1018 08:53:57.805817 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1604
I1018 08:53:57.805823 4097533 var_handle.cc:23] deleting var handle name:assign_25.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805827 4097533 var_handle.cc:23] deleting var handle name:relu_6.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805831 4097533 var_handle.cc:23] deleting var handle name:assign_29.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805835 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_7.b_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805840 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_5.w_2, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805843 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_7.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805847 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_5.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805851 4097533 var_handle.cc:23] deleting var handle name:conv2d_4.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805855 4097533 var_handle.cc:23] deleting var handle name:conv2d_0.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805859 4097533 var_handle.cc:23] deleting var handle name:conv2d_3.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805863 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_0.w_2, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805867 4097533 var_handle.cc:23] deleting var handle name:_jst.0.x.0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805872 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1527
I1018 08:53:57.805876 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1601
I1018 08:53:57.805877 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_0.b_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805881 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1583
I1018 08:53:57.805884 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1611
I1018 08:53:57.805886 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_6.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805891 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_6.w_2, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805894 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_2.w_2, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805898 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_4.w_2, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805902 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_1.w_1, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805907 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_1.b_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805912 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1554
I1018 08:53:57.805913 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1608
I1018 08:53:57.805917 4097533 var_handle.cc:23] deleting var handle name:relu_7.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805920 4097533 var_handle.cc:23] deleting var handle name:assign_33.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805924 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_8.b_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805928 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_8.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805932 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_8.w_1, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805935 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_8.w_2, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805940 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1531
I1018 08:53:57.805946 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1618
I1018 08:53:57.805948 4097533 var_handle.cc:23] deleting var handle name:relu_1.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805953 4097533 var_handle.cc:23] deleting var handle name:assign_5.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805958 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_1.w_2, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805965 4097533 var_handle.cc:23] deleting var handle name:assign_9.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805969 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_2.w_1, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805974 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_2.b_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805977 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_2.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805981 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1619
I1018 08:53:57.805984 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1607
I1018 08:53:57.805989 4097533 var_handle.cc:23] deleting var handle name:conv2d_5.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805992 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1621
I1018 08:53:57.805994 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_7.w_2, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.805999 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_7.w_1, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.806002 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_5.w_1, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.806006 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_5.b_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.806010 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1551
I1018 08:53:57.806012 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1545
I1018 08:53:57.806015 4097533 var_handle.cc:23] deleting var handle name:conv2d_1.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.806020 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1536
I1018 08:53:57.806022 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1623
I1018 08:53:57.806025 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1592
I1018 08:53:57.806027 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1624
I1018 08:53:57.806030 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1577
I1018 08:53:57.806035 4097533 var_handle.cc:23] deleting var handle name:relu_5.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.806039 4097533 var_handle.cc:23] deleting var handle name:assign_21.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.806043 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_6.b_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.806047 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_6.w_1, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.806051 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_1.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.806056 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1519
I1018 08:53:57.806058 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1528
I1018 08:53:57.806061 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1591
I1018 08:53:57.806063 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1626
I1018 08:53:57.806066 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1589
I1018 08:53:57.806067 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1628
I1018 08:53:57.806070 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1596
I1018 08:53:57.806079 4097533 var_handle.cc:23] deleting var handle name:conv2d_11.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.806083 4097533 var_handle.cc:23] deleting var handle name:conv2d_12.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.806087 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1629
I1018 08:53:57.806090 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1594
I1018 08:53:57.806092 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1568
I1018 08:53:57.806095 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1581
I1018 08:53:57.806097 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1586
I1018 08:53:57.806100 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1631
I1018 08:53:57.806102 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1584
I1018 08:53:57.806104 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1633
I1018 08:53:57.806109 4097533 var_handle.cc:23] deleting var handle name:conv2d_2.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.806113 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1597
I1018 08:53:57.806115 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1634
I1018 08:53:57.806118 4097533 var_handle.cc:23] deleting var handle name:conv2d_7.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.806123 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1605
I1018 08:53:57.806125 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1602
I1018 08:53:57.806128 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1636
I1018 08:53:57.806130 4097533 var_handle.cc:23] deleting var handle name:conv2d_10.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.806134 4097533 var_handle.cc:23] deleting var handle name:conv2d_11.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.806139 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1573
I1018 08:53:57.806141 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1637
I1018 08:53:57.806145 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1534
I1018 08:53:57.806149 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1639
I1018 08:53:57.806152 4097533 var_handle.cc:23] deleting var handle name:conv2d_1.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.806156 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_3.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.806160 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_3.w_1, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.806164 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_3.w_2, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.806169 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1640
I1018 08:53:57.806175 4097533 var_handle.cc:23] deleting var handle name:relu_2.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.806180 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1642
I1018 08:53:57.806183 4097533 var_handle.cc:23] deleting var handle name:conv2d_4.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.806188 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1575
I1018 08:53:57.806190 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1644
I1018 08:53:57.806193 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1587
I1018 08:53:57.806196 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1645
I1018 08:53:57.806200 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1613
I1018 08:53:57.806201 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1533
I1018 08:53:57.806205 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1647
I1018 08:53:57.806206 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1570
I1018 08:53:57.806211 4097533 var_handle.cc:23] deleting var handle name:conv2d_3.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.806216 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1649
I1018 08:53:57.806218 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1650
I1018 08:53:57.806221 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1522
I1018 08:53:57.806223 4097533 var_handle.cc:23] deleting var handle name:assign_1.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.806227 4097533 var_handle.cc:23] deleting var handle name:pool2d_0.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.806231 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1521
I1018 08:53:57.806234 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1562
I1018 08:53:57.806239 4097533 var_handle.cc:23] deleting var handle name:relu_3.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.806244 4097533 var_handle.cc:23] deleting var handle name:assign_13.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.806248 4097533 var_handle.cc:23] deleting var handle name:batch_norm2d_3.b_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.806252 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1560
I1018 08:53:57.806254 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1616
I1018 08:53:57.806257 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1530
I1018 08:53:57.806259 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1559
I1018 08:53:57.806263 4097533 var_handle.cc:23] deleting var handle name:conv2d_7.tmp_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.806267 4097533 var_handle.cc:23] deleting var handle name:conv2d_5.w_0, place:Place(mlu:0), version:0, scope_idx:0
I1018 08:53:57.806272 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1565
I1018 08:53:57.806275 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1537
I1018 08:53:57.806278 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1549
I1018 08:53:57.806280 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1546
I1018 08:53:57.806283 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1525
I1018 08:53:57.806286 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1543
I1018 08:53:57.806288 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1542
I1018 08:53:57.806291 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1548
I1018 08:53:57.806293 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1540
I1018 08:53:57.806295 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1539
I1018 08:53:57.806298 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1513
I1018 08:53:57.806301 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1552
I1018 08:53:57.806303 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1514
I1018 08:53:57.806306 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1516
I1018 08:53:57.806309 4097533 var_handle.cc:35] deleting dummy var handle __control_var@1518
I1018 08:53:57.810904 4097533 var_desc.cc:415] Flush  conv2d_0.tmp_0 1
I1018 08:53:57.810917 4097533 var_desc.cc:415] Flush  conv2d_0.tmp_0 0
I1018 08:53:57.810923 4097533 var_desc.cc:415] Flush  assign_1.tmp_0 1
I1018 08:53:57.810926 4097533 var_desc.cc:415] Flush  pool2d_0.tmp_0 1
I1018 08:53:57.810930 4097533 var_desc.cc:415] Flush  pool2d_0.tmp_0 0
I1018 08:53:57.810932 4097533 var_desc.cc:415] Flush  conv2d_1.tmp_0 1
I1018 08:53:57.810935 4097533 var_desc.cc:415] Flush  conv2d_1.tmp_0 0
I1018 08:53:57.810940 4097533 var_desc.cc:415] Flush  assign_5.tmp_0 1
I1018 08:53:57.810941 4097533 var_desc.cc:415] Flush  relu_1.tmp_0 1
I1018 08:53:57.810945 4097533 var_desc.cc:415] Flush  relu_1.tmp_0 0
I1018 08:53:57.810947 4097533 var_desc.cc:415] Flush  conv2d_2.tmp_0 1
I1018 08:53:57.810956 4097533 var_desc.cc:415] Flush  conv2d_2.tmp_0 0
I1018 08:53:57.810959 4097533 var_desc.cc:415] Flush  pool2d_0.tmp_0 0
I1018 08:53:57.810962 4097533 var_desc.cc:415] Flush  assign_9.tmp_0 1
I1018 08:53:57.810964 4097533 var_desc.cc:415] Flush  relu_2.tmp_0 1
I1018 08:53:57.810967 4097533 var_desc.cc:415] Flush  relu_2.tmp_0 0
I1018 08:53:57.810971 4097533 var_desc.cc:415] Flush  conv2d_3.tmp_0 1
I1018 08:53:57.810972 4097533 var_desc.cc:415] Flush  conv2d_3.tmp_0 0
I1018 08:53:57.810976 4097533 var_desc.cc:415] Flush  assign_13.tmp_0 1
I1018 08:53:57.810979 4097533 var_desc.cc:415] Flush  relu_3.tmp_0 1
I1018 08:53:57.810982 4097533 var_desc.cc:415] Flush  relu_3.tmp_0 0
I1018 08:53:57.810984 4097533 var_desc.cc:415] Flush  conv2d_4.tmp_0 1
I1018 08:53:57.810987 4097533 var_desc.cc:415] Flush  relu_2.tmp_0 0
I1018 08:53:57.810990 4097533 var_desc.cc:415] Flush  conv2d_4.tmp_0 0
I1018 08:53:57.810993 4097533 var_desc.cc:415] Flush  assign_17.tmp_0 1
I1018 08:53:57.810997 4097533 var_desc.cc:415] Flush  relu_4.tmp_0 1
I1018 08:53:57.810999 4097533 var_desc.cc:415] Flush  relu_4.tmp_0 0
I1018 08:53:57.811002 4097533 var_desc.cc:415] Flush  conv2d_5.tmp_0 1
I1018 08:53:57.811004 4097533 var_desc.cc:415] Flush  conv2d_5.tmp_0 0
I1018 08:53:57.811008 4097533 var_desc.cc:415] Flush  assign_21.tmp_0 1
I1018 08:53:57.811010 4097533 var_desc.cc:415] Flush  relu_5.tmp_0 1
I1018 08:53:57.811013 4097533 var_desc.cc:415] Flush  relu_5.tmp_0 0
I1018 08:53:57.811017 4097533 var_desc.cc:415] Flush  conv2d_6.tmp_0 1
I1018 08:53:57.811018 4097533 var_desc.cc:415] Flush  relu_4.tmp_0 0
I1018 08:53:57.811021 4097533 var_desc.cc:415] Flush  conv2d_7.tmp_0 1
I1018 08:53:57.811023 4097533 var_desc.cc:415] Flush  conv2d_7.tmp_0 0
I1018 08:53:57.811028 4097533 var_desc.cc:415] Flush  conv2d_6.tmp_0 0
I1018 08:53:57.811031 4097533 var_desc.cc:415] Flush  assign_29.tmp_0 1
I1018 08:53:57.811033 4097533 var_desc.cc:415] Flush  relu_6.tmp_0 1
I1018 08:53:57.811035 4097533 var_desc.cc:415] Flush  assign_25.tmp_0 1
I1018 08:53:57.811039 4097533 var_desc.cc:415] Flush  relu_6.tmp_0 0
I1018 08:53:57.811041 4097533 var_desc.cc:415] Flush  conv2d_8.tmp_0 1
I1018 08:53:57.811044 4097533 var_desc.cc:415] Flush  conv2d_8.tmp_0 0
I1018 08:53:57.811048 4097533 var_desc.cc:415] Flush  assign_33.tmp_0 1
I1018 08:53:57.811050 4097533 var_desc.cc:415] Flush  relu_7.tmp_0 1
I1018 08:53:57.811053 4097533 var_desc.cc:415] Flush  relu_7.tmp_0 0
I1018 08:53:57.811055 4097533 var_desc.cc:415] Flush  conv2d_9.tmp_0 1
I1018 08:53:57.811057 4097533 var_desc.cc:415] Flush  conv2d_9.tmp_0 0
I1018 08:53:57.811061 4097533 var_desc.cc:415] Flush  relu_6.tmp_0 0
I1018 08:53:57.811064 4097533 var_desc.cc:415] Flush  assign_37.tmp_0 1
I1018 08:53:57.811066 4097533 var_desc.cc:415] Flush  relu_8.tmp_0 1
I1018 08:53:57.811069 4097533 var_desc.cc:415] Flush  relu_8.tmp_0 0
I1018 08:53:57.811071 4097533 var_desc.cc:415] Flush  conv2d_10.tmp_0 1
I1018 08:53:57.811074 4097533 var_desc.cc:415] Flush  conv2d_10.tmp_0 0
I1018 08:53:57.811079 4097533 var_desc.cc:415] Flush  assign_41.tmp_0 1
I1018 08:53:57.811081 4097533 var_desc.cc:415] Flush  relu_9.tmp_0 1
I1018 08:53:57.811084 4097533 var_desc.cc:415] Flush  relu_9.tmp_0 0
I1018 08:53:57.811086 4097533 var_desc.cc:415] Flush  conv2d_11.tmp_0 1
I1018 08:53:57.811089 4097533 var_desc.cc:415] Flush  relu_8.tmp_0 0
I1018 08:53:57.811091 4097533 var_desc.cc:415] Flush  conv2d_12.tmp_0 1
I1018 08:53:57.811094 4097533 var_desc.cc:415] Flush  conv2d_12.tmp_0 0
I1018 08:53:57.811100 4097533 var_desc.cc:415] Flush  conv2d_11.tmp_0 0
I1018 08:53:57.811101 4097533 var_desc.cc:415] Flush  assign_49.tmp_0 1
I1018 08:53:57.811105 4097533 var_desc.cc:415] Flush  relu_10.tmp_0 1
I1018 08:53:57.811106 4097533 var_desc.cc:415] Flush  assign_45.tmp_0 1
I1018 08:53:57.811110 4097533 var_desc.cc:415] Flush  relu_10.tmp_0 0
I1018 08:53:57.811112 4097533 var_desc.cc:415] Flush  conv2d_13.tmp_0 1
I1018 08:53:57.811115 4097533 var_desc.cc:415] Flush  conv2d_13.tmp_0 0
I1018 08:53:57.811118 4097533 var_desc.cc:415] Flush  assign_53.tmp_0 1
I1018 08:53:57.811125 4097533 var_desc.cc:415] Flush  relu_11.tmp_0 1
I1018 08:53:57.811128 4097533 var_desc.cc:415] Flush  relu_11.tmp_0 0
I1018 08:53:57.811131 4097533 var_desc.cc:415] Flush  conv2d_14.tmp_0 1
I1018 08:53:57.811133 4097533 var_desc.cc:415] Flush  conv2d_14.tmp_0 0
I1018 08:53:57.811137 4097533 var_desc.cc:415] Flush  relu_10.tmp_0 0
I1018 08:53:57.811141 4097533 var_desc.cc:415] Flush  assign_57.tmp_0 1
I1018 08:53:57.811142 4097533 var_desc.cc:415] Flush  relu_12.tmp_0 1
I1018 08:53:57.811146 4097533 var_desc.cc:415] Flush  relu_12.tmp_0 0
I1018 08:53:57.811147 4097533 var_desc.cc:415] Flush  conv2d_15.tmp_0 1
I1018 08:53:57.811151 4097533 var_desc.cc:415] Flush  conv2d_15.tmp_0 0
I1018 08:53:57.811154 4097533 var_desc.cc:415] Flush  assign_61.tmp_0 1
I1018 08:53:57.811156 4097533 var_desc.cc:415] Flush  relu_13.tmp_0 1
I1018 08:53:57.811159 4097533 var_desc.cc:415] Flush  relu_13.tmp_0 0
I1018 08:53:57.811161 4097533 var_desc.cc:415] Flush  conv2d_16.tmp_0 1
I1018 08:53:57.811164 4097533 var_desc.cc:415] Flush  relu_12.tmp_0 0
I1018 08:53:57.811167 4097533 var_desc.cc:415] Flush  conv2d_17.tmp_0 1
I1018 08:53:57.811169 4097533 var_desc.cc:415] Flush  conv2d_17.tmp_0 0
I1018 08:53:57.811174 4097533 var_desc.cc:415] Flush  conv2d_16.tmp_0 0
I1018 08:53:57.811177 4097533 var_desc.cc:415] Flush  assign_69.tmp_0 1
I1018 08:53:57.811179 4097533 var_desc.cc:415] Flush  relu_14.tmp_0 1
I1018 08:53:57.811182 4097533 var_desc.cc:415] Flush  assign_65.tmp_0 1
I1018 08:53:57.811184 4097533 var_desc.cc:415] Flush  relu_14.tmp_0 0
I1018 08:53:57.811187 4097533 var_desc.cc:415] Flush  conv2d_18.tmp_0 1
I1018 08:53:57.811189 4097533 var_desc.cc:415] Flush  conv2d_18.tmp_0 0
I1018 08:53:57.811193 4097533 var_desc.cc:415] Flush  assign_73.tmp_0 1
I1018 08:53:57.811195 4097533 var_desc.cc:415] Flush  relu_15.tmp_0 1
I1018 08:53:57.811198 4097533 var_desc.cc:415] Flush  relu_15.tmp_0 0
I1018 08:53:57.811200 4097533 var_desc.cc:415] Flush  conv2d_19.tmp_0 1
I1018 08:53:57.811203 4097533 var_desc.cc:415] Flush  conv2d_19.tmp_0 0
I1018 08:53:57.811208 4097533 var_desc.cc:415] Flush  relu_14.tmp_0 0
I1018 08:53:57.811210 4097533 var_desc.cc:415] Flush  assign_77.tmp_0 1
I1018 08:53:57.811223 4097533 var_desc.cc:415] Flush  assign_0.tmp_0 1
I1018 08:53:57.811226 4097533 var_desc.cc:415] Flush  assign_4.tmp_0 1
I1018 08:53:57.811228 4097533 var_desc.cc:415] Flush  assign_8.tmp_0 1
I1018 08:53:57.811230 4097533 var_desc.cc:415] Flush  assign_12.tmp_0 1
I1018 08:53:57.811234 4097533 var_desc.cc:415] Flush  assign_16.tmp_0 1
I1018 08:53:57.811236 4097533 var_desc.cc:415] Flush  assign_20.tmp_0 1
I1018 08:53:57.811239 4097533 var_desc.cc:415] Flush  assign_24.tmp_0 1
I1018 08:53:57.811241 4097533 var_desc.cc:415] Flush  assign_28.tmp_0 1
I1018 08:53:57.811244 4097533 var_desc.cc:415] Flush  assign_32.tmp_0 1
I1018 08:53:57.811246 4097533 var_desc.cc:415] Flush  assign_36.tmp_0 1
I1018 08:53:57.811249 4097533 var_desc.cc:415] Flush  assign_40.tmp_0 1
I1018 08:53:57.811250 4097533 var_desc.cc:415] Flush  assign_44.tmp_0 1
I1018 08:53:57.811254 4097533 var_desc.cc:415] Flush  assign_48.tmp_0 1
I1018 08:53:57.811255 4097533 var_desc.cc:415] Flush  assign_52.tmp_0 1
I1018 08:53:57.811259 4097533 var_desc.cc:415] Flush  assign_56.tmp_0 1
I1018 08:53:57.811260 4097533 var_desc.cc:415] Flush  assign_60.tmp_0 1
I1018 08:53:57.811264 4097533 var_desc.cc:415] Flush  assign_64.tmp_0 1
I1018 08:53:57.811265 4097533 var_desc.cc:415] Flush  assign_68.tmp_0 1
I1018 08:53:57.811267 4097533 var_desc.cc:415] Flush  assign_72.tmp_0 1
I1018 08:53:57.811270 4097533 var_desc.cc:415] Flush  assign_76.tmp_0 1
I1018 08:53:57.812562 4097533 block_desc.cc:205] vars in desc 180
I1018 08:53:57.812575 4097533 block_desc.cc:209] Flush assign_72.tmp_0
I1018 08:53:57.812578 4097533 var_desc.cc:415] Flush  assign_72.tmp_0 1
I1018 08:53:57.812582 4097533 block_desc.cc:209] Flush assign_68.tmp_0
I1018 08:53:57.812583 4097533 var_desc.cc:415] Flush  assign_68.tmp_0 1
I1018 08:53:57.812587 4097533 block_desc.cc:209] Flush assign_64.tmp_0
I1018 08:53:57.812588 4097533 var_desc.cc:415] Flush  assign_64.tmp_0 1
I1018 08:53:57.812595 4097533 block_desc.cc:209] Flush assign_60.tmp_0
I1018 08:53:57.812597 4097533 var_desc.cc:415] Flush  assign_60.tmp_0 1
I1018 08:53:57.812600 4097533 block_desc.cc:209] Flush assign_52.tmp_0
I1018 08:53:57.812602 4097533 var_desc.cc:415] Flush  assign_52.tmp_0 1
I1018 08:53:57.812605 4097533 block_desc.cc:209] Flush assign_44.tmp_0
I1018 08:53:57.812608 4097533 var_desc.cc:415] Flush  assign_44.tmp_0 1
I1018 08:53:57.812609 4097533 block_desc.cc:209] Flush assign_40.tmp_0
I1018 08:53:57.812611 4097533 var_desc.cc:415] Flush  assign_40.tmp_0 1
I1018 08:53:57.812614 4097533 block_desc.cc:209] Flush assign_36.tmp_0
I1018 08:53:57.812616 4097533 var_desc.cc:415] Flush  assign_36.tmp_0 1
I1018 08:53:57.812618 4097533 block_desc.cc:209] Flush assign_32.tmp_0
I1018 08:53:57.812621 4097533 var_desc.cc:415] Flush  assign_32.tmp_0 1
I1018 08:53:57.812623 4097533 block_desc.cc:209] Flush assign_28.tmp_0
I1018 08:53:57.812625 4097533 var_desc.cc:415] Flush  assign_28.tmp_0 1
I1018 08:53:57.812628 4097533 block_desc.cc:209] Flush assign_24.tmp_0
I1018 08:53:57.812630 4097533 var_desc.cc:415] Flush  assign_24.tmp_0 1
I1018 08:53:57.812633 4097533 block_desc.cc:209] Flush assign_20.tmp_0
I1018 08:53:57.812634 4097533 var_desc.cc:415] Flush  assign_20.tmp_0 1
I1018 08:53:57.812637 4097533 block_desc.cc:209] Flush assign_16.tmp_0
I1018 08:53:57.812639 4097533 var_desc.cc:415] Flush  assign_16.tmp_0 1
I1018 08:53:57.812641 4097533 block_desc.cc:209] Flush assign_12.tmp_0
I1018 08:53:57.812644 4097533 var_desc.cc:415] Flush  assign_12.tmp_0 1
I1018 08:53:57.812646 4097533 block_desc.cc:209] Flush assign_8.tmp_0
I1018 08:53:57.812649 4097533 var_desc.cc:415] Flush  assign_8.tmp_0 1
I1018 08:53:57.812651 4097533 block_desc.cc:209] Flush batch_norm2d_19.w_2
I1018 08:53:57.812654 4097533 var_desc.cc:415] Flush  batch_norm2d_19.w_2 1
I1018 08:53:57.812657 4097533 block_desc.cc:209] Flush batch_norm2d_18.w_2
I1018 08:53:57.812659 4097533 var_desc.cc:415] Flush  batch_norm2d_18.w_2 1
I1018 08:53:57.812662 4097533 block_desc.cc:209] Flush batch_norm2d_18.w_1
I1018 08:53:57.812664 4097533 var_desc.cc:415] Flush  batch_norm2d_18.w_1 1
I1018 08:53:57.812666 4097533 block_desc.cc:209] Flush batch_norm2d_15.w_2
I1018 08:53:57.812669 4097533 var_desc.cc:415] Flush  batch_norm2d_15.w_2 1
I1018 08:53:57.812671 4097533 block_desc.cc:209] Flush batch_norm2d_15.w_1
I1018 08:53:57.812673 4097533 var_desc.cc:415] Flush  batch_norm2d_15.w_1 1
I1018 08:53:57.812676 4097533 block_desc.cc:209] Flush batch_norm2d_17.w_2
I1018 08:53:57.812678 4097533 var_desc.cc:415] Flush  batch_norm2d_17.w_2 1
I1018 08:53:57.812681 4097533 block_desc.cc:209] Flush batch_norm2d_17.w_1
I1018 08:53:57.812683 4097533 var_desc.cc:415] Flush  batch_norm2d_17.w_1 1
I1018 08:53:57.812685 4097533 block_desc.cc:209] Flush batch_norm2d_16.w_2
I1018 08:53:57.812687 4097533 var_desc.cc:415] Flush  batch_norm2d_16.w_2 1
I1018 08:53:57.812690 4097533 block_desc.cc:209] Flush batch_norm2d_16.w_1
I1018 08:53:57.812692 4097533 var_desc.cc:415] Flush  batch_norm2d_16.w_1 1
I1018 08:53:57.812695 4097533 block_desc.cc:209] Flush batch_norm2d_14.w_2
I1018 08:53:57.812697 4097533 var_desc.cc:415] Flush  batch_norm2d_14.w_2 1
I1018 08:53:57.812700 4097533 block_desc.cc:209] Flush batch_norm2d_14.w_1
I1018 08:53:57.812701 4097533 var_desc.cc:415] Flush  batch_norm2d_14.w_1 1
I1018 08:53:57.812705 4097533 block_desc.cc:209] Flush batch_norm2d_13.w_2
I1018 08:53:57.812706 4097533 var_desc.cc:415] Flush  batch_norm2d_13.w_2 1
I1018 08:53:57.812709 4097533 block_desc.cc:209] Flush batch_norm2d_13.w_1
I1018 08:53:57.812711 4097533 var_desc.cc:415] Flush  batch_norm2d_13.w_1 1
I1018 08:53:57.812714 4097533 block_desc.cc:209] Flush batch_norm2d_10.w_2
I1018 08:53:57.812716 4097533 var_desc.cc:415] Flush  batch_norm2d_10.w_2 1
I1018 08:53:57.812718 4097533 block_desc.cc:209] Flush batch_norm2d_12.w_2
I1018 08:53:57.812721 4097533 var_desc.cc:415] Flush  batch_norm2d_12.w_2 1
I1018 08:53:57.812723 4097533 block_desc.cc:209] Flush batch_norm2d_12.w_1
I1018 08:53:57.812727 4097533 var_desc.cc:415] Flush  batch_norm2d_12.w_1 1
I1018 08:53:57.812731 4097533 block_desc.cc:209] Flush batch_norm2d_9.w_2
I1018 08:53:57.812732 4097533 var_desc.cc:415] Flush  batch_norm2d_9.w_2 1
I1018 08:53:57.812736 4097533 block_desc.cc:209] Flush batch_norm2d_9.w_1
I1018 08:53:57.812737 4097533 var_desc.cc:415] Flush  batch_norm2d_9.w_1 1
I1018 08:53:57.812740 4097533 block_desc.cc:209] Flush batch_norm2d_8.w_2
I1018 08:53:57.812742 4097533 var_desc.cc:415] Flush  batch_norm2d_8.w_2 1
I1018 08:53:57.812745 4097533 block_desc.cc:209] Flush batch_norm2d_8.w_1
I1018 08:53:57.812747 4097533 var_desc.cc:415] Flush  batch_norm2d_8.w_1 1
I1018 08:53:57.812749 4097533 block_desc.cc:209] Flush batch_norm2d_5.w_2
I1018 08:53:57.812752 4097533 var_desc.cc:415] Flush  batch_norm2d_5.w_2 1
I1018 08:53:57.812754 4097533 block_desc.cc:209] Flush batch_norm2d_5.w_1
I1018 08:53:57.812757 4097533 var_desc.cc:415] Flush  batch_norm2d_5.w_1 1
I1018 08:53:57.812759 4097533 block_desc.cc:209] Flush batch_norm2d_7.w_2
I1018 08:53:57.812762 4097533 var_desc.cc:415] Flush  batch_norm2d_7.w_2 1
I1018 08:53:57.812763 4097533 block_desc.cc:209] Flush batch_norm2d_7.w_1
I1018 08:53:57.812767 4097533 var_desc.cc:415] Flush  batch_norm2d_7.w_1 1
I1018 08:53:57.812768 4097533 block_desc.cc:209] Flush batch_norm2d_6.w_2
I1018 08:53:57.812770 4097533 var_desc.cc:415] Flush  batch_norm2d_6.w_2 1
I1018 08:53:57.812773 4097533 block_desc.cc:209] Flush batch_norm2d_6.w_1
I1018 08:53:57.812775 4097533 var_desc.cc:415] Flush  batch_norm2d_6.w_1 1
I1018 08:53:57.812778 4097533 block_desc.cc:209] Flush batch_norm2d_4.w_1
I1018 08:53:57.812780 4097533 var_desc.cc:415] Flush  batch_norm2d_4.w_1 1
I1018 08:53:57.812783 4097533 block_desc.cc:209] Flush batch_norm2d_3.w_2
I1018 08:53:57.812784 4097533 var_desc.cc:415] Flush  batch_norm2d_3.w_2 1
I1018 08:53:57.812788 4097533 block_desc.cc:209] Flush assign_76.tmp_0
I1018 08:53:57.812789 4097533 var_desc.cc:415] Flush  assign_76.tmp_0 1
I1018 08:53:57.812793 4097533 block_desc.cc:209] Flush batch_norm2d_2.w_2
I1018 08:53:57.812794 4097533 var_desc.cc:415] Flush  batch_norm2d_2.w_2 1
I1018 08:53:57.812798 4097533 block_desc.cc:209] Flush batch_norm2d_2.w_1
I1018 08:53:57.812799 4097533 var_desc.cc:415] Flush  batch_norm2d_2.w_1 1
I1018 08:53:57.812801 4097533 block_desc.cc:209] Flush batch_norm2d_1.w_2
I1018 08:53:57.812803 4097533 var_desc.cc:415] Flush  batch_norm2d_1.w_2 1
I1018 08:53:57.812806 4097533 block_desc.cc:209] Flush batch_norm2d_1.w_1
I1018 08:53:57.812808 4097533 var_desc.cc:415] Flush  batch_norm2d_1.w_1 1
I1018 08:53:57.812811 4097533 block_desc.cc:209] Flush batch_norm2d_0.w_2
I1018 08:53:57.812813 4097533 var_desc.cc:415] Flush  batch_norm2d_0.w_2 1
I1018 08:53:57.812816 4097533 block_desc.cc:209] Flush batch_norm2d_0.w_1
I1018 08:53:57.812819 4097533 var_desc.cc:415] Flush  batch_norm2d_0.w_1 1
I1018 08:53:57.812820 4097533 block_desc.cc:209] Flush linear_0.tmp_1
I1018 08:53:57.812824 4097533 var_desc.cc:415] Flush  linear_0.tmp_1 1
I1018 08:53:57.812825 4097533 block_desc.cc:209] Flush assign_77.tmp_0
I1018 08:53:57.812827 4097533 var_desc.cc:415] Flush  assign_77.tmp_0 1
I1018 08:53:57.812830 4097533 block_desc.cc:209] Flush batch_norm2d_19.w_0
I1018 08:53:57.812832 4097533 var_desc.cc:415] Flush  batch_norm2d_19.w_0 1
I1018 08:53:57.812835 4097533 block_desc.cc:209] Flush batch_norm2d_19.w_1
I1018 08:53:57.812837 4097533 var_desc.cc:415] Flush  batch_norm2d_19.w_1 1
I1018 08:53:57.812840 4097533 block_desc.cc:209] Flush linear_0.w_0
I1018 08:53:57.812842 4097533 var_desc.cc:415] Flush  linear_0.w_0 1
I1018 08:53:57.812845 4097533 block_desc.cc:209] Flush conv2d_19.tmp_0
I1018 08:53:57.812847 4097533 var_desc.cc:415] Flush  conv2d_19.tmp_0 1
I1018 08:53:57.812849 4097533 block_desc.cc:209] Flush conv2d_19.w_0
I1018 08:53:57.812852 4097533 var_desc.cc:415] Flush  conv2d_19.w_0 1
I1018 08:53:57.812855 4097533 block_desc.cc:209] Flush relu_15.tmp_0
I1018 08:53:57.812856 4097533 var_desc.cc:415] Flush  relu_15.tmp_0 1
I1018 08:53:57.812860 4097533 block_desc.cc:209] Flush assign_73.tmp_0
I1018 08:53:57.812863 4097533 var_desc.cc:415] Flush  assign_73.tmp_0 1
I1018 08:53:57.812866 4097533 block_desc.cc:209] Flush batch_norm2d_18.w_0
I1018 08:53:57.812868 4097533 var_desc.cc:415] Flush  batch_norm2d_18.w_0 1
I1018 08:53:57.812870 4097533 block_desc.cc:209] Flush batch_norm2d_11.w_1
I1018 08:53:57.812873 4097533 var_desc.cc:415] Flush  batch_norm2d_11.w_1 1
I1018 08:53:57.812875 4097533 block_desc.cc:209] Flush conv2d_18.tmp_0
I1018 08:53:57.812877 4097533 var_desc.cc:415] Flush  conv2d_18.tmp_0 1
I1018 08:53:57.812880 4097533 block_desc.cc:209] Flush conv2d_18.w_0
I1018 08:53:57.812882 4097533 var_desc.cc:415] Flush  conv2d_18.w_0 1
I1018 08:53:57.812884 4097533 block_desc.cc:209] Flush assign_65.tmp_0
I1018 08:53:57.812887 4097533 var_desc.cc:415] Flush  assign_65.tmp_0 1
I1018 08:53:57.812889 4097533 block_desc.cc:209] Flush relu_14.tmp_0
I1018 08:53:57.812891 4097533 var_desc.cc:415] Flush  relu_14.tmp_0 1
I1018 08:53:57.812894 4097533 block_desc.cc:209] Flush assign_69.tmp_0
I1018 08:53:57.812896 4097533 var_desc.cc:415] Flush  assign_69.tmp_0 1
I1018 08:53:57.812899 4097533 block_desc.cc:209] Flush batch_norm2d_17.b_0
I1018 08:53:57.812901 4097533 var_desc.cc:415] Flush  batch_norm2d_17.b_0 1
I1018 08:53:57.812903 4097533 block_desc.cc:209] Flush batch_norm2d_17.w_0
I1018 08:53:57.812906 4097533 var_desc.cc:415] Flush  batch_norm2d_17.w_0 1
I1018 08:53:57.812908 4097533 block_desc.cc:209] Flush batch_norm2d_15.w_0
I1018 08:53:57.812911 4097533 var_desc.cc:415] Flush  batch_norm2d_15.w_0 1
I1018 08:53:57.812913 4097533 block_desc.cc:209] Flush linear_0.b_0
I1018 08:53:57.812916 4097533 var_desc.cc:415] Flush  linear_0.b_0 1
I1018 08:53:57.812918 4097533 block_desc.cc:209] Flush batch_norm2d_15.b_0
I1018 08:53:57.812920 4097533 var_desc.cc:415] Flush  batch_norm2d_15.b_0 1
I1018 08:53:57.812922 4097533 block_desc.cc:209] Flush conv2d_17.tmp_0
I1018 08:53:57.812924 4097533 var_desc.cc:415] Flush  conv2d_17.tmp_0 1
I1018 08:53:57.812927 4097533 block_desc.cc:209] Flush conv2d_15.w_0
I1018 08:53:57.812929 4097533 var_desc.cc:415] Flush  conv2d_15.w_0 1
I1018 08:53:57.812932 4097533 block_desc.cc:209] Flush conv2d_16.tmp_0
I1018 08:53:57.812934 4097533 var_desc.cc:415] Flush  conv2d_16.tmp_0 1
I1018 08:53:57.812937 4097533 block_desc.cc:209] Flush conv2d_17.w_0
I1018 08:53:57.812938 4097533 var_desc.cc:415] Flush  conv2d_17.w_0 1
I1018 08:53:57.812942 4097533 block_desc.cc:209] Flush relu_13.tmp_0
I1018 08:53:57.812943 4097533 var_desc.cc:415] Flush  relu_13.tmp_0 1
I1018 08:53:57.812945 4097533 block_desc.cc:209] Flush batch_norm2d_16.b_0
I1018 08:53:57.812947 4097533 var_desc.cc:415] Flush  batch_norm2d_16.b_0 1
I1018 08:53:57.812950 4097533 block_desc.cc:209] Flush conv2d_15.tmp_0
I1018 08:53:57.812952 4097533 var_desc.cc:415] Flush  conv2d_15.tmp_0 1
I1018 08:53:57.812955 4097533 block_desc.cc:209] Flush conv2d_16.w_0
I1018 08:53:57.812958 4097533 var_desc.cc:415] Flush  conv2d_16.w_0 1
I1018 08:53:57.812959 4097533 block_desc.cc:209] Flush relu_12.tmp_0
I1018 08:53:57.812961 4097533 var_desc.cc:415] Flush  relu_12.tmp_0 1
I1018 08:53:57.812964 4097533 block_desc.cc:209] Flush assign_57.tmp_0
I1018 08:53:57.812966 4097533 var_desc.cc:415] Flush  assign_57.tmp_0 1
I1018 08:53:57.812969 4097533 block_desc.cc:209] Flush batch_norm2d_14.b_0
I1018 08:53:57.812971 4097533 var_desc.cc:415] Flush  batch_norm2d_14.b_0 1
I1018 08:53:57.812973 4097533 block_desc.cc:209] Flush batch_norm2d_14.w_0
I1018 08:53:57.812975 4097533 var_desc.cc:415] Flush  batch_norm2d_14.w_0 1
I1018 08:53:57.812978 4097533 block_desc.cc:209] Flush conv2d_14.tmp_0
I1018 08:53:57.812980 4097533 var_desc.cc:415] Flush  conv2d_14.tmp_0 1
I1018 08:53:57.812983 4097533 block_desc.cc:209] Flush conv2d_14.w_0
I1018 08:53:57.812985 4097533 var_desc.cc:415] Flush  conv2d_14.w_0 1
I1018 08:53:57.812987 4097533 block_desc.cc:209] Flush relu_11.tmp_0
I1018 08:53:57.812989 4097533 var_desc.cc:415] Flush  relu_11.tmp_0 1
I1018 08:53:57.812992 4097533 block_desc.cc:209] Flush assign_53.tmp_0
I1018 08:53:57.812994 4097533 var_desc.cc:415] Flush  assign_53.tmp_0 1
I1018 08:53:57.812999 4097533 block_desc.cc:209] Flush batch_norm2d_13.b_0
I1018 08:53:57.813001 4097533 var_desc.cc:415] Flush  batch_norm2d_13.b_0 1
I1018 08:53:57.813004 4097533 block_desc.cc:209] Flush assign_0.tmp_0
I1018 08:53:57.813005 4097533 var_desc.cc:415] Flush  assign_0.tmp_0 1
I1018 08:53:57.813009 4097533 block_desc.cc:209] Flush batch_norm2d_13.w_0
I1018 08:53:57.813010 4097533 var_desc.cc:415] Flush  batch_norm2d_13.w_0 1
I1018 08:53:57.813014 4097533 block_desc.cc:209] Flush conv2d_13.tmp_0
I1018 08:53:57.813015 4097533 var_desc.cc:415] Flush  conv2d_13.tmp_0 1
I1018 08:53:57.813017 4097533 block_desc.cc:209] Flush conv2d_13.w_0
I1018 08:53:57.813019 4097533 var_desc.cc:415] Flush  conv2d_13.w_0 1
I1018 08:53:57.813022 4097533 block_desc.cc:209] Flush relu_10.tmp_0
I1018 08:53:57.813024 4097533 var_desc.cc:415] Flush  relu_10.tmp_0 1
I1018 08:53:57.813026 4097533 block_desc.cc:209] Flush assign_49.tmp_0
I1018 08:53:57.813030 4097533 var_desc.cc:415] Flush  assign_49.tmp_0 1
I1018 08:53:57.813031 4097533 block_desc.cc:209] Flush batch_norm2d_12.b_0
I1018 08:53:57.813033 4097533 var_desc.cc:415] Flush  batch_norm2d_12.b_0 1
I1018 08:53:57.813036 4097533 block_desc.cc:209] Flush batch_norm2d_3.w_1
I1018 08:53:57.813038 4097533 var_desc.cc:415] Flush  batch_norm2d_3.w_1 1
I1018 08:53:57.813041 4097533 block_desc.cc:209] Flush batch_norm2d_12.w_0
I1018 08:53:57.813043 4097533 var_desc.cc:415] Flush  batch_norm2d_12.w_0 1
I1018 08:53:57.813045 4097533 block_desc.cc:209] Flush batch_norm2d_10.w_0
I1018 08:53:57.813048 4097533 var_desc.cc:415] Flush  batch_norm2d_10.w_0 1
I1018 08:53:57.813050 4097533 block_desc.cc:209] Flush batch_norm2d_10.w_1
I1018 08:53:57.813052 4097533 var_desc.cc:415] Flush  batch_norm2d_10.w_1 1
I1018 08:53:57.813055 4097533 block_desc.cc:209] Flush batch_norm2d_10.b_0
I1018 08:53:57.813057 4097533 var_desc.cc:415] Flush  batch_norm2d_10.b_0 1
I1018 08:53:57.813060 4097533 block_desc.cc:209] Flush conv2d_12.tmp_0
I1018 08:53:57.813062 4097533 var_desc.cc:415] Flush  conv2d_12.tmp_0 1
I1018 08:53:57.813064 4097533 block_desc.cc:209] Flush conv2d_10.w_0
I1018 08:53:57.813066 4097533 var_desc.cc:415] Flush  conv2d_10.w_0 1
I1018 08:53:57.813069 4097533 block_desc.cc:209] Flush conv2d_11.tmp_0
I1018 08:53:57.813071 4097533 var_desc.cc:415] Flush  conv2d_11.tmp_0 1
I1018 08:53:57.813074 4097533 block_desc.cc:209] Flush conv2d_12.w_0
I1018 08:53:57.813076 4097533 var_desc.cc:415] Flush  conv2d_12.w_0 1
I1018 08:53:57.813078 4097533 block_desc.cc:209] Flush relu_9.tmp_0
I1018 08:53:57.813081 4097533 var_desc.cc:415] Flush  relu_9.tmp_0 1
I1018 08:53:57.813083 4097533 block_desc.cc:209] Flush assign_41.tmp_0
I1018 08:53:57.813086 4097533 var_desc.cc:415] Flush  assign_41.tmp_0 1
I1018 08:53:57.813088 4097533 block_desc.cc:209] Flush batch_norm2d_19.b_0
I1018 08:53:57.813091 4097533 var_desc.cc:415] Flush  batch_norm2d_19.b_0 1
I1018 08:53:57.813092 4097533 block_desc.cc:209] Flush batch_norm2d_11.w_0
I1018 08:53:57.813094 4097533 var_desc.cc:415] Flush  batch_norm2d_11.w_0 1
I1018 08:53:57.813097 4097533 block_desc.cc:209] Flush assign_45.tmp_0
I1018 08:53:57.813099 4097533 var_desc.cc:415] Flush  assign_45.tmp_0 1
I1018 08:53:57.813102 4097533 block_desc.cc:209] Flush batch_norm2d_11.b_0
I1018 08:53:57.813104 4097533 var_desc.cc:415] Flush  batch_norm2d_11.b_0 1
I1018 08:53:57.813107 4097533 block_desc.cc:209] Flush batch_norm2d_11.w_2
I1018 08:53:57.813109 4097533 var_desc.cc:415] Flush  batch_norm2d_11.w_2 1
I1018 08:53:57.813112 4097533 block_desc.cc:209] Flush conv2d_10.tmp_0
I1018 08:53:57.813113 4097533 var_desc.cc:415] Flush  conv2d_10.tmp_0 1
I1018 08:53:57.813117 4097533 block_desc.cc:209] Flush conv2d_11.w_0
I1018 08:53:57.813118 4097533 var_desc.cc:415] Flush  conv2d_11.w_0 1
I1018 08:53:57.813120 4097533 block_desc.cc:209] Flush relu_8.tmp_0
I1018 08:53:57.813122 4097533 var_desc.cc:415] Flush  relu_8.tmp_0 1
I1018 08:53:57.813125 4097533 block_desc.cc:209] Flush batch_norm2d_18.b_0
I1018 08:53:57.813127 4097533 var_desc.cc:415] Flush  batch_norm2d_18.b_0 1
I1018 08:53:57.813133 4097533 block_desc.cc:209] Flush assign_37.tmp_0
I1018 08:53:57.813134 4097533 var_desc.cc:415] Flush  assign_37.tmp_0 1
I1018 08:53:57.813138 4097533 block_desc.cc:209] Flush batch_norm2d_9.b_0
I1018 08:53:57.813139 4097533 var_desc.cc:415] Flush  batch_norm2d_9.b_0 1
I1018 08:53:57.813143 4097533 block_desc.cc:209] Flush batch_norm2d_9.w_0
I1018 08:53:57.813144 4097533 var_desc.cc:415] Flush  batch_norm2d_9.w_0 1
I1018 08:53:57.813148 4097533 block_desc.cc:209] Flush conv2d_9.tmp_0
I1018 08:53:57.813149 4097533 var_desc.cc:415] Flush  conv2d_9.tmp_0 1
I1018 08:53:57.813151 4097533 block_desc.cc:209] Flush conv2d_9.w_0
I1018 08:53:57.813153 4097533 var_desc.cc:415] Flush  conv2d_9.w_0 1
I1018 08:53:57.813156 4097533 block_desc.cc:209] Flush batch_norm2d_8.b_0
I1018 08:53:57.813158 4097533 var_desc.cc:415] Flush  batch_norm2d_8.b_0 1
I1018 08:53:57.813161 4097533 block_desc.cc:209] Flush batch_norm2d_8.w_0
I1018 08:53:57.813163 4097533 var_desc.cc:415] Flush  batch_norm2d_8.w_0 1
I1018 08:53:57.813166 4097533 block_desc.cc:209] Flush conv2d_8.tmp_0
I1018 08:53:57.813169 4097533 var_desc.cc:415] Flush  conv2d_8.tmp_0 1
I1018 08:53:57.813170 4097533 block_desc.cc:209] Flush conv2d_8.w_0
I1018 08:53:57.813172 4097533 var_desc.cc:415] Flush  conv2d_8.w_0 1
I1018 08:53:57.813175 4097533 block_desc.cc:209] Flush assign_25.tmp_0
I1018 08:53:57.813177 4097533 var_desc.cc:415] Flush  assign_25.tmp_0 1
I1018 08:53:57.813180 4097533 block_desc.cc:209] Flush relu_6.tmp_0
I1018 08:53:57.813182 4097533 var_desc.cc:415] Flush  relu_6.tmp_0 1
I1018 08:53:57.813184 4097533 block_desc.cc:209] Flush assign_29.tmp_0
I1018 08:53:57.813187 4097533 var_desc.cc:415] Flush  assign_29.tmp_0 1
I1018 08:53:57.813189 4097533 block_desc.cc:209] Flush batch_norm2d_7.b_0
I1018 08:53:57.813191 4097533 var_desc.cc:415] Flush  batch_norm2d_7.b_0 1
I1018 08:53:57.813194 4097533 block_desc.cc:209] Flush batch_norm2d_7.w_0
I1018 08:53:57.813196 4097533 var_desc.cc:415] Flush  batch_norm2d_7.w_0 1
I1018 08:53:57.813199 4097533 block_desc.cc:209] Flush batch_norm2d_5.w_0
I1018 08:53:57.813201 4097533 var_desc.cc:415] Flush  batch_norm2d_5.w_0 1
I1018 08:53:57.813205 4097533 block_desc.cc:209] Flush batch_norm2d_5.b_0
I1018 08:53:57.813211 4097533 var_desc.cc:415] Flush  batch_norm2d_5.b_0 1
I1018 08:53:57.813215 4097533 block_desc.cc:209] Flush conv2d_7.tmp_0
I1018 08:53:57.813216 4097533 var_desc.cc:415] Flush  conv2d_7.tmp_0 1
I1018 08:53:57.813220 4097533 block_desc.cc:209] Flush assign_4.tmp_0
I1018 08:53:57.813221 4097533 var_desc.cc:415] Flush  assign_4.tmp_0 1
I1018 08:53:57.813223 4097533 block_desc.cc:209] Flush conv2d_5.w_0
I1018 08:53:57.813226 4097533 var_desc.cc:415] Flush  conv2d_5.w_0 1
I1018 08:53:57.813228 4097533 block_desc.cc:209] Flush conv2d_6.tmp_0
I1018 08:53:57.813230 4097533 var_desc.cc:415] Flush  conv2d_6.tmp_0 1
I1018 08:53:57.813233 4097533 block_desc.cc:209] Flush conv2d_7.w_0
I1018 08:53:57.813235 4097533 var_desc.cc:415] Flush  conv2d_7.w_0 1
I1018 08:53:57.813237 4097533 block_desc.cc:209] Flush relu_5.tmp_0
I1018 08:53:57.813241 4097533 var_desc.cc:415] Flush  relu_5.tmp_0 1
I1018 08:53:57.813242 4097533 block_desc.cc:209] Flush assign_21.tmp_0
I1018 08:53:57.813244 4097533 var_desc.cc:415] Flush  assign_21.tmp_0 1
I1018 08:53:57.813247 4097533 block_desc.cc:209] Flush batch_norm2d_6.b_0
I1018 08:53:57.813249 4097533 var_desc.cc:415] Flush  batch_norm2d_6.b_0 1
I1018 08:53:57.813252 4097533 block_desc.cc:209] Flush batch_norm2d_6.w_0
I1018 08:53:57.813254 4097533 var_desc.cc:415] Flush  batch_norm2d_6.w_0 1
I1018 08:53:57.813257 4097533 block_desc.cc:209] Flush assign_48.tmp_0
I1018 08:53:57.813259 4097533 var_desc.cc:415] Flush  assign_48.tmp_0 1
I1018 08:53:57.813261 4097533 block_desc.cc:209] Flush batch_norm2d_4.w_2
I1018 08:53:57.813263 4097533 var_desc.cc:415] Flush  batch_norm2d_4.w_2 1
I1018 08:53:57.813266 4097533 block_desc.cc:209] Flush batch_norm2d_16.w_0
I1018 08:53:57.813268 4097533 var_desc.cc:415] Flush  batch_norm2d_16.w_0 1
I1018 08:53:57.813271 4097533 block_desc.cc:209] Flush conv2d_5.tmp_0
I1018 08:53:57.813275 4097533 var_desc.cc:415] Flush  conv2d_5.tmp_0 1
I1018 08:53:57.813278 4097533 block_desc.cc:209] Flush conv2d_6.w_0
I1018 08:53:57.813280 4097533 var_desc.cc:415] Flush  conv2d_6.w_0 1
I1018 08:53:57.813282 4097533 block_desc.cc:209] Flush relu_4.tmp_0
I1018 08:53:57.813285 4097533 var_desc.cc:415] Flush  relu_4.tmp_0 1
I1018 08:53:57.813287 4097533 block_desc.cc:209] Flush assign_17.tmp_0
I1018 08:53:57.813289 4097533 var_desc.cc:415] Flush  assign_17.tmp_0 1
I1018 08:53:57.813292 4097533 block_desc.cc:209] Flush batch_norm2d_4.b_0
I1018 08:53:57.813294 4097533 var_desc.cc:415] Flush  batch_norm2d_4.b_0 1
I1018 08:53:57.813297 4097533 block_desc.cc:209] Flush batch_norm2d_4.w_0
I1018 08:53:57.813299 4097533 var_desc.cc:415] Flush  batch_norm2d_4.w_0 1
I1018 08:53:57.813302 4097533 block_desc.cc:209] Flush assign_61.tmp_0
I1018 08:53:57.813303 4097533 var_desc.cc:415] Flush  assign_61.tmp_0 1
I1018 08:53:57.813306 4097533 block_desc.cc:209] Flush conv2d_4.tmp_0
I1018 08:53:57.813308 4097533 var_desc.cc:415] Flush  conv2d_4.tmp_0 1
I1018 08:53:57.813311 4097533 block_desc.cc:209] Flush conv2d_4.w_0
I1018 08:53:57.813313 4097533 var_desc.cc:415] Flush  conv2d_4.w_0 1
I1018 08:53:57.813315 4097533 block_desc.cc:209] Flush relu_7.tmp_0
I1018 08:53:57.813318 4097533 var_desc.cc:415] Flush  relu_7.tmp_0 1
I1018 08:53:57.813320 4097533 block_desc.cc:209] Flush relu_3.tmp_0
I1018 08:53:57.813323 4097533 var_desc.cc:415] Flush  relu_3.tmp_0 1
I1018 08:53:57.813324 4097533 block_desc.cc:209] Flush assign_33.tmp_0
I1018 08:53:57.813328 4097533 var_desc.cc:415] Flush  assign_33.tmp_0 1
I1018 08:53:57.813329 4097533 block_desc.cc:209] Flush assign_13.tmp_0
I1018 08:53:57.813331 4097533 var_desc.cc:415] Flush  assign_13.tmp_0 1
I1018 08:53:57.813334 4097533 block_desc.cc:209] Flush batch_norm2d_3.b_0
I1018 08:53:57.813336 4097533 var_desc.cc:415] Flush  batch_norm2d_3.b_0 1
I1018 08:53:57.813338 4097533 block_desc.cc:209] Flush batch_norm2d_3.w_0
I1018 08:53:57.813341 4097533 var_desc.cc:415] Flush  batch_norm2d_3.w_0 1
I1018 08:53:57.813344 4097533 block_desc.cc:209] Flush conv2d_3.tmp_0
I1018 08:53:57.813345 4097533 var_desc.cc:415] Flush  conv2d_3.tmp_0 1
I1018 08:53:57.813349 4097533 block_desc.cc:209] Flush conv2d_3.w_0
I1018 08:53:57.813350 4097533 var_desc.cc:415] Flush  conv2d_3.w_0 1
I1018 08:53:57.813352 4097533 block_desc.cc:209] Flush relu_2.tmp_0
I1018 08:53:57.813355 4097533 var_desc.cc:415] Flush  relu_2.tmp_0 1
I1018 08:53:57.813357 4097533 block_desc.cc:209] Flush assign_9.tmp_0
I1018 08:53:57.813359 4097533 var_desc.cc:415] Flush  assign_9.tmp_0 1
I1018 08:53:57.813362 4097533 block_desc.cc:209] Flush batch_norm2d_2.b_0
I1018 08:53:57.813364 4097533 var_desc.cc:415] Flush  batch_norm2d_2.b_0 1
I1018 08:53:57.813367 4097533 block_desc.cc:209] Flush batch_norm2d_2.w_0
I1018 08:53:57.813369 4097533 var_desc.cc:415] Flush  batch_norm2d_2.w_0 1
I1018 08:53:57.813371 4097533 block_desc.cc:209] Flush conv2d_2.tmp_0
I1018 08:53:57.813374 4097533 var_desc.cc:415] Flush  conv2d_2.tmp_0 1
I1018 08:53:57.813376 4097533 block_desc.cc:209] Flush conv2d_2.w_0
I1018 08:53:57.813378 4097533 var_desc.cc:415] Flush  conv2d_2.w_0 1
I1018 08:53:57.813381 4097533 block_desc.cc:209] Flush relu_1.tmp_0
I1018 08:53:57.813383 4097533 var_desc.cc:415] Flush  relu_1.tmp_0 1
I1018 08:53:57.813385 4097533 block_desc.cc:209] Flush assign_5.tmp_0
I1018 08:53:57.813387 4097533 var_desc.cc:415] Flush  assign_5.tmp_0 1
I1018 08:53:57.813390 4097533 block_desc.cc:209] Flush batch_norm2d_1.b_0
I1018 08:53:57.813392 4097533 var_desc.cc:415] Flush  batch_norm2d_1.b_0 1
I1018 08:53:57.813395 4097533 block_desc.cc:209] Flush batch_norm2d_1.w_0
I1018 08:53:57.813397 4097533 var_desc.cc:415] Flush  batch_norm2d_1.w_0 1
I1018 08:53:57.813400 4097533 block_desc.cc:209] Flush conv2d_1.tmp_0
I1018 08:53:57.813402 4097533 var_desc.cc:415] Flush  conv2d_1.tmp_0 1
I1018 08:53:57.813405 4097533 block_desc.cc:209] Flush conv2d_1.w_0
I1018 08:53:57.813406 4097533 var_desc.cc:415] Flush  conv2d_1.w_0 1
I1018 08:53:57.813409 4097533 block_desc.cc:209] Flush pool2d_0.tmp_0
I1018 08:53:57.813413 4097533 var_desc.cc:415] Flush  pool2d_0.tmp_0 1
I1018 08:53:57.813416 4097533 block_desc.cc:209] Flush assign_56.tmp_0
I1018 08:53:57.813418 4097533 var_desc.cc:415] Flush  assign_56.tmp_0 1
I1018 08:53:57.813421 4097533 block_desc.cc:209] Flush assign_1.tmp_0
I1018 08:53:57.813422 4097533 var_desc.cc:415] Flush  assign_1.tmp_0 1
I1018 08:53:57.813426 4097533 block_desc.cc:209] Flush batch_norm2d_0.b_0
I1018 08:53:57.813427 4097533 var_desc.cc:415] Flush  batch_norm2d_0.b_0 1
I1018 08:53:57.813431 4097533 block_desc.cc:209] Flush batch_norm2d_0.w_0
I1018 08:53:57.813432 4097533 var_desc.cc:415] Flush  batch_norm2d_0.w_0 1
I1018 08:53:57.813436 4097533 block_desc.cc:209] Flush conv2d_0.tmp_0
I1018 08:53:57.813437 4097533 var_desc.cc:415] Flush  conv2d_0.tmp_0 1
I1018 08:53:57.813439 4097533 block_desc.cc:209] Flush _jst.0.x.0
I1018 08:53:57.813442 4097533 var_desc.cc:415] Flush  _jst.0.x.0 1
I1018 08:53:57.813444 4097533 block_desc.cc:209] Flush conv2d_0.w_0
I1018 08:53:57.813446 4097533 var_desc.cc:415] Flush  conv2d_0.w_0 1
I1018 08:53:57.822283 4097533 block_desc.cc:205] vars in desc 180
I1018 08:53:57.822300 4097533 block_desc.cc:209] Flush assign_72.tmp_0
I1018 08:53:57.822304 4097533 var_desc.cc:415] Flush  assign_72.tmp_0 1
I1018 08:53:57.822306 4097533 block_desc.cc:209] Flush assign_68.tmp_0
I1018 08:53:57.822309 4097533 var_desc.cc:415] Flush  assign_68.tmp_0 1
I1018 08:53:57.822311 4097533 block_desc.cc:209] Flush assign_64.tmp_0
I1018 08:53:57.822314 4097533 var_desc.cc:415] Flush  assign_64.tmp_0 1
I1018 08:53:57.822315 4097533 block_desc.cc:209] Flush assign_60.tmp_0
I1018 08:53:57.822319 4097533 var_desc.cc:415] Flush  assign_60.tmp_0 1
I1018 08:53:57.822320 4097533 block_desc.cc:209] Flush assign_52.tmp_0
I1018 08:53:57.822322 4097533 var_desc.cc:415] Flush  assign_52.tmp_0 1
I1018 08:53:57.822324 4097533 block_desc.cc:209] Flush assign_44.tmp_0
I1018 08:53:57.822326 4097533 var_desc.cc:415] Flush  assign_44.tmp_0 1
I1018 08:53:57.822329 4097533 block_desc.cc:209] Flush assign_40.tmp_0
I1018 08:53:57.822331 4097533 var_desc.cc:415] Flush  assign_40.tmp_0 1
I1018 08:53:57.822333 4097533 block_desc.cc:209] Flush assign_36.tmp_0
I1018 08:53:57.822335 4097533 var_desc.cc:415] Flush  assign_36.tmp_0 1
I1018 08:53:57.822338 4097533 block_desc.cc:209] Flush assign_32.tmp_0
I1018 08:53:57.822340 4097533 var_desc.cc:415] Flush  assign_32.tmp_0 1
I1018 08:53:57.822342 4097533 block_desc.cc:209] Flush assign_28.tmp_0
I1018 08:53:57.822345 4097533 var_desc.cc:415] Flush  assign_28.tmp_0 1
I1018 08:53:57.822346 4097533 block_desc.cc:209] Flush assign_24.tmp_0
I1018 08:53:57.822350 4097533 var_desc.cc:415] Flush  assign_24.tmp_0 1
I1018 08:53:57.822351 4097533 block_desc.cc:209] Flush assign_20.tmp_0
I1018 08:53:57.822353 4097533 var_desc.cc:415] Flush  assign_20.tmp_0 1
I1018 08:53:57.822355 4097533 block_desc.cc:209] Flush assign_16.tmp_0
I1018 08:53:57.822357 4097533 var_desc.cc:415] Flush  assign_16.tmp_0 1
I1018 08:53:57.822360 4097533 block_desc.cc:209] Flush assign_12.tmp_0
I1018 08:53:57.822362 4097533 var_desc.cc:415] Flush  assign_12.tmp_0 1
I1018 08:53:57.822364 4097533 block_desc.cc:209] Flush assign_8.tmp_0
I1018 08:53:57.822366 4097533 var_desc.cc:415] Flush  assign_8.tmp_0 1
I1018 08:53:57.822369 4097533 block_desc.cc:209] Flush batch_norm2d_19.w_2
I1018 08:53:57.822371 4097533 var_desc.cc:415] Flush  batch_norm2d_19.w_2 1
I1018 08:53:57.822374 4097533 block_desc.cc:209] Flush batch_norm2d_18.w_2
I1018 08:53:57.822376 4097533 var_desc.cc:415] Flush  batch_norm2d_18.w_2 1
I1018 08:53:57.822378 4097533 block_desc.cc:209] Flush batch_norm2d_18.w_1
I1018 08:53:57.822381 4097533 var_desc.cc:415] Flush  batch_norm2d_18.w_1 1
I1018 08:53:57.822382 4097533 block_desc.cc:209] Flush batch_norm2d_15.w_2
I1018 08:53:57.822386 4097533 var_desc.cc:415] Flush  batch_norm2d_15.w_2 1
I1018 08:53:57.822387 4097533 block_desc.cc:209] Flush batch_norm2d_15.w_1
I1018 08:53:57.822389 4097533 var_desc.cc:415] Flush  batch_norm2d_15.w_1 1
I1018 08:53:57.822391 4097533 block_desc.cc:209] Flush batch_norm2d_17.w_2
I1018 08:53:57.822398 4097533 var_desc.cc:415] Flush  batch_norm2d_17.w_2 1
I1018 08:53:57.822402 4097533 block_desc.cc:209] Flush batch_norm2d_17.w_1
I1018 08:53:57.822403 4097533 var_desc.cc:415] Flush  batch_norm2d_17.w_1 1
I1018 08:53:57.822405 4097533 block_desc.cc:209] Flush batch_norm2d_16.w_2
I1018 08:53:57.822407 4097533 var_desc.cc:415] Flush  batch_norm2d_16.w_2 1
I1018 08:53:57.822409 4097533 block_desc.cc:209] Flush batch_norm2d_16.w_1
I1018 08:53:57.822412 4097533 var_desc.cc:415] Flush  batch_norm2d_16.w_1 1
I1018 08:53:57.822414 4097533 block_desc.cc:209] Flush batch_norm2d_14.w_2
I1018 08:53:57.822417 4097533 var_desc.cc:415] Flush  batch_norm2d_14.w_2 1
I1018 08:53:57.822418 4097533 block_desc.cc:209] Flush batch_norm2d_14.w_1
I1018 08:53:57.822420 4097533 var_desc.cc:415] Flush  batch_norm2d_14.w_1 1
I1018 08:53:57.822423 4097533 block_desc.cc:209] Flush batch_norm2d_13.w_2
I1018 08:53:57.822425 4097533 var_desc.cc:415] Flush  batch_norm2d_13.w_2 1
I1018 08:53:57.822427 4097533 block_desc.cc:209] Flush batch_norm2d_13.w_1
I1018 08:53:57.822429 4097533 var_desc.cc:415] Flush  batch_norm2d_13.w_1 1
I1018 08:53:57.822432 4097533 block_desc.cc:209] Flush batch_norm2d_10.w_2
I1018 08:53:57.822434 4097533 var_desc.cc:415] Flush  batch_norm2d_10.w_2 1
I1018 08:53:57.822436 4097533 block_desc.cc:209] Flush batch_norm2d_12.w_2
I1018 08:53:57.822438 4097533 var_desc.cc:415] Flush  batch_norm2d_12.w_2 1
I1018 08:53:57.822441 4097533 block_desc.cc:209] Flush batch_norm2d_12.w_1
I1018 08:53:57.822443 4097533 var_desc.cc:415] Flush  batch_norm2d_12.w_1 1
I1018 08:53:57.822445 4097533 block_desc.cc:209] Flush batch_norm2d_9.w_2
I1018 08:53:57.822448 4097533 var_desc.cc:415] Flush  batch_norm2d_9.w_2 1
I1018 08:53:57.822450 4097533 block_desc.cc:209] Flush batch_norm2d_9.w_1
I1018 08:53:57.822453 4097533 var_desc.cc:415] Flush  batch_norm2d_9.w_1 1
I1018 08:53:57.822454 4097533 block_desc.cc:209] Flush batch_norm2d_8.w_2
I1018 08:53:57.822456 4097533 var_desc.cc:415] Flush  batch_norm2d_8.w_2 1
I1018 08:53:57.822459 4097533 block_desc.cc:209] Flush batch_norm2d_8.w_1
I1018 08:53:57.822461 4097533 var_desc.cc:415] Flush  batch_norm2d_8.w_1 1
I1018 08:53:57.822463 4097533 block_desc.cc:209] Flush batch_norm2d_5.w_2
I1018 08:53:57.822465 4097533 var_desc.cc:415] Flush  batch_norm2d_5.w_2 1
I1018 08:53:57.822468 4097533 block_desc.cc:209] Flush batch_norm2d_5.w_1
I1018 08:53:57.822470 4097533 var_desc.cc:415] Flush  batch_norm2d_5.w_1 1
I1018 08:53:57.822472 4097533 block_desc.cc:209] Flush batch_norm2d_7.w_2
I1018 08:53:57.822474 4097533 var_desc.cc:415] Flush  batch_norm2d_7.w_2 1
I1018 08:53:57.822477 4097533 block_desc.cc:209] Flush batch_norm2d_7.w_1
I1018 08:53:57.822479 4097533 var_desc.cc:415] Flush  batch_norm2d_7.w_1 1
I1018 08:53:57.822481 4097533 block_desc.cc:209] Flush batch_norm2d_6.w_2
I1018 08:53:57.822484 4097533 var_desc.cc:415] Flush  batch_norm2d_6.w_2 1
I1018 08:53:57.822485 4097533 block_desc.cc:209] Flush batch_norm2d_6.w_1
I1018 08:53:57.822487 4097533 var_desc.cc:415] Flush  batch_norm2d_6.w_1 1
I1018 08:53:57.822490 4097533 block_desc.cc:209] Flush batch_norm2d_4.w_1
I1018 08:53:57.822492 4097533 var_desc.cc:415] Flush  batch_norm2d_4.w_1 1
I1018 08:53:57.822494 4097533 block_desc.cc:209] Flush batch_norm2d_3.w_2
I1018 08:53:57.822496 4097533 var_desc.cc:415] Flush  batch_norm2d_3.w_2 1
I1018 08:53:57.822499 4097533 block_desc.cc:209] Flush assign_76.tmp_0
I1018 08:53:57.822501 4097533 var_desc.cc:415] Flush  assign_76.tmp_0 1
I1018 08:53:57.822503 4097533 block_desc.cc:209] Flush batch_norm2d_2.w_2
I1018 08:53:57.822505 4097533 var_desc.cc:415] Flush  batch_norm2d_2.w_2 1
I1018 08:53:57.822508 4097533 block_desc.cc:209] Flush batch_norm2d_2.w_1
I1018 08:53:57.822510 4097533 var_desc.cc:415] Flush  batch_norm2d_2.w_1 1
I1018 08:53:57.822512 4097533 block_desc.cc:209] Flush batch_norm2d_1.w_2
I1018 08:53:57.822515 4097533 var_desc.cc:415] Flush  batch_norm2d_1.w_2 1
I1018 08:53:57.822517 4097533 block_desc.cc:209] Flush batch_norm2d_1.w_1
I1018 08:53:57.822520 4097533 var_desc.cc:415] Flush  batch_norm2d_1.w_1 1
I1018 08:53:57.822523 4097533 block_desc.cc:209] Flush batch_norm2d_0.w_2
I1018 08:53:57.822525 4097533 var_desc.cc:415] Flush  batch_norm2d_0.w_2 1
I1018 08:53:57.822528 4097533 block_desc.cc:209] Flush batch_norm2d_0.w_1
I1018 08:53:57.822530 4097533 var_desc.cc:415] Flush  batch_norm2d_0.w_1 1
I1018 08:53:57.822532 4097533 block_desc.cc:209] Flush linear_0.tmp_1
I1018 08:53:57.822535 4097533 var_desc.cc:415] Flush  linear_0.tmp_1 1
I1018 08:53:57.822537 4097533 block_desc.cc:209] Flush assign_77.tmp_0
I1018 08:53:57.822539 4097533 var_desc.cc:415] Flush  assign_77.tmp_0 1
I1018 08:53:57.822541 4097533 block_desc.cc:209] Flush batch_norm2d_19.w_0
I1018 08:53:57.822543 4097533 var_desc.cc:415] Flush  batch_norm2d_19.w_0 1
I1018 08:53:57.822546 4097533 block_desc.cc:209] Flush batch_norm2d_19.w_1
I1018 08:53:57.822548 4097533 var_desc.cc:415] Flush  batch_norm2d_19.w_1 1
I1018 08:53:57.822551 4097533 block_desc.cc:209] Flush linear_0.w_0
I1018 08:53:57.822553 4097533 var_desc.cc:415] Flush  linear_0.w_0 1
I1018 08:53:57.822556 4097533 block_desc.cc:209] Flush conv2d_19.tmp_0
I1018 08:53:57.822557 4097533 var_desc.cc:415] Flush  conv2d_19.tmp_0 1
I1018 08:53:57.822561 4097533 block_desc.cc:209] Flush conv2d_19.w_0
I1018 08:53:57.822562 4097533 var_desc.cc:415] Flush  conv2d_19.w_0 1
I1018 08:53:57.822564 4097533 block_desc.cc:209] Flush relu_15.tmp_0
I1018 08:53:57.822566 4097533 var_desc.cc:415] Flush  relu_15.tmp_0 1
I1018 08:53:57.822569 4097533 block_desc.cc:209] Flush assign_73.tmp_0
I1018 08:53:57.822571 4097533 var_desc.cc:415] Flush  assign_73.tmp_0 1
I1018 08:53:57.822573 4097533 block_desc.cc:209] Flush batch_norm2d_18.w_0
I1018 08:53:57.822575 4097533 var_desc.cc:415] Flush  batch_norm2d_18.w_0 1
I1018 08:53:57.822578 4097533 block_desc.cc:209] Flush batch_norm2d_11.w_1
I1018 08:53:57.822580 4097533 var_desc.cc:415] Flush  batch_norm2d_11.w_1 1
I1018 08:53:57.822582 4097533 block_desc.cc:209] Flush conv2d_18.tmp_0
I1018 08:53:57.822584 4097533 var_desc.cc:415] Flush  conv2d_18.tmp_0 1
I1018 08:53:57.822587 4097533 block_desc.cc:209] Flush conv2d_18.w_0
I1018 08:53:57.822589 4097533 var_desc.cc:415] Flush  conv2d_18.w_0 1
I1018 08:53:57.822592 4097533 block_desc.cc:209] Flush assign_65.tmp_0
I1018 08:53:57.822593 4097533 var_desc.cc:415] Flush  assign_65.tmp_0 1
I1018 08:53:57.822595 4097533 block_desc.cc:209] Flush relu_14.tmp_0
I1018 08:53:57.822597 4097533 var_desc.cc:415] Flush  relu_14.tmp_0 1
I1018 08:53:57.822600 4097533 block_desc.cc:209] Flush assign_69.tmp_0
I1018 08:53:57.822602 4097533 var_desc.cc:415] Flush  assign_69.tmp_0 1
I1018 08:53:57.822604 4097533 block_desc.cc:209] Flush batch_norm2d_17.b_0
I1018 08:53:57.822607 4097533 var_desc.cc:415] Flush  batch_norm2d_17.b_0 1
I1018 08:53:57.822609 4097533 block_desc.cc:209] Flush batch_norm2d_17.w_0
I1018 08:53:57.822611 4097533 var_desc.cc:415] Flush  batch_norm2d_17.w_0 1
I1018 08:53:57.822613 4097533 block_desc.cc:209] Flush batch_norm2d_15.w_0
I1018 08:53:57.822615 4097533 var_desc.cc:415] Flush  batch_norm2d_15.w_0 1
I1018 08:53:57.822618 4097533 block_desc.cc:209] Flush linear_0.b_0
I1018 08:53:57.822620 4097533 var_desc.cc:415] Flush  linear_0.b_0 1
I1018 08:53:57.822623 4097533 block_desc.cc:209] Flush batch_norm2d_15.b_0
I1018 08:53:57.822624 4097533 var_desc.cc:415] Flush  batch_norm2d_15.b_0 1
I1018 08:53:57.822628 4097533 block_desc.cc:209] Flush conv2d_17.tmp_0
I1018 08:53:57.822629 4097533 var_desc.cc:415] Flush  conv2d_17.tmp_0 1
I1018 08:53:57.822631 4097533 block_desc.cc:209] Flush conv2d_15.w_0
I1018 08:53:57.822633 4097533 var_desc.cc:415] Flush  conv2d_15.w_0 1
I1018 08:53:57.822636 4097533 block_desc.cc:209] Flush conv2d_16.tmp_0
I1018 08:53:57.822638 4097533 var_desc.cc:415] Flush  conv2d_16.tmp_0 1
I1018 08:53:57.822640 4097533 block_desc.cc:209] Flush conv2d_17.w_0
I1018 08:53:57.822642 4097533 var_desc.cc:415] Flush  conv2d_17.w_0 1
I1018 08:53:57.822644 4097533 block_desc.cc:209] Flush relu_13.tmp_0
I1018 08:53:57.822646 4097533 var_desc.cc:415] Flush  relu_13.tmp_0 1
I1018 08:53:57.822651 4097533 block_desc.cc:209] Flush batch_norm2d_16.b_0
I1018 08:53:57.822654 4097533 var_desc.cc:415] Flush  batch_norm2d_16.b_0 1
I1018 08:53:57.822655 4097533 block_desc.cc:209] Flush conv2d_15.tmp_0
I1018 08:53:57.822657 4097533 var_desc.cc:415] Flush  conv2d_15.tmp_0 1
I1018 08:53:57.822660 4097533 block_desc.cc:209] Flush conv2d_16.w_0
I1018 08:53:57.822662 4097533 var_desc.cc:415] Flush  conv2d_16.w_0 1
I1018 08:53:57.822664 4097533 block_desc.cc:209] Flush relu_12.tmp_0
I1018 08:53:57.822666 4097533 var_desc.cc:415] Flush  relu_12.tmp_0 1
I1018 08:53:57.822669 4097533 block_desc.cc:209] Flush assign_57.tmp_0
I1018 08:53:57.822670 4097533 var_desc.cc:415] Flush  assign_57.tmp_0 1
I1018 08:53:57.822674 4097533 block_desc.cc:209] Flush batch_norm2d_14.b_0
I1018 08:53:57.822675 4097533 var_desc.cc:415] Flush  batch_norm2d_14.b_0 1
I1018 08:53:57.822677 4097533 block_desc.cc:209] Flush batch_norm2d_14.w_0
I1018 08:53:57.822679 4097533 var_desc.cc:415] Flush  batch_norm2d_14.w_0 1
I1018 08:53:57.822682 4097533 block_desc.cc:209] Flush conv2d_14.tmp_0
I1018 08:53:57.822685 4097533 var_desc.cc:415] Flush  conv2d_14.tmp_0 1
I1018 08:53:57.822686 4097533 block_desc.cc:209] Flush conv2d_14.w_0
I1018 08:53:57.822688 4097533 var_desc.cc:415] Flush  conv2d_14.w_0 1
I1018 08:53:57.822691 4097533 block_desc.cc:209] Flush relu_11.tmp_0
I1018 08:53:57.822693 4097533 var_desc.cc:415] Flush  relu_11.tmp_0 1
I1018 08:53:57.822695 4097533 block_desc.cc:209] Flush assign_53.tmp_0
I1018 08:53:57.822697 4097533 var_desc.cc:415] Flush  assign_53.tmp_0 1
I1018 08:53:57.822700 4097533 block_desc.cc:209] Flush batch_norm2d_13.b_0
I1018 08:53:57.822702 4097533 var_desc.cc:415] Flush  batch_norm2d_13.b_0 1
I1018 08:53:57.822705 4097533 block_desc.cc:209] Flush assign_0.tmp_0
I1018 08:53:57.822706 4097533 var_desc.cc:415] Flush  assign_0.tmp_0 1
I1018 08:53:57.822708 4097533 block_desc.cc:209] Flush batch_norm2d_13.w_0
I1018 08:53:57.822711 4097533 var_desc.cc:415] Flush  batch_norm2d_13.w_0 1
I1018 08:53:57.822713 4097533 block_desc.cc:209] Flush conv2d_13.tmp_0
I1018 08:53:57.822715 4097533 var_desc.cc:415] Flush  conv2d_13.tmp_0 1
I1018 08:53:57.822717 4097533 block_desc.cc:209] Flush conv2d_13.w_0
I1018 08:53:57.822719 4097533 var_desc.cc:415] Flush  conv2d_13.w_0 1
I1018 08:53:57.822722 4097533 block_desc.cc:209] Flush relu_10.tmp_0
I1018 08:53:57.822724 4097533 var_desc.cc:415] Flush  relu_10.tmp_0 1
I1018 08:53:57.822726 4097533 block_desc.cc:209] Flush assign_49.tmp_0
I1018 08:53:57.822728 4097533 var_desc.cc:415] Flush  assign_49.tmp_0 1
I1018 08:53:57.822731 4097533 block_desc.cc:209] Flush batch_norm2d_12.b_0
I1018 08:53:57.822733 4097533 var_desc.cc:415] Flush  batch_norm2d_12.b_0 1
I1018 08:53:57.822736 4097533 block_desc.cc:209] Flush batch_norm2d_3.w_1
I1018 08:53:57.822737 4097533 var_desc.cc:415] Flush  batch_norm2d_3.w_1 1
I1018 08:53:57.822741 4097533 block_desc.cc:209] Flush batch_norm2d_12.w_0
I1018 08:53:57.822742 4097533 var_desc.cc:415] Flush  batch_norm2d_12.w_0 1
I1018 08:53:57.822744 4097533 block_desc.cc:209] Flush batch_norm2d_10.w_0
I1018 08:53:57.822746 4097533 var_desc.cc:415] Flush  batch_norm2d_10.w_0 1
I1018 08:53:57.822749 4097533 block_desc.cc:209] Flush batch_norm2d_10.w_1
I1018 08:53:57.822751 4097533 var_desc.cc:415] Flush  batch_norm2d_10.w_1 1
I1018 08:53:57.822753 4097533 block_desc.cc:209] Flush batch_norm2d_10.b_0
I1018 08:53:57.822755 4097533 var_desc.cc:415] Flush  batch_norm2d_10.b_0 1
I1018 08:53:57.822758 4097533 block_desc.cc:209] Flush conv2d_12.tmp_0
I1018 08:53:57.822760 4097533 var_desc.cc:415] Flush  conv2d_12.tmp_0 1
I1018 08:53:57.822762 4097533 block_desc.cc:209] Flush conv2d_10.w_0
I1018 08:53:57.822764 4097533 var_desc.cc:415] Flush  conv2d_10.w_0 1
I1018 08:53:57.822767 4097533 block_desc.cc:209] Flush conv2d_11.tmp_0
I1018 08:53:57.822768 4097533 var_desc.cc:415] Flush  conv2d_11.tmp_0 1
I1018 08:53:57.822772 4097533 block_desc.cc:209] Flush conv2d_12.w_0
I1018 08:53:57.822773 4097533 var_desc.cc:415] Flush  conv2d_12.w_0 1
I1018 08:53:57.822775 4097533 block_desc.cc:209] Flush relu_9.tmp_0
I1018 08:53:57.822782 4097533 var_desc.cc:415] Flush  relu_9.tmp_0 1
I1018 08:53:57.822783 4097533 block_desc.cc:209] Flush assign_41.tmp_0
I1018 08:53:57.822785 4097533 var_desc.cc:415] Flush  assign_41.tmp_0 1
I1018 08:53:57.822788 4097533 block_desc.cc:209] Flush batch_norm2d_19.b_0
I1018 08:53:57.822790 4097533 var_desc.cc:415] Flush  batch_norm2d_19.b_0 1
I1018 08:53:57.822793 4097533 block_desc.cc:209] Flush batch_norm2d_11.w_0
I1018 08:53:57.822794 4097533 var_desc.cc:415] Flush  batch_norm2d_11.w_0 1
I1018 08:53:57.822798 4097533 block_desc.cc:209] Flush assign_45.tmp_0
I1018 08:53:57.822799 4097533 var_desc.cc:415] Flush  assign_45.tmp_0 1
I1018 08:53:57.822801 4097533 block_desc.cc:209] Flush batch_norm2d_11.b_0
I1018 08:53:57.822803 4097533 var_desc.cc:415] Flush  batch_norm2d_11.b_0 1
I1018 08:53:57.822806 4097533 block_desc.cc:209] Flush batch_norm2d_11.w_2
I1018 08:53:57.822808 4097533 var_desc.cc:415] Flush  batch_norm2d_11.w_2 1
I1018 08:53:57.822811 4097533 block_desc.cc:209] Flush conv2d_10.tmp_0
I1018 08:53:57.822813 4097533 var_desc.cc:415] Flush  conv2d_10.tmp_0 1
I1018 08:53:57.822815 4097533 block_desc.cc:209] Flush conv2d_11.w_0
I1018 08:53:57.822818 4097533 var_desc.cc:415] Flush  conv2d_11.w_0 1
I1018 08:53:57.822820 4097533 block_desc.cc:209] Flush relu_8.tmp_0
I1018 08:53:57.822822 4097533 var_desc.cc:415] Flush  relu_8.tmp_0 1
I1018 08:53:57.822824 4097533 block_desc.cc:209] Flush batch_norm2d_18.b_0
I1018 08:53:57.822826 4097533 var_desc.cc:415] Flush  batch_norm2d_18.b_0 1
I1018 08:53:57.822829 4097533 block_desc.cc:209] Flush assign_37.tmp_0
I1018 08:53:57.822831 4097533 var_desc.cc:415] Flush  assign_37.tmp_0 1
I1018 08:53:57.822834 4097533 block_desc.cc:209] Flush batch_norm2d_9.b_0
I1018 08:53:57.822835 4097533 var_desc.cc:415] Flush  batch_norm2d_9.b_0 1
I1018 08:53:57.822839 4097533 block_desc.cc:209] Flush batch_norm2d_9.w_0
I1018 08:53:57.822840 4097533 var_desc.cc:415] Flush  batch_norm2d_9.w_0 1
I1018 08:53:57.822842 4097533 block_desc.cc:209] Flush conv2d_9.tmp_0
I1018 08:53:57.822844 4097533 var_desc.cc:415] Flush  conv2d_9.tmp_0 1
I1018 08:53:57.822847 4097533 block_desc.cc:209] Flush conv2d_9.w_0
I1018 08:53:57.822849 4097533 var_desc.cc:415] Flush  conv2d_9.w_0 1
I1018 08:53:57.822851 4097533 block_desc.cc:209] Flush batch_norm2d_8.b_0
I1018 08:53:57.822853 4097533 var_desc.cc:415] Flush  batch_norm2d_8.b_0 1
I1018 08:53:57.822856 4097533 block_desc.cc:209] Flush batch_norm2d_8.w_0
I1018 08:53:57.822858 4097533 var_desc.cc:415] Flush  batch_norm2d_8.w_0 1
I1018 08:53:57.822860 4097533 block_desc.cc:209] Flush conv2d_8.tmp_0
I1018 08:53:57.822862 4097533 var_desc.cc:415] Flush  conv2d_8.tmp_0 1
I1018 08:53:57.822865 4097533 block_desc.cc:209] Flush conv2d_8.w_0
I1018 08:53:57.822867 4097533 var_desc.cc:415] Flush  conv2d_8.w_0 1
I1018 08:53:57.822870 4097533 block_desc.cc:209] Flush assign_25.tmp_0
I1018 08:53:57.822871 4097533 var_desc.cc:415] Flush  assign_25.tmp_0 1
I1018 08:53:57.822875 4097533 block_desc.cc:209] Flush relu_6.tmp_0
I1018 08:53:57.822876 4097533 var_desc.cc:415] Flush  relu_6.tmp_0 1
I1018 08:53:57.822878 4097533 block_desc.cc:209] Flush assign_29.tmp_0
I1018 08:53:57.822880 4097533 var_desc.cc:415] Flush  assign_29.tmp_0 1
I1018 08:53:57.822883 4097533 block_desc.cc:209] Flush batch_norm2d_7.b_0
I1018 08:53:57.822885 4097533 var_desc.cc:415] Flush  batch_norm2d_7.b_0 1
I1018 08:53:57.822887 4097533 block_desc.cc:209] Flush batch_norm2d_7.w_0
I1018 08:53:57.822889 4097533 var_desc.cc:415] Flush  batch_norm2d_7.w_0 1
I1018 08:53:57.822892 4097533 block_desc.cc:209] Flush batch_norm2d_5.w_0
I1018 08:53:57.822894 4097533 var_desc.cc:415] Flush  batch_norm2d_5.w_0 1
I1018 08:53:57.822896 4097533 block_desc.cc:209] Flush batch_norm2d_5.b_0
I1018 08:53:57.822898 4097533 var_desc.cc:415] Flush  batch_norm2d_5.b_0 1
I1018 08:53:57.822901 4097533 block_desc.cc:209] Flush conv2d_7.tmp_0
I1018 08:53:57.822903 4097533 var_desc.cc:415] Flush  conv2d_7.tmp_0 1
I1018 08:53:57.822906 4097533 block_desc.cc:209] Flush assign_4.tmp_0
I1018 08:53:57.822907 4097533 var_desc.cc:415] Flush  assign_4.tmp_0 1
I1018 08:53:57.822912 4097533 block_desc.cc:209] Flush conv2d_5.w_0
I1018 08:53:57.822914 4097533 var_desc.cc:415] Flush  conv2d_5.w_0 1
I1018 08:53:57.822916 4097533 block_desc.cc:209] Flush conv2d_6.tmp_0
I1018 08:53:57.822918 4097533 var_desc.cc:415] Flush  conv2d_6.tmp_0 1
I1018 08:53:57.822921 4097533 block_desc.cc:209] Flush conv2d_7.w_0
I1018 08:53:57.822923 4097533 var_desc.cc:415] Flush  conv2d_7.w_0 1
I1018 08:53:57.822925 4097533 block_desc.cc:209] Flush relu_5.tmp_0
I1018 08:53:57.822927 4097533 var_desc.cc:415] Flush  relu_5.tmp_0 1
I1018 08:53:57.822929 4097533 block_desc.cc:209] Flush assign_21.tmp_0
I1018 08:53:57.822932 4097533 var_desc.cc:415] Flush  assign_21.tmp_0 1
I1018 08:53:57.822934 4097533 block_desc.cc:209] Flush batch_norm2d_6.b_0
I1018 08:53:57.822937 4097533 var_desc.cc:415] Flush  batch_norm2d_6.b_0 1
I1018 08:53:57.822938 4097533 block_desc.cc:209] Flush batch_norm2d_6.w_0
I1018 08:53:57.822942 4097533 var_desc.cc:415] Flush  batch_norm2d_6.w_0 1
I1018 08:53:57.822943 4097533 block_desc.cc:209] Flush assign_48.tmp_0
I1018 08:53:57.822945 4097533 var_desc.cc:415] Flush  assign_48.tmp_0 1
I1018 08:53:57.822947 4097533 block_desc.cc:209] Flush batch_norm2d_4.w_2
I1018 08:53:57.822950 4097533 var_desc.cc:415] Flush  batch_norm2d_4.w_2 1
I1018 08:53:57.822952 4097533 block_desc.cc:209] Flush batch_norm2d_16.w_0
I1018 08:53:57.822954 4097533 var_desc.cc:415] Flush  batch_norm2d_16.w_0 1
I1018 08:53:57.822957 4097533 block_desc.cc:209] Flush conv2d_5.tmp_0
I1018 08:53:57.822959 4097533 var_desc.cc:415] Flush  conv2d_5.tmp_0 1
I1018 08:53:57.822961 4097533 block_desc.cc:209] Flush conv2d_6.w_0
I1018 08:53:57.822963 4097533 var_desc.cc:415] Flush  conv2d_6.w_0 1
I1018 08:53:57.822966 4097533 block_desc.cc:209] Flush relu_4.tmp_0
I1018 08:53:57.822968 4097533 var_desc.cc:415] Flush  relu_4.tmp_0 1
I1018 08:53:57.822970 4097533 block_desc.cc:209] Flush assign_17.tmp_0
I1018 08:53:57.822973 4097533 var_desc.cc:415] Flush  assign_17.tmp_0 1
I1018 08:53:57.822975 4097533 block_desc.cc:209] Flush batch_norm2d_4.b_0
I1018 08:53:57.822978 4097533 var_desc.cc:415] Flush  batch_norm2d_4.b_0 1
I1018 08:53:57.822979 4097533 block_desc.cc:209] Flush batch_norm2d_4.w_0
I1018 08:53:57.822981 4097533 var_desc.cc:415] Flush  batch_norm2d_4.w_0 1
I1018 08:53:57.822984 4097533 block_desc.cc:209] Flush assign_61.tmp_0
I1018 08:53:57.822986 4097533 var_desc.cc:415] Flush  assign_61.tmp_0 1
I1018 08:53:57.822988 4097533 block_desc.cc:209] Flush conv2d_4.tmp_0
I1018 08:53:57.822990 4097533 var_desc.cc:415] Flush  conv2d_4.tmp_0 1
I1018 08:53:57.822993 4097533 block_desc.cc:209] Flush conv2d_4.w_0
I1018 08:53:57.822995 4097533 var_desc.cc:415] Flush  conv2d_4.w_0 1
I1018 08:53:57.822997 4097533 block_desc.cc:209] Flush relu_7.tmp_0
I1018 08:53:57.822999 4097533 var_desc.cc:415] Flush  relu_7.tmp_0 1
I1018 08:53:57.823001 4097533 block_desc.cc:209] Flush relu_3.tmp_0
I1018 08:53:57.823004 4097533 var_desc.cc:415] Flush  relu_3.tmp_0 1
I1018 08:53:57.823006 4097533 block_desc.cc:209] Flush assign_33.tmp_0
I1018 08:53:57.823009 4097533 var_desc.cc:415] Flush  assign_33.tmp_0 1
I1018 08:53:57.823010 4097533 block_desc.cc:209] Flush assign_13.tmp_0
I1018 08:53:57.823012 4097533 var_desc.cc:415] Flush  assign_13.tmp_0 1
I1018 08:53:57.823015 4097533 block_desc.cc:209] Flush batch_norm2d_3.b_0
I1018 08:53:57.823017 4097533 var_desc.cc:415] Flush  batch_norm2d_3.b_0 1
I1018 08:53:57.823019 4097533 block_desc.cc:209] Flush batch_norm2d_3.w_0
I1018 08:53:57.823021 4097533 var_desc.cc:415] Flush  batch_norm2d_3.w_0 1
I1018 08:53:57.823024 4097533 block_desc.cc:209] Flush conv2d_3.tmp_0
I1018 08:53:57.823026 4097533 var_desc.cc:415] Flush  conv2d_3.tmp_0 1
I1018 08:53:57.823028 4097533 block_desc.cc:209] Flush conv2d_3.w_0
I1018 08:53:57.823030 4097533 var_desc.cc:415] Flush  conv2d_3.w_0 1
I1018 08:53:57.823033 4097533 block_desc.cc:209] Flush relu_2.tmp_0
I1018 08:53:57.823035 4097533 var_desc.cc:415] Flush  relu_2.tmp_0 1
I1018 08:53:57.823037 4097533 block_desc.cc:209] Flush assign_9.tmp_0
I1018 08:53:57.823041 4097533 var_desc.cc:415] Flush  assign_9.tmp_0 1
I1018 08:53:57.823045 4097533 block_desc.cc:209] Flush batch_norm2d_2.b_0
I1018 08:53:57.823046 4097533 var_desc.cc:415] Flush  batch_norm2d_2.b_0 1
I1018 08:53:57.823048 4097533 block_desc.cc:209] Flush batch_norm2d_2.w_0
I1018 08:53:57.823050 4097533 var_desc.cc:415] Flush  batch_norm2d_2.w_0 1
I1018 08:53:57.823053 4097533 block_desc.cc:209] Flush conv2d_2.tmp_0
I1018 08:53:57.823055 4097533 var_desc.cc:415] Flush  conv2d_2.tmp_0 1
I1018 08:53:57.823057 4097533 block_desc.cc:209] Flush conv2d_2.w_0
I1018 08:53:57.823060 4097533 var_desc.cc:415] Flush  conv2d_2.w_0 1
I1018 08:53:57.823062 4097533 block_desc.cc:209] Flush relu_1.tmp_0
I1018 08:53:57.823064 4097533 var_desc.cc:415] Flush  relu_1.tmp_0 1
I1018 08:53:57.823066 4097533 block_desc.cc:209] Flush assign_5.tmp_0
I1018 08:53:57.823068 4097533 var_desc.cc:415] Flush  assign_5.tmp_0 1
I1018 08:53:57.823071 4097533 block_desc.cc:209] Flush batch_norm2d_1.b_0
I1018 08:53:57.823073 4097533 var_desc.cc:415] Flush  batch_norm2d_1.b_0 1
I1018 08:53:57.823076 4097533 block_desc.cc:209] Flush batch_norm2d_1.w_0
I1018 08:53:57.823078 4097533 var_desc.cc:415] Flush  batch_norm2d_1.w_0 1
I1018 08:53:57.823081 4097533 block_desc.cc:209] Flush conv2d_1.tmp_0
I1018 08:53:57.823082 4097533 var_desc.cc:415] Flush  conv2d_1.tmp_0 1
I1018 08:53:57.823086 4097533 block_desc.cc:209] Flush conv2d_1.w_0
I1018 08:53:57.823087 4097533 var_desc.cc:415] Flush  conv2d_1.w_0 1
I1018 08:53:57.823089 4097533 block_desc.cc:209] Flush pool2d_0.tmp_0
I1018 08:53:57.823091 4097533 var_desc.cc:415] Flush  pool2d_0.tmp_0 1
I1018 08:53:57.823094 4097533 block_desc.cc:209] Flush assign_56.tmp_0
I1018 08:53:57.823096 4097533 var_desc.cc:415] Flush  assign_56.tmp_0 1
I1018 08:53:57.823098 4097533 block_desc.cc:209] Flush assign_1.tmp_0
I1018 08:53:57.823100 4097533 var_desc.cc:415] Flush  assign_1.tmp_0 1
I1018 08:53:57.823103 4097533 block_desc.cc:209] Flush batch_norm2d_0.b_0
I1018 08:53:57.823105 4097533 var_desc.cc:415] Flush  batch_norm2d_0.b_0 1
I1018 08:53:57.823107 4097533 block_desc.cc:209] Flush batch_norm2d_0.w_0
I1018 08:53:57.823109 4097533 var_desc.cc:415] Flush  batch_norm2d_0.w_0 1
I1018 08:53:57.823112 4097533 block_desc.cc:209] Flush conv2d_0.tmp_0
I1018 08:53:57.823114 4097533 var_desc.cc:415] Flush  conv2d_0.tmp_0 1
I1018 08:53:57.823117 4097533 block_desc.cc:209] Flush _jst.0.x.0
I1018 08:53:57.823118 4097533 var_desc.cc:415] Flush  _jst.0.x.0 1
I1018 08:53:57.823122 4097533 block_desc.cc:209] Flush conv2d_0.w_0
I1018 08:53:57.823123 4097533 var_desc.cc:415] Flush  conv2d_0.w_0 1
I1018 08:53:57.823616 4097533 block_desc.cc:205] vars in desc 180
I1018 08:53:57.828332 4097533 block_desc.cc:205] vars in desc 0
I1018 08:53:57.828341 4097533 block_desc.cc:205] vars in desc 0
I1018 08:53:57.829093 4097533 op_function_common.cc:910] Start Process 5
I1018 08:53:57.829102 4097533 op_function_common.cc:926] Start Process forward_global_block
I1018 08:53:57.829109 4097533 op_function_common.cc:910] Start Process 7
I1018 08:53:57.829113 4097533 op_function_common.cc:926] Start Process backward_global_block
I1018 08:53:57.829114 4097533 op_function_common.cc:910] Start Process 9
I1018 08:53:57.829116 4097533 op_function_common.cc:926] Start Process is_test
I1018 08:53:57.829119 4097533 op_function_common.cc:910] Start Process 11
I1018 08:53:57.829121 4097533 op_function_common.cc:926] Start Process program_id
I1018 08:53:57.829124 4097533 op_function_common.cc:910] Start Process 13
I1018 08:53:57.829126 4097533 op_function_common.cc:926] Start Process in_pir_pt_mode
I1018 08:53:57.829128 4097533 op_function_common.cc:910] Start Process 15
I1018 08:53:57.829130 4097533 op_function_common.cc:926] Start Process x_names
I1018 08:53:57.829135 4097533 run_program_op_func.h:147] start run run_program ad function.
I1018 08:53:57.829140 4097533 run_program_op_func.h:160] start run run_program with require_any_grad = 1
W1018 08:53:57.829144 4097533 tensor.cc:405] The `is_initialized` method is deprecated since version 2.3, and will be removed in version 2.4! Please use `initialized` method instead.
I1018 08:53:57.829180 4097533 run_program_op_node.h:635] RunProgramOpKernel Compute
I1018 08:53:57.829183 4097533 run_program_op_node.h:662] RunProgramOp use interpretercore to execute program.
I1018 08:53:57.829185 4097533 run_program_op_node.h:666] global_inner_scope:0x55b7c51793b0
I1018 08:53:57.829197 4097533 run_program_op_node.h:718] No interpretercore cache, so create a new interpretercore for program: -1040920367077787911
I1018 08:53:57.829200 4097533 run_program_op_node.h:215] Share Tensor Into Scope: _jst.0.x.0
I1018 08:53:57.829203 4097533 scope.cc:203] Create variable _jst.0.x.0
I1018 08:53:57.829221 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_0.b_0
I1018 08:53:57.829223 4097533 scope.cc:203] Create variable batch_norm2d_0.b_0
I1018 08:53:57.829226 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_0.w_0
I1018 08:53:57.829228 4097533 scope.cc:203] Create variable batch_norm2d_0.w_0
I1018 08:53:57.829231 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_0.w_1
I1018 08:53:57.829233 4097533 scope.cc:203] Create variable batch_norm2d_0.w_1
I1018 08:53:57.829236 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_0.w_2
I1018 08:53:57.829238 4097533 scope.cc:203] Create variable batch_norm2d_0.w_2
I1018 08:53:57.829241 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_1.b_0
I1018 08:53:57.829243 4097533 scope.cc:203] Create variable batch_norm2d_1.b_0
I1018 08:53:57.829245 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_1.w_0
I1018 08:53:57.829248 4097533 scope.cc:203] Create variable batch_norm2d_1.w_0
I1018 08:53:57.829250 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_1.w_1
I1018 08:53:57.829252 4097533 scope.cc:203] Create variable batch_norm2d_1.w_1
I1018 08:53:57.829255 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_1.w_2
I1018 08:53:57.829257 4097533 scope.cc:203] Create variable batch_norm2d_1.w_2
I1018 08:53:57.829260 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_10.b_0
I1018 08:53:57.829262 4097533 scope.cc:203] Create variable batch_norm2d_10.b_0
I1018 08:53:57.829264 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_10.w_0
I1018 08:53:57.829267 4097533 scope.cc:203] Create variable batch_norm2d_10.w_0
I1018 08:53:57.829270 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_10.w_1
I1018 08:53:57.829272 4097533 scope.cc:203] Create variable batch_norm2d_10.w_1
I1018 08:53:57.829274 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_10.w_2
I1018 08:53:57.829277 4097533 scope.cc:203] Create variable batch_norm2d_10.w_2
I1018 08:53:57.829279 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_11.b_0
I1018 08:53:57.829282 4097533 scope.cc:203] Create variable batch_norm2d_11.b_0
I1018 08:53:57.829284 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_11.w_0
I1018 08:53:57.829286 4097533 scope.cc:203] Create variable batch_norm2d_11.w_0
I1018 08:53:57.829289 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_11.w_1
I1018 08:53:57.829293 4097533 scope.cc:203] Create variable batch_norm2d_11.w_1
I1018 08:53:57.829294 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_11.w_2
I1018 08:53:57.829296 4097533 scope.cc:203] Create variable batch_norm2d_11.w_2
I1018 08:53:57.829299 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_12.b_0
I1018 08:53:57.829301 4097533 scope.cc:203] Create variable batch_norm2d_12.b_0
I1018 08:53:57.829303 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_12.w_0
I1018 08:53:57.829306 4097533 scope.cc:203] Create variable batch_norm2d_12.w_0
I1018 08:53:57.829309 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_12.w_1
I1018 08:53:57.829311 4097533 scope.cc:203] Create variable batch_norm2d_12.w_1
I1018 08:53:57.829315 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_12.w_2
I1018 08:53:57.829319 4097533 scope.cc:203] Create variable batch_norm2d_12.w_2
I1018 08:53:57.829320 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_13.b_0
I1018 08:53:57.829324 4097533 scope.cc:203] Create variable batch_norm2d_13.b_0
I1018 08:53:57.829325 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_13.w_0
I1018 08:53:57.829327 4097533 scope.cc:203] Create variable batch_norm2d_13.w_0
I1018 08:53:57.829330 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_13.w_1
I1018 08:53:57.829332 4097533 scope.cc:203] Create variable batch_norm2d_13.w_1
I1018 08:53:57.829335 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_13.w_2
I1018 08:53:57.829337 4097533 scope.cc:203] Create variable batch_norm2d_13.w_2
I1018 08:53:57.829339 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_14.b_0
I1018 08:53:57.829342 4097533 scope.cc:203] Create variable batch_norm2d_14.b_0
I1018 08:53:57.829344 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_14.w_0
I1018 08:53:57.829347 4097533 scope.cc:203] Create variable batch_norm2d_14.w_0
I1018 08:53:57.829349 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_14.w_1
I1018 08:53:57.829351 4097533 scope.cc:203] Create variable batch_norm2d_14.w_1
I1018 08:53:57.829355 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_14.w_2
I1018 08:53:57.829356 4097533 scope.cc:203] Create variable batch_norm2d_14.w_2
I1018 08:53:57.829360 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_15.b_0
I1018 08:53:57.829361 4097533 scope.cc:203] Create variable batch_norm2d_15.b_0
I1018 08:53:57.829365 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_15.w_0
I1018 08:53:57.829366 4097533 scope.cc:203] Create variable batch_norm2d_15.w_0
I1018 08:53:57.829370 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_15.w_1
I1018 08:53:57.829371 4097533 scope.cc:203] Create variable batch_norm2d_15.w_1
I1018 08:53:57.829375 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_15.w_2
I1018 08:53:57.829376 4097533 scope.cc:203] Create variable batch_norm2d_15.w_2
I1018 08:53:57.829378 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_16.b_0
I1018 08:53:57.829380 4097533 scope.cc:203] Create variable batch_norm2d_16.b_0
I1018 08:53:57.829383 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_16.w_0
I1018 08:53:57.829385 4097533 scope.cc:203] Create variable batch_norm2d_16.w_0
I1018 08:53:57.829388 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_16.w_1
I1018 08:53:57.829391 4097533 scope.cc:203] Create variable batch_norm2d_16.w_1
I1018 08:53:57.829392 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_16.w_2
I1018 08:53:57.829394 4097533 scope.cc:203] Create variable batch_norm2d_16.w_2
I1018 08:53:57.829397 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_17.b_0
I1018 08:53:57.829401 4097533 scope.cc:203] Create variable batch_norm2d_17.b_0
I1018 08:53:57.829402 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_17.w_0
I1018 08:53:57.829404 4097533 scope.cc:203] Create variable batch_norm2d_17.w_0
I1018 08:53:57.829407 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_17.w_1
I1018 08:53:57.829409 4097533 scope.cc:203] Create variable batch_norm2d_17.w_1
I1018 08:53:57.829412 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_17.w_2
I1018 08:53:57.829414 4097533 scope.cc:203] Create variable batch_norm2d_17.w_2
I1018 08:53:57.829417 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_18.b_0
I1018 08:53:57.829419 4097533 scope.cc:203] Create variable batch_norm2d_18.b_0
I1018 08:53:57.829421 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_18.w_0
I1018 08:53:57.829427 4097533 scope.cc:203] Create variable batch_norm2d_18.w_0
I1018 08:53:57.829428 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_18.w_1
I1018 08:53:57.829430 4097533 scope.cc:203] Create variable batch_norm2d_18.w_1
I1018 08:53:57.829433 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_18.w_2
I1018 08:53:57.829435 4097533 scope.cc:203] Create variable batch_norm2d_18.w_2
I1018 08:53:57.829438 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_19.b_0
I1018 08:53:57.829440 4097533 scope.cc:203] Create variable batch_norm2d_19.b_0
I1018 08:53:57.829442 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_19.w_0
I1018 08:53:57.829445 4097533 scope.cc:203] Create variable batch_norm2d_19.w_0
I1018 08:53:57.829447 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_19.w_1
I1018 08:53:57.829449 4097533 scope.cc:203] Create variable batch_norm2d_19.w_1
I1018 08:53:57.829453 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_19.w_2
I1018 08:53:57.829455 4097533 scope.cc:203] Create variable batch_norm2d_19.w_2
I1018 08:53:57.829457 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_2.b_0
I1018 08:53:57.829459 4097533 scope.cc:203] Create variable batch_norm2d_2.b_0
I1018 08:53:57.829463 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_2.w_0
I1018 08:53:57.829464 4097533 scope.cc:203] Create variable batch_norm2d_2.w_0
I1018 08:53:57.829468 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_2.w_1
I1018 08:53:57.829469 4097533 scope.cc:203] Create variable batch_norm2d_2.w_1
I1018 08:53:57.829471 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_2.w_2
I1018 08:53:57.829473 4097533 scope.cc:203] Create variable batch_norm2d_2.w_2
I1018 08:53:57.829476 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_3.b_0
I1018 08:53:57.829478 4097533 scope.cc:203] Create variable batch_norm2d_3.b_0
I1018 08:53:57.829481 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_3.w_0
I1018 08:53:57.829483 4097533 scope.cc:203] Create variable batch_norm2d_3.w_0
I1018 08:53:57.829486 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_3.w_1
I1018 08:53:57.829488 4097533 scope.cc:203] Create variable batch_norm2d_3.w_1
I1018 08:53:57.829491 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_3.w_2
I1018 08:53:57.829493 4097533 scope.cc:203] Create variable batch_norm2d_3.w_2
I1018 08:53:57.829495 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_4.b_0
I1018 08:53:57.829497 4097533 scope.cc:203] Create variable batch_norm2d_4.b_0
I1018 08:53:57.829500 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_4.w_0
I1018 08:53:57.829502 4097533 scope.cc:203] Create variable batch_norm2d_4.w_0
I1018 08:53:57.829505 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_4.w_1
I1018 08:53:57.829509 4097533 scope.cc:203] Create variable batch_norm2d_4.w_1
I1018 08:53:57.829510 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_4.w_2
I1018 08:53:57.829512 4097533 scope.cc:203] Create variable batch_norm2d_4.w_2
I1018 08:53:57.829515 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_5.b_0
I1018 08:53:57.829517 4097533 scope.cc:203] Create variable batch_norm2d_5.b_0
I1018 08:53:57.829520 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_5.w_0
I1018 08:53:57.829522 4097533 scope.cc:203] Create variable batch_norm2d_5.w_0
I1018 08:53:57.829524 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_5.w_1
I1018 08:53:57.829527 4097533 scope.cc:203] Create variable batch_norm2d_5.w_1
I1018 08:53:57.829530 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_5.w_2
I1018 08:53:57.829532 4097533 scope.cc:203] Create variable batch_norm2d_5.w_2
I1018 08:53:57.829538 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_6.b_0
I1018 08:53:57.829540 4097533 scope.cc:203] Create variable batch_norm2d_6.b_0
I1018 08:53:57.829543 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_6.w_0
I1018 08:53:57.829545 4097533 scope.cc:203] Create variable batch_norm2d_6.w_0
I1018 08:53:57.829548 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_6.w_1
I1018 08:53:57.829550 4097533 scope.cc:203] Create variable batch_norm2d_6.w_1
I1018 08:53:57.829552 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_6.w_2
I1018 08:53:57.829555 4097533 scope.cc:203] Create variable batch_norm2d_6.w_2
I1018 08:53:57.829557 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_7.b_0
I1018 08:53:57.829559 4097533 scope.cc:203] Create variable batch_norm2d_7.b_0
I1018 08:53:57.829562 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_7.w_0
I1018 08:53:57.829564 4097533 scope.cc:203] Create variable batch_norm2d_7.w_0
I1018 08:53:57.829566 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_7.w_1
I1018 08:53:57.829569 4097533 scope.cc:203] Create variable batch_norm2d_7.w_1
I1018 08:53:57.829572 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_7.w_2
I1018 08:53:57.829574 4097533 scope.cc:203] Create variable batch_norm2d_7.w_2
I1018 08:53:57.829576 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_8.b_0
I1018 08:53:57.829578 4097533 scope.cc:203] Create variable batch_norm2d_8.b_0
I1018 08:53:57.829581 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_8.w_0
I1018 08:53:57.829584 4097533 scope.cc:203] Create variable batch_norm2d_8.w_0
I1018 08:53:57.829586 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_8.w_1
I1018 08:53:57.829588 4097533 scope.cc:203] Create variable batch_norm2d_8.w_1
I1018 08:53:57.829591 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_8.w_2
I1018 08:53:57.829593 4097533 scope.cc:203] Create variable batch_norm2d_8.w_2
I1018 08:53:57.829596 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_9.b_0
I1018 08:53:57.829598 4097533 scope.cc:203] Create variable batch_norm2d_9.b_0
I1018 08:53:57.829600 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_9.w_0
I1018 08:53:57.829602 4097533 scope.cc:203] Create variable batch_norm2d_9.w_0
I1018 08:53:57.829605 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_9.w_1
I1018 08:53:57.829607 4097533 scope.cc:203] Create variable batch_norm2d_9.w_1
I1018 08:53:57.829610 4097533 run_program_op_node.h:215] Share Tensor Into Scope: batch_norm2d_9.w_2
I1018 08:53:57.829612 4097533 scope.cc:203] Create variable batch_norm2d_9.w_2
I1018 08:53:57.829615 4097533 run_program_op_node.h:215] Share Tensor Into Scope: conv2d_0.w_0
I1018 08:53:57.829617 4097533 scope.cc:203] Create variable conv2d_0.w_0
I1018 08:53:57.829620 4097533 run_program_op_node.h:215] Share Tensor Into Scope: conv2d_1.w_0
I1018 08:53:57.829622 4097533 scope.cc:203] Create variable conv2d_1.w_0
I1018 08:53:57.829624 4097533 run_program_op_node.h:215] Share Tensor Into Scope: conv2d_10.w_0
I1018 08:53:57.829627 4097533 scope.cc:203] Create variable conv2d_10.w_0
I1018 08:53:57.829629 4097533 run_program_op_node.h:215] Share Tensor Into Scope: conv2d_11.w_0
I1018 08:53:57.829631 4097533 scope.cc:203] Create variable conv2d_11.w_0
I1018 08:53:57.829634 4097533 run_program_op_node.h:215] Share Tensor Into Scope: conv2d_12.w_0
I1018 08:53:57.829636 4097533 scope.cc:203] Create variable conv2d_12.w_0
I1018 08:53:57.829638 4097533 run_program_op_node.h:215] Share Tensor Into Scope: conv2d_13.w_0
I1018 08:53:57.829641 4097533 scope.cc:203] Create variable conv2d_13.w_0
I1018 08:53:57.829643 4097533 run_program_op_node.h:215] Share Tensor Into Scope: conv2d_14.w_0
I1018 08:53:57.829645 4097533 scope.cc:203] Create variable conv2d_14.w_0
I1018 08:53:57.829648 4097533 run_program_op_node.h:215] Share Tensor Into Scope: conv2d_15.w_0
I1018 08:53:57.829653 4097533 scope.cc:203] Create variable conv2d_15.w_0
I1018 08:53:57.829654 4097533 run_program_op_node.h:215] Share Tensor Into Scope: conv2d_16.w_0
I1018 08:53:57.829658 4097533 scope.cc:203] Create variable conv2d_16.w_0
I1018 08:53:57.829659 4097533 run_program_op_node.h:215] Share Tensor Into Scope: conv2d_17.w_0
I1018 08:53:57.829661 4097533 scope.cc:203] Create variable conv2d_17.w_0
I1018 08:53:57.829664 4097533 run_program_op_node.h:215] Share Tensor Into Scope: conv2d_18.w_0
I1018 08:53:57.829666 4097533 scope.cc:203] Create variable conv2d_18.w_0
I1018 08:53:57.829669 4097533 run_program_op_node.h:215] Share Tensor Into Scope: conv2d_19.w_0
I1018 08:53:57.829671 4097533 scope.cc:203] Create variable conv2d_19.w_0
I1018 08:53:57.829674 4097533 run_program_op_node.h:215] Share Tensor Into Scope: conv2d_2.w_0
I1018 08:53:57.829675 4097533 scope.cc:203] Create variable conv2d_2.w_0
I1018 08:53:57.829679 4097533 run_program_op_node.h:215] Share Tensor Into Scope: conv2d_3.w_0
I1018 08:53:57.829680 4097533 scope.cc:203] Create variable conv2d_3.w_0
I1018 08:53:57.829682 4097533 run_program_op_node.h:215] Share Tensor Into Scope: conv2d_4.w_0
I1018 08:53:57.829685 4097533 scope.cc:203] Create variable conv2d_4.w_0
I1018 08:53:57.829689 4097533 run_program_op_node.h:215] Share Tensor Into Scope: conv2d_5.w_0
I1018 08:53:57.829690 4097533 scope.cc:203] Create variable conv2d_5.w_0
I1018 08:53:57.829692 4097533 run_program_op_node.h:215] Share Tensor Into Scope: conv2d_6.w_0
I1018 08:53:57.829694 4097533 scope.cc:203] Create variable conv2d_6.w_0
I1018 08:53:57.829697 4097533 run_program_op_node.h:215] Share Tensor Into Scope: conv2d_7.w_0
I1018 08:53:57.829699 4097533 scope.cc:203] Create variable conv2d_7.w_0
I1018 08:53:57.829701 4097533 run_program_op_node.h:215] Share Tensor Into Scope: conv2d_8.w_0
I1018 08:53:57.829704 4097533 scope.cc:203] Create variable conv2d_8.w_0
I1018 08:53:57.829706 4097533 run_program_op_node.h:215] Share Tensor Into Scope: conv2d_9.w_0
I1018 08:53:57.829710 4097533 scope.cc:203] Create variable conv2d_9.w_0
I1018 08:53:57.829711 4097533 run_program_op_node.h:215] Share Tensor Into Scope: linear_0.b_0
I1018 08:53:57.829713 4097533 scope.cc:203] Create variable linear_0.b_0
I1018 08:53:57.829716 4097533 run_program_op_node.h:215] Share Tensor Into Scope: linear_0.w_0
I1018 08:53:57.829718 4097533 scope.cc:203] Create variable linear_0.w_0
I1018 08:53:57.829723 4097533 interpretercore.cc:45] InterpreterCore(): 0x55b7cfa180f0 on Place(mlu:0)
I1018 08:53:57.829735 4097533 program_interpreter.cc:61] ProgramInterpreter(): 0x55b7c4ea2dd0 on Place(mlu:0)
I1018 08:53:57.829768 4097533 execution_config.cc:106] place:Place(mlu:0), processor_count:112, device_count:8, serial_run:0, num_host_threads:4, num_device_threads:1
I1018 08:53:57.829775 4097533 new_executor_defs.cc:49] Set local scope: 0
I1018 08:53:57.829782 4097533 run_program_op_node.h:785] Get skip GC vars size is: 1
I1018 08:53:57.829798 4097533 program_interpreter.cc:221] New Executor is Running.
I1018 08:53:57.829802 4097533 interpreter_util.cc:1177] Creating Variables
I1018 08:53:57.829813 4097533 interpreter_util.cc:1214] Create Variable _jst.0.x.0 locally, which pointer is 0x55b7b85df050 type is 7
I1018 08:53:57.829818 4097533 scope.cc:203] Create variable assign_0.tmp_0
I1018 08:53:57.829820 4097533 interpreter_util.cc:1214] Create Variable assign_0.tmp_0 locally, which pointer is 0x55b7b75b2ab0 type is 7
I1018 08:53:57.829823 4097533 scope.cc:203] Create variable assign_1.tmp_0
I1018 08:53:57.829825 4097533 interpreter_util.cc:1214] Create Variable assign_1.tmp_0 locally, which pointer is 0x55b7d0e9dbe0 type is 7
I1018 08:53:57.829828 4097533 scope.cc:203] Create variable assign_12.tmp_0
I1018 08:53:57.829830 4097533 interpreter_util.cc:1214] Create Variable assign_12.tmp_0 locally, which pointer is 0x55b7b5910130 type is 7
I1018 08:53:57.829833 4097533 scope.cc:203] Create variable assign_13.tmp_0
I1018 08:53:57.829835 4097533 interpreter_util.cc:1214] Create Variable assign_13.tmp_0 locally, which pointer is 0x55b7d0a55360 type is 7
I1018 08:53:57.829841 4097533 scope.cc:203] Create variable assign_16.tmp_0
I1018 08:53:57.829844 4097533 interpreter_util.cc:1214] Create Variable assign_16.tmp_0 locally, which pointer is 0x55b7d0e95650 type is 7
I1018 08:53:57.829846 4097533 scope.cc:203] Create variable assign_17.tmp_0
I1018 08:53:57.829849 4097533 interpreter_util.cc:1214] Create Variable assign_17.tmp_0 locally, which pointer is 0x55b7d0978990 type is 7
I1018 08:53:57.829851 4097533 scope.cc:203] Create variable assign_20.tmp_0
I1018 08:53:57.829854 4097533 interpreter_util.cc:1214] Create Variable assign_20.tmp_0 locally, which pointer is 0x55b7d0d13440 type is 7
I1018 08:53:57.829856 4097533 scope.cc:203] Create variable assign_21.tmp_0
I1018 08:53:57.829859 4097533 interpreter_util.cc:1214] Create Variable assign_21.tmp_0 locally, which pointer is 0x55b7d0d96260 type is 7
I1018 08:53:57.829861 4097533 scope.cc:203] Create variable assign_24.tmp_0
I1018 08:53:57.829864 4097533 interpreter_util.cc:1214] Create Variable assign_24.tmp_0 locally, which pointer is 0x55b7a60eb560 type is 7
I1018 08:53:57.829866 4097533 scope.cc:203] Create variable assign_25.tmp_0
I1018 08:53:57.829869 4097533 interpreter_util.cc:1214] Create Variable assign_25.tmp_0 locally, which pointer is 0x55b7d0abe400 type is 7
I1018 08:53:57.829871 4097533 scope.cc:203] Create variable assign_28.tmp_0
I1018 08:53:57.829874 4097533 interpreter_util.cc:1214] Create Variable assign_28.tmp_0 locally, which pointer is 0x55b7d0aeda80 type is 7
I1018 08:53:57.829876 4097533 scope.cc:203] Create variable assign_29.tmp_0
I1018 08:53:57.829878 4097533 interpreter_util.cc:1214] Create Variable assign_29.tmp_0 locally, which pointer is 0x55b7d0d0a280 type is 7
I1018 08:53:57.829881 4097533 scope.cc:203] Create variable assign_32.tmp_0
I1018 08:53:57.829883 4097533 interpreter_util.cc:1214] Create Variable assign_32.tmp_0 locally, which pointer is 0x55b7a2c1cb70 type is 7
I1018 08:53:57.829885 4097533 scope.cc:203] Create variable assign_33.tmp_0
I1018 08:53:57.829888 4097533 interpreter_util.cc:1214] Create Variable assign_33.tmp_0 locally, which pointer is 0x55b7983b49a0 type is 7
I1018 08:53:57.829890 4097533 scope.cc:203] Create variable assign_36.tmp_0
I1018 08:53:57.829893 4097533 interpreter_util.cc:1214] Create Variable assign_36.tmp_0 locally, which pointer is 0x55b7d0d4b780 type is 7
I1018 08:53:57.829896 4097533 scope.cc:203] Create variable assign_37.tmp_0
I1018 08:53:57.829898 4097533 interpreter_util.cc:1214] Create Variable assign_37.tmp_0 locally, which pointer is 0x55b7a35ceb90 type is 7
I1018 08:53:57.829901 4097533 scope.cc:203] Create variable assign_4.tmp_0
I1018 08:53:57.829903 4097533 interpreter_util.cc:1214] Create Variable assign_4.tmp_0 locally, which pointer is 0x55b7d0e1a000 type is 7
I1018 08:53:57.829906 4097533 scope.cc:203] Create variable assign_40.tmp_0
I1018 08:53:57.829908 4097533 interpreter_util.cc:1214] Create Variable assign_40.tmp_0 locally, which pointer is 0x55b7b209b590 type is 7
I1018 08:53:57.829911 4097533 scope.cc:203] Create variable assign_41.tmp_0
I1018 08:53:57.829913 4097533 interpreter_util.cc:1214] Create Variable assign_41.tmp_0 locally, which pointer is 0x55b7b7d9d9a0 type is 7
I1018 08:53:57.829916 4097533 scope.cc:203] Create variable assign_44.tmp_0
I1018 08:53:57.829918 4097533 interpreter_util.cc:1214] Create Variable assign_44.tmp_0 locally, which pointer is 0x55b77462ccf0 type is 7
I1018 08:53:57.829921 4097533 scope.cc:203] Create variable assign_45.tmp_0
I1018 08:53:57.829922 4097533 interpreter_util.cc:1214] Create Variable assign_45.tmp_0 locally, which pointer is 0x55b7751dbf50 type is 7
I1018 08:53:57.829926 4097533 scope.cc:203] Create variable assign_48.tmp_0
I1018 08:53:57.829928 4097533 interpreter_util.cc:1214] Create Variable assign_48.tmp_0 locally, which pointer is 0x55b7d0ea6990 type is 7
I1018 08:53:57.829931 4097533 scope.cc:203] Create variable assign_49.tmp_0
I1018 08:53:57.829932 4097533 interpreter_util.cc:1214] Create Variable assign_49.tmp_0 locally, which pointer is 0x55b7987e6280 type is 7
I1018 08:53:57.829937 4097533 scope.cc:203] Create variable assign_5.tmp_0
I1018 08:53:57.829939 4097533 interpreter_util.cc:1214] Create Variable assign_5.tmp_0 locally, which pointer is 0x55b773d8be80 type is 7
I1018 08:53:57.829944 4097533 scope.cc:203] Create variable assign_52.tmp_0
I1018 08:53:57.829946 4097533 interpreter_util.cc:1214] Create Variable assign_52.tmp_0 locally, which pointer is 0x55b7c309eee0 type is 7
I1018 08:53:57.829949 4097533 scope.cc:203] Create variable assign_53.tmp_0
I1018 08:53:57.829952 4097533 interpreter_util.cc:1214] Create Variable assign_53.tmp_0 locally, which pointer is 0x55b775269320 type is 7
I1018 08:53:57.829954 4097533 scope.cc:203] Create variable assign_56.tmp_0
I1018 08:53:57.829957 4097533 interpreter_util.cc:1214] Create Variable assign_56.tmp_0 locally, which pointer is 0x55b784d3fcd0 type is 7
I1018 08:53:57.829959 4097533 scope.cc:203] Create variable assign_57.tmp_0
I1018 08:53:57.829962 4097533 interpreter_util.cc:1214] Create Variable assign_57.tmp_0 locally, which pointer is 0x55b7d0ba0600 type is 7
I1018 08:53:57.829964 4097533 scope.cc:203] Create variable assign_60.tmp_0
I1018 08:53:57.829967 4097533 interpreter_util.cc:1214] Create Variable assign_60.tmp_0 locally, which pointer is 0x55b785b349c0 type is 7
I1018 08:53:57.829969 4097533 scope.cc:203] Create variable assign_61.tmp_0
I1018 08:53:57.829972 4097533 interpreter_util.cc:1214] Create Variable assign_61.tmp_0 locally, which pointer is 0x55b77472b710 type is 7
I1018 08:53:57.829973 4097533 scope.cc:203] Create variable assign_64.tmp_0
I1018 08:53:57.829977 4097533 interpreter_util.cc:1214] Create Variable assign_64.tmp_0 locally, which pointer is 0x55b7b0c57ef0 type is 7
I1018 08:53:57.829979 4097533 scope.cc:203] Create variable assign_65.tmp_0
I1018 08:53:57.829981 4097533 interpreter_util.cc:1214] Create Variable assign_65.tmp_0 locally, which pointer is 0x55b7d0e828b0 type is 7
I1018 08:53:57.829984 4097533 scope.cc:203] Create variable assign_68.tmp_0
I1018 08:53:57.829986 4097533 interpreter_util.cc:1214] Create Variable assign_68.tmp_0 locally, which pointer is 0x55b7b8784180 type is 7
I1018 08:53:57.829989 4097533 scope.cc:203] Create variable assign_69.tmp_0
I1018 08:53:57.829991 4097533 interpreter_util.cc:1214] Create Variable assign_69.tmp_0 locally, which pointer is 0x55b7af1635a0 type is 7
I1018 08:53:57.829993 4097533 scope.cc:203] Create variable assign_72.tmp_0
I1018 08:53:57.829996 4097533 interpreter_util.cc:1214] Create Variable assign_72.tmp_0 locally, which pointer is 0x55b7bae009d0 type is 7
I1018 08:53:57.829998 4097533 scope.cc:203] Create variable assign_73.tmp_0
I1018 08:53:57.830001 4097533 interpreter_util.cc:1214] Create Variable assign_73.tmp_0 locally, which pointer is 0x55b797af2920 type is 7
I1018 08:53:57.830003 4097533 scope.cc:203] Create variable assign_76.tmp_0
I1018 08:53:57.830005 4097533 interpreter_util.cc:1214] Create Variable assign_76.tmp_0 locally, which pointer is 0x55b783b79040 type is 7
I1018 08:53:57.830008 4097533 scope.cc:203] Create variable assign_77.tmp_0
I1018 08:53:57.830011 4097533 interpreter_util.cc:1214] Create Variable assign_77.tmp_0 locally, which pointer is 0x55b7d0d15760 type is 7
I1018 08:53:57.830013 4097533 scope.cc:203] Create variable assign_8.tmp_0
I1018 08:53:57.830015 4097533 interpreter_util.cc:1214] Create Variable assign_8.tmp_0 locally, which pointer is 0x55b7874b6d10 type is 7
I1018 08:53:57.830018 4097533 scope.cc:203] Create variable assign_9.tmp_0
I1018 08:53:57.830020 4097533 interpreter_util.cc:1214] Create Variable assign_9.tmp_0 locally, which pointer is 0x55b7790ec6f0 type is 7
I1018 08:53:57.830024 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_0.b_0 global, which pointer is 0x55b7b77702f0 type is 7
I1018 08:53:57.830026 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_0.w_0 global, which pointer is 0x55b77453b290 type is 7
I1018 08:53:57.830029 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_0.w_1 global, which pointer is 0x55b7790ed900 type is 7
I1018 08:53:57.830034 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_0.w_2 global, which pointer is 0x55b784ec4490 type is 7
I1018 08:53:57.830036 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_1.b_0 global, which pointer is 0x55b7a3f6baa0 type is 7
I1018 08:53:57.830039 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_1.w_0 global, which pointer is 0x55b79370a280 type is 7
I1018 08:53:57.830042 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_1.w_1 global, which pointer is 0x55b7b11a1830 type is 7
I1018 08:53:57.830045 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_1.w_2 global, which pointer is 0x55b7b75953f0 type is 7
I1018 08:53:57.830047 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_10.b_0 global, which pointer is 0x55b775d63b70 type is 7
I1018 08:53:57.830050 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_10.w_0 global, which pointer is 0x55b773d88f40 type is 7
I1018 08:53:57.830053 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_10.w_1 global, which pointer is 0x55b7b11a8510 type is 7
I1018 08:53:57.830055 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_10.w_2 global, which pointer is 0x55b7b75943a0 type is 7
I1018 08:53:57.830058 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_11.b_0 global, which pointer is 0x55b78dc17070 type is 7
I1018 08:53:57.830061 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_11.w_0 global, which pointer is 0x55b7ab2bfba0 type is 7
I1018 08:53:57.830063 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_11.w_1 global, which pointer is 0x55b773f59810 type is 7
I1018 08:53:57.830066 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_11.w_2 global, which pointer is 0x55b7766ee3e0 type is 7
I1018 08:53:57.830068 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_12.b_0 global, which pointer is 0x55b7791e3b70 type is 7
I1018 08:53:57.830071 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_12.w_0 global, which pointer is 0x55b7b14138d0 type is 7
I1018 08:53:57.830075 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_12.w_1 global, which pointer is 0x55b7803ad300 type is 7
I1018 08:53:57.830076 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_12.w_2 global, which pointer is 0x55b7739e1760 type is 7
I1018 08:53:57.830080 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_13.b_0 global, which pointer is 0x55b7a6de0320 type is 7
I1018 08:53:57.830082 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_13.w_0 global, which pointer is 0x55b7b234af20 type is 7
I1018 08:53:57.830085 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_13.w_1 global, which pointer is 0x55b7863c50a0 type is 7
I1018 08:53:57.830088 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_13.w_2 global, which pointer is 0x55b7c3b0aa60 type is 7
I1018 08:53:57.830090 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_14.b_0 global, which pointer is 0x55b775d6b760 type is 7
I1018 08:53:57.830093 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_14.w_0 global, which pointer is 0x55b7b046e220 type is 7
I1018 08:53:57.830096 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_14.w_1 global, which pointer is 0x55b7b25c1930 type is 7
I1018 08:53:57.830098 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_14.w_2 global, which pointer is 0x55b7b7dab1f0 type is 7
I1018 08:53:57.830101 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_15.b_0 global, which pointer is 0x55b7d0a26320 type is 7
I1018 08:53:57.830103 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_15.w_0 global, which pointer is 0x55b7d0aebd70 type is 7
I1018 08:53:57.830106 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_15.w_1 global, which pointer is 0x55b7d093ae60 type is 7
I1018 08:53:57.830109 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_15.w_2 global, which pointer is 0x55b7d0978a70 type is 7
I1018 08:53:57.830113 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_16.b_0 global, which pointer is 0x55b7d0a12280 type is 7
I1018 08:53:57.830116 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_16.w_0 global, which pointer is 0x55b7d0dca7b0 type is 7
I1018 08:53:57.830118 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_16.w_1 global, which pointer is 0x55b7d0cec650 type is 7
I1018 08:53:57.830121 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_16.w_2 global, which pointer is 0x55b7d0a55e50 type is 7
I1018 08:53:57.830123 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_17.b_0 global, which pointer is 0x55b7d0e8e500 type is 7
I1018 08:53:57.830127 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_17.w_0 global, which pointer is 0x55b7d0e74080 type is 7
I1018 08:53:57.830129 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_17.w_1 global, which pointer is 0x55b7d09c5940 type is 7
I1018 08:53:57.830132 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_17.w_2 global, which pointer is 0x55b7d0b33a10 type is 7
I1018 08:53:57.830134 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_18.b_0 global, which pointer is 0x55b7b7d8bdd0 type is 7
I1018 08:53:57.830137 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_18.w_0 global, which pointer is 0x55b7d0b54da0 type is 7
I1018 08:53:57.830139 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_18.w_1 global, which pointer is 0x55b7a819e1d0 type is 7
I1018 08:53:57.830142 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_18.w_2 global, which pointer is 0x55b7d0e49a00 type is 7
I1018 08:53:57.830145 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_19.b_0 global, which pointer is 0x55b7d0b16420 type is 7
I1018 08:53:57.830147 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_19.w_0 global, which pointer is 0x55b784ec1a90 type is 7
I1018 08:53:57.830150 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_19.w_1 global, which pointer is 0x55b7d0e499a0 type is 7
I1018 08:53:57.830153 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_19.w_2 global, which pointer is 0x55b78dbb50d0 type is 7
I1018 08:53:57.830155 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_2.b_0 global, which pointer is 0x55b7d0e116a0 type is 7
I1018 08:53:57.830158 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_2.w_0 global, which pointer is 0x55b7c0140cc0 type is 7
I1018 08:53:57.830161 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_2.w_1 global, which pointer is 0x55b7739f29c0 type is 7
I1018 08:53:57.830163 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_2.w_2 global, which pointer is 0x55b7d0f3da30 type is 7
I1018 08:53:57.830166 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_3.b_0 global, which pointer is 0x55b7d0e5a300 type is 7
I1018 08:53:57.830168 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_3.w_0 global, which pointer is 0x55b7987ea6c0 type is 7
I1018 08:53:57.830171 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_3.w_1 global, which pointer is 0x55b7d09f7140 type is 7
I1018 08:53:57.830174 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_3.w_2 global, which pointer is 0x55b784a8d0a0 type is 7
I1018 08:53:57.830178 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_4.b_0 global, which pointer is 0x55b7d0c48d60 type is 7
I1018 08:53:57.830179 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_4.w_0 global, which pointer is 0x55b7b32f3d20 type is 7
I1018 08:53:57.830183 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_4.w_1 global, which pointer is 0x55b7d0bb6030 type is 7
I1018 08:53:57.830184 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_4.w_2 global, which pointer is 0x55b783ea18f0 type is 7
I1018 08:53:57.830188 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_5.b_0 global, which pointer is 0x55b7a8165340 type is 7
I1018 08:53:57.830191 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_5.w_0 global, which pointer is 0x55b7d0b9f080 type is 7
I1018 08:53:57.830194 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_5.w_1 global, which pointer is 0x55b7b774d1f0 type is 7
I1018 08:53:57.830197 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_5.w_2 global, which pointer is 0x55b7d0dd8a40 type is 7
I1018 08:53:57.830199 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_6.b_0 global, which pointer is 0x55b7d0a77c00 type is 7
I1018 08:53:57.830202 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_6.w_0 global, which pointer is 0x55b7d0cac380 type is 7
I1018 08:53:57.830205 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_6.w_1 global, which pointer is 0x55b7d0e94e90 type is 7
I1018 08:53:57.830207 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_6.w_2 global, which pointer is 0x55b7b6216eb0 type is 7
I1018 08:53:57.830210 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_7.b_0 global, which pointer is 0x55b7ad72fc30 type is 7
I1018 08:53:57.830212 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_7.w_0 global, which pointer is 0x55b7d0d99370 type is 7
I1018 08:53:57.830215 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_7.w_1 global, which pointer is 0x55b77472a190 type is 7
I1018 08:53:57.830217 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_7.w_2 global, which pointer is 0x55b7d0d60340 type is 7
I1018 08:53:57.830220 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_8.b_0 global, which pointer is 0x55b7a95039b0 type is 7
I1018 08:53:57.830224 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_8.w_0 global, which pointer is 0x55b7d0e055d0 type is 7
I1018 08:53:57.830225 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_8.w_1 global, which pointer is 0x55b7d0cfa6a0 type is 7
I1018 08:53:57.830228 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_8.w_2 global, which pointer is 0x55b7d0ca1170 type is 7
I1018 08:53:57.830230 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_9.b_0 global, which pointer is 0x55b7d0be0ae0 type is 7
I1018 08:53:57.830233 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_9.w_0 global, which pointer is 0x55b7d0f002e0 type is 7
I1018 08:53:57.830235 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_9.w_1 global, which pointer is 0x55b7d0ccc240 type is 7
I1018 08:53:57.830238 4097533 interpreter_util.cc:1209] Create Variable batch_norm2d_9.w_2 global, which pointer is 0x55b7d0b78eb0 type is 7
I1018 08:53:57.830241 4097533 scope.cc:203] Create variable conv2d_0.tmp_0
I1018 08:53:57.830245 4097533 interpreter_util.cc:1214] Create Variable conv2d_0.tmp_0 locally, which pointer is 0x55b773ea9af0 type is 7
I1018 08:53:57.830247 4097533 interpreter_util.cc:1209] Create Variable conv2d_0.w_0 global, which pointer is 0x55b7875d5970 type is 7
I1018 08:53:57.830250 4097533 scope.cc:203] Create variable conv2d_1.tmp_0
I1018 08:53:57.830252 4097533 interpreter_util.cc:1214] Create Variable conv2d_1.tmp_0 locally, which pointer is 0x55b7b1b4d220 type is 7
I1018 08:53:57.830255 4097533 interpreter_util.cc:1209] Create Variable conv2d_1.w_0 global, which pointer is 0x55b7ccbc68c0 type is 7
I1018 08:53:57.830257 4097533 scope.cc:203] Create variable conv2d_10.tmp_0
I1018 08:53:57.830260 4097533 interpreter_util.cc:1214] Create Variable conv2d_10.tmp_0 locally, which pointer is 0x55b7a35d2610 type is 7
I1018 08:53:57.830262 4097533 interpreter_util.cc:1209] Create Variable conv2d_10.w_0 global, which pointer is 0x55b7d0efa960 type is 7
I1018 08:53:57.830265 4097533 scope.cc:203] Create variable conv2d_11.tmp_0
I1018 08:53:57.830267 4097533 interpreter_util.cc:1214] Create Variable conv2d_11.tmp_0 locally, which pointer is 0x55b7b2c28050 type is 7
I1018 08:53:57.830271 4097533 interpreter_util.cc:1209] Create Variable conv2d_11.w_0 global, which pointer is 0x55b7d0b9f2c0 type is 7
I1018 08:53:57.830276 4097533 scope.cc:203] Create variable conv2d_12.tmp_0
I1018 08:53:57.830277 4097533 interpreter_util.cc:1214] Create Variable conv2d_12.tmp_0 locally, which pointer is 0x55b7a184bc60 type is 7
I1018 08:53:57.830281 4097533 interpreter_util.cc:1209] Create Variable conv2d_12.w_0 global, which pointer is 0x55b7c4ec69c0 type is 7
I1018 08:53:57.830282 4097533 scope.cc:203] Create variable conv2d_13.tmp_0
I1018 08:53:57.830286 4097533 interpreter_util.cc:1214] Create Variable conv2d_13.tmp_0 locally, which pointer is 0x55b7cd73d5c0 type is 7
I1018 08:53:57.830287 4097533 interpreter_util.cc:1209] Create Variable conv2d_13.w_0 global, which pointer is 0x55b7d0e9e100 type is 7
I1018 08:53:57.830291 4097533 scope.cc:203] Create variable conv2d_14.tmp_0
I1018 08:53:57.830292 4097533 interpreter_util.cc:1214] Create Variable conv2d_14.tmp_0 locally, which pointer is 0x55b7b78fde80 type is 7
I1018 08:53:57.830296 4097533 interpreter_util.cc:1209] Create Variable conv2d_14.w_0 global, which pointer is 0x55b79d22beb0 type is 7
I1018 08:53:57.830297 4097533 scope.cc:203] Create variable conv2d_15.tmp_0
I1018 08:53:57.830300 4097533 interpreter_util.cc:1214] Create Variable conv2d_15.tmp_0 locally, which pointer is 0x55b7746141b0 type is 7
I1018 08:53:57.830302 4097533 interpreter_util.cc:1209] Create Variable conv2d_15.w_0 global, which pointer is 0x55b7d0e36bc0 type is 7
I1018 08:53:57.830305 4097533 scope.cc:203] Create variable conv2d_16.tmp_0
I1018 08:53:57.830308 4097533 interpreter_util.cc:1214] Create Variable conv2d_16.tmp_0 locally, which pointer is 0x55b7b209eb20 type is 7
I1018 08:53:57.830310 4097533 interpreter_util.cc:1209] Create Variable conv2d_16.w_0 global, which pointer is 0x55b7d0e9d6f0 type is 7
I1018 08:53:57.830313 4097533 scope.cc:203] Create variable conv2d_17.tmp_0
I1018 08:53:57.830315 4097533 interpreter_util.cc:1214] Create Variable conv2d_17.tmp_0 locally, which pointer is 0x55b77ef498a0 type is 7
I1018 08:53:57.830318 4097533 interpreter_util.cc:1209] Create Variable conv2d_17.w_0 global, which pointer is 0x55b773f67180 type is 7
I1018 08:53:57.830320 4097533 scope.cc:203] Create variable conv2d_18.tmp_0
I1018 08:53:57.830323 4097533 interpreter_util.cc:1214] Create Variable conv2d_18.tmp_0 locally, which pointer is 0x55b7b1532880 type is 7
I1018 08:53:57.830325 4097533 interpreter_util.cc:1209] Create Variable conv2d_18.w_0 global, which pointer is 0x55b7d0d380e0 type is 7
I1018 08:53:57.830328 4097533 scope.cc:203] Create variable conv2d_19.tmp_0
I1018 08:53:57.830330 4097533 interpreter_util.cc:1214] Create Variable conv2d_19.tmp_0 locally, which pointer is 0x55b7cfb66bc0 type is 7
I1018 08:53:57.830333 4097533 interpreter_util.cc:1209] Create Variable conv2d_19.w_0 global, which pointer is 0x55b7ad729390 type is 7
I1018 08:53:57.830335 4097533 scope.cc:203] Create variable conv2d_2.tmp_0
I1018 08:53:57.830338 4097533 interpreter_util.cc:1214] Create Variable conv2d_2.tmp_0 locally, which pointer is 0x55b7c517b9d0 type is 7
I1018 08:53:57.830340 4097533 interpreter_util.cc:1209] Create Variable conv2d_2.w_0 global, which pointer is 0x55b797be4a70 type is 7
I1018 08:53:57.830343 4097533 scope.cc:203] Create variable conv2d_3.tmp_0
I1018 08:53:57.830345 4097533 interpreter_util.cc:1214] Create Variable conv2d_3.tmp_0 locally, which pointer is 0x55b7ab7b7de0 type is 7
I1018 08:53:57.830348 4097533 interpreter_util.cc:1209] Create Variable conv2d_3.w_0 global, which pointer is 0x55b7d0e96fe0 type is 7
I1018 08:53:57.830350 4097533 scope.cc:203] Create variable conv2d_4.tmp_0
I1018 08:53:57.830353 4097533 interpreter_util.cc:1214] Create Variable conv2d_4.tmp_0 locally, which pointer is 0x55b7cfb512d0 type is 7
I1018 08:53:57.830355 4097533 interpreter_util.cc:1209] Create Variable conv2d_4.w_0 global, which pointer is 0x55b7d0ea3370 type is 7
I1018 08:53:57.830358 4097533 scope.cc:203] Create variable conv2d_5.tmp_0
I1018 08:53:57.830360 4097533 interpreter_util.cc:1214] Create Variable conv2d_5.tmp_0 locally, which pointer is 0x55b7cfb57280 type is 7
I1018 08:53:57.830364 4097533 interpreter_util.cc:1209] Create Variable conv2d_5.w_0 global, which pointer is 0x55b7d0baf620 type is 7
I1018 08:53:57.830368 4097533 scope.cc:203] Create variable conv2d_6.tmp_0
I1018 08:53:57.830370 4097533 interpreter_util.cc:1214] Create Variable conv2d_6.tmp_0 locally, which pointer is 0x55b7cfb57b10 type is 7
I1018 08:53:57.830372 4097533 interpreter_util.cc:1209] Create Variable conv2d_6.w_0 global, which pointer is 0x55b7d0c23680 type is 7
I1018 08:53:57.830375 4097533 scope.cc:203] Create variable conv2d_7.tmp_0
I1018 08:53:57.830377 4097533 interpreter_util.cc:1214] Create Variable conv2d_7.tmp_0 locally, which pointer is 0x55b7cfb5aa90 type is 7
I1018 08:53:57.830379 4097533 interpreter_util.cc:1209] Create Variable conv2d_7.w_0 global, which pointer is 0x55b7d0e15ea0 type is 7
I1018 08:53:57.830382 4097533 scope.cc:203] Create variable conv2d_8.tmp_0
I1018 08:53:57.830385 4097533 interpreter_util.cc:1214] Create Variable conv2d_8.tmp_0 locally, which pointer is 0x55b7cfbaec50 type is 7
I1018 08:53:57.830387 4097533 interpreter_util.cc:1209] Create Variable conv2d_8.w_0 global, which pointer is 0x55b7b70f4a50 type is 7
I1018 08:53:57.830390 4097533 scope.cc:203] Create variable conv2d_9.tmp_0
I1018 08:53:57.830392 4097533 interpreter_util.cc:1214] Create Variable conv2d_9.tmp_0 locally, which pointer is 0x55b7cfc74d50 type is 7
I1018 08:53:57.830394 4097533 interpreter_util.cc:1209] Create Variable conv2d_9.w_0 global, which pointer is 0x55b7b85d5830 type is 7
I1018 08:53:57.830397 4097533 interpreter_util.cc:1209] Create Variable linear_0.b_0 global, which pointer is 0x55b7d0e05130 type is 7
I1018 08:53:57.830400 4097533 scope.cc:203] Create variable linear_0.tmp_1
I1018 08:53:57.830402 4097533 interpreter_util.cc:1214] Create Variable linear_0.tmp_1 locally, which pointer is 0x55b7cfbdfca0 type is 7
I1018 08:53:57.830405 4097533 interpreter_util.cc:1209] Create Variable linear_0.w_0 global, which pointer is 0x55b783e8d900 type is 7
I1018 08:53:57.830407 4097533 scope.cc:203] Create variable pool2d_0.tmp_0
I1018 08:53:57.830410 4097533 interpreter_util.cc:1214] Create Variable pool2d_0.tmp_0 locally, which pointer is 0x55b7cfc25d30 type is 7
I1018 08:53:57.830412 4097533 scope.cc:203] Create variable relu_1.tmp_0
I1018 08:53:57.830415 4097533 interpreter_util.cc:1214] Create Variable relu_1.tmp_0 locally, which pointer is 0x55b7cfc1a260 type is 7
I1018 08:53:57.830417 4097533 scope.cc:203] Create variable relu_10.tmp_0
I1018 08:53:57.830420 4097533 interpreter_util.cc:1214] Create Variable relu_10.tmp_0 locally, which pointer is 0x55b7cfc52490 type is 7
I1018 08:53:57.830422 4097533 scope.cc:203] Create variable relu_11.tmp_0
I1018 08:53:57.830425 4097533 interpreter_util.cc:1214] Create Variable relu_11.tmp_0 locally, which pointer is 0x55b7cfc99280 type is 7
I1018 08:53:57.830427 4097533 scope.cc:203] Create variable relu_12.tmp_0
I1018 08:53:57.830430 4097533 interpreter_util.cc:1214] Create Variable relu_12.tmp_0 locally, which pointer is 0x55b7cfcb1f70 type is 7
I1018 08:53:57.830432 4097533 scope.cc:203] Create variable relu_13.tmp_0
I1018 08:53:57.830435 4097533 interpreter_util.cc:1214] Create Variable relu_13.tmp_0 locally, which pointer is 0x55b7cfc6f0a0 type is 7
I1018 08:53:57.830437 4097533 scope.cc:203] Create variable relu_14.tmp_0
I1018 08:53:57.830439 4097533 interpreter_util.cc:1214] Create Variable relu_14.tmp_0 locally, which pointer is 0x55b7cfcbb000 type is 7
I1018 08:53:57.830441 4097533 scope.cc:203] Create variable relu_15.tmp_0
I1018 08:53:57.830444 4097533 interpreter_util.cc:1214] Create Variable relu_15.tmp_0 locally, which pointer is 0x55b7d08dfe90 type is 7
I1018 08:53:57.830446 4097533 scope.cc:203] Create variable relu_2.tmp_0
I1018 08:53:57.830449 4097533 interpreter_util.cc:1214] Create Variable relu_2.tmp_0 locally, which pointer is 0x55b7916795e0 type is 7
I1018 08:53:57.830451 4097533 scope.cc:203] Create variable relu_3.tmp_0
I1018 08:53:57.830454 4097533 interpreter_util.cc:1214] Create Variable relu_3.tmp_0 locally, which pointer is 0x55b773c499a0 type is 7
I1018 08:53:57.830458 4097533 scope.cc:203] Create variable relu_4.tmp_0
I1018 08:53:57.830461 4097533 interpreter_util.cc:1214] Create Variable relu_4.tmp_0 locally, which pointer is 0x55b7bae2f9b0 type is 7
I1018 08:53:57.830463 4097533 scope.cc:203] Create variable relu_5.tmp_0
I1018 08:53:57.830466 4097533 interpreter_util.cc:1214] Create Variable relu_5.tmp_0 locally, which pointer is 0x55b77390c080 type is 7
I1018 08:53:57.830468 4097533 scope.cc:203] Create variable relu_6.tmp_0
I1018 08:53:57.830471 4097533 interpreter_util.cc:1214] Create Variable relu_6.tmp_0 locally, which pointer is 0x55b78495b820 type is 7
I1018 08:53:57.830473 4097533 scope.cc:203] Create variable relu_7.tmp_0
I1018 08:53:57.830475 4097533 interpreter_util.cc:1214] Create Variable relu_7.tmp_0 locally, which pointer is 0x55b798c878c0 type is 7
I1018 08:53:57.830478 4097533 scope.cc:203] Create variable relu_8.tmp_0
I1018 08:53:57.830480 4097533 interpreter_util.cc:1214] Create Variable relu_8.tmp_0 locally, which pointer is 0x55b773ee3900 type is 7
I1018 08:53:57.830482 4097533 scope.cc:203] Create variable relu_9.tmp_0
I1018 08:53:57.830485 4097533 interpreter_util.cc:1214] Create Variable relu_9.tmp_0 locally, which pointer is 0x55b775d67a90 type is 7
I1018 08:53:57.830868 4097533 interpreter_util.cc:597] Static build: 0
I1018 08:53:57.830883 4097533 var_desc.cc:415] Flush  _jst.0.x.0 1
I1018 08:53:57.830886 4097533 var_desc.cc:415] Flush  conv2d_0.tmp_0 1
I1018 08:53:57.830889 4097533 var_desc.cc:415] Flush  conv2d_0.tmp_0 0
I1018 08:53:57.830895 4097533 var_desc.cc:415] Flush  assign_1.tmp_0 1
I1018 08:53:57.830897 4097533 var_desc.cc:415] Flush  pool2d_0.tmp_0 1
I1018 08:53:57.830900 4097533 var_desc.cc:415] Flush  pool2d_0.tmp_0 0
I1018 08:53:57.830904 4097533 var_desc.cc:415] Flush  conv2d_1.tmp_0 1
I1018 08:53:57.830906 4097533 var_desc.cc:415] Flush  conv2d_1.tmp_0 0
I1018 08:53:57.830909 4097533 var_desc.cc:415] Flush  assign_5.tmp_0 1
I1018 08:53:57.830912 4097533 var_desc.cc:415] Flush  relu_1.tmp_0 1
I1018 08:53:57.830914 4097533 var_desc.cc:415] Flush  relu_1.tmp_0 0
I1018 08:53:57.830917 4097533 var_desc.cc:415] Flush  conv2d_2.tmp_0 1
I1018 08:53:57.830919 4097533 var_desc.cc:415] Flush  conv2d_2.tmp_0 0
I1018 08:53:57.830924 4097533 var_desc.cc:415] Flush  pool2d_0.tmp_0 0
I1018 08:53:57.830926 4097533 var_desc.cc:415] Flush  assign_9.tmp_0 1
I1018 08:53:57.830929 4097533 var_desc.cc:415] Flush  relu_2.tmp_0 1
I1018 08:53:57.830931 4097533 var_desc.cc:415] Flush  relu_2.tmp_0 0
I1018 08:53:57.830935 4097533 var_desc.cc:415] Flush  conv2d_3.tmp_0 1
I1018 08:53:57.830936 4097533 var_desc.cc:415] Flush  conv2d_3.tmp_0 0
I1018 08:53:57.830940 4097533 var_desc.cc:415] Flush  assign_13.tmp_0 1
I1018 08:53:57.830942 4097533 var_desc.cc:415] Flush  relu_3.tmp_0 1
I1018 08:53:57.830945 4097533 var_desc.cc:415] Flush  relu_3.tmp_0 0
I1018 08:53:57.830947 4097533 var_desc.cc:415] Flush  conv2d_4.tmp_0 1
I1018 08:53:57.830950 4097533 var_desc.cc:415] Flush  relu_2.tmp_0 0
I1018 08:53:57.830955 4097533 var_desc.cc:415] Flush  conv2d_4.tmp_0 0
I1018 08:53:57.830956 4097533 var_desc.cc:415] Flush  assign_17.tmp_0 1
I1018 08:53:57.830960 4097533 var_desc.cc:415] Flush  relu_4.tmp_0 1
I1018 08:53:57.830962 4097533 var_desc.cc:415] Flush  relu_4.tmp_0 0
I1018 08:53:57.830964 4097533 var_desc.cc:415] Flush  conv2d_5.tmp_0 1
I1018 08:53:57.830966 4097533 var_desc.cc:415] Flush  conv2d_5.tmp_0 0
I1018 08:53:57.830971 4097533 var_desc.cc:415] Flush  assign_21.tmp_0 1
I1018 08:53:57.830972 4097533 var_desc.cc:415] Flush  relu_5.tmp_0 1
I1018 08:53:57.830976 4097533 var_desc.cc:415] Flush  relu_5.tmp_0 0
I1018 08:53:57.830977 4097533 var_desc.cc:415] Flush  conv2d_6.tmp_0 1
I1018 08:53:57.830981 4097533 var_desc.cc:415] Flush  relu_4.tmp_0 0
I1018 08:53:57.830982 4097533 var_desc.cc:415] Flush  conv2d_7.tmp_0 1
I1018 08:53:57.830986 4097533 var_desc.cc:415] Flush  conv2d_7.tmp_0 0
I1018 08:53:57.830989 4097533 var_desc.cc:415] Flush  conv2d_6.tmp_0 0
I1018 08:53:57.830992 4097533 var_desc.cc:415] Flush  assign_29.tmp_0 1
I1018 08:53:57.830997 4097533 var_desc.cc:415] Flush  relu_6.tmp_0 1
I1018 08:53:57.831000 4097533 var_desc.cc:415] Flush  assign_25.tmp_0 1
I1018 08:53:57.831003 4097533 var_desc.cc:415] Flush  relu_6.tmp_0 0
I1018 08:53:57.831005 4097533 var_desc.cc:415] Flush  conv2d_8.tmp_0 1
I1018 08:53:57.831008 4097533 var_desc.cc:415] Flush  conv2d_8.tmp_0 0
I1018 08:53:57.831012 4097533 var_desc.cc:415] Flush  assign_33.tmp_0 1
I1018 08:53:57.831013 4097533 var_desc.cc:415] Flush  relu_7.tmp_0 1
I1018 08:53:57.831017 4097533 var_desc.cc:415] Flush  relu_7.tmp_0 0
I1018 08:53:57.831018 4097533 var_desc.cc:415] Flush  conv2d_9.tmp_0 1
I1018 08:53:57.831022 4097533 var_desc.cc:415] Flush  conv2d_9.tmp_0 0
I1018 08:53:57.831024 4097533 var_desc.cc:415] Flush  relu_6.tmp_0 0
I1018 08:53:57.831027 4097533 var_desc.cc:415] Flush  assign_37.tmp_0 1
I1018 08:53:57.831029 4097533 var_desc.cc:415] Flush  relu_8.tmp_0 1
I1018 08:53:57.831032 4097533 var_desc.cc:415] Flush  relu_8.tmp_0 0
I1018 08:53:57.831034 4097533 var_desc.cc:415] Flush  conv2d_10.tmp_0 1
I1018 08:53:57.831037 4097533 var_desc.cc:415] Flush  conv2d_10.tmp_0 0
I1018 08:53:57.831040 4097533 var_desc.cc:415] Flush  assign_41.tmp_0 1
I1018 08:53:57.831043 4097533 var_desc.cc:415] Flush  relu_9.tmp_0 1
I1018 08:53:57.831045 4097533 var_desc.cc:415] Flush  relu_9.tmp_0 0
I1018 08:53:57.831048 4097533 var_desc.cc:415] Flush  conv2d_11.tmp_0 1
I1018 08:53:57.831050 4097533 var_desc.cc:415] Flush  relu_8.tmp_0 0
I1018 08:53:57.831053 4097533 var_desc.cc:415] Flush  conv2d_12.tmp_0 1
I1018 08:53:57.831055 4097533 var_desc.cc:415] Flush  conv2d_12.tmp_0 0
I1018 08:53:57.831060 4097533 var_desc.cc:415] Flush  conv2d_11.tmp_0 0
I1018 08:53:57.831063 4097533 var_desc.cc:415] Flush  assign_49.tmp_0 1
I1018 08:53:57.831064 4097533 var_desc.cc:415] Flush  relu_10.tmp_0 1
I1018 08:53:57.831068 4097533 var_desc.cc:415] Flush  assign_45.tmp_0 1
I1018 08:53:57.831070 4097533 var_desc.cc:415] Flush  relu_10.tmp_0 0
I1018 08:53:57.831072 4097533 var_desc.cc:415] Flush  conv2d_13.tmp_0 1
I1018 08:53:57.831074 4097533 var_desc.cc:415] Flush  conv2d_13.tmp_0 0
I1018 08:53:57.831079 4097533 var_desc.cc:415] Flush  assign_53.tmp_0 1
I1018 08:53:57.831080 4097533 var_desc.cc:415] Flush  relu_11.tmp_0 1
I1018 08:53:57.831084 4097533 var_desc.cc:415] Flush  relu_11.tmp_0 0
I1018 08:53:57.831085 4097533 var_desc.cc:415] Flush  conv2d_14.tmp_0 1
I1018 08:53:57.831089 4097533 var_desc.cc:415] Flush  conv2d_14.tmp_0 0
I1018 08:53:57.831091 4097533 var_desc.cc:415] Flush  relu_10.tmp_0 0
I1018 08:53:57.831094 4097533 var_desc.cc:415] Flush  assign_57.tmp_0 1
I1018 08:53:57.831096 4097533 var_desc.cc:415] Flush  relu_12.tmp_0 1
I1018 08:53:57.831099 4097533 var_desc.cc:415] Flush  relu_12.tmp_0 0
I1018 08:53:57.831101 4097533 var_desc.cc:415] Flush  conv2d_15.tmp_0 1
I1018 08:53:57.831104 4097533 var_desc.cc:415] Flush  conv2d_15.tmp_0 0
I1018 08:53:57.831107 4097533 var_desc.cc:415] Flush  assign_61.tmp_0 1
I1018 08:53:57.831110 4097533 var_desc.cc:415] Flush  relu_13.tmp_0 1
I1018 08:53:57.831112 4097533 var_desc.cc:415] Flush  relu_13.tmp_0 0
I1018 08:53:57.831115 4097533 var_desc.cc:415] Flush  conv2d_16.tmp_0 1
I1018 08:53:57.831117 4097533 var_desc.cc:415] Flush  relu_12.tmp_0 0
I1018 08:53:57.831120 4097533 var_desc.cc:415] Flush  conv2d_17.tmp_0 1
I1018 08:53:57.831122 4097533 var_desc.cc:415] Flush  conv2d_17.tmp_0 0
I1018 08:53:57.831126 4097533 var_desc.cc:415] Flush  conv2d_16.tmp_0 0
I1018 08:53:57.831128 4097533 var_desc.cc:415] Flush  assign_69.tmp_0 1
I1018 08:53:57.831131 4097533 var_desc.cc:415] Flush  relu_14.tmp_0 1
I1018 08:53:57.831133 4097533 var_desc.cc:415] Flush  assign_65.tmp_0 1
I1018 08:53:57.831136 4097533 var_desc.cc:415] Flush  relu_14.tmp_0 0
I1018 08:53:57.831138 4097533 var_desc.cc:415] Flush  conv2d_18.tmp_0 1
I1018 08:53:57.831140 4097533 var_desc.cc:415] Flush  conv2d_18.tmp_0 0
I1018 08:53:57.831144 4097533 var_desc.cc:415] Flush  assign_73.tmp_0 1
I1018 08:53:57.831146 4097533 var_desc.cc:415] Flush  relu_15.tmp_0 1
I1018 08:53:57.831149 4097533 var_desc.cc:415] Flush  relu_15.tmp_0 0
I1018 08:53:57.831153 4097533 var_desc.cc:415] Flush  conv2d_19.tmp_0 1
I1018 08:53:57.831156 4097533 var_desc.cc:415] Flush  conv2d_19.tmp_0 0
I1018 08:53:57.831161 4097533 var_desc.cc:415] Flush  relu_14.tmp_0 0
I1018 08:53:57.831162 4097533 var_desc.cc:415] Flush  assign_77.tmp_0 1
I1018 08:53:57.831164 4097533 var_desc.cc:415] Flush  linear_0.tmp_1 1
I1018 08:53:57.831177 4097533 var_desc.cc:415] Flush  assign_0.tmp_0 1
I1018 08:53:57.831180 4097533 var_desc.cc:415] Flush  assign_4.tmp_0 1
I1018 08:53:57.831183 4097533 var_desc.cc:415] Flush  assign_8.tmp_0 1
I1018 08:53:57.831185 4097533 var_desc.cc:415] Flush  assign_12.tmp_0 1
I1018 08:53:57.831188 4097533 var_desc.cc:415] Flush  assign_16.tmp_0 1
I1018 08:53:57.831190 4097533 var_desc.cc:415] Flush  assign_20.tmp_0 1
I1018 08:53:57.831192 4097533 var_desc.cc:415] Flush  assign_24.tmp_0 1
I1018 08:53:57.831194 4097533 var_desc.cc:415] Flush  assign_28.tmp_0 1
I1018 08:53:57.831197 4097533 var_desc.cc:415] Flush  assign_32.tmp_0 1
I1018 08:53:57.831199 4097533 var_desc.cc:415] Flush  assign_36.tmp_0 1
I1018 08:53:57.831202 4097533 var_desc.cc:415] Flush  assign_40.tmp_0 1
I1018 08:53:57.831204 4097533 var_desc.cc:415] Flush  assign_44.tmp_0 1
I1018 08:53:57.831207 4097533 var_desc.cc:415] Flush  assign_48.tmp_0 1
I1018 08:53:57.831209 4097533 var_desc.cc:415] Flush  assign_52.tmp_0 1
I1018 08:53:57.831211 4097533 var_desc.cc:415] Flush  assign_56.tmp_0 1
I1018 08:53:57.831213 4097533 var_desc.cc:415] Flush  assign_60.tmp_0 1
I1018 08:53:57.831216 4097533 var_desc.cc:415] Flush  assign_64.tmp_0 1
I1018 08:53:57.831218 4097533 var_desc.cc:415] Flush  assign_68.tmp_0 1
I1018 08:53:57.831220 4097533 var_desc.cc:415] Flush  assign_72.tmp_0 1
I1018 08:53:57.831223 4097533 var_desc.cc:415] Flush  assign_76.tmp_0 1
I1018 08:53:57.831226 4097533 interpreter_util.cc:306] cinn_launch assign_64.tmp_0
I1018 08:53:57.831228 4097533 interpreter_util.cc:306] cinn_launch assign_60.tmp_0
I1018 08:53:57.831230 4097533 interpreter_util.cc:306] cinn_launch assign_56.tmp_0
I1018 08:53:57.831233 4097533 interpreter_util.cc:306] cinn_launch assign_68.tmp_0
I1018 08:53:57.831234 4097533 interpreter_util.cc:306] cinn_launch assign_52.tmp_0
I1018 08:53:57.831236 4097533 interpreter_util.cc:306] cinn_launch assign_48.tmp_0
I1018 08:53:57.831238 4097533 interpreter_util.cc:306] cinn_launch assign_44.tmp_0
I1018 08:53:57.831240 4097533 interpreter_util.cc:306] cinn_launch assign_28.tmp_0
I1018 08:53:57.831243 4097533 interpreter_util.cc:306] cinn_launch assign_12.tmp_0
I1018 08:53:57.831244 4097533 interpreter_util.cc:306] cinn_launch assign_8.tmp_0
I1018 08:53:57.831246 4097533 interpreter_util.cc:306] cinn_launch assign_4.tmp_0
I1018 08:53:57.831249 4097533 interpreter_util.cc:306] cinn_launch conv2d_9.tmp_0
I1018 08:53:57.831251 4097533 interpreter_util.cc:306] cinn_launch assign_25.tmp_0
I1018 08:53:57.831254 4097533 interpreter_util.cc:306] cinn_launch relu_14.tmp_0
I1018 08:53:57.831255 4097533 interpreter_util.cc:306] cinn_launch conv2d_6.tmp_0
I1018 08:53:57.831257 4097533 interpreter_util.cc:306] cinn_launch pool2d_0.tmp_0
I1018 08:53:57.831259 4097533 interpreter_util.cc:306] cinn_launch assign_76.tmp_0
I1018 08:53:57.831261 4097533 interpreter_util.cc:306] cinn_launch assign_21.tmp_0
I1018 08:53:57.831264 4097533 interpreter_util.cc:306] cinn_launch conv2d_7.tmp_0
I1018 08:53:57.831266 4097533 interpreter_util.cc:306] conv2d relu_4.tmp_0
I1018 08:53:57.831269 4097533 interpreter_util.cc:306] conv2d relu_7.tmp_0
I1018 08:53:57.831270 4097533 interpreter_util.cc:306] conv2d relu_11.tmp_0
I1018 08:53:57.831272 4097533 interpreter_util.cc:306] cinn_launch assign_17.tmp_0
I1018 08:53:57.831274 4097533 interpreter_util.cc:306] cinn_launch relu_2.tmp_0
I1018 08:53:57.831276 4097533 interpreter_util.cc:306] cinn_launch relu_6.tmp_0
I1018 08:53:57.831279 4097533 interpreter_util.cc:306] conv2d relu_5.tmp_0
I1018 08:53:57.831280 4097533 interpreter_util.cc:306] cinn_launch assign_49.tmp_0
I1018 08:53:57.831282 4097533 interpreter_util.cc:306] cinn_launch assign_33.tmp_0
I1018 08:53:57.831286 4097533 interpreter_util.cc:306] cinn_launch conv2d_4.tmp_0
I1018 08:53:57.831288 4097533 interpreter_util.cc:306] cinn_launch assign_37.tmp_0
I1018 08:53:57.831290 4097533 interpreter_util.cc:306] cinn_launch conv2d_19.tmp_0
I1018 08:53:57.831292 4097533 interpreter_util.cc:306] conv2d relu_9.tmp_0
I1018 08:53:57.831295 4097533 interpreter_util.cc:306] cinn_launch assign_72.tmp_0
I1018 08:53:57.831296 4097533 interpreter_util.cc:306] cinn_launch conv2d_0.tmp_0
I1018 08:53:57.831298 4097533 interpreter_util.cc:306] conv2d relu_3.tmp_0
I1018 08:53:57.831300 4097533 interpreter_util.cc:306] cinn_launch assign_32.tmp_0
I1018 08:53:57.831302 4097533 interpreter_util.cc:306] cinn_launch assign_1.tmp_0
I1018 08:53:57.831305 4097533 interpreter_util.cc:306] conv2d relu_1.tmp_0
I1018 08:53:57.831307 4097533 interpreter_util.cc:306] cinn_launch assign_0.tmp_0
I1018 08:53:57.831310 4097533 interpreter_util.cc:306] cinn_launch conv2d_5.tmp_0
I1018 08:53:57.831311 4097533 interpreter_util.cc:306] cinn_launch assign_13.tmp_0
I1018 08:53:57.831313 4097533 interpreter_util.cc:306] cinn_launch conv2d_1.tmp_0
I1018 08:53:57.831315 4097533 interpreter_util.cc:306] conv2d relu_12.tmp_0
I1018 08:53:57.831317 4097533 interpreter_util.cc:306] cinn_launch assign_5.tmp_0
I1018 08:53:57.831319 4097533 interpreter_util.cc:306] cinn_launch conv2d_8.tmp_0
I1018 08:53:57.831321 4097533 interpreter_util.cc:306] cinn_launch assign_16.tmp_0
I1018 08:53:57.831324 4097533 interpreter_util.cc:306] cinn_launch conv2d_2.tmp_0
I1018 08:53:57.831326 4097533 interpreter_util.cc:306] cinn_launch assign_29.tmp_0
I1018 08:53:57.831328 4097533 interpreter_util.cc:306] cinn_launch assign_73.tmp_0
I1018 08:53:57.831331 4097533 interpreter_util.cc:306] cinn_launch conv2d_17.tmp_0
I1018 08:53:57.831332 4097533 interpreter_util.cc:306] cinn_launch linear_0.tmp_1
I1018 08:53:57.831334 4097533 interpreter_util.cc:306] cinn_launch assign_9.tmp_0
I1018 08:53:57.831336 4097533 interpreter_util.cc:306] conv2d _jst.0.x.0
I1018 08:53:57.831338 4097533 interpreter_util.cc:306] conv2d relu_8.tmp_0
I1018 08:53:57.831341 4097533 interpreter_util.cc:306] cinn_launch conv2d_10.tmp_0
I1018 08:53:57.831342 4097533 interpreter_util.cc:306] cinn_launch assign_41.tmp_0
I1018 08:53:57.831344 4097533 interpreter_util.cc:306] cinn_launch assign_45.tmp_0
I1018 08:53:57.831346 4097533 interpreter_util.cc:306] cinn_launch assign_20.tmp_0
I1018 08:53:57.831348 4097533 interpreter_util.cc:306] cinn_launch conv2d_11.tmp_0
I1018 08:53:57.831350 4097533 interpreter_util.cc:306] cinn_launch conv2d_12.tmp_0
I1018 08:53:57.831352 4097533 interpreter_util.cc:306] cinn_launch relu_10.tmp_0
I1018 08:53:57.831354 4097533 interpreter_util.cc:306] cinn_launch assign_36.tmp_0
I1018 08:53:57.831357 4097533 interpreter_util.cc:306] conv2d relu_15.tmp_0
I1018 08:53:57.831359 4097533 interpreter_util.cc:306] cinn_launch conv2d_18.tmp_0
I1018 08:53:57.831362 4097533 interpreter_util.cc:306] cinn_launch conv2d_13.tmp_0
I1018 08:53:57.831363 4097533 interpreter_util.cc:306] cinn_launch assign_69.tmp_0
I1018 08:53:57.831365 4097533 interpreter_util.cc:306] cinn_launch assign_24.tmp_0
I1018 08:53:57.831367 4097533 interpreter_util.cc:306] cinn_launch assign_53.tmp_0
I1018 08:53:57.831369 4097533 interpreter_util.cc:306] cinn_launch conv2d_14.tmp_0
I1018 08:53:57.831372 4097533 interpreter_util.cc:306] cinn_launch assign_40.tmp_0
I1018 08:53:57.831373 4097533 interpreter_util.cc:306] cinn_launch assign_57.tmp_0
I1018 08:53:57.831375 4097533 interpreter_util.cc:306] cinn_launch conv2d_15.tmp_0
I1018 08:53:57.831377 4097533 interpreter_util.cc:306] cinn_launch conv2d_16.tmp_0
I1018 08:53:57.831379 4097533 interpreter_util.cc:306] cinn_launch assign_61.tmp_0
I1018 08:53:57.831382 4097533 interpreter_util.cc:306] conv2d relu_13.tmp_0
I1018 08:53:57.831383 4097533 interpreter_util.cc:306] cinn_launch conv2d_3.tmp_0
I1018 08:53:57.831385 4097533 interpreter_util.cc:306] cinn_launch assign_65.tmp_0
I1018 08:53:57.831388 4097533 interpreter_util.cc:306] cinn_launch assign_77.tmp_0
I1018 08:53:57.831394 4097533 interpreter_util.cc:308] gc map size:49
I1018 08:53:57.831410 4097533 interpreter_util.cc:732] Start run Place(mlu:0) Op(conv2d), inputs:{Filter[conv2d_0.w_0:float[64, 3, 7, 7]({})(Place(mlu:0))], Input[_jst.0.x.0:float[1, 3, 224, 224]({})(Place(mlu:0))]}, outputs:{Output[conv2d_0.tmp_0:[]({})()]}.
I1018 08:53:57.831439 4097533 interpreter_util.cc:749] OP is not null
I1018 08:53:57.831441 4097533 interpreter_util.cc:752] get op_with_kernel
I1018 08:53:57.831444 4097533 interpreter_util.cc:757] get RuntimeContext
I1018 08:53:57.831456 4097533 interpreter_util.cc:786] expected_kernel_key : {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(mlu:0)]; library_type[PLAIN]}
I1018 08:53:57.831480 4097533 operator.cc:2297] op type:conv2d, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(mlu:0)]; library_type[PLAIN]}
I1018 08:53:57.831486 4097533 interpreter_util.cc:834] if run phi kernel? : 1
I1018 08:53:57.831490 4097533 interpreter_util.cc:851] conv2d : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(mlu:0)]; library_type[PLAIN]}
I1018 08:53:57.831516 4097533 interpreter_util.cc:866] apply data transform done. 
I1018 08:53:57.831517 4097533 interpreter_util.cc:870] infer shape
I1018 08:53:57.831521 4097533 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: conv2d; inputs: Input, Filter; attributes: strides, paddings, padding_algorithm, dilations, groups, data_format; outputs: Output
I1018 08:53:57.831543 4097533 operator.cc:3270] Done inputs
I1018 08:53:57.831547 4097533 operator.cc:3329] Done outputs
I1018 08:53:57.831552 4097533 operator.cc:3580] Done attributes
I1018 08:53:57.831744 4097533 interpreter_util.cc:1069] End run Place(mlu:0) Op(conv2d), inputs:{Filter[conv2d_0.w_0:float[64, 3, 7, 7]({})(Place(mlu:0))], Input[_jst.0.x.0:float[1, 3, 224, 224]({})(Place(mlu:0))]}, outputs:{Output[conv2d_0.tmp_0:float[1, 64, 112, 112]({})(Place(mlu:0))]}.
I1018 08:53:57.831775 4097533 interpreter_util.cc:732] Start run Place(mlu:0) Op(cinn_launch), inputs:{NoNeedBufferX[], X[batch_norm2d_0.w_0:float[64]({})(Place(mlu:0)), conv2d_0.tmp_0:float[1, 64, 112, 112]({})(Place(mlu:0)), batch_norm2d_0.b_0:float[64]({})(Place(mlu:0)), batch_norm2d_0.w_1:float[64]({})(Place(mlu:0)), batch_norm2d_0.w_2:float[64]({})(Place(mlu:0))]}, outputs:{Out[assign_1.tmp_0:[]({})(), pool2d_0.tmp_0:[]({})()]}.
I1018 08:53:57.831792 4097533 interpreter_util.cc:749] OP is not null
I1018 08:53:57.831794 4097533 interpreter_util.cc:752] get op_with_kernel
I1018 08:53:57.831796 4097533 interpreter_util.cc:757] get RuntimeContext
I1018 08:53:57.831800 4097533 interpreter_util.cc:786] expected_kernel_key : {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(mlu:0)]; library_type[PLAIN]}
I1018 08:53:57.831827 4097533 operator.cc:2297] op type:cinn_launch, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(mlu:0)]; library_type[PLAIN]}
I1018 08:53:57.831835 4097533 interpreter_util.cc:814] fluid missing (mlu, Undefined(AnyLayout), float32) kernel: cinn_launch
I1018 08:53:57.831840 4097533 phi_utils.cc:130] phi missing mlu kernel: cinn_launch, expected_kernel_key:(mlu, Undefined(AnyLayout), float32), fallback to CPU one!
I1018 08:53:57.831843 4097533 interpreter_util.cc:834] if run phi kernel? : 1
I1018 08:53:57.831846 4097533 interpreter_util.cc:851] cinn_launch : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(mlu:0)]; library_type[PLAIN]}
I1018 08:53:57.831856 4097533 interpreter_util.cc:866] apply data transform done. 
I1018 08:53:57.831858 4097533 interpreter_util.cc:870] infer shape
I1018 08:53:57.831868 4097533 var_desc.cc:415] Flush  pool2d_0.tmp_0 0
I1018 08:53:57.831872 4097533 var_desc.cc:415] Flush  assign_1.tmp_0 0
I1018 08:53:57.831876 4097533 var_desc.cc:415] Flush  batch_norm2d_0.w_2 0
I1018 08:53:57.831879 4097533 var_desc.cc:415] Flush  batch_norm2d_0.w_1 0
I1018 08:53:57.831881 4097533 var_desc.cc:415] Flush  batch_norm2d_0.b_0 0
I1018 08:53:57.831888 4097533 var_desc.cc:415] Flush  conv2d_0.tmp_0 0
I1018 08:53:57.831892 4097533 var_desc.cc:415] Flush  fill_constant_7.tmp_0 0
I1018 08:53:57.831893 4097533 var_desc.cc:415] Flush  tmp_11 0
I1018 08:53:57.831897 4097533 var_desc.cc:415] Flush  reshape2_0.tmp_0 0
I1018 08:53:57.831899 4097533 var_desc.cc:415] Flush  elementwise_pow_0.tmp_0 0
I1018 08:53:57.831902 4097533 var_desc.cc:415] Flush  reshape2_1.tmp_1 0
I1018 08:53:57.831905 4097533 var_desc.cc:415] Flush  reshape2_3.tmp_0 0
I1018 08:53:57.831907 4097533 var_desc.cc:415] Flush  reshape2_1.tmp_0 0
I1018 08:53:57.831910 4097533 var_desc.cc:415] Flush  tmp_8 0
I1018 08:53:57.831913 4097533 var_desc.cc:415] Flush  tmp_10 0
I1018 08:53:57.831916 4097533 var_desc.cc:415] Flush  batch_norm2d_0.w_0 0
I1018 08:53:57.831919 4097533 var_desc.cc:415] Flush  tmp_12 0
I1018 08:53:57.831923 4097533 var_desc.cc:415] Flush  fill_constant_5.tmp_0 0
I1018 08:53:57.831925 4097533 var_desc.cc:415] Flush  reshape2_0.tmp_1 0
I1018 08:53:57.831928 4097533 var_desc.cc:415] Flush  tmp_9 0
I1018 08:53:57.831930 4097533 var_desc.cc:415] Flush  batch_norm_0.tmp_2 0
I1018 08:53:57.831933 4097533 var_desc.cc:415] Flush  relu_0.tmp_0 0
I1018 08:53:57.831935 4097533 var_desc.cc:415] Flush  fill_constant_1.tmp_0 0
I1018 08:53:57.831938 4097533 var_desc.cc:415] Flush  reshape2_3.tmp_1 0
I1018 08:53:57.831941 4097533 var_desc.cc:415] Flush  reshape2_2.tmp_1 0
I1018 08:53:57.831943 4097533 var_desc.cc:415] Flush  reshape2_2.tmp_0 0
I1018 08:53:57.831945 4097533 var_desc.cc:415] Flush  elementwise_pow_2.tmp_0 0
I1018 08:53:57.831966 4097533 graph_helper.h:104] adj pool2d0x55b7d095cb80 -> fetch0x55b7d0ef6f90  via pool2d_0.tmp_00x55b7d0ef6080
I1018 08:53:57.831974 4097533 graph_helper.h:104] adj feed0x55b7d0ef64a0 -> reshape20x55b7d0eedd60  via batch_norm2d_0.w_00x55b7d0ef54e0
I1018 08:53:57.831982 4097533 graph_helper.h:104] adj reshape20x55b7d0edf4a0 -> elementwise_sub0x55b7d0ee7510  via reshape2_0.tmp_00x55b7d0ef46e0
I1018 08:53:57.831990 4097533 graph_helper.h:104] adj feed0x55b7d0ef6270 -> elementwise_sub0x55b7d0ee7510  via conv2d_0.tmp_00x55b7d0ef56d0
I1018 08:53:57.831995 4097533 graph_helper.h:104] adj scale0x55b7b87a3dd0 -> elementwise_pow0x55b7d0ee5080  via tmp_80x55b7d0ef2bd0
I1018 08:53:57.832000 4097533 graph_helper.h:104] adj fill_constant0x55b7d0ef0110 -> elementwise_pow0x55b7d0ee5080  via fill_constant_1.tmp_00x55b7d0ef3fb0
I1018 08:53:57.832005 4097533 graph_helper.h:104] adj reshape20x55b7d0943960 -> scale0x55b7d0941260  via reshape2_1.tmp_00x55b7d0ef3e40
I1018 08:53:57.832013 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d093d210 -> elementwise_add0x55b7d0945ba0  via tmp_120x55b7d0ef4d20
I1018 08:53:57.832019 4097533 graph_helper.h:104] adj reshape20x55b7d0ee2c90 -> elementwise_add0x55b7d0945ba0  via reshape2_3.tmp_00x55b7d0ef28f0
I1018 08:53:57.832026 4097533 graph_helper.h:104] adj elementwise_add0x55b7d0945ba0 -> elementwise_max0x55b7d0947b00  via batch_norm_0.tmp_20x55b7d0ef2780
I1018 08:53:57.832031 4097533 graph_helper.h:104] adj fill_constant0x55b7d094a1b0 -> elementwise_max0x55b7d0947b00  via fill_constant_7.tmp_00x55b7d0ef5100
I1018 08:53:57.832036 4097533 graph_helper.h:104] adj fill_constant0x55b7b87a17e0 -> scale0x55b7b87a3dd0  via fill_constant_5.tmp_00x55b7d0ef26c0
I1018 08:53:57.832042 4097533 graph_helper.h:104] adj reshape20x55b7d0eedd60 -> elementwise_mul0x55b7d093d210  via reshape2_2.tmp_00x55b7d0ef4f10
I1018 08:53:57.832048 4097533 graph_helper.h:104] adj elementwise_mul0x55b7d094d0e0 -> elementwise_mul0x55b7d093d210  via tmp_110x55b7d0ef4b30
I1018 08:53:57.832053 4097533 graph_helper.h:104] adj feed0x55b7d0ef6900 -> reshape20x55b7d0943960  via batch_norm2d_0.w_20x55b7d0ef5ca0
I1018 08:53:57.832060 4097533 graph_helper.h:104] adj elementwise_sub0x55b7d0ee7510 -> elementwise_mul0x55b7d094d0e0  via tmp_90x55b7d0ef2a60
I1018 08:53:57.832067 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0ee98d0 -> elementwise_mul0x55b7d094d0e0  via elementwise_pow_2.tmp_00x55b7d0ef4400
I1018 08:53:57.832072 4097533 graph_helper.h:104] adj scale0x55b7d0941260 -> elementwise_pow0x55b7d0ee98d0  via tmp_100x55b7d0ef49c0
I1018 08:53:57.832080 4097533 graph_helper.h:104] adj fill_constant0x55b7d0ef0110 -> elementwise_pow0x55b7d0ee98d0  via fill_constant_1.tmp_00x55b7d0ef3fb0
I1018 08:53:57.832087 4097533 graph_helper.h:104] adj elementwise_pow0x55b7d0ee5080 -> assign0x55b7d0eebd50  via elementwise_pow_0.tmp_00x55b7d0ef4570
I1018 08:53:57.832091 4097533 graph_helper.h:104] adj feed0x55b7d0ef6b30 -> reshape20x55b7d0edf4a0  via batch_norm2d_0.w_10x55b7d0ef5ab0
I1018 08:53:57.832098 4097533 graph_helper.h:104] adj assign0x55b7d0eebd50 -> fetch0x55b7d0ef6d60  via assign_1.tmp_00x55b7d0ef5e90
I1018 08:53:57.832103 4097533 graph_helper.h:104] adj feed0x55b7d0ef66d0 -> reshape20x55b7d0ee2c90  via batch_norm2d_0.b_00x55b7d0ef58c0
I1018 08:53:57.832110 4097533 graph_helper.h:104] adj elementwise_max0x55b7d0947b00 -> pool2d0x55b7d095cb80  via relu_0.tmp_00x55b7d0ef2d40
I1018 08:53:57.832252 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.832254 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.832260 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.832263 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.832273 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.832276 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.832288 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.832291 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.832299 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.832302 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.832329 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.832330 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.832334 4097533 graph_helper.cc:698] convert op node to desc elementwise_sub
I1018 08:53:57.832336 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.832345 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.832346 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.832355 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.832357 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.832406 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.832409 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.832417 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.832418 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.832422 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.832424 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.832432 4097533 graph_helper.cc:698] convert op node to desc elementwise_mul
I1018 08:53:57.832435 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.832442 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.832445 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.832453 4097533 graph_helper.cc:698] convert op node to desc feed
I1018 08:53:57.832454 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.832458 4097533 graph_helper.cc:698] convert op node to desc reshape2
I1018 08:53:57.832460 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.832468 4097533 graph_helper.cc:698] convert op node to desc elementwise_add
I1018 08:53:57.832471 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.832571 4097533 graph_helper.cc:698] convert op node to desc elementwise_max
I1018 08:53:57.832574 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.832580 4097533 graph_helper.cc:698] convert op node to desc pool2d
I1018 08:53:57.832583 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.832592 4097533 graph_helper.cc:698] convert op node to desc fill_constant
I1018 08:53:57.832593 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.832600 4097533 graph_helper.cc:698] convert op node to desc scale
I1018 08:53:57.832602 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.832610 4097533 graph_helper.cc:698] convert op node to desc elementwise_pow
I1018 08:53:57.832613 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.832620 4097533 graph_helper.cc:698] convert op node to desc assign
I1018 08:53:57.832625 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.832633 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.832636 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.832639 4097533 graph_helper.cc:698] convert op node to desc fetch
I1018 08:53:57.832643 4097533 graph_helper.cc:676] 0 0
I1018 08:53:57.833489 4097533 graph_helper.cc:829] Graph to program need convert 1 sub graph
I1018 08:53:57.834091 4097533 block_desc.cc:205] vars in desc 27
I1018 08:53:57.834100 4097533 block_desc.cc:209] Flush pool2d_0.tmp_0
I1018 08:53:57.834102 4097533 var_desc.cc:415] Flush  pool2d_0.tmp_0 1
I1018 08:53:57.834105 4097533 block_desc.cc:209] Flush assign_1.tmp_0
I1018 08:53:57.834107 4097533 var_desc.cc:415] Flush  assign_1.tmp_0 1
I1018 08:53:57.834110 4097533 block_desc.cc:209] Flush batch_norm2d_0.w_2
I1018 08:53:57.834113 4097533 var_desc.cc:415] Flush  batch_norm2d_0.w_2 1
I1018 08:53:57.834116 4097533 block_desc.cc:209] Flush batch_norm2d_0.w_1
I1018 08:53:57.834118 4097533 var_desc.cc:415] Flush  batch_norm2d_0.w_1 1
I1018 08:53:57.834120 4097533 block_desc.cc:209] Flush batch_norm2d_0.b_0
I1018 08:53:57.834123 4097533 var_desc.cc:415] Flush  batch_norm2d_0.b_0 1
I1018 08:53:57.834125 4097533 block_desc.cc:209] Flush conv2d_0.tmp_0
I1018 08:53:57.834127 4097533 var_desc.cc:415] Flush  conv2d_0.tmp_0 1
I1018 08:53:57.834131 4097533 block_desc.cc:209] Flush fill_constant_7.tmp_0
I1018 08:53:57.834132 4097533 var_desc.cc:415] Flush  fill_constant_7.tmp_0 1
I1018 08:53:57.834136 4097533 block_desc.cc:209] Flush tmp_11
I1018 08:53:57.834137 4097533 var_desc.cc:415] Flush  tmp_11 1
I1018 08:53:57.834141 4097533 block_desc.cc:209] Flush reshape2_0.tmp_0
I1018 08:53:57.834142 4097533 var_desc.cc:415] Flush  reshape2_0.tmp_0 1
I1018 08:53:57.834144 4097533 block_desc.cc:209] Flush elementwise_pow_0.tmp_0
I1018 08:53:57.834147 4097533 var_desc.cc:415] Flush  elementwise_pow_0.tmp_0 1
I1018 08:53:57.834151 4097533 block_desc.cc:209] Flush reshape2_1.tmp_1
I1018 08:53:57.834152 4097533 var_desc.cc:415] Flush  reshape2_1.tmp_1 1
I1018 08:53:57.834156 4097533 block_desc.cc:209] Flush reshape2_3.tmp_0
I1018 08:53:57.834157 4097533 var_desc.cc:415] Flush  reshape2_3.tmp_0 1
I1018 08:53:57.834160 4097533 block_desc.cc:209] Flush reshape2_1.tmp_0
I1018 08:53:57.834162 4097533 var_desc.cc:415] Flush  reshape2_1.tmp_0 1
I1018 08:53:57.834165 4097533 block_desc.cc:209] Flush tmp_8
I1018 08:53:57.834167 4097533 var_desc.cc:415] Flush  tmp_8 1
I1018 08:53:57.834169 4097533 block_desc.cc:209] Flush tmp_10
I1018 08:53:57.834172 4097533 var_desc.cc:415] Flush  tmp_10 1
I1018 08:53:57.834174 4097533 block_desc.cc:209] Flush batch_norm2d_0.w_0
I1018 08:53:57.834177 4097533 var_desc.cc:415] Flush  batch_norm2d_0.w_0 1
I1018 08:53:57.834179 4097533 block_desc.cc:209] Flush tmp_12
I1018 08:53:57.834182 4097533 var_desc.cc:415] Flush  tmp_12 1
I1018 08:53:57.834184 4097533 block_desc.cc:209] Flush fill_constant_5.tmp_0
I1018 08:53:57.834187 4097533 var_desc.cc:415] Flush  fill_constant_5.tmp_0 1
I1018 08:53:57.834188 4097533 block_desc.cc:209] Flush reshape2_0.tmp_1
I1018 08:53:57.834192 4097533 var_desc.cc:415] Flush  reshape2_0.tmp_1 1
I1018 08:53:57.834193 4097533 block_desc.cc:209] Flush tmp_9
I1018 08:53:57.834195 4097533 var_desc.cc:415] Flush  tmp_9 1
I1018 08:53:57.834198 4097533 block_desc.cc:209] Flush batch_norm_0.tmp_2
I1018 08:53:57.834200 4097533 var_desc.cc:415] Flush  batch_norm_0.tmp_2 1
I1018 08:53:57.834203 4097533 block_desc.cc:209] Flush relu_0.tmp_0
I1018 08:53:57.834205 4097533 var_desc.cc:415] Flush  relu_0.tmp_0 1
I1018 08:53:57.834208 4097533 block_desc.cc:209] Flush fill_constant_1.tmp_0
I1018 08:53:57.834209 4097533 var_desc.cc:415] Flush  fill_constant_1.tmp_0 1
I1018 08:53:57.834213 4097533 block_desc.cc:209] Flush reshape2_3.tmp_1
I1018 08:53:57.834214 4097533 var_desc.cc:415] Flush  reshape2_3.tmp_1 1
I1018 08:53:57.834218 4097533 block_desc.cc:209] Flush reshape2_2.tmp_1
I1018 08:53:57.834219 4097533 var_desc.cc:415] Flush  reshape2_2.tmp_1 1
I1018 08:53:57.834228 4097533 block_desc.cc:209] Flush reshape2_2.tmp_0
I1018 08:53:57.834230 4097533 var_desc.cc:415] Flush  reshape2_2.tmp_0 1
I1018 08:53:57.834232 4097533 block_desc.cc:209] Flush elementwise_pow_2.tmp_0
I1018 08:53:57.834234 4097533 var_desc.cc:415] Flush  elementwise_pow_2.tmp_0 1
I1018 08:53:57.831862 4097533 cinn_launch_op.h:85] CinnLaunchOp attribute(compilation_key) value:
blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "pool2d_0.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 56
          dims: 56
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "assign_1.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "batch_norm2d_0.w_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "batch_norm2d_0.w_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: true
  }
  vars {
    name: "batch_norm2d_0.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "conv2d_0.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 112
          dims: 112
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "fill_constant_7.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_11"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 112
          dims: 112
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "reshape2_0.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "elementwise_pow_0.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_1.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_3.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_1.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_8"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  vars {
    name: "tmp_10"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    stop_gradient: true
  }
  vars {
    name: "batch_norm2d_0.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
      }
    }
    persistable: true
    is_parameter: true
    stop_gradient: false
  }
  vars {
    name: "tmp_12"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 112
          dims: 112
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "fill_constant_5.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_0.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "tmp_9"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 112
          dims: 112
        }
        lod_level: 0
      }
    }
    stop_gradient: false
  }
  vars {
    name: "batch_norm_0.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 112
          dims: 112
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "relu_0.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 112
          dims: 112
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "fill_constant_1.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: true
  }
  vars {
    name: "reshape2_3.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_2.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 0
          dims: 64
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "reshape2_2.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 1
          dims: 1
        }
      }
    }
    persistable: false
    stop_gradient: false
  }
  vars {
    name: "elementwise_pow_2.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
          dims: 1
          dims: 1
        }
        lod_level: 0
      }
    }
    persistable: false
    stop_gradient: true
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_0.w_1"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_0.w_1"
    }
    outputs {
      parameter: "Out"
      arguments: "reshape2_0.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_0.tmp_1"
    }
    type: "reshape2"
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 123, in composite_batchnorm"
      strings: "    x_hat = (x - reshape(run_mean, stats_shape)) * pow("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/manipulation.py\", line 4518, in reshape"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 1
      ints: 64
      ints: 1
      ints: 1
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "batch_norm2d_0.w_2"
    }
    type: "feed"
  }
  ops {
    inputs {
      parameter: "Shape"
    }
    inputs {
      parameter: "ShapeTensor"
    }
    inputs {
      parameter: "X"
      arguments: "batch_norm2d_0.w_2"
    }
    outputs {
      parameter: "Out"
      arguments: "reshape2_1.tmp_0"
    }
    outputs {
      parameter: "XShape"
      arguments: "reshape2_1.tmp_1"
    }
    type: "reshape2"
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "mkldnn_data_type"
      type: STRING
      s: "float32"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 124, in composite_batchnorm"
      strings: "    (reshape(run_var, stats_shape) + epsilon), half"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/tensor/manipulation.py\", line 4518, in reshape"
      strings: "    helper.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layer_helper.py\", line 50, in append_op"
      strings: "    return self.main_program.current_block().append_op(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "shape"
      type: INTS
      ints: 1
      ints: 64
      ints: 1
      ints: 1
    }
    attrs {
      name: "use_quantizer"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "with_quant_attr"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "BiasTensor"
    }
    inputs {
      parameter: "ScaleTensor"
    }
    inputs {
      parameter: "X"
      arguments: "reshape2_1.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "tmp_10"
    }
    type: "scale"
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "bias"
      type: FLOAT
      f: 1e-05
    }
    attrs {
      name: "bias_after_scale"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 127, in <module>"
      strings: "    model.benchmark(use_cinn=True)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 84, in benchmark"
      strings: "    benchmark(net, self.input)"
      strings: "  File \"/home/wzy/paddle_tests/models/benchmark.py\", line 37, in benchmark"
      strings: "    net(input)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/nn/layer/layers.py\", line 1426, in __call__"
      strings: "    return self.forward(*inputs, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 493, in __call__"
      strings: "    return self._perform_call(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 797, in _perform_call"
      strings: "    return partial_program_layer(args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 229, in __call__"
      strings: "    attrs = self._prepare_attributes()"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 782, in _prepare_attributes"
      strings: "    self.forward_program.desc.block(0),"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 549, in forward_program"
      strings: "    forward_program = self.infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 531, in infer_program"
      strings: "    infer_program = self._infer_program"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 106, in __get__"
      strings: "    val = self.function(instance)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 400, in _infer_program"
      strings: "    program, op_size = self._infer_info(\'fp32\', self._create_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 131, in __call__"
      strings: "    infer_prog = prog_creator(is_infer_mode=True)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/partial_program.py\", line 317, in _create_program"
      strings: "    infer_program = self._hooker.after_infer(infer_program)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1676, in after_infer"
      strings: "    _to_prim(infer_program.block(0))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/dygraph/base.py\", line 66, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/jit/dy2static/program_translator.py\", line 1800, in _to_prim"
      strings: "    primapi.to_prim("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/decorator.py\", line 232, in fun"
      strings: "    return caller(func, *(extras + args), **kw)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/wrapped_decorator.py\", line 26, in __impl__"
      strings: "    return wrapped_func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 642, in __impl__"
      strings: "    return func(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primapi.py\", line 279, in to_prim"
      strings: "    primx._lower_composite("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primx.py\", line 660, in _lower_composite"
      strings: "    as_tensors(lower_fn(op, *input_args))"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 31, in _composite"
      strings: "    return _lowerrule(op, *args)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/primreg.py\", line 266, in _lower"
      strings: "    return f(*args, **kwargs)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/incubate/autograd/composite_rules.py\", line 124, in composite_batchnorm"
      strings: "    (reshape(run_var, stats_shape) + epsilon), half"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 557, in __impl__"
      strings: "    return scalar_method(self, other_var)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 530, in _scalar_add_"
      strings: "    return _scalar_op_(var, 1.0, value)"
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/layers/math_op_patch.py\", line 451, in _scalar_op_"
      strings: "    block.append_op("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 4580, in append_op"
      strings: "    op = Operator("
      strings: "  File \"/home/wzy/.local/lib/python3.9/site-packages/paddle/base/framework.py\", line 3134, in __init__"
      strings: "    for frame in traceback.extract_stack():"
    }
    attrs {
      name: "op_device"
      type: STRING
      s: ""
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
I1018 08:53:57.835539 4097533 cinn_launch_op.cc:108] Set CINN compile target to Target<linux,CambriconMLU,sycl,64>
I1018 08:53:57.835565 4097533 cinn_launch_op.h:128] Starts to compile at thread 140653012227904
I1018 08:53:57.835584 4097533 cinn_cache_key.cc:183] The graph's hash value by graph address is: 94246622736768
I1018 08:53:57.835595 4097533 cinn_cache_key.cc:110] CinnCacheKey : batch_norm2d_0.b_0,[64],float32;batch_norm2d_0.w_0,[64],float32;batch_norm2d_0.w_1,[64],float32;batch_norm2d_0.w_2,[64],float32;conv2d_0.tmp_0,[1, 64, 112, 112],float32;CambriconMLU,94246622736768
I1018 08:53:57.835599 4097533 cinn_compiler.cc:90] Not found CinnCompiledObject in cache_by_address_.
I1018 08:53:57.835831 4097533 cinn_cache_key.cc:174] The hash graph:
assign:inputs=[elementwise_pow_0.tmp_0],outputs=[assign_1.tmp_0],;elementwise_add:inputs=[reshape2_3.tmp_0,tmp_12],outputs=[batch_norm_0.tmp_2],axis=-1,;elementwise_max:inputs=[batch_norm_0.tmp_2,fill_constant_7.tmp_0],outputs=[relu_0.tmp_0],axis=-1,;elementwise_mul:inputs=[elementwise_pow_2.tmp_0,tmp_9],outputs=[tmp_11],axis=-1,;elementwise_mul:inputs=[reshape2_2.tmp_0,tmp_11],outputs=[tmp_12],axis=-1,;elementwise_pow:inputs=[fill_constant_1.tmp_0,tmp_8],outputs=[elementwise_pow_0.tmp_0],axis=-1,;elementwise_pow:inputs=[fill_constant_1.tmp_0,tmp_10],outputs=[elementwise_pow_2.tmp_0],axis=-1,;elementwise_sub:inputs=[conv2d_0.tmp_0,reshape2_0.tmp_0],outputs=[tmp_9],axis=-1,;feed:inputs=[],outputs=[conv2d_0.tmp_0],;feed:inputs=[],outputs=[batch_norm2d_0.w_0],;feed:inputs=[],outputs=[batch_norm2d_0.b_0],;feed:inputs=[],outputs=[batch_norm2d_0.w_2],;feed:inputs=[],outputs=[batch_norm2d_0.w_1],;fetch:inputs=[assign_1.tmp_0],outputs=[],;fetch:inputs=[pool2d_0.tmp_0],outputs=[],;fill_constant:inputs=[],outputs=[fill_constant_7.tmp_0],dtype=5,force_cpu=false,place_type=-1,shape=[1],str_value=0.0,value=0,;fill_constant:inputs=[],outputs=[fill_constant_5.tmp_0],dtype=5,force_cpu=false,place_type=-1,shape=[64],str_value=0.0,value=0,;fill_constant:inputs=[],outputs=[fill_constant_1.tmp_0],dtype=5,force_cpu=false,place_type=-1,shape=[1],str_value=-0.5,value=-0.5,;pool2d:inputs=[relu_0.tmp_0],outputs=[pool2d_0.tmp_0],adaptive=false,ceil_mode=false,data_format=NCHW,exclusive=true,global_pooling=false,ksize=[3, 3],padding_algorithm=EXPLICIT,paddings=[1, 1],pooling_type=max,strides=[2, 2],use_cudnn=true,;reshape2:inputs=[batch_norm2d_0.w_1],outputs=[reshape2_0.tmp_0,reshape2_0.tmp_1],mkldnn_data_type=float32,shape=[1, 64, 1, 1],use_quantizer=false,;reshape2:inputs=[batch_norm2d_0.w_2],outputs=[reshape2_1.tmp_0,reshape2_1.tmp_1],mkldnn_data_type=float32,shape=[1, 64, 1, 1],use_quantizer=false,;reshape2:inputs=[batch_norm2d_0.b_0],outputs=[reshape2_3.tmp_0,reshape2_3.tmp_1],mkldnn_data_type=float32,shape=[1, 64, 1, 1],use_quantizer=false,;reshape2:inputs=[batch_norm2d_0.w_0],outputs=[reshape2_2.tmp_0,reshape2_2.tmp_1],mkldnn_data_type=float32,shape=[1, 64, 1, 1],use_quantizer=false,;scale:inputs=[reshape2_1.tmp_0],outputs=[tmp_10],bias=1e-05,bias_after_scale=true,scale=1,;scale:inputs=[fill_constant_5.tmp_0],outputs=[tmp_8],bias=1e-05,bias_after_scale=true,scale=1,;
I1018 08:53:57.835839 4097533 cinn_cache_key.cc:177] The graph's hash value by graph structure is: 776652776595299646
I1018 08:53:57.835847 4097533 cinn_cache_key.cc:110] CinnCacheKey : batch_norm2d_0.b_0,[64],float32;batch_norm2d_0.w_0,[64],float32;batch_norm2d_0.w_1,[64],float32;batch_norm2d_0.w_2,[64],float32;conv2d_0.tmp_0,[1, 64, 112, 112],float32;CambriconMLU,776652776595299646
I1018 08:53:57.835850 4097533 cinn_compiler.cc:94] Not found CinnCompiledObject in cache_by_struct_.
I1018 08:53:57.835855 4097533 cinn_graph_symbolization.cc:307] NetBuilder Name NetBuilder_of_graph_0
I1018 08:53:57.835866 4097533 cinn_graph_symbolization.cc:87] Get feed info from input: batch_norm2d_0.b_0
I1018 08:53:57.835886 4097533 cinn_graph_symbolization.cc:87] Get feed info from input: batch_norm2d_0.w_0
I1018 08:53:57.835889 4097533 cinn_graph_symbolization.cc:87] Get feed info from input: batch_norm2d_0.w_1
I1018 08:53:57.835892 4097533 cinn_graph_symbolization.cc:87] Get feed info from input: batch_norm2d_0.w_2
I1018 08:53:57.835894 4097533 cinn_graph_symbolization.cc:87] Get feed info from input: conv2d_0.tmp_0
I1018 08:53:57.835912 4097533 scope.h:65] Scope insert Var [batch_norm2d_0__b_0]
I1018 08:53:57.835934 4097533 cinn_graph_symbolization.cc:151] add paddle param var [batch_norm2d_0.b_0] info cinn scope var[batch_norm2d_0__b_0]
I1018 08:53:57.835938 4097533 scope.h:65] Scope insert Var [batch_norm2d_0__w_1]
I1018 08:53:57.835942 4097533 cinn_graph_symbolization.cc:151] add paddle param var [batch_norm2d_0.w_1] info cinn scope var[batch_norm2d_0__w_1]
I1018 08:53:57.835944 4097533 scope.h:65] Scope insert Var [batch_norm2d_0__w_2]
I1018 08:53:57.835947 4097533 cinn_graph_symbolization.cc:151] add paddle param var [batch_norm2d_0.w_2] info cinn scope var[batch_norm2d_0__w_2]
I1018 08:53:57.835949 4097533 scope.h:65] Scope insert Var [batch_norm2d_0__w_0]
I1018 08:53:57.835953 4097533 cinn_graph_symbolization.cc:151] add paddle param var [batch_norm2d_0.w_0] info cinn scope var[batch_norm2d_0__w_0]
I1018 08:53:57.835975 4097533 cinn_graph_symbolization.cc:323] add feed var [conv2d_0.tmp_0] info context
I1018 08:53:57.835978 4097533 cinn_graph_symbolization.cc:323] add feed var [batch_norm2d_0.w_2] info context
I1018 08:53:57.835980 4097533 cinn_graph_symbolization.cc:323] add feed var [batch_norm2d_0.w_1] info context
I1018 08:53:57.835983 4097533 cinn_graph_symbolization.cc:323] add feed var [batch_norm2d_0.w_0] info context
I1018 08:53:57.835985 4097533 cinn_graph_symbolization.cc:323] add feed var [batch_norm2d_0.b_0] info context
I1018 08:53:57.836020 4097533 cinn_graph_symbolization.cc:233] topological sort insert: feed 0x55b7d0ef66d0 input 0
I1018 08:53:57.836023 4097533 cinn_graph_symbolization.cc:233] topological sort insert: feed 0x55b7d0ef64a0 input 0
I1018 08:53:57.836025 4097533 cinn_graph_symbolization.cc:233] topological sort insert: feed 0x55b7d0ef6b30 input 0
I1018 08:53:57.836027 4097533 cinn_graph_symbolization.cc:233] topological sort insert: feed 0x55b7d0ef6900 input 0
I1018 08:53:57.836030 4097533 cinn_graph_symbolization.cc:233] topological sort insert: feed 0x55b7d0ef6270 input 0
I1018 08:53:57.836032 4097533 cinn_graph_symbolization.cc:233] topological sort insert: fill_constant 0x55b7d0ef0110 input 0
I1018 08:53:57.836035 4097533 cinn_graph_symbolization.cc:233] topological sort insert: fill_constant 0x55b7b87a17e0 input 0
I1018 08:53:57.836036 4097533 cinn_graph_symbolization.cc:233] topological sort insert: fill_constant 0x55b7d094a1b0 input 0
I1018 08:53:57.836041 4097533 cinn_graph_symbolization.cc:233] topological sort insert: reshape2 0x55b7d0ee2c90 input 1
I1018 08:53:57.836043 4097533 cinn_graph_symbolization.cc:233] topological sort insert: reshape2 0x55b7d0eedd60 input 1
I1018 08:53:57.836046 4097533 cinn_graph_symbolization.cc:233] topological sort insert: reshape2 0x55b7d0edf4a0 input 1
I1018 08:53:57.836048 4097533 cinn_graph_symbolization.cc:233] topological sort insert: reshape2 0x55b7d0943960 input 1
I1018 08:53:57.836050 4097533 cinn_graph_symbolization.cc:233] topological sort insert: scale 0x55b7b87a3dd0 input 1
I1018 08:53:57.836052 4097533 cinn_graph_symbolization.cc:233] topological sort insert: elementwise_sub 0x55b7d0ee7510 input 2
I1018 08:53:57.836055 4097533 cinn_graph_symbolization.cc:233] topological sort insert: scale 0x55b7d0941260 input 1
I1018 08:53:57.836056 4097533 cinn_graph_symbolization.cc:233] topological sort insert: elementwise_pow 0x55b7d0ee5080 input 2
I1018 08:53:57.836058 4097533 cinn_graph_symbolization.cc:233] topological sort insert: elementwise_pow 0x55b7d0ee98d0 input 2
I1018 08:53:57.836061 4097533 cinn_graph_symbolization.cc:233] topological sort insert: assign 0x55b7d0eebd50 input 1
I1018 08:53:57.836063 4097533 cinn_graph_symbolization.cc:233] topological sort insert: elementwise_mul 0x55b7d094d0e0 input 2
I1018 08:53:57.836066 4097533 cinn_graph_symbolization.cc:233] topological sort insert: fetch 0x55b7d0ef6d60 input 1
I1018 08:53:57.836067 4097533 cinn_graph_symbolization.cc:233] topological sort insert: elementwise_mul 0x55b7d093d210 input 2
I1018 08:53:57.836069 4097533 cinn_graph_symbolization.cc:233] topological sort insert: elementwise_add 0x55b7d0945ba0 input 2
I1018 08:53:57.836071 4097533 cinn_graph_symbolization.cc:233] topological sort insert: elementwise_max 0x55b7d0947b00 input 2
I1018 08:53:57.836074 4097533 cinn_graph_symbolization.cc:233] topological sort insert: pool2d 0x55b7d095cb80 input 1
I1018 08:53:57.836076 4097533 cinn_graph_symbolization.cc:233] topological sort insert: fetch 0x55b7d0ef6f90 input 1
I1018 08:53:57.836442 4097533 cinn_graph_symbolization.cc:276] Running Op feed
I1018 08:53:57.836460 4097533 fetch_feed.cc:35] Model get feed [batch_norm2d_0.b_0]
I1018 08:53:57.836570 4097533 op_mapper_registry.cc:36] The Paddle var "batch_norm2d_0.b_0" is mapped to CINN var "batch_norm2d_0__b_0" with shape=[64], dtype=float32
I1018 08:53:57.836597 4097533 fetch_feed.cc:41] Create param [batch_norm2d_0.b_0] to batch_norm2d_0__b_0 with shape=[64], dtype=float32
I1018 08:53:57.836604 4097533 cinn_graph_symbolization.cc:276] Running Op feed
I1018 08:53:57.836607 4097533 fetch_feed.cc:35] Model get feed [batch_norm2d_0.w_0]
I1018 08:53:57.836611 4097533 op_mapper_registry.cc:36] The Paddle var "batch_norm2d_0.w_0" is mapped to CINN var "batch_norm2d_0__w_0" with shape=[64], dtype=float32
I1018 08:53:57.836616 4097533 fetch_feed.cc:41] Create param [batch_norm2d_0.w_0] to batch_norm2d_0__w_0 with shape=[64], dtype=float32
I1018 08:53:57.836619 4097533 cinn_graph_symbolization.cc:276] Running Op feed
I1018 08:53:57.836621 4097533 fetch_feed.cc:35] Model get feed [batch_norm2d_0.w_1]
I1018 08:53:57.836624 4097533 op_mapper_registry.cc:36] The Paddle var "batch_norm2d_0.w_1" is mapped to CINN var "batch_norm2d_0__w_1" with shape=[64], dtype=float32
I1018 08:53:57.836628 4097533 fetch_feed.cc:41] Create param [batch_norm2d_0.w_1] to batch_norm2d_0__w_1 with shape=[64], dtype=float32
I1018 08:53:57.836632 4097533 cinn_graph_symbolization.cc:276] Running Op feed
I1018 08:53:57.836634 4097533 fetch_feed.cc:35] Model get feed [batch_norm2d_0.w_2]
I1018 08:53:57.836637 4097533 op_mapper_registry.cc:36] The Paddle var "batch_norm2d_0.w_2" is mapped to CINN var "batch_norm2d_0__w_2" with shape=[64], dtype=float32
I1018 08:53:57.836642 4097533 fetch_feed.cc:41] Create param [batch_norm2d_0.w_2] to batch_norm2d_0__w_2 with shape=[64], dtype=float32
I1018 08:53:57.836645 4097533 cinn_graph_symbolization.cc:276] Running Op feed
I1018 08:53:57.836650 4097533 fetch_feed.cc:35] Model get feed [conv2d_0.tmp_0]
I1018 08:53:57.836664 4097533 op_mapper_registry.cc:36] The Paddle var "conv2d_0.tmp_0" is mapped to CINN var "conv2d_0__tmp_0" with shape=[1, 64, 112, 112], dtype=float32
I1018 08:53:57.836673 4097533 op_mapper_registry.cc:50] Paddle name [conv2d_0.tmp_0] map to program id conv2d_0__tmp_0
I1018 08:53:57.836675 4097533 cinn_graph_symbolization.cc:276] Running Op fill_constant
I1018 08:53:57.836697 4097533 constant.cc:101] fill constant (-0.5) with shape (1) and dtype [float32]
I1018 08:53:57.836818 4097533 elementwise.cc:603] FillConstant output dtype (from [dtype]): float32
I1018 08:53:57.836824 4097533 op_mapper_registry.cc:36] The Paddle var "fill_constant_1.tmp_0" is mapped to CINN var "fill_constant_1__tmp_0" with shape=[1], dtype=float32
I1018 08:53:57.836828 4097533 op_mapper_registry.cc:50] Paddle name [fill_constant_1.tmp_0] map to program id fill_constant_1__tmp_0
I1018 08:53:57.836831 4097533 cinn_graph_symbolization.cc:276] Running Op fill_constant
I1018 08:53:57.836838 4097533 constant.cc:101] fill constant (0.0) with shape (64) and dtype [float32]
I1018 08:53:57.836844 4097533 elementwise.cc:603] FillConstant output dtype (from [dtype]): float32
I1018 08:53:57.836848 4097533 op_mapper_registry.cc:36] The Paddle var "fill_constant_5.tmp_0" is mapped to CINN var "fill_constant_5__tmp_0" with shape=[64], dtype=float32
I1018 08:53:57.836851 4097533 op_mapper_registry.cc:50] Paddle name [fill_constant_5.tmp_0] map to program id fill_constant_5__tmp_0
I1018 08:53:57.836854 4097533 cinn_graph_symbolization.cc:276] Running Op fill_constant
I1018 08:53:57.836858 4097533 constant.cc:101] fill constant (0.0) with shape (1) and dtype [float32]
I1018 08:53:57.836864 4097533 elementwise.cc:603] FillConstant output dtype (from [dtype]): float32
I1018 08:53:57.836867 4097533 op_mapper_registry.cc:36] The Paddle var "fill_constant_7.tmp_0" is mapped to CINN var "fill_constant_7__tmp_0" with shape=[1], dtype=float32
I1018 08:53:57.836871 4097533 op_mapper_registry.cc:50] Paddle name [fill_constant_7.tmp_0] map to program id fill_constant_7__tmp_0
I1018 08:53:57.836874 4097533 cinn_graph_symbolization.cc:276] Running Op reshape2
I1018 08:53:57.836879 4097533 reshape.cc:76] x shape: 64
I1018 08:53:57.836881 4097533 reshape.cc:77] reshape to : 1,64,1,1
I1018 08:53:57.836897 4097533 op_mapper_registry.cc:36] The Paddle var "reshape2_3.tmp_0" is mapped to CINN var "var_19" with shape=[1, 64, 1, 1], dtype=float32
I1018 08:53:57.836901 4097533 op_mapper_registry.cc:50] Paddle name [reshape2_3.tmp_0] map to program id var_19
I1018 08:53:57.836911 4097533 op_mapper_registry.cc:36] The Paddle var "reshape2_3.tmp_1" is mapped to CINN var "var_21" with shape=[64], dtype=float32
I1018 08:53:57.836915 4097533 op_mapper_registry.cc:50] Paddle name [reshape2_3.tmp_1] map to program id var_21
I1018 08:53:57.836917 4097533 cinn_graph_symbolization.cc:276] Running Op reshape2
I1018 08:53:57.836920 4097533 reshape.cc:76] x shape: 64
I1018 08:53:57.836923 4097533 reshape.cc:77] reshape to : 1,64,1,1
I1018 08:53:57.836930 4097533 op_mapper_registry.cc:36] The Paddle var "reshape2_2.tmp_0" is mapped to CINN var "var_23" with shape=[1, 64, 1, 1], dtype=float32
I1018 08:53:57.836933 4097533 op_mapper_registry.cc:50] Paddle name [reshape2_2.tmp_0] map to program id var_23
I1018 08:53:57.836938 4097533 op_mapper_registry.cc:36] The Paddle var "reshape2_2.tmp_1" is mapped to CINN var "var_25" with shape=[64], dtype=float32
I1018 08:53:57.836941 4097533 op_mapper_registry.cc:50] Paddle name [reshape2_2.tmp_1] map to program id var_25
I1018 08:53:57.836944 4097533 cinn_graph_symbolization.cc:276] Running Op reshape2
I1018 08:53:57.836947 4097533 reshape.cc:76] x shape: 64
I1018 08:53:57.836949 4097533 reshape.cc:77] reshape to : 1,64,1,1
I1018 08:53:57.836954 4097533 op_mapper_registry.cc:36] The Paddle var "reshape2_0.tmp_0" is mapped to CINN var "var_27" with shape=[1, 64, 1, 1], dtype=float32
I1018 08:53:57.836958 4097533 op_mapper_registry.cc:50] Paddle name [reshape2_0.tmp_0] map to program id var_27
I1018 08:53:57.836966 4097533 op_mapper_registry.cc:36] The Paddle var "reshape2_0.tmp_1" is mapped to CINN var "var_29" with shape=[64], dtype=float32
I1018 08:53:57.836971 4097533 op_mapper_registry.cc:50] Paddle name [reshape2_0.tmp_1] map to program id var_29
I1018 08:53:57.836974 4097533 cinn_graph_symbolization.cc:276] Running Op reshape2
I1018 08:53:57.836977 4097533 reshape.cc:76] x shape: 64
I1018 08:53:57.836979 4097533 reshape.cc:77] reshape to : 1,64,1,1
I1018 08:53:57.836984 4097533 op_mapper_registry.cc:36] The Paddle var "reshape2_1.tmp_0" is mapped to CINN var "var_31" with shape=[1, 64, 1, 1], dtype=float32
I1018 08:53:57.836988 4097533 op_mapper_registry.cc:50] Paddle name [reshape2_1.tmp_0] map to program id var_31
I1018 08:53:57.836992 4097533 op_mapper_registry.cc:36] The Paddle var "reshape2_1.tmp_1" is mapped to CINN var "var_33" with shape=[64], dtype=float32
I1018 08:53:57.836997 4097533 op_mapper_registry.cc:50] Paddle name [reshape2_1.tmp_1] map to program id var_33
I1018 08:53:57.836999 4097533 cinn_graph_symbolization.cc:276] Running Op scale
I1018 08:53:57.837004 4097533 scale.cc:72] tmp_8 = scale(fill_constant_5.tmp_0=Var(fill_constant_5__tmp_0: shape=[64], dtype=float32), scale=1, bias=1e-05, bias_after_scale=1
I1018 08:53:57.837015 4097533 op_mapper_registry.cc:36] The Paddle var "tmp_8" is mapped to CINN var "var_35" with shape=[64], dtype=float32
I1018 08:53:57.837020 4097533 op_mapper_registry.cc:50] Paddle name [tmp_8] map to program id var_35
I1018 08:53:57.837023 4097533 cinn_graph_symbolization.cc:276] Running Op elementwise_sub
I1018 08:53:57.837028 4097533 elementwise.cc:108] tmp_9 = conv2d_0.tmp_0 - reshape2_0.tmp_0 at -1
I1018 08:53:57.837035 4097533 broadcast.cc:166] broadcast input shapes are : 1, 64, 112, 112; 1, 64, 1, 1
I1018 08:53:57.837179 4097533 broadcast.cc:170] broadcast out shape: 1, 64, 112, 112
I1018 08:53:57.837184 4097533 op_mapper_registry.cc:36] The Paddle var "tmp_9" is mapped to CINN var "var_37" with shape=[1, 64, 112, 112], dtype=float32
I1018 08:53:57.837189 4097533 op_mapper_registry.cc:50] Paddle name [tmp_9] map to program id var_37
I1018 08:53:57.837191 4097533 cinn_graph_symbolization.cc:276] Running Op scale
I1018 08:53:57.837195 4097533 scale.cc:72] tmp_10 = scale(reshape2_1.tmp_0=Var(var_31: shape=[1, 64, 1, 1], dtype=float32), scale=1, bias=1e-05, bias_after_scale=1
I1018 08:53:57.837204 4097533 op_mapper_registry.cc:36] The Paddle var "tmp_10" is mapped to CINN var "var_39" with shape=[1, 64, 1, 1], dtype=float32
I1018 08:53:57.837214 4097533 op_mapper_registry.cc:50] Paddle name [tmp_10] map to program id var_39
I1018 08:53:57.837217 4097533 cinn_graph_symbolization.cc:276] Running Op elementwise_pow
I1018 08:53:57.837221 4097533 elementwise.cc:108] elementwise_pow_0.tmp_0 = tmp_8 pow fill_constant_1.tmp_0 at -1
I1018 08:53:57.837231 4097533 broadcast.cc:166] broadcast input shapes are : 64; 1
I1018 08:53:57.837272 4097533 broadcast.cc:170] broadcast out shape: 64
I1018 08:53:57.837276 4097533 op_mapper_registry.cc:36] The Paddle var "elementwise_pow_0.tmp_0" is mapped to CINN var "var_41" with shape=[64], dtype=float32
I1018 08:53:57.837280 4097533 op_mapper_registry.cc:50] Paddle name [elementwise_pow_0.tmp_0] map to program id var_41
I1018 08:53:57.837282 4097533 cinn_graph_symbolization.cc:276] Running Op elementwise_pow
I1018 08:53:57.837286 4097533 elementwise.cc:108] elementwise_pow_2.tmp_0 = tmp_10 pow fill_constant_1.tmp_0 at -1
I1018 08:53:57.837291 4097533 broadcast.cc:166] broadcast input shapes are : 1, 64, 1, 1; 1
I1018 08:53:57.837307 4097533 broadcast.cc:170] broadcast out shape: 1, 64, 1, 1
I1018 08:53:57.837312 4097533 op_mapper_registry.cc:36] The Paddle var "elementwise_pow_2.tmp_0" is mapped to CINN var "var_43" with shape=[1, 64, 1, 1], dtype=float32
I1018 08:53:57.837316 4097533 op_mapper_registry.cc:50] Paddle name [elementwise_pow_2.tmp_0] map to program id var_43
I1018 08:53:57.837318 4097533 cinn_graph_symbolization.cc:276] Running Op assign
I1018 08:53:57.837324 4097533 op_mapper_registry.cc:36] The Paddle var "assign_1.tmp_0" is mapped to CINN var "var_45" with shape=[64], dtype=float32
I1018 08:53:57.837332 4097533 op_mapper_registry.cc:50] Paddle name [assign_1.tmp_0] map to program id var_45
I1018 08:53:57.837334 4097533 cinn_graph_symbolization.cc:276] Running Op elementwise_mul
I1018 08:53:57.837337 4097533 elementwise.cc:108] tmp_11 = tmp_9 * elementwise_pow_2.tmp_0 at -1
I1018 08:53:57.837343 4097533 broadcast.cc:166] broadcast input shapes are : 1, 64, 112, 112; 1, 64, 1, 1
I1018 08:53:57.837432 4097533 broadcast.cc:170] broadcast out shape: 1, 64, 112, 112
I1018 08:53:57.837436 4097533 op_mapper_registry.cc:36] The Paddle var "tmp_11" is mapped to CINN var "var_47" with shape=[1, 64, 112, 112], dtype=float32
I1018 08:53:57.837440 4097533 op_mapper_registry.cc:50] Paddle name [tmp_11] map to program id var_47
I1018 08:53:57.837443 4097533 cinn_graph_symbolization.cc:276] Running Op fetch
I1018 08:53:57.837447 4097533 fetch_feed.cc:28] detect model output: [assign_1.tmp_0]
I1018 08:53:57.837450 4097533 cinn_graph_symbolization.cc:276] Running Op elementwise_mul
I1018 08:53:57.837452 4097533 elementwise.cc:108] tmp_12 = reshape2_2.tmp_0 * tmp_11 at -1
I1018 08:53:57.837457 4097533 broadcast.cc:166] broadcast input shapes are : 1, 64, 1, 1; 1, 64, 112, 112
I1018 08:53:57.837530 4097533 broadcast.cc:170] broadcast out shape: 1, 64, 112, 112
I1018 08:53:57.837534 4097533 op_mapper_registry.cc:36] The Paddle var "tmp_12" is mapped to CINN var "var_49" with shape=[1, 64, 112, 112], dtype=float32
I1018 08:53:57.837538 4097533 op_mapper_registry.cc:50] Paddle name [tmp_12] map to program id var_49
I1018 08:53:57.837541 4097533 cinn_graph_symbolization.cc:276] Running Op elementwise_add
I1018 08:53:57.837544 4097533 elementwise.cc:108] batch_norm_0.tmp_2 = tmp_12 + reshape2_3.tmp_0 at -1
I1018 08:53:57.837549 4097533 broadcast.cc:166] broadcast input shapes are : 1, 64, 112, 112; 1, 64, 1, 1
I1018 08:53:57.837638 4097533 broadcast.cc:170] broadcast out shape: 1, 64, 112, 112
I1018 08:53:57.837642 4097533 op_mapper_registry.cc:36] The Paddle var "batch_norm_0.tmp_2" is mapped to CINN var "var_51" with shape=[1, 64, 112, 112], dtype=float32
I1018 08:53:57.837646 4097533 op_mapper_registry.cc:50] Paddle name [batch_norm_0.tmp_2] map to program id var_51
I1018 08:53:57.837649 4097533 cinn_graph_symbolization.cc:276] Running Op elementwise_max
I1018 08:53:57.837653 4097533 elementwise.cc:108] relu_0.tmp_0 = batch_norm_0.tmp_2 max fill_constant_7.tmp_0 at -1
I1018 08:53:57.837659 4097533 broadcast.cc:166] broadcast input shapes are : 1, 64, 112, 112; 1
I1018 08:53:57.837699 4097533 broadcast.cc:170] broadcast out shape: 1, 64, 112, 112
I1018 08:53:57.837703 4097533 op_mapper_registry.cc:36] The Paddle var "relu_0.tmp_0" is mapped to CINN var "var_53" with shape=[1, 64, 112, 112], dtype=float32
I1018 08:53:57.837707 4097533 op_mapper_registry.cc:50] Paddle name [relu_0.tmp_0] map to program id var_53
I1018 08:53:57.837710 4097533 cinn_graph_symbolization.cc:276] Running Op pool2d
I1018 08:53:57.837738 4097533 nn.cc:1671] y[1, 64, 56, 56] = pool2d(x[1, 64, 112, 112], kernel_size=[3, 3], stride_size=[2, 2], padding_size=[1, 1, 1, 1], pool_type=max, ceil_mode=false, exclusive=true, data_format=NCHW, global_pooling=false, adaptive=false
I1018 08:53:57.837749 4097533 op_mapper_registry.cc:36] The Paddle var "pool2d_0.tmp_0" is mapped to CINN var "var_55" with shape=[1, 64, 56, 56], dtype=float32
I1018 08:53:57.837754 4097533 op_mapper_registry.cc:50] Paddle name [pool2d_0.tmp_0] map to program id var_55
I1018 08:53:57.837756 4097533 cinn_graph_symbolization.cc:276] Running Op fetch
I1018 08:53:57.837759 4097533 fetch_feed.cc:28] detect model output: [pool2d_0.tmp_0]
I1018 08:53:57.837810 4097533 cinn_compiler.cc:315] All fetch var ids in CINN: var_45,var_55
I1018 08:53:57.837831 4097533 visualize_helper.cc:41] No set "FLAGS_cinn_pass_visualize_dir", the pass visualize information will print directly.
I1018 08:53:57.837833 4097533 optimize.cc:148] Before frontend::ProgramPass::Apply
I1018 08:53:57.837858 4097533 program_pass.cc:42] Apply ExpandZeroDim pass, program size: 22 -> 22, diff: 0
I1018 08:53:57.837877 4097533 program_pass.cc:42] Apply AutoCast pass, program size: 22 -> 22, diff: 0
I1018 08:53:57.837888 4097533 decomposer.cc:51] Don't run decomposer of op fill_constant
I1018 08:53:57.837890 4097533 decomposer.cc:51] Don't run decomposer of op fill_constant
I1018 08:53:57.837893 4097533 decomposer.cc:51] Don't run decomposer of op fill_constant
I1018 08:53:57.837895 4097533 decomposer.cc:51] Don't run decomposer of op reshape
I1018 08:53:57.837898 4097533 decomposer.cc:51] Don't run decomposer of op identity
I1018 08:53:57.837900 4097533 decomposer.cc:51] Don't run decomposer of op reshape
I1018 08:53:57.837903 4097533 decomposer.cc:51] Don't run decomposer of op identity
I1018 08:53:57.837905 4097533 decomposer.cc:51] Don't run decomposer of op reshape
I1018 08:53:57.837908 4097533 decomposer.cc:51] Don't run decomposer of op identity
I1018 08:53:57.837910 4097533 decomposer.cc:51] Don't run decomposer of op reshape
I1018 08:53:57.837913 4097533 decomposer.cc:51] Don't run decomposer of op identity
I1018 08:53:57.837915 4097533 decomposer.cc:51] Don't run decomposer of op scale
I1018 08:53:57.837917 4097533 decomposer.cc:51] Don't run decomposer of op subtract
I1018 08:53:57.837920 4097533 decomposer.cc:51] Don't run decomposer of op scale
I1018 08:53:57.837922 4097533 decomposer.cc:51] Don't run decomposer of op pow
I1018 08:53:57.837925 4097533 decomposer.cc:51] Don't run decomposer of op pow
I1018 08:53:57.837927 4097533 decomposer.cc:51] Don't run decomposer of op identity
I1018 08:53:57.837930 4097533 decomposer.cc:51] Don't run decomposer of op elementwise_mul
I1018 08:53:57.837932 4097533 decomposer.cc:51] Don't run decomposer of op elementwise_mul
I1018 08:53:57.837935 4097533 decomposer.cc:48] Run decomposer of op elementwise_add
I1018 08:53:57.837946 4097533 broadcast.cc:360] broadcast input shape: 1, 64, 1, 1
I1018 08:53:57.837949 4097533 broadcast.cc:361] broadcast out shape: 1, 64, 112, 112
I1018 08:53:57.837952 4097533 broadcast.cc:362] broadcast_axes shape: 0, 1, 2, 3
I1018 08:53:57.837958 4097533 broadcast.cc:166] broadcast input shapes are : 1, 64, 112, 112; 1, 64, 112, 112
I1018 08:53:57.838007 4097533 broadcast.cc:170] broadcast out shape: 1, 64, 112, 112
I1018 08:53:57.838014 4097533 decomposer.cc:51] Don't run decomposer of op max
I1018 08:53:57.838017 4097533 decomposer.cc:51] Don't run decomposer of op pool2d
I1018 08:53:57.838019 4097533 decomposer.cc:55] Before builder.Build()
I1018 08:53:57.838022 4097533 decomposer.cc:57] After builder.Build()
I1018 08:53:57.838027 4097533 program_pass.cc:42] Apply Decomposer pass, program size: 22 -> 23, diff: 1
I1018 08:53:57.838037 4097533 remove_identity.cc:278] Add var_21 -> batch_norm2d_0__b_0
I1018 08:53:57.838040 4097533 remove_identity.cc:246] Remove the 4-th instruction: var_21 = identity(batch_norm2d_0__b_0)
I1018 08:53:57.838047 4097533 remove_identity.cc:278] Add var_25 -> batch_norm2d_0__w_0
I1018 08:53:57.838049 4097533 remove_identity.cc:246] Remove the 6-th instruction: var_25 = identity(batch_norm2d_0__w_0)
I1018 08:53:57.838053 4097533 remove_identity.cc:278] Add var_29 -> batch_norm2d_0__w_1
I1018 08:53:57.838055 4097533 remove_identity.cc:246] Remove the 8-th instruction: var_29 = identity(batch_norm2d_0__w_1)
I1018 08:53:57.838059 4097533 remove_identity.cc:278] Add var_33 -> batch_norm2d_0__w_2
I1018 08:53:57.838063 4097533 remove_identity.cc:246] Remove the 10-th instruction: var_33 = identity(batch_norm2d_0__w_2)
I1018 08:53:57.838068 4097533 remove_identity.cc:278] Add var_41 -> var_45
I1018 08:53:57.838069 4097533 remove_identity.cc:246] Remove the 16-th instruction: var_45 = identity(var_41)
I1018 08:53:57.838074 4097533 remove_identity.cc:264] origin2new_ {
I1018 08:53:57.838076 4097533 remove_identity.cc:266]   var_33 -> batch_norm2d_0__w_2
I1018 08:53:57.838078 4097533 remove_identity.cc:266]   var_29 -> batch_norm2d_0__w_1
I1018 08:53:57.838080 4097533 remove_identity.cc:266]   var_25 -> batch_norm2d_0__w_0
I1018 08:53:57.838086 4097533 remove_identity.cc:266]   var_41 -> var_45
I1018 08:53:57.838088 4097533 remove_identity.cc:266]   var_21 -> batch_norm2d_0__b_0
I1018 08:53:57.838090 4097533 remove_identity.cc:268] }
I1018 08:53:57.838093 4097533 remove_identity.cc:150] Total remove 5 instructions.
I1018 08:53:57.838106 4097533 program_pass.cc:42] Apply RemoveIdentity pass, program size: 23 -> 18, diff: -5
I1018 08:53:57.838124 4097533 program_pass.cc:42] Apply CastCollapsing pass, program size: 18 -> 18, diff: 0
I1018 08:53:57.838140 4097533 program_pass.cc:42] Apply TransposeCollapsing pass, program size: 18 -> 18, diff: 0
I1018 08:53:57.838143 4097533 remove_identity.cc:264] origin2new_ {
I1018 08:53:57.838145 4097533 remove_identity.cc:268] }
I1018 08:53:57.838147 4097533 remove_identity.cc:150] Total remove 0 instructions.
I1018 08:53:57.838155 4097533 program_pass.cc:42] Apply RemoveIdentity pass, program size: 18 -> 18, diff: 0
I1018 08:53:57.838167 4097533 broadcast.cc:360] broadcast input shape: 1, 64, 1, 1
I1018 08:53:57.838171 4097533 broadcast.cc:361] broadcast out shape: 1, 64, 112, 112
I1018 08:53:57.838173 4097533 broadcast.cc:362] broadcast_axes shape: 0, 1, 2, 3
I1018 08:53:57.838177 4097533 auto_broadcast.cc:108] Before Insert broadcast_to: var_37 = subtract(conv2d_0__tmp_0, var_27, axis=-1)
I1018 08:53:57.838186 4097533 auto_broadcast.cc:112] After Insert broadcast_to: var_37 = subtract(conv2d_0__tmp_0, var_101, axis=-1)
I1018 08:53:57.838196 4097533 broadcast.cc:360] broadcast input shape: 1
I1018 08:53:57.838198 4097533 broadcast.cc:361] broadcast out shape: 64
I1018 08:53:57.838200 4097533 broadcast.cc:362] broadcast_axes shape: 0
I1018 08:53:57.838203 4097533 auto_broadcast.cc:108] Before Insert broadcast_to: var_45 = pow(var_35, fill_constant_1__tmp_0, axis=-1)
I1018 08:53:57.838208 4097533 auto_broadcast.cc:112] After Insert broadcast_to: var_45 = pow(var_35, var_102, axis=-1)
I1018 08:53:57.838215 4097533 broadcast.cc:360] broadcast input shape: 1
I1018 08:53:57.838218 4097533 broadcast.cc:361] broadcast out shape: 1, 64, 1, 1
I1018 08:53:57.838220 4097533 broadcast.cc:362] broadcast_axes shape: 3
I1018 08:53:57.838223 4097533 auto_broadcast.cc:108] Before Insert broadcast_to: var_43 = pow(var_39, fill_constant_1__tmp_0, axis=-1)
I1018 08:53:57.838228 4097533 auto_broadcast.cc:112] After Insert broadcast_to: var_43 = pow(var_39, var_103, axis=-1)
I1018 08:53:57.838235 4097533 broadcast.cc:360] broadcast input shape: 1, 64, 1, 1
I1018 08:53:57.838238 4097533 broadcast.cc:361] broadcast out shape: 1, 64, 112, 112
I1018 08:53:57.838241 4097533 broadcast.cc:362] broadcast_axes shape: 0, 1, 2, 3
I1018 08:53:57.838244 4097533 auto_broadcast.cc:108] Before Insert broadcast_to: var_47 = elementwise_mul(var_37, var_43, axis=-1)
I1018 08:53:57.838248 4097533 auto_broadcast.cc:112] After Insert broadcast_to: var_47 = elementwise_mul(var_37, var_104, axis=-1)
I1018 08:53:57.838256 4097533 broadcast.cc:360] broadcast input shape: 1, 64, 1, 1
I1018 08:53:57.838259 4097533 broadcast.cc:361] broadcast out shape: 1, 64, 112, 112
I1018 08:53:57.838261 4097533 broadcast.cc:362] broadcast_axes shape: 0, 1, 2, 3
I1018 08:53:57.838265 4097533 auto_broadcast.cc:108] Before Insert broadcast_to: var_49 = elementwise_mul(var_23, var_47, axis=-1)
I1018 08:53:57.838270 4097533 auto_broadcast.cc:112] After Insert broadcast_to: var_49 = elementwise_mul(var_105, var_47, axis=-1)
I1018 08:53:57.838277 4097533 broadcast.cc:360] broadcast input shape: 1
I1018 08:53:57.838280 4097533 broadcast.cc:361] broadcast out shape: 1, 64, 112, 112
I1018 08:53:57.838284 4097533 broadcast.cc:362] broadcast_axes shape: 3
I1018 08:53:57.838286 4097533 auto_broadcast.cc:108] Before Insert broadcast_to: var_53 = max(var_51, fill_constant_7__tmp_0, axis=-1)
I1018 08:53:57.838290 4097533 auto_broadcast.cc:112] After Insert broadcast_to: var_53 = max(var_51, var_106, axis=-1)
I1018 08:53:57.838296 4097533 program_pass.cc:42] Apply AutoBroadcast pass, program size: 18 -> 24, diff: 6
I1018 08:53:57.838306 4097533 fill_constant_rewriter.cc:222] Try folding var_103 = broadcast_to(fill_constant_1__tmp_0, broadcast_axes=[3], out_shape=[1,64,1,1]) into fill_constant_1__tmp_0 = fill_constant(dtype=float32, force_cpu=false, shape=[1], value=-0.5)
I1018 08:53:57.838330 4097533 fill_constant_rewriter.cc:222] Try folding var_102 = broadcast_to(fill_constant_1__tmp_0, broadcast_axes=[0], out_shape=[64]) into fill_constant_1__tmp_0 = fill_constant(dtype=float32, force_cpu=false, shape=[1], value=-0.5)
I1018 08:53:57.838343 4097533 fill_constant_rewriter.cc:222] Try folding var_35 = scale(fill_constant_5__tmp_0, bias=1e-05, bias_after_scale=true, scale=1) into fill_constant_5__tmp_0 = fill_constant(dtype=float32, force_cpu=false, shape=[64], value=0)
I1018 08:53:57.838358 4097533 fill_constant_rewriter.cc:222] Try folding var_106 = broadcast_to(fill_constant_7__tmp_0, broadcast_axes=[3], out_shape=[1,64,112,112]) into fill_constant_7__tmp_0 = fill_constant(dtype=float32, force_cpu=false, shape=[1], value=0)
I1018 08:53:57.838372 4097533 fill_constant_rewriter.cc:169] FillConstantRewriterPass Remove 3 instruction
I1018 08:53:57.838382 4097533 program_pass.cc:42] Apply FillConstantRewriter pass, program size: 24 -> 21, diff: -3
I1018 08:53:57.838387 4097533 remove_identity.cc:264] origin2new_ {
I1018 08:53:57.838388 4097533 remove_identity.cc:268] }
I1018 08:53:57.838390 4097533 remove_identity.cc:150] Total remove 0 instructions.
I1018 08:53:57.838397 4097533 program_pass.cc:42] Apply RemoveIdentity pass, program size: 21 -> 21, diff: 0
I1018 08:53:57.838407 4097533 dead_code_eliminate.cc:64] Total remove 0 instructions.
I1018 08:53:57.838409 4097533 program_pass.cc:42] Apply DeadCodeEliminate pass, program size: 21 -> 21, diff: 0
I1018 08:53:57.838413 4097533 graph.cc:46] operator [reshape] has [1] inputs, and [1] outputs
I1018 08:53:57.838423 4097533 graph.cc:46] operator [reshape] has [1] inputs, and [1] outputs
I1018 08:53:57.838428 4097533 graph.cc:46] operator [reshape] has [1] inputs, and [1] outputs
I1018 08:53:57.838433 4097533 graph.cc:46] operator [reshape] has [1] inputs, and [1] outputs
I1018 08:53:57.838438 4097533 graph.cc:46] operator [fill_constant] has [0] inputs, and [1] outputs
I1018 08:53:57.838441 4097533 graph.cc:46] operator [broadcast_to] has [1] inputs, and [1] outputs
I1018 08:53:57.838446 4097533 graph.cc:46] operator [subtract] has [2] inputs, and [1] outputs
I1018 08:53:57.838452 4097533 graph.cc:46] operator [scale] has [1] inputs, and [1] outputs
I1018 08:53:57.838456 4097533 graph.cc:46] operator [fill_constant] has [0] inputs, and [1] outputs
I1018 08:53:57.838460 4097533 graph.cc:46] operator [pow] has [2] inputs, and [1] outputs
I1018 08:53:57.838506 4097533 graph.cc:46] operator [fill_constant] has [0] inputs, and [1] outputs
I1018 08:53:57.838511 4097533 graph.cc:46] operator [pow] has [2] inputs, and [1] outputs
I1018 08:53:57.838516 4097533 graph.cc:46] operator [broadcast_to] has [1] inputs, and [1] outputs
I1018 08:53:57.838521 4097533 graph.cc:46] operator [elementwise_mul] has [2] inputs, and [1] outputs
I1018 08:53:57.838524 4097533 graph.cc:46] operator [broadcast_to] has [1] inputs, and [1] outputs
I1018 08:53:57.838528 4097533 graph.cc:46] operator [elementwise_mul] has [2] inputs, and [1] outputs
I1018 08:53:57.838533 4097533 graph.cc:46] operator [broadcast_to] has [1] inputs, and [1] outputs
I1018 08:53:57.838536 4097533 graph.cc:46] operator [elementwise_add] has [2] inputs, and [1] outputs
I1018 08:53:57.838541 4097533 graph.cc:46] operator [fill_constant] has [0] inputs, and [1] outputs
I1018 08:53:57.838546 4097533 graph.cc:46] operator [max] has [2] inputs, and [1] outputs
I1018 08:53:57.838549 4097533 graph.cc:46] operator [pool2d] has [1] inputs, and [1] outputs
I1018 08:53:57.838562 4097533 optimize.cc:155] Before hlir::framework::ApplyPasses
I1018 08:53:57.838564 4097533 pass.cc:27] Run Pass -> ConstantFolding
I1018 08:53:57.838572 4097533 pass.cc:27] Run Pass -> TransToCustomCallPass
I1018 08:53:57.838580 4097533 pass.cc:27] Run Pass -> OpFusionPass
I1018 08:53:57.838582 4097533 pass.cc:27] Run Pass -> GeneralFusionMergePass
I1018 08:53:57.838590 4097533 pass.cc:27] Run Pass -> SingleGroupOptimizePass
I1018 08:53:57.838637 4097533 custom_call_pass.cc:93] TransToCustomCallPass...!
I1018 08:53:57.838661 4097533 custom_call_pass.cc:95] TransToCustomCallPass Finish...!
I1018 08:53:57.838663 4097533 op_fusion_pass.cc:373] OpFusionPass...!
I1018 08:53:57.838729 4097533 op_fusion_pass.cc:156] Producer Op: fill_constant_18, Op Pattern: 0 -> Consumer Op: max_19, Op Pattern: 0
I1018 08:53:57.838733 4097533 op_fusion_pass.cc:349] Call ConditionFunction, Producer Op Pattern : 0 , Consumer Group Pattern : 0
I1018 08:53:57.838737 4097533 op_fusion_pass.cc:174] Fuse Op fill_constant_18 into Op max_19
I1018 08:53:57.838740 4097533 op_fusion_pass.cc:156] Producer Op: elementwise_add_17, Op Pattern: 0 -> Consumer Op: max_19, Op Pattern: 0
I1018 08:53:57.838742 4097533 op_fusion_pass.cc:349] Call ConditionFunction, Producer Op Pattern : 0 , Consumer Group Pattern : 0
I1018 08:53:57.838744 4097533 op_fusion_pass.cc:174] Fuse Op elementwise_add_17 into Op max_19
I1018 08:53:57.838748 4097533 op_fusion_pass.cc:156] Producer Op: elementwise_mul_15, Op Pattern: 0 -> Consumer Op: elementwise_add_17, Op Pattern: 0
I1018 08:53:57.838750 4097533 op_fusion_pass.cc:349] Call ConditionFunction, Producer Op Pattern : 0 , Consumer Group Pattern : 0
I1018 08:53:57.838752 4097533 op_fusion_pass.cc:174] Fuse Op elementwise_mul_15 into Op elementwise_add_17
I1018 08:53:57.838755 4097533 op_fusion_pass.cc:156] Producer Op: broadcast_to_16, Op Pattern: 1 -> Consumer Op: elementwise_add_17, Op Pattern: 0
I1018 08:53:57.838758 4097533 op_fusion_pass.cc:349] Call ConditionFunction, Producer Op Pattern : 1 , Consumer Group Pattern : 0
I1018 08:53:57.838761 4097533 op_fusion_pass.cc:174] Fuse Op broadcast_to_16 into Op elementwise_add_17
I1018 08:53:57.838764 4097533 op_fusion_pass.cc:156] Producer Op: broadcast_to_14, Op Pattern: 1 -> Consumer Op: elementwise_mul_15, Op Pattern: 0
I1018 08:53:57.838766 4097533 op_fusion_pass.cc:349] Call ConditionFunction, Producer Op Pattern : 1 , Consumer Group Pattern : 1
I1018 08:53:57.838768 4097533 op_fusion_pass.cc:174] Fuse Op broadcast_to_14 into Op elementwise_mul_15
I1018 08:53:57.838771 4097533 op_fusion_pass.cc:156] Producer Op: elementwise_mul_13, Op Pattern: 0 -> Consumer Op: elementwise_mul_15, Op Pattern: 0
I1018 08:53:57.838774 4097533 op_fusion_pass.cc:349] Call ConditionFunction, Producer Op Pattern : 0 , Consumer Group Pattern : 1
I1018 08:53:57.838776 4097533 op_fusion_pass.cc:174] Fuse Op elementwise_mul_13 into Op elementwise_mul_15
I1018 08:53:57.838779 4097533 op_fusion_pass.cc:156] Producer Op: broadcast_to_12, Op Pattern: 1 -> Consumer Op: elementwise_mul_13, Op Pattern: 0
I1018 08:53:57.838781 4097533 op_fusion_pass.cc:349] Call ConditionFunction, Producer Op Pattern : 1 , Consumer Group Pattern : 1
I1018 08:53:57.838784 4097533 op_fusion_pass.cc:174] Fuse Op broadcast_to_12 into Op elementwise_mul_13
I1018 08:53:57.838788 4097533 op_fusion_pass.cc:156] Producer Op: subtract_6, Op Pattern: 0 -> Consumer Op: elementwise_mul_13, Op Pattern: 0
I1018 08:53:57.838789 4097533 op_fusion_pass.cc:349] Call ConditionFunction, Producer Op Pattern : 0 , Consumer Group Pattern : 1
I1018 08:53:57.838793 4097533 op_fusion_pass.cc:174] Fuse Op subtract_6 into Op elementwise_mul_13
I1018 08:53:57.838795 4097533 op_fusion_pass.cc:156] Producer Op: pow_11, Op Pattern: 0 -> Consumer Op: broadcast_to_12, Op Pattern: 1
I1018 08:53:57.838797 4097533 op_fusion_pass.cc:349] Call ConditionFunction, Producer Op Pattern : 0 , Consumer Group Pattern : 1
I1018 08:53:57.838800 4097533 op_fusion_pass.cc:174] Fuse Op pow_11 into Op broadcast_to_12
I1018 08:53:57.838802 4097533 op_fusion_pass.cc:156] Producer Op: fill_constant_10, Op Pattern: 0 -> Consumer Op: pow_11, Op Pattern: 0
I1018 08:53:57.838805 4097533 op_fusion_pass.cc:349] Call ConditionFunction, Producer Op Pattern : 0 , Consumer Group Pattern : 1
I1018 08:53:57.838807 4097533 op_fusion_pass.cc:174] Fuse Op fill_constant_10 into Op pow_11
I1018 08:53:57.838810 4097533 op_fusion_pass.cc:156] Producer Op: scale_7, Op Pattern: 0 -> Consumer Op: pow_11, Op Pattern: 0
I1018 08:53:57.838815 4097533 op_fusion_pass.cc:349] Call ConditionFunction, Producer Op Pattern : 0 , Consumer Group Pattern : 1
I1018 08:53:57.838819 4097533 op_fusion_pass.cc:174] Fuse Op scale_7 into Op pow_11
I1018 08:53:57.838821 4097533 op_fusion_pass.cc:156] Producer Op: broadcast_to_5, Op Pattern: 1 -> Consumer Op: subtract_6, Op Pattern: 0
I1018 08:53:57.838824 4097533 op_fusion_pass.cc:349] Call ConditionFunction, Producer Op Pattern : 1 , Consumer Group Pattern : 1
I1018 08:53:57.838826 4097533 op_fusion_pass.cc:174] Fuse Op broadcast_to_5 into Op subtract_6
I1018 08:53:57.838829 4097533 op_fusion_pass.cc:156] Producer Op: reshape_3, Op Pattern: 0 -> Consumer Op: scale_7, Op Pattern: 0
I1018 08:53:57.838831 4097533 op_fusion_pass.cc:349] Call ConditionFunction, Producer Op Pattern : 0 , Consumer Group Pattern : 1
I1018 08:53:57.838833 4097533 op_fusion_pass.cc:174] Fuse Op reshape_3 into Op scale_7
I1018 08:53:57.838837 4097533 op_fusion_pass.cc:156] Producer Op: reshape_2, Op Pattern: 0 -> Consumer Op: broadcast_to_5, Op Pattern: 1
I1018 08:53:57.838840 4097533 op_fusion_pass.cc:349] Call ConditionFunction, Producer Op Pattern : 0 , Consumer Group Pattern : 1
I1018 08:53:57.838841 4097533 op_fusion_pass.cc:174] Fuse Op reshape_2 into Op broadcast_to_5
I1018 08:53:57.838845 4097533 op_fusion_pass.cc:156] Producer Op: reshape_1, Op Pattern: 0 -> Consumer Op: broadcast_to_14, Op Pattern: 1
I1018 08:53:57.838846 4097533 op_fusion_pass.cc:349] Call ConditionFunction, Producer Op Pattern : 0 , Consumer Group Pattern : 1
I1018 08:53:57.838850 4097533 op_fusion_pass.cc:174] Fuse Op reshape_1 into Op broadcast_to_14
I1018 08:53:57.838851 4097533 op_fusion_pass.cc:156] Producer Op: reshape_0, Op Pattern: 0 -> Consumer Op: broadcast_to_16, Op Pattern: 1
I1018 08:53:57.838855 4097533 op_fusion_pass.cc:349] Call ConditionFunction, Producer Op Pattern : 0 , Consumer Group Pattern : 1
I1018 08:53:57.838856 4097533 op_fusion_pass.cc:174] Fuse Op reshape_0 into Op broadcast_to_16
I1018 08:53:57.838860 4097533 op_fusion_pass.cc:156] Producer Op: fill_constant_8, Op Pattern: 0 -> Consumer Op: pow_9, Op Pattern: 0
I1018 08:53:57.838861 4097533 op_fusion_pass.cc:349] Call ConditionFunction, Producer Op Pattern : 0 , Consumer Group Pattern : 0
I1018 08:53:57.838863 4097533 op_fusion_pass.cc:174] Fuse Op fill_constant_8 into Op pow_9
I1018 08:53:57.838866 4097533 op_fusion_pass.cc:156] Producer Op: fill_constant_4, Op Pattern: 0 -> Consumer Op: pow_9, Op Pattern: 0
I1018 08:53:57.838868 4097533 op_fusion_pass.cc:349] Call ConditionFunction, Producer Op Pattern : 0 , Consumer Group Pattern : 0
I1018 08:53:57.838871 4097533 op_fusion_pass.cc:174] Fuse Op fill_constant_4 into Op pow_9
I1018 08:53:57.838877 4097533 op_fusion_pass.cc:378] Group Id : fill_constant_4_fill_constant_8_pow_9
I1018 08:53:57.838879 4097533 op_fusion_pass.cc:378] Group Id : reshape_0_reshape_1_reshape_2_reshape_3_broadcast_to_5_scale_7_fill_constant_10_pow_11_subtract_6_broadcast_to_12_elementwise_mul_13_broadcast_to_14_broadcast_to_16_elementwise_mul_15_elementwise_add_17_fill_constant_18_max_19
I1018 08:53:57.838881 4097533 op_fusion_pass.cc:383]   consumer group -> pool2d_20
I1018 08:53:57.838883 4097533 op_fusion_pass.cc:378] Group Id : pool2d_20
I1018 08:53:57.838886 4097533 op_fusion_pass.cc:380]   producer group -> reshape_0_reshape_1_reshape_2_reshape_3_broadcast_to_5_scale_7_fill_constant_10_pow_11_subtract_6_broadcast_to_12_elementwise_mul_13_broadcast_to_14_broadcast_to_16_elementwise_mul_15_elementwise_add_17_fill_constant_18_max_19
I1018 08:53:57.838887 4097533 op_fusion_pass.cc:386] OpFusionPass Finish...!
I1018 08:53:57.838893 4097533 general_fusion_merge_pass.cc:983] InitInputToConsumers...!
I1018 08:53:57.838901 4097533 general_fusion_merge_pass.cc:1001] InitFusionGroupsAndIndex...!
I1018 08:53:57.838912 4097533 general_fusion_merge_pass.cc:91] DoFusionMerge...!
I1018 08:53:57.838914 4097533 general_fusion_merge_pass.cc:101] DoGeneralHorizontalFusion...!
I1018 08:53:57.838918 4097533 general_fusion_merge_pass.cc:105] Fusion Producer idx 0 Group -> fill_constant_4_fill_constant_8_pow_9
I1018 08:53:57.838920 4097533 general_fusion_merge_pass.cc:245] GeneralHorizontalFuse handling producer : fill_constant_4_fill_constant_8_pow_9
I1018 08:53:57.838924 4097533 general_fusion_merge_pass.cc:105] Fusion Producer idx 1 Group -> reshape_0_reshape_1_reshape_2_reshape_3_broadcast_to_5_scale_7_fill_constant_10_pow_11_subtract_6_broadcast_to_12_elementwise_mul_13_broadcast_to_14_broadcast_to_16_elementwise_mul_15_elementwise_add_17_fill_constant_18_max_19
I1018 08:53:57.838927 4097533 general_fusion_merge_pass.cc:245] GeneralHorizontalFuse handling producer : reshape_0_reshape_1_reshape_2_reshape_3_broadcast_to_5_scale_7_fill_constant_10_pow_11_subtract_6_broadcast_to_12_elementwise_mul_13_broadcast_to_14_broadcast_to_16_elementwise_mul_15_elementwise_add_17_fill_constant_18_max_19
I1018 08:53:57.838928 4097533 general_fusion_merge_pass.cc:105] Fusion Producer idx 2 Group -> pool2d_20
I1018 08:53:57.838930 4097533 general_fusion_merge_pass.cc:245] GeneralHorizontalFuse handling producer : pool2d_20
I1018 08:53:57.838932 4097533 general_fusion_merge_pass.cc:122] DoGeneralVerticalFusion...!
I1018 08:53:57.838934 4097533 general_fusion_merge_pass.cc:126] Fusion Producer idx 0 Group -> fill_constant_4_fill_constant_8_pow_9
I1018 08:53:57.838937 4097533 general_fusion_merge_pass.cc:245] GeneralHorizontalFuse handling producer : fill_constant_4_fill_constant_8_pow_9
I1018 08:53:57.838940 4097533 general_fusion_merge_pass.cc:523] GeneralVerticalFuse...!
I1018 08:53:57.838943 4097533 general_fusion_merge_pass.cc:126] Fusion Producer idx 1 Group -> reshape_0_reshape_1_reshape_2_reshape_3_broadcast_to_5_scale_7_fill_constant_10_pow_11_subtract_6_broadcast_to_12_elementwise_mul_13_broadcast_to_14_broadcast_to_16_elementwise_mul_15_elementwise_add_17_fill_constant_18_max_19
I1018 08:53:57.838944 4097533 general_fusion_merge_pass.cc:245] GeneralHorizontalFuse handling producer : reshape_0_reshape_1_reshape_2_reshape_3_broadcast_to_5_scale_7_fill_constant_10_pow_11_subtract_6_broadcast_to_12_elementwise_mul_13_broadcast_to_14_broadcast_to_16_elementwise_mul_15_elementwise_add_17_fill_constant_18_max_19
I1018 08:53:57.838948 4097533 general_fusion_merge_pass.cc:523] GeneralVerticalFuse...!
I1018 08:53:57.838976 4097533 general_fusion_merge_pass.cc:126] Fusion Producer idx 2 Group -> pool2d_20
I1018 08:53:57.838979 4097533 general_fusion_merge_pass.cc:245] GeneralHorizontalFuse handling producer : pool2d_20
I1018 08:53:57.838981 4097533 general_fusion_merge_pass.cc:523] GeneralVerticalFuse...!
I1018 08:53:57.838984 4097533 general_fusion_merge_pass.cc:934] GeneralInputFuse...!
I1018 08:53:57.838989 4097533 general_fusion_merge_pass.cc:147] DoGeneralRecomputeAndVerticalFusion...!
I1018 08:53:57.838990 4097533 general_fusion_merge_pass.cc:151] Fusion Producer idx 0 Group -> fill_constant_4_fill_constant_8_pow_9
I1018 08:53:57.838992 4097533 general_fusion_merge_pass.cc:771] GeneralRecomputeFuse handling producer : fill_constant_4_fill_constant_8_pow_9
I1018 08:53:57.838996 4097533 general_fusion_merge_pass.cc:523] GeneralVerticalFuse...!
I1018 08:53:57.838999 4097533 general_fusion_merge_pass.cc:151] Fusion Producer idx 1 Group -> reshape_0_reshape_1_reshape_2_reshape_3_broadcast_to_5_scale_7_fill_constant_10_pow_11_subtract_6_broadcast_to_12_elementwise_mul_13_broadcast_to_14_broadcast_to_16_elementwise_mul_15_elementwise_add_17_fill_constant_18_max_19
I1018 08:53:57.839000 4097533 general_fusion_merge_pass.cc:771] GeneralRecomputeFuse handling producer : reshape_0_reshape_1_reshape_2_reshape_3_broadcast_to_5_scale_7_fill_constant_10_pow_11_subtract_6_broadcast_to_12_elementwise_mul_13_broadcast_to_14_broadcast_to_16_elementwise_mul_15_elementwise_add_17_fill_constant_18_max_19
I1018 08:53:57.839004 4097533 general_fusion_merge_pass.cc:523] GeneralVerticalFuse...!
I1018 08:53:57.839005 4097533 general_fusion_merge_pass.cc:151] Fusion Producer idx 2 Group -> pool2d_20
I1018 08:53:57.839008 4097533 general_fusion_merge_pass.cc:771] GeneralRecomputeFuse handling producer : pool2d_20
I1018 08:53:57.839011 4097533 general_fusion_merge_pass.cc:523] GeneralVerticalFuse...!
I1018 08:53:57.839013 4097533 general_fusion_merge_pass.cc:934] GeneralInputFuse...!
I1018 08:53:57.839017 4097533 general_fusion_merge_pass.cc:75] Fusion Group -> fill_constant_4_fill_constant_8_pow_9
I1018 08:53:57.839020 4097533 general_fusion_merge_pass.cc:77]   Fused Sub-Group -> fill_constant_4_fill_constant_8_pow_9
I1018 08:53:57.839021 4097533 general_fusion_merge_pass.cc:75] Fusion Group -> reshape_0_reshape_1_reshape_2_reshape_3_broadcast_to_5_scale_7_fill_constant_10_pow_11_subtract_6_broadcast_to_12_elementwise_mul_13_broadcast_to_14_broadcast_to_16_elementwise_mul_15_elementwise_add_17_fill_constant_18_max_19
I1018 08:53:57.839023 4097533 general_fusion_merge_pass.cc:77]   Fused Sub-Group -> reshape_0_reshape_1_reshape_2_reshape_3_broadcast_to_5_scale_7_fill_constant_10_pow_11_subtract_6_broadcast_to_12_elementwise_mul_13_broadcast_to_14_broadcast_to_16_elementwise_mul_15_elementwise_add_17_fill_constant_18_max_19
I1018 08:53:57.839025 4097533 general_fusion_merge_pass.cc:83]   Consumer -> pool2d_20
I1018 08:53:57.839027 4097533 general_fusion_merge_pass.cc:75] Fusion Group -> pool2d_20
I1018 08:53:57.839030 4097533 general_fusion_merge_pass.cc:77]   Fused Sub-Group -> pool2d_20
I1018 08:53:57.839031 4097533 general_fusion_merge_pass.cc:80]   Producer -> reshape_0_reshape_1_reshape_2_reshape_3_broadcast_to_5_scale_7_fill_constant_10_pow_11_subtract_6_broadcast_to_12_elementwise_mul_13_broadcast_to_14_broadcast_to_16_elementwise_mul_15_elementwise_add_17_fill_constant_18_max_19
I1018 08:53:57.839046 4097533 scope.h:65] Scope insert Var [conv2d_0__tmp_0]
I1018 08:53:57.839049 4097533 graph_compiler.cc:337] Tensor [conv2d_0__tmp_0] resize to 1,64,112,112
I1018 08:53:57.839056 4097533 scope.h:65] Scope insert Var [var_43]
I1018 08:53:57.839057 4097533 graph_compiler.cc:337] Tensor [var_43] resize to 1,64,1,1
I1018 08:53:57.839061 4097533 scope.h:65] Scope insert Var [var_55]
I1018 08:53:57.839063 4097533 graph_compiler.cc:337] Tensor [var_55] resize to 1,64,56,56
I1018 08:53:57.839067 4097533 scope.h:65] Scope insert Var [var_23]
I1018 08:53:57.839069 4097533 graph_compiler.cc:337] Tensor [var_23] resize to 1,64,1,1
I1018 08:53:57.839072 4097533 scope.h:65] Scope insert Var [var_106]
I1018 08:53:57.839074 4097533 graph_compiler.cc:337] Tensor [var_106] resize to 1,64,112,112
I1018 08:53:57.839077 4097533 scope.h:65] Scope insert Var [var_105]
I1018 08:53:57.839080 4097533 graph_compiler.cc:337] Tensor [var_105] resize to 1,64,112,112
I1018 08:53:57.839083 4097533 scope.h:65] Scope insert Var [var_103]
I1018 08:53:57.839085 4097533 graph_compiler.cc:337] Tensor [var_103] resize to 1,64,1,1
I1018 08:53:57.839088 4097533 scope.h:65] Scope insert Var [var_27]
I1018 08:53:57.839092 4097533 graph_compiler.cc:337] Tensor [var_27] resize to 1,64,1,1
I1018 08:53:57.839094 4097533 scope.h:65] Scope insert Var [var_49]
I1018 08:53:57.839097 4097533 graph_compiler.cc:337] Tensor [var_49] resize to 1,64,112,112
I1018 08:53:57.839099 4097533 scope.h:65] Scope insert Var [var_39]
I1018 08:53:57.839102 4097533 graph_compiler.cc:337] Tensor [var_39] resize to 1,64,1,1
I1018 08:53:57.839105 4097533 scope.h:65] Scope insert Var [var_19]
I1018 08:53:57.839107 4097533 graph_compiler.cc:337] Tensor [var_19] resize to 1,64,1,1
I1018 08:53:57.839110 4097533 scope.h:65] Scope insert Var [batch_norm2d_0__b_0]
I1018 08:53:57.839112 4097533 graph_compiler.cc:337] Tensor [batch_norm2d_0__b_0] resize to 64
I1018 08:53:57.839116 4097533 scope.h:65] Scope insert Var [batch_norm2d_0__w_2]
I1018 08:53:57.839118 4097533 graph_compiler.cc:337] Tensor [batch_norm2d_0__w_2] resize to 64
I1018 08:53:57.839121 4097533 scope.h:65] Scope insert Var [var_104]
I1018 08:53:57.839123 4097533 graph_compiler.cc:337] Tensor [var_104] resize to 1,64,112,112
I1018 08:53:57.839126 4097533 scope.h:65] Scope insert Var [var_47]
I1018 08:53:57.839133 4097533 graph_compiler.cc:337] Tensor [var_47] resize to 1,64,112,112
I1018 08:53:57.839138 4097533 scope.h:65] Scope insert Var [var_53]
I1018 08:53:57.839140 4097533 graph_compiler.cc:337] Tensor [var_53] resize to 1,64,112,112
I1018 08:53:57.839143 4097533 scope.h:65] Scope insert Var [var_51]
I1018 08:53:57.839146 4097533 graph_compiler.cc:337] Tensor [var_51] resize to 1,64,112,112
I1018 08:53:57.839149 4097533 scope.h:65] Scope insert Var [var_45]
I1018 08:53:57.839151 4097533 graph_compiler.cc:337] Tensor [var_45] resize to 64
I1018 08:53:57.839154 4097533 scope.h:65] Scope insert Var [var_73]
I1018 08:53:57.839157 4097533 graph_compiler.cc:337] Tensor [var_73] resize to 1,64,112,112
I1018 08:53:57.839160 4097533 scope.h:65] Scope insert Var [var_31]
I1018 08:53:57.839162 4097533 graph_compiler.cc:337] Tensor [var_31] resize to 1,64,1,1
I1018 08:53:57.839165 4097533 scope.h:65] Scope insert Var [var_37]
I1018 08:53:57.839167 4097533 graph_compiler.cc:337] Tensor [var_37] resize to 1,64,112,112
I1018 08:53:57.839170 4097533 scope.h:65] Scope insert Var [batch_norm2d_0__w_0]
I1018 08:53:57.839172 4097533 graph_compiler.cc:337] Tensor [batch_norm2d_0__w_0] resize to 64
I1018 08:53:57.839176 4097533 scope.h:65] Scope insert Var [batch_norm2d_0__w_1]
I1018 08:53:57.839179 4097533 graph_compiler.cc:337] Tensor [batch_norm2d_0__w_1] resize to 64
I1018 08:53:57.839181 4097533 scope.h:65] Scope insert Var [var_101]
I1018 08:53:57.839184 4097533 graph_compiler.cc:337] Tensor [var_101] resize to 1,64,112,112
I1018 08:53:57.839187 4097533 scope.h:65] Scope insert Var [var_102]
I1018 08:53:57.839190 4097533 graph_compiler.cc:337] Tensor [var_102] resize to 64
I1018 08:53:57.839192 4097533 scope.h:65] Scope insert Var [var_35]
I1018 08:53:57.839195 4097533 graph_compiler.cc:337] Tensor [var_35] resize to 64
I1018 08:53:57.839201 4097533 graph_compiler.cc:63] Compile With Parallel Compiler!
I1018 08:53:57.839239 4097533 parallel_compiler.cc:174] Compile with 56 threads
I1018 08:53:57.839463 4097768 parallel_compiler.cc:126] Start run task 0 on thread: 140640350574336
I1018 08:53:57.839506 4097768 parallel_compiler.cc:128] Start lowering on task 0
I1018 08:53:57.839715 4097770 parallel_compiler.cc:126] Start run task 1 on thread: 140640367359744
I1018 08:53:57.839757 4097770 parallel_compiler.cc:128] Start lowering on task 1
I1018 08:53:57.839751 4097769 parallel_compiler.cc:126] Start run task 2 on thread: 140640358967040
I1018 08:53:57.839792 4097769 parallel_compiler.cc:128] Start lowering on task 2
I1018 08:53:57.839524 4097768 parallel_compiler.cc:222] Start Lowering Group 0 at 140640350574336 :
Group 0 {

    var_35 = builder.fill_constant(dtype="float32", force_cpu=False, shape=[64], value=9.9999997473787516e-06)
    var_102 = builder.fill_constant(dtype="float32", force_cpu=False, shape=[64], value=-0.50000000000000000)
    var_45 = builder.pow(var_35, var_102, axis=-1)

    feed_list = []
    fetch_list = [var_45]
}
I1018 08:53:57.839841 4097768 op_lowering_impl.cc:54] Lowering Group : fill_constant_4_fill_constant_8_pow_9 , Op Pattern : 0
I1018 08:53:57.839852 4097768 op_lowering_impl.cc:108] group->fused_sub_groups.size() is : 1
I1018 08:53:57.839934 4097768 op_lowering_impl.cc:373] Do lower with Compute, op: fill_constant
I1018 08:53:57.839805 4097769 parallel_compiler.cc:222] Start Lowering Group 2 at 140640358967040 :
Group 2 {
    var_53 = builder.create_input(type="float32", shape=[1, 64, 112, 112], id_hint="var_53")

    var_55 = builder.pool2d(var_53, adaptive=False, ceil_mode=False, data_format="NCHW", exclusive=True, global_pooling=False, kernel_size=[3, 3], origin_adaptive=False, origin_global_pooling=False, origin_kernel_size=[3, 3], origin_padding_size=[1, 1], padding_algorithm="EXPLICIT", padding_size=[1, 1, 1, 1], pool_type="max", stride_size=[2, 2])

    feed_list = [var_53]
    fetch_list = [var_55]
}
I1018 08:53:57.839974 4097769 op_lowering_impl.cc:54] Lowering Group : pool2d_20 , Op Pattern : 8
I1018 08:53:57.839985 4097769 op_lowering_impl.cc:108] group->fused_sub_groups.size() is : 1
I1018 08:53:57.840010 4097768 ir_simplify.cc:467] Begin Simplify 64
I1018 08:53:57.840052 4097768 compute.cc:209] tensor var_35's domain is : 64
I1018 08:53:57.839773 4097770 parallel_compiler.cc:222] Start Lowering Group 1 at 140640367359744 :
Group 1 {
    conv2d_0__tmp_0 = builder.create_input(type="float32", shape=[1, 64, 112, 112], id_hint="conv2d_0__tmp_0")
    batch_norm2d_0__w_2 = builder.create_input(type="float32", shape=[64], id_hint="batch_norm2d_0__w_2")
    batch_norm2d_0__w_1 = builder.create_input(type="float32", shape=[64], id_hint="batch_norm2d_0__w_1")
    batch_norm2d_0__w_0 = builder.create_input(type="float32", shape=[64], id_hint="batch_norm2d_0__w_0")
    batch_norm2d_0__b_0 = builder.create_input(type="float32", shape=[64], id_hint="batch_norm2d_0__b_0")

    var_19 = builder.reshape(batch_norm2d_0__b_0, shape=[1, 64, 1, 1])
    var_23 = builder.reshape(batch_norm2d_0__w_0, shape=[1, 64, 1, 1])
    var_27 = builder.reshape(batch_norm2d_0__w_1, shape=[1, 64, 1, 1])
    var_31 = builder.reshape(batch_norm2d_0__w_2, shape=[1, 64, 1, 1])
    var_101 = builder.broadcast_to(var_27, broadcast_axes=[0, 1, 2, 3], out_shape=[1, 64, 112, 112])
    var_39 = builder.scale(var_31, bias=9.99999975e-06, bias_after_scale=True, scale=1.00000000)
    var_103 = builder.fill_constant(dtype="float32", force_cpu=False, shape=[1, 64, 1, 1], value=-0.50000000000000000)
    var_43 = builder.pow(var_39, var_103, axis=-1)
    var_37 = builder.subtract(conv2d_0__tmp_0, var_101, axis=-1)
    var_104 = builder.broadcast_to(var_43, broadcast_axes=[0, 1, 2, 3], out_shape=[1, 64, 112, 112])
    var_47 = builder.elementwise_mul(var_37, var_104, axis=-1)
    var_105 = builder.broadcast_to(var_23, broadcast_axes=[0, 1, 2, 3], out_shape=[1, 64, 112, 112])
    var_73 = builder.broadcast_to(var_19, broadcast_axes=[0, 1, 2, 3], out_shape=[1, 64, 112, 112])
    var_49 = builder.elementwise_mul(var_105, var_47, axis=-1)
    var_51 = builder.elementwise_add(var_49, var_73, axis=-1)
    var_106 = builder.fill_constant(dtype="float32", force_cpu=False, shape=[1, 64, 112, 112], value=0.0000000000000000)
    var_53 = builder.max(var_51, var_106, axis=-1)

    feed_list = [conv2d_0__tmp_0, batch_norm2d_0__w_2, batch_norm2d_0__w_1, batch_norm2d_0__w_0, batch_norm2d_0__b_0]
    fetch_list = [var_53]
}
I1018 08:53:57.840219 4097768 tensor.cc:259] name:var_35, domain: []->{ var_35[i]: 0 <= i <= 63 }
I1018 08:53:57.840253 4097770 op_lowering_impl.cc:54] Lowering Group : reshape_0_reshape_1_reshape_2_reshape_3_broadcast_to_5_scale_7_fill_constant_10_pow_11_subtract_6_broadcast_to_12_elementwise_mul_13_broadcast_to_14_broadcast_to_16_elementwise_mul_15_elementwise_add_17_fill_constant_18_max_19 , Op Pattern : 1
I1018 08:53:57.840283 4097770 op_lowering_impl.cc:108] group->fused_sub_groups.size() is : 1
I1018 08:53:57.840269 4097768 domain.cc:65] isl::set []->{ var_35[i]: 0 <= i <= 63 }
I1018 08:53:57.840298 4097769 op_lowering_impl.cc:373] Do lower with Compute, op: pool2d
I1018 08:53:57.840459 4097770 op_lowering_impl.cc:373] Do lower with Compute, op: reshape
I1018 08:53:57.840502 4097770 tensor.cc:259] name:batch_norm2d_0__b_0, domain: []->{ batch_norm2d_0__b_0[]:  }
I1018 08:53:57.840519 4097770 domain.cc:65] isl::set []->{ batch_norm2d_0__b_0[]:  }
I1018 08:53:57.840715 4097770 elementwise.cc:992] A shape: 64, output_shapes: 1, 64, 1, 1
I1018 08:53:57.840816 4097770 ir_simplify.cc:467] Begin Simplify 1
I1018 08:53:57.840831 4097770 ir_simplify.cc:467] Begin Simplify 64
I1018 08:53:57.840844 4097770 ir_simplify.cc:467] Begin Simplify 1
I1018 08:53:57.840857 4097770 ir_simplify.cc:467] Begin Simplify 1
I1018 08:53:57.840870 4097770 compute.cc:209] tensor var_19's domain is : 1, 64, 1, 1
I1018 08:53:57.840975 4097770 tensor.cc:259] name:var_19, domain: []->{ var_19[i, j, k, a]: 0 <= i <= 0 and 0 <= j <= 63 and 0 <= k <= 0 and 0 <= a <= 0 }
I1018 08:53:57.841068 4097769 ir_simplify.cc:467] Begin Simplify 1
I1018 08:53:57.841044 4097770 domain.cc:65] isl::set []->{ var_19[i, j, k, a]: 0 <= i <= 0 and 0 <= j <= 63 and 0 <= k <= 0 and 0 <= a <= 0 }
I1018 08:53:57.841092 4097769 ir_simplify.cc:467] Begin Simplify 64
I1018 08:53:57.841120 4097769 ir_simplify.cc:467] Begin Simplify 114
I1018 08:53:57.841131 4097769 ir_simplify.cc:467] Begin Simplify 114
I1018 08:53:57.841146 4097769 compute.cc:209] tensor pad_temp_0's domain is : 1, 64, 114, 114
I1018 08:53:57.841305 4097768 lower_tensor_group.cc:103] In store_exprs, its name is : var_35
I1018 08:53:57.841348 4097768 lower_tensor_group.cc:134] func_args is : _var_35
I1018 08:53:57.841403 4097768 lowered_func.cc:230] Function used 1 buffers
I1018 08:53:57.841468 4097769 ir_simplify.cc:467] Begin Simplify 1
I1018 08:53:57.841485 4097769 ir_simplify.cc:467] Begin Simplify 64
I1018 08:53:57.841503 4097769 ir_simplify.cc:467] Begin Simplify 56
I1018 08:53:57.841488 4097768 op_lowering_impl.cc:420] Lower op: fill_constant, get 1 LoweredFunc:
I1018 08:53:57.841516 4097769 ir_simplify.cc:467] Begin Simplify 56
I1018 08:53:57.841547 4097769 compute.cc:209] tensor var_55's domain is : 1, 64, 56, 56
I1018 08:53:57.841529 4097768 op_lowering_impl.cc:423] function fn_fill_constant_4 (_var_35)
{
  ScheduleBlock(root)
  {
    serial for (i, 0, 64)
    {
      ScheduleBlock(var_35)
      {
        i0 = axis.bind(i)
        var_35[i0] = float32(9.9999997473787516e-06)
      }
    }
  }
}
I1018 08:53:57.841614 4097768 op_lowering_impl.cc:450] Do op schedule
I1018 08:53:57.841621 4097769 tensor.cc:259] name:var_53, domain: []->{ var_53[]:  }
I1018 08:53:57.841636 4097769 domain.cc:65] isl::set []->{ var_53[]:  }
I1018 08:53:57.841643 4097768 ir_schedule_pe.cc:70] Before IRElementwiseSchedule, new ir is : {
  ScheduleBlock(root)
  {
    serial for (i, 0, 64)
    {
      ScheduleBlock(var_35)
      {
        i0 = axis.bind(i)
        var_35[i0] = float32(9.9999997473787516e-06)
      }
    }
  }
}
I1018 08:53:57.841682 4097768 ir_analyzer.cc:101] it_expr is : {
  ScheduleBlock(root)
  {
    serial for (i, 0, 64)
    {
      ScheduleBlock(var_35)
      {
        i0 = axis.bind(i)
        var_35[i0] = float32(9.9999997473787516e-06)
      }
    }
  }
}
I1018 08:53:57.841723 4097768 loop_transformation.cc:688] Before FlattenLoops, ir is:
serial for (i, 0, 64)
{
  ScheduleBlock(var_35)
  {
    i0 = axis.bind(i)
    var_35[i0] = float32(9.9999997473787516e-06)
  }
}
I1018 08:53:57.841807 4097768 loop_transformation.cc:864] After FlattenLoops, ir is:
serial for (flat_i, 0, 64)
{
  ScheduleBlock(var_35)
  {
    _flat_i = axis.bind(flat_i)
    var_35[_flat_i] = float32(9.9999997473787516e-06)
  }
}
I1018 08:53:57.841845 4097768 ir_schedule_pe.cc:109] After IRElementwiseSchedule, new ir is : {
  ScheduleBlock(root)
  {
    serial for (flat_i, 0, 64)
    {
      ScheduleBlock(var_35)
      {
        _flat_i = axis.bind(flat_i)
        var_35[_flat_i] = float32(9.9999997473787516e-06)
      }
    }
  }
}
I1018 08:53:57.841804 4097769 tensor.cc:259] name:var_55, domain: []->{ var_55[i, j, k, a, kernel_idx, kernel_idx_0]: 0 <= i <= 0 and 0 <= j <= 63 and 0 <= k <= 55 and 0 <= a <= 55 and 0 <= kernel_idx <= 2 and 0 <= kernel_idx_0 <= 2 }
I1018 08:53:57.841881 4097768 op_lowering_impl.cc:463] After op schedule: {
  ScheduleBlock(root)
  {
    serial for (flat_i, 0, 64)
    {
      ScheduleBlock(var_35)
      {
        _flat_i = axis.bind(flat_i)
        var_35[_flat_i] = float32(9.9999997473787516e-06)
      }
    }
  }
}
I1018 08:53:57.841948 4097768 op_lowering_impl.cc:373] Do lower with Compute, op: fill_constant
I1018 08:53:57.841889 4097769 domain.cc:65] isl::set []->{ var_55[i, j, k, a, kernel_idx, kernel_idx_0]: 0 <= i <= 0 and 0 <= j <= 63 and 0 <= k <= 55 and 0 <= a <= 55 and 0 <= kernel_idx <= 2 and 0 <= kernel_idx_0 <= 2 }
I1018 08:53:57.841979 4097768 ir_simplify.cc:467] Begin Simplify 64
I1018 08:53:57.841995 4097768 compute.cc:209] tensor var_102's domain is : 64
I1018 08:53:57.842039 4097768 tensor.cc:259] name:var_102, domain: []->{ var_102[i]: 0 <= i <= 63 }
I1018 08:53:57.842072 4097768 domain.cc:65] isl::set []->{ var_102[i]: 0 <= i <= 63 }
I1018 08:53:57.842731 4097768 lower_tensor_group.cc:103] In store_exprs, its name is : var_102
I1018 08:53:57.842764 4097768 lower_tensor_group.cc:134] func_args is : _var_102
I1018 08:53:57.842824 4097768 lowered_func.cc:230] Function used 1 buffers
I1018 08:53:57.842898 4097768 op_lowering_impl.cc:420] Lower op: fill_constant, get 1 LoweredFunc:
I1018 08:53:57.842909 4097768 op_lowering_impl.cc:423] function fn_fill_constant_8 (_var_102)
{
  ScheduleBlock(root_0)
  {
    serial for (i, 0, 64)
    {
      ScheduleBlock(var_102)
      {
        i0_0 = axis.bind(i)
        var_102[i0_0] = float32(-0.50000000000000000)
      }
    }
  }
}
I1018 08:53:57.842963 4097768 op_lowering_impl.cc:450] Do op schedule
I1018 08:53:57.842984 4097768 ir_schedule_pe.cc:70] Before IRElementwiseSchedule, new ir is : {
  ScheduleBlock(root_0)
  {
    serial for (i, 0, 64)
    {
      ScheduleBlock(var_102)
      {
        i0_0 = axis.bind(i)
        var_102[i0_0] = float32(-0.50000000000000000)
      }
    }
  }
}
I1018 08:53:57.843017 4097768 ir_analyzer.cc:101] it_expr is : {
  ScheduleBlock(root_0)
  {
    serial for (i, 0, 64)
    {
      ScheduleBlock(var_102)
      {
        i0_0 = axis.bind(i)
        var_102[i0_0] = float32(-0.50000000000000000)
      }
    }
  }
}
I1018 08:53:57.843053 4097768 loop_transformation.cc:688] Before FlattenLoops, ir is:
serial for (i, 0, 64)
{
  ScheduleBlock(var_102)
  {
    i0_0 = axis.bind(i)
    var_102[i0_0] = float32(-0.50000000000000000)
  }
}
I1018 08:53:57.843077 4097770 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.843098 4097770 ir_util.cc:142] shape is : 1,64,1,1
I1018 08:53:57.843112 4097768 loop_transformation.cc:864] After FlattenLoops, ir is:
serial for (flat_i, 0, 64)
{
  ScheduleBlock(var_102)
  {
    _flat_i = axis.bind(flat_i)
    var_102[_flat_i] = float32(-0.50000000000000000)
  }
}
I1018 08:53:57.843127 4097770 ir_util.cc:143] indices is : i,j,k,a
I1018 08:53:57.843155 4097768 ir_schedule_pe.cc:109] After IRElementwiseSchedule, new ir is : {
  ScheduleBlock(root_0)
  {
    serial for (flat_i, 0, 64)
    {
      ScheduleBlock(var_102)
      {
        _flat_i = axis.bind(flat_i)
        var_102[_flat_i] = float32(-0.50000000000000000)
      }
    }
  }
}
I1018 08:53:57.843189 4097768 op_lowering_impl.cc:463] After op schedule: {
  ScheduleBlock(root_0)
  {
    serial for (flat_i, 0, 64)
    {
      ScheduleBlock(var_102)
      {
        _flat_i = axis.bind(flat_i)
        var_102[_flat_i] = float32(-0.50000000000000000)
      }
    }
  }
}
I1018 08:53:57.843317 4097768 op_lowering_impl.cc:373] Do lower with Compute, op: pow
I1018 08:53:57.843430 4097768 ir_simplify.cc:467] Begin Simplify 64
I1018 08:53:57.843449 4097768 compute.cc:209] tensor var_45's domain is : 64
I1018 08:53:57.843513 4097768 tensor.cc:259] name:var_102, domain: []->{ var_102[]:  }
I1018 08:53:57.843528 4097768 domain.cc:65] isl::set []->{ var_102[]:  }
I1018 08:53:57.843587 4097770 lower_tensor_group.cc:103] In store_exprs, its name is : var_19
I1018 08:53:57.843644 4097768 tensor.cc:259] name:var_35, domain: []->{ var_35[]:  }
I1018 08:53:57.843660 4097768 domain.cc:65] isl::set []->{ var_35[]:  }
I1018 08:53:57.843647 4097770 lower_tensor_group.cc:134] func_args is : _batch_norm2d_0__b_0
I1018 08:53:57.843678 4097770 lower_tensor_group.cc:134] func_args is : _var_19
I1018 08:53:57.843750 4097768 tensor.cc:259] name:var_45, domain: []->{ var_45[i]: 0 <= i <= 63 }
I1018 08:53:57.843775 4097770 lowered_func.cc:230] Function used 2 buffers
I1018 08:53:57.843786 4097768 domain.cc:65] isl::set []->{ var_45[i]: 0 <= i <= 63 }
I1018 08:53:57.843910 4097770 op_lowering_impl.cc:420] Lower op: reshape, get 1 LoweredFunc:
I1018 08:53:57.843922 4097770 op_lowering_impl.cc:423] function fn_reshape_0 (_batch_norm2d_0__b_0, _var_19)
{
  ScheduleBlock(root_1)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 1)
        {
          serial for (a, 0, 1)
          {
            ScheduleBlock(var_19)
            {
              i0_1, i1, i2, i3 = axis.bind(0, j, 0, 0)
              var_19[i0_1, i1, i2, i3] = batch_norm2d_0__b_0[(((((i0_1 * 64) + (i1 * 1)) + (i2 * 1)) + (i3 * 1)) % 64)]
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.844024 4097770 op_lowering_impl.cc:450] Do op schedule
I1018 08:53:57.844051 4097770 ir_schedule_pe.cc:70] Before IRElementwiseSchedule, new ir is : {
  ScheduleBlock(root_1)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 1)
        {
          serial for (a, 0, 1)
          {
            ScheduleBlock(var_19)
            {
              i0_1, i1, i2, i3 = axis.bind(0, j, 0, 0)
              var_19[i0_1, i1, i2, i3] = batch_norm2d_0__b_0[(((((i0_1 * 64) + (i1 * 1)) + (i2 * 1)) + (i3 * 1)) % 64)]
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.844101 4097770 ir_analyzer.cc:101] it_expr is : {
  ScheduleBlock(root_1)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 1)
        {
          serial for (a, 0, 1)
          {
            ScheduleBlock(var_19)
            {
              i0_1, i1, i2, i3 = axis.bind(0, j, 0, 0)
              var_19[i0_1, i1, i2, i3] = batch_norm2d_0__b_0[(((((i0_1 * 64) + (i1 * 1)) + (i2 * 1)) + (i3 * 1)) % 64)]
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.844159 4097770 loop_transformation.cc:688] Before FlattenLoops, ir is:
serial for (i, 0, 1)
{
  serial for (j, 0, 64)
  {
    serial for (k, 0, 1)
    {
      serial for (a, 0, 1)
      {
        ScheduleBlock(var_19)
        {
          i0_1, i1, i2, i3 = axis.bind(0, j, 0, 0)
          var_19[i0_1, i1, i2, i3] = batch_norm2d_0__b_0[(((((i0_1 * 64) + (i1 * 1)) + (i2 * 1)) + (i3 * 1)) % 64)]
        }
      }
    }
  }
}
I1018 08:53:57.844287 4097770 loop_transformation.cc:864] After FlattenLoops, ir is:
serial for (flat_i, 0, 64)
{
  ScheduleBlock(var_19)
  {
    _flat_i = axis.bind(flat_i)
    var_19[_flat_i] = batch_norm2d_0__b_0[_flat_i]
  }
}
I1018 08:53:57.844321 4097770 ir_schedule_pe.cc:109] After IRElementwiseSchedule, new ir is : {
  ScheduleBlock(root_1)
  {
    serial for (flat_i, 0, 64)
    {
      ScheduleBlock(var_19)
      {
        _flat_i = axis.bind(flat_i)
        var_19[_flat_i] = batch_norm2d_0__b_0[_flat_i]
      }
    }
  }
}
I1018 08:53:57.844352 4097770 op_lowering_impl.cc:463] After op schedule: {
  ScheduleBlock(root_1)
  {
    serial for (flat_i, 0, 64)
    {
      ScheduleBlock(var_19)
      {
        _flat_i = axis.bind(flat_i)
        var_19[_flat_i] = batch_norm2d_0__b_0[_flat_i]
      }
    }
  }
}
I1018 08:53:57.844453 4097770 op_lowering_impl.cc:373] Do lower with Compute, op: reshape
I1018 08:53:57.844461 4097768 lower_tensor_group.cc:103] In store_exprs, its name is : var_45
I1018 08:53:57.844477 4097770 tensor.cc:259] name:batch_norm2d_0__w_0, domain: []->{ batch_norm2d_0__w_0[]:  }
I1018 08:53:57.844492 4097770 domain.cc:65] isl::set []->{ batch_norm2d_0__w_0[]:  }
I1018 08:53:57.844502 4097768 lower_tensor_group.cc:134] func_args is : _var_35
I1018 08:53:57.844512 4097768 lower_tensor_group.cc:134] func_args is : _var_102
I1018 08:53:57.844520 4097768 lower_tensor_group.cc:134] func_args is : _var_45
I1018 08:53:57.844576 4097768 lowered_func.cc:230] Function used 3 buffers
I1018 08:53:57.844616 4097770 elementwise.cc:992] A shape: 64, output_shapes: 1, 64, 1, 1
I1018 08:53:57.844702 4097770 ir_simplify.cc:467] Begin Simplify 1
I1018 08:53:57.844720 4097770 ir_simplify.cc:467] Begin Simplify 64
I1018 08:53:57.844681 4097769 tensor.cc:259] name:pad_temp_0, domain: []->{ pad_temp_0[i, j, k, a]: 0 <= i <= 0 and 0 <= j <= 63 and 0 <= k <= 113 and 0 <= a <= 113 }
I1018 08:53:57.844705 4097768 op_lowering_impl.cc:420] Lower op: pow, get 1 LoweredFunc:
I1018 08:53:57.844735 4097770 ir_simplify.cc:467] Begin Simplify 1
I1018 08:53:57.844789 4097770 ir_simplify.cc:467] Begin Simplify 1
I1018 08:53:57.844769 4097768 op_lowering_impl.cc:423] function fn_pow_9 (_var_35, _var_102, _var_45)
{
  ScheduleBlock(root_2)
  {
    serial for (i, 0, 64)
    {
      ScheduleBlock(var_45)
      {
        i0_2 = axis.bind(i)
        var_45[i0_2] = pow(var_35[i0_2], var_102[i0_2])
      }
    }
  }
}
I1018 08:53:57.844753 4097769 domain.cc:65] isl::set []->{ pad_temp_0[i, j, k, a]: 0 <= i <= 0 and 0 <= j <= 63 and 0 <= k <= 113 and 0 <= a <= 113 }
I1018 08:53:57.844813 4097770 compute.cc:209] tensor var_23's domain is : 1, 64, 1, 1
I1018 08:53:57.844823 4097768 op_lowering_impl.cc:450] Do op schedule
I1018 08:53:57.844882 4097768 ir_schedule_pe.cc:116] Before IRInjectiveSchedule, new ir is : {
  ScheduleBlock(root_2)
  {
    serial for (i, 0, 64)
    {
      ScheduleBlock(var_45)
      {
        i0_2 = axis.bind(i)
        var_45[i0_2] = pow(var_35[i0_2], var_102[i0_2])
      }
    }
  }
}
I1018 08:53:57.844908 4097768 ir_analyzer.cc:101] it_expr is : {
  ScheduleBlock(root_2)
  {
    serial for (i, 0, 64)
    {
      ScheduleBlock(var_45)
      {
        i0_2 = axis.bind(i)
        var_45[i0_2] = pow(var_35[i0_2], var_102[i0_2])
      }
    }
  }
}
I1018 08:53:57.844938 4097768 loop_transformation.cc:688] Before FlattenLoops, ir is:
serial for (i, 0, 64)
{
  ScheduleBlock(var_45)
  {
    i0_2 = axis.bind(i)
    var_45[i0_2] = pow(var_35[i0_2], var_102[i0_2])
  }
}
I1018 08:53:57.844928 4097770 tensor.cc:259] name:var_23, domain: []->{ var_23[i, j, k, a]: 0 <= i <= 0 and 0 <= j <= 63 and 0 <= k <= 0 and 0 <= a <= 0 }
I1018 08:53:57.845002 4097768 loop_transformation.cc:864] After FlattenLoops, ir is:
serial for (flat_i, 0, 64)
{
  ScheduleBlock(var_45)
  {
    _flat_i = axis.bind(flat_i)
    var_45[_flat_i] = pow(var_35[_flat_i], var_102[_flat_i])
  }
}
I1018 08:53:57.844993 4097770 domain.cc:65] isl::set []->{ var_23[i, j, k, a]: 0 <= i <= 0 and 0 <= j <= 63 and 0 <= k <= 0 and 0 <= a <= 0 }
I1018 08:53:57.845029 4097768 ir_schedule_pe.cc:155] After IRInjectiveSchedule, new ir is : {
  ScheduleBlock(root_2)
  {
    serial for (flat_i, 0, 64)
    {
      ScheduleBlock(var_45)
      {
        _flat_i = axis.bind(flat_i)
        var_45[_flat_i] = pow(var_35[_flat_i], var_102[_flat_i])
      }
    }
  }
}
I1018 08:53:57.845072 4097768 op_lowering_impl.cc:463] After op schedule: {
  ScheduleBlock(root_2)
  {
    serial for (flat_i, 0, 64)
    {
      ScheduleBlock(var_45)
      {
        _flat_i = axis.bind(flat_i)
        var_45[_flat_i] = pow(var_35[_flat_i], var_102[_flat_i])
      }
    }
  }
}
I1018 08:53:57.845114 4097768 base.cc:528] Before merging, exprs[0] is : {
  ScheduleBlock(root)
  {
    serial for (flat_i, 0, 64)
    {
      ScheduleBlock(var_35)
      {
        _flat_i = axis.bind(flat_i)
        var_35[_flat_i] = float32(9.9999997473787516e-06)
      }
    }
  }
}
I1018 08:53:57.845155 4097768 base.cc:546] in merged_block, it has serial for (flat_i, 0, 64)
{
  ScheduleBlock(var_35)
  {
    _flat_i = axis.bind(flat_i)
    var_35[_flat_i] = float32(9.9999997473787516e-06)
  }
}
I1018 08:53:57.845181 4097768 base.cc:546] in merged_block, it has serial for (flat_i, 0, 64)
{
  ScheduleBlock(var_102)
  {
    _flat_i = axis.bind(flat_i)
    var_102[_flat_i] = float32(-0.50000000000000000)
  }
}
I1018 08:53:57.845218 4097768 base.cc:546] in merged_block, it has serial for (flat_i, 0, 64)
{
  ScheduleBlock(var_45)
  {
    _flat_i = axis.bind(flat_i)
    var_45[_flat_i] = pow(var_35[_flat_i], var_102[_flat_i])
  }
}
I1018 08:53:57.845242 4097768 base.cc:555] After merging, exprs[0] is : {
  ScheduleBlock(root)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_35)
        {
          _flat_i = axis.bind(flat_i)
          var_35[_flat_i] = float32(9.9999997473787516e-06)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_102)
        {
          _flat_i = axis.bind(flat_i)
          var_102[_flat_i] = float32(-0.50000000000000000)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_45)
        {
          _flat_i = axis.bind(flat_i)
          var_45[_flat_i] = pow(var_35[_flat_i], var_102[_flat_i])
        }
      }
    }
  }
}
I1018 08:53:57.845306 4097768 op_lowering_impl.cc:127] After lower, ir is: 
{
  ScheduleBlock(root)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_35)
        {
          _flat_i = axis.bind(flat_i)
          var_35[_flat_i] = float32(9.9999997473787516e-06)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_102)
        {
          _flat_i = axis.bind(flat_i)
          var_102[_flat_i] = float32(-0.50000000000000000)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_45)
        {
          _flat_i = axis.bind(flat_i)
          var_45[_flat_i] = pow(var_35[_flat_i], var_102[_flat_i])
        }
      }
    }
  }
}
I1018 08:53:57.845399 4097768 op_lowering_impl.cc:503] Try FUSION pow
I1018 08:53:57.845413 4097768 op_lowering_impl.cc:609] Before loop fusion, ir is:
{
  ScheduleBlock(root)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_35)
        {
          _flat_i = axis.bind(flat_i)
          var_35[_flat_i] = float32(9.9999997473787516e-06)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_102)
        {
          _flat_i = axis.bind(flat_i)
          var_102[_flat_i] = float32(-0.50000000000000000)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_45)
        {
          _flat_i = axis.bind(flat_i)
          var_45[_flat_i] = pow(var_35[_flat_i], var_102[_flat_i])
        }
      }
    }
  }
}
I1018 08:53:57.845461 4097768 op_lowering_impl.cc:618] After loop fusion, ir is:
{
  ScheduleBlock(root)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_35)
        {
          _flat_i = axis.bind(flat_i)
          var_35[_flat_i] = float32(9.9999997473787516e-06)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_102)
        {
          _flat_i = axis.bind(flat_i)
          var_102[_flat_i] = float32(-0.50000000000000000)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_45)
        {
          _flat_i = axis.bind(flat_i)
          var_45[_flat_i] = pow(var_35[_flat_i], var_102[_flat_i])
        }
      }
    }
  }
}
I1018 08:53:57.845504 4097768 op_lowering_impl.cc:503] Try FUSION fill_constant
I1018 08:53:57.845521 4097768 op_lowering_impl.cc:525] Before compute inline, ir is:
{
  ScheduleBlock(root)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_35)
        {
          _flat_i = axis.bind(flat_i)
          var_35[_flat_i] = float32(9.9999997473787516e-06)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_102)
        {
          _flat_i = axis.bind(flat_i)
          var_102[_flat_i] = float32(-0.50000000000000000)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_45)
        {
          _flat_i = axis.bind(flat_i)
          var_45[_flat_i] = pow(var_35[_flat_i], var_102[_flat_i])
        }
      }
    }
  }
}
I1018 08:53:57.845717 4097768 op_lowering_impl.cc:546] After compute inline, ir is:
{
  ScheduleBlock(root)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_35)
        {
          _flat_i = axis.bind(flat_i)
          var_35[_flat_i] = float32(9.9999997473787516e-06)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_45)
        {
          _flat_i = axis.bind(flat_i)
          var_45[_flat_i] = pow(var_35[_flat_i], float32(-0.50000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.845763 4097768 op_lowering_impl.cc:503] Try FUSION fill_constant
I1018 08:53:57.845775 4097768 op_lowering_impl.cc:525] Before compute inline, ir is:
{
  ScheduleBlock(root)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_35)
        {
          _flat_i = axis.bind(flat_i)
          var_35[_flat_i] = float32(9.9999997473787516e-06)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_45)
        {
          _flat_i = axis.bind(flat_i)
          var_45[_flat_i] = pow(var_35[_flat_i], float32(-0.50000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.845878 4097768 op_lowering_impl.cc:546] After compute inline, ir is:
{
  ScheduleBlock(root)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_45)
        {
          _flat_i = axis.bind(flat_i)
          var_45[_flat_i] = pow(float32(9.9999997473787516e-06), float32(-0.50000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.845929 4097768 ir_analyzer.cc:101] it_expr is : {
  ScheduleBlock(root)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_45)
        {
          _flat_i = axis.bind(flat_i)
          var_45[_flat_i] = pow(float32(9.9999997473787516e-06), float32(-0.50000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.845965 4097768 op_lowering_impl.cc:624] Size of blocks: 1
I1018 08:53:57.845973 4097768 op_lowering_impl.cc:625] Op Pattern : 0
I1018 08:53:57.846000 4097768 op_lowering_impl.cc:690] Op Pattern : 1
I1018 08:53:57.846009 4097768 op_lowering_impl.cc:692] Before vectorize, ir is: 
{
  ScheduleBlock(root)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_45)
        {
          _flat_i = axis.bind(flat_i)
          var_45[_flat_i] = pow(float32(9.9999997473787516e-06), float32(-0.50000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.846055 4097768 op_lowering_impl.cc:706] var_45 dtype float32
I1018 08:53:57.846134 4097768 op_lowering_impl.cc:708] After vectorize, ir is: 
{
  ScheduleBlock(root)
  {
    {
      vectorize[64] for (flat_i, 0, 64)
      {
        ScheduleBlock(var_45)
        {
          _flat_i = axis.bind(flat_i)
          var_45[_flat_i] = pow(float32(9.9999997473787516e-06), float32(-0.50000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.846176 4097768 op_lowering_impl.cc:713] Before Sync IRLowerOp schedule, ir is: 
{
  ScheduleBlock(root)
  {
    {
      vectorize[64] for (flat_i, 0, 64)
      {
        ScheduleBlock(var_45)
        {
          _flat_i = axis.bind(flat_i)
          var_45[_flat_i] = pow(float32(9.9999997473787516e-06), float32(-0.50000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.846213 4097768 ir_analyzer.cc:101] it_expr is : {
  ScheduleBlock(root)
  {
    {
      vectorize[64] for (flat_i, 0, 64)
      {
        ScheduleBlock(var_45)
        {
          _flat_i = axis.bind(flat_i)
          var_45[_flat_i] = pow(float32(9.9999997473787516e-06), float32(-0.50000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.846252 4097768 op_lowering_impl.cc:717] After IRSchedule,  ir is: 
{
  ScheduleBlock(root)
  {
    {
      vectorize[64] for (flat_i, 0, 64)
      {
        ScheduleBlock(var_45)
        {
          _flat_i = axis.bind(flat_i)
          var_45[_flat_i] = pow(float32(9.9999997473787516e-06), float32(-0.50000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.846287 4097768 op_lowering_impl.cc:133] After group schedule, ir is: 
{
  ScheduleBlock(root)
  {
    {
      vectorize[64] for (flat_i, 0, 64)
      {
        ScheduleBlock(var_45)
        {
          _flat_i = axis.bind(flat_i)
          var_45[_flat_i] = pow(float32(9.9999997473787516e-06), float32(-0.50000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.846330 4097768 transform_gpu_forloop.cc:429] Before Optimize Expr:
{
  ScheduleBlock(root)
  {
    {
      vectorize[64] for (flat_i, 0, 64)
      {
        ScheduleBlock(var_45)
        {
          _flat_i = axis.bind(flat_i)
          var_45[_flat_i] = pow(float32(9.9999997473787516e-06), float32(-0.50000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.846398 4097768 eliminate_common_factor_of_local_index.cc:289] Before EliminateCommonFactorOfLocalIndex, Expr = 
{
  ScheduleBlock(root)
  {
    {
      vectorize[64] for (flat_i, 0, 64)
      {
        ScheduleBlock(var_45)
        {
          _flat_i = axis.bind(flat_i)
          var_45[flat_i] = pow(float32(9.9999997473787516e-06), float32(-0.50000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.846441 4097768 eliminate_common_factor_of_local_index.cc:301] After EliminateCommonFactorOfLocalIndex, Expr = 
{
  ScheduleBlock(root)
  {
    {
      vectorize[64] for (flat_i, 0, 64)
      {
        ScheduleBlock(var_45)
        {
          _flat_i = axis.bind(flat_i)
          var_45[flat_i] = pow(float32(9.9999997473787516e-06), float32(-0.50000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.846590 4097768 transform_gpu_forloop.cc:461] After Optimize Expr: 
{
  ScheduleBlock(root)
  {
    {
      vectorize[64] for (flat_i, 0, 64)
      {
        ScheduleBlock(var_45)
        {
          _flat_i = axis.bind(flat_i)
          var_45[flat_i] = pow(float32(9.9999997473787516e-06), float32(-0.50000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.846647 4097769 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.846665 4097769 ir_util.cc:142] shape is : 1,64,114,114
I1018 08:53:57.846688 4097769 ir_util.cc:143] indices is : i,j,k,a
I1018 08:53:57.846735 4097768 lowered_func.cc:230] Function used 1 buffers
I1018 08:53:57.846869 4097770 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.846889 4097770 ir_util.cc:142] shape is : 1,64,1,1
I1018 08:53:57.846916 4097770 ir_util.cc:143] indices is : i,j,k,a
I1018 08:53:57.846912 4097768 ir_simplify.cc:467] Begin Simplify function fn_fill_constant_4_fill_constant_8_pow_9_20 (_var_45)
{
  ScheduleBlock(root)
  {
    {
      vectorize[64] for (flat_i, 0, 64)
      {
        ScheduleBlock(var_45)
        {
          _flat_i = axis.bind(flat_i)
          var_45[flat_i] = pow(float32(9.9999997473787516e-06), float32(-0.50000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.847043 4097768 optimize.cc:57] After Optimize UnrollLoop:function fn_fill_constant_4_fill_constant_8_pow_9_20 (_var_45)
{
  ScheduleBlock(root)
  {
    {
      vectorize[64] for (flat_i, 0, 64)
      {
        ScheduleBlock(var_45)
        {
          _flat_i = axis.bind(flat_i)
          var_45[flat_i] = pow(9.99999975e-06f, -0.500000000f)
        }
      }
    }
  }
}
I1018 08:53:57.847105 4097768 ir_simplify.cc:467] Begin Simplify 64
I1018 08:53:57.847127 4097768 ir_simplify.cc:467] Begin Simplify flat_i
I1018 08:53:57.847157 4097769 ir_simplify.cc:467] Begin Simplify 1
I1018 08:53:57.847175 4097769 ir_simplify.cc:467] Begin Simplify 64
I1018 08:53:57.847186 4097769 ir_simplify.cc:467] Begin Simplify 56
I1018 08:53:57.847196 4097769 ir_simplify.cc:467] Begin Simplify 56
I1018 08:53:57.847208 4097769 compute.cc:209] tensor var_55__reduce_init's domain is : 1, 64, 56, 56
I1018 08:53:57.847224 4097768 vectorize_loops.cc:820] Vectorizing vi extent 64
I1018 08:53:57.847259 4097769 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.847237 4097768 vectorize_loops.cc:822] before vectorize body:
{
  vectorize[64] for (vi, 0, 64)
  {
    ScheduleBlock(var_45)
    {
      _flat_i = axis.bind(vi)
      var_45[vi] = pow(9.99999975e-06f, -0.500000000f)
    }
  }
}
I1018 08:53:57.847277 4097769 ir_util.cc:142] shape is : 1,64,56,56
I1018 08:53:57.847298 4097769 ir_util.cc:143] indices is : i,j,k,a
I1018 08:53:57.847309 4097768 vectorize_loops.cc:543] Vectorizer Call: pow is_changed: 0 lanes: 1
I1018 08:53:57.847318 4097770 lower_tensor_group.cc:103] In store_exprs, its name is : var_23
I1018 08:53:57.847332 4097768 vectorize_loops.cc:855] after vectorize body:
{
  vectorize[64] for (vi, 0, 64)
  {
    ScheduleBlock(var_45)
    {
      _flat_i = axis.bind(Ramp(0,1,64))
      var_45[Ramp(0,1,64)] = Broadcast(pow(9.99999975e-06f, -0.500000000f),64)
    }
  }
}
I1018 08:53:57.847375 4097770 lower_tensor_group.cc:134] func_args is : _batch_norm2d_0__w_0
I1018 08:53:57.847405 4097770 lower_tensor_group.cc:134] func_args is : _var_23
I1018 08:53:57.847432 4097768 optimize.cc:60] After Optimize VectorizeLoops:function fn_fill_constant_4_fill_constant_8_pow_9_20 (_var_45)
{
  ScheduleBlock(root)
  {
    {
      {
        ScheduleBlock(var_45)
        {
          _flat_i = axis.bind(Ramp(0,1,64))
          var_45[Ramp(0,1,64)] = Broadcast(pow(9.99999975e-06f, -0.500000000f),64)
        }
      }
    }
  }
}
I1018 08:53:57.847496 4097770 lowered_func.cc:230] Function used 2 buffers
I1018 08:53:57.847519 4097768 optimize.cc:74] After SimplifyBlocks:function fn_fill_constant_4_fill_constant_8_pow_9_20 (_var_45)
{
  ScheduleBlock(root)
  {
    {
      ScheduleBlock(var_45)
      {
        _flat_i = axis.bind(Ramp(0,1,64))
        var_45[Ramp(0,1,64)] = Broadcast(pow(9.99999975e-06f, -0.500000000f),64)
      }
    }
  }
}
I1018 08:53:57.847587 4097769 ast_gen.cc:100] FLAGS_group_schedule_tiling_first = 0
I1018 08:53:57.847604 4097770 op_lowering_impl.cc:420] Lower op: reshape, get 1 LoweredFunc:
I1018 08:53:57.847605 4097769 ast_gen.cc:104] ast gen: tensor init_body is var_55__reduce_init[i, j, k, a] = -3.40282347e+38f
I1018 08:53:57.847625 4097768 ir_simplify.cc:467] Begin Simplify function fn_fill_constant_4_fill_constant_8_pow_9_20 (_var_45)
{
  ScheduleBlock(root)
  {
    {
      ScheduleBlock(var_45)
      {
        _flat_i = axis.bind(Ramp(0,1,64))
        var_45[Ramp(0,1,64)] = Broadcast(cinn_sycl_rsqrt_fp32(9.99999975e-06f),64)
      }
    }
  }
}
I1018 08:53:57.847615 4097770 op_lowering_impl.cc:423] function fn_reshape_1 (_batch_norm2d_0__w_0, _var_23)
{
  ScheduleBlock(root_3)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 1)
        {
          serial for (a, 0, 1)
          {
            ScheduleBlock(var_23)
            {
              i0_4, i1_1, i2_1, i3_1 = axis.bind(0, j, 0, 0)
              var_23[i0_4, i1_1, i2_1, i3_1] = batch_norm2d_0__w_0[(((((i0_4 * 64) + (i1_1 * 1)) + (i2_1 * 1)) + (i3_1 * 1)) % 64)]
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.847684 4097769 ast_gen.cc:123] iter_value.size() and block_vars.size() is 4 4
I1018 08:53:57.847702 4097770 op_lowering_impl.cc:450] Do op schedule
I1018 08:53:57.847712 4097769 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.847720 4097769 ir_util.cc:142] shape is : 1,64,56,56
I1018 08:53:57.847740 4097769 ir_util.cc:143] indices is : i,j,k,a
I1018 08:53:57.847725 4097770 ir_schedule_pe.cc:70] Before IRElementwiseSchedule, new ir is : {
  ScheduleBlock(root_3)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 1)
        {
          serial for (a, 0, 1)
          {
            ScheduleBlock(var_23)
            {
              i0_4, i1_1, i2_1, i3_1 = axis.bind(0, j, 0, 0)
              var_23[i0_4, i1_1, i2_1, i3_1] = batch_norm2d_0__w_0[(((((i0_4 * 64) + (i1_1 * 1)) + (i2_1 * 1)) + (i3_1 * 1)) % 64)]
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.847767 4097768 parallel_compiler.cc:137] Start CodegenAndJit
I1018 08:53:57.847771 4097770 ir_analyzer.cc:101] it_expr is : {
  ScheduleBlock(root_3)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 1)
        {
          serial for (a, 0, 1)
          {
            ScheduleBlock(var_23)
            {
              i0_4, i1_1, i2_1, i3_1 = axis.bind(0, j, 0, 0)
              var_23[i0_4, i1_1, i2_1, i3_1] = batch_norm2d_0__w_0[(((((i0_4 * 64) + (i1_1 * 1)) + (i2_1 * 1)) + (i3_1 * 1)) % 64)]
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.847783 4097768 ir_simplify.cc:467] Begin Simplify {
  ScheduleBlock(root)
  {
    {
      ScheduleBlock(var_45)
      {
        _flat_i = axis.bind(Ramp(0,1,64))
        var_45[Ramp(0,1,64)] = Broadcast(cinn_sycl_rsqrt_fp32(9.99999975e-06f),64)
      }
    }
  }
}
I1018 08:53:57.847821 4097770 loop_transformation.cc:688] Before FlattenLoops, ir is:
serial for (i, 0, 1)
{
  serial for (j, 0, 64)
  {
    serial for (k, 0, 1)
    {
      serial for (a, 0, 1)
      {
        ScheduleBlock(var_23)
        {
          i0_4, i1_1, i2_1, i3_1 = axis.bind(0, j, 0, 0)
          var_23[i0_4, i1_1, i2_1, i3_1] = batch_norm2d_0__w_0[(((((i0_4 * 64) + (i1_1 * 1)) + (i2_1 * 1)) + (i3_1 * 1)) % 64)]
        }
      }
    }
  }
}
I1018 08:53:57.847924 4097768 ir_simplify.cc:467] Begin Simplify {
  ScheduleBlock(root)
  {
    ScheduleBlock(var_45)
    {
      _flat_i = axis.bind(Ramp(0,1,64))
      var_45[Ramp(0,1,64)] = Broadcast(cinn_sycl_rsqrt_fp32(9.99999975e-06f),64)
    }
  }
}
I1018 08:53:57.847949 4097770 loop_transformation.cc:864] After FlattenLoops, ir is:
serial for (flat_i, 0, 64)
{
  ScheduleBlock(var_23)
  {
    _flat_i = axis.bind(flat_i)
    var_23[_flat_i] = batch_norm2d_0__w_0[_flat_i]
  }
}
I1018 08:53:57.847993 4097770 ir_schedule_pe.cc:109] After IRElementwiseSchedule, new ir is : {
  ScheduleBlock(root_3)
  {
    serial for (flat_i, 0, 64)
    {
      ScheduleBlock(var_23)
      {
        _flat_i = axis.bind(flat_i)
        var_23[_flat_i] = batch_norm2d_0__w_0[_flat_i]
      }
    }
  }
}
I1018 08:53:57.848008 4097768 optimize.cc:57] After Optimize UnrollLoop:{
  ScheduleBlock(root)
  {
    ScheduleBlock(var_45)
    {
      _flat_i = axis.bind(Ramp(0,1,64))
      var_45[Ramp(0,1,64)] = Broadcast(cinn_sycl_rsqrt_fp32(9.99999975e-06f),64)
    }
  }
}
I1018 08:53:57.848026 4097770 op_lowering_impl.cc:463] After op schedule: {
  ScheduleBlock(root_3)
  {
    serial for (flat_i, 0, 64)
    {
      ScheduleBlock(var_23)
      {
        _flat_i = axis.bind(flat_i)
        var_23[_flat_i] = batch_norm2d_0__w_0[_flat_i]
      }
    }
  }
}
I1018 08:53:57.848032 4097769 ast_gen.cc:134] ast gen: reduce body is var_55[i, j, k, a] = cinn_max(var_55[i, j, k, a], pad_temp_0[i, j, ((k * 2) + kernel_idx), ((a * 2) + kernel_idx_0)])
I1018 08:53:57.848055 4097768 ir_simplify.cc:467] Begin Simplify Ramp(0,1,64)
I1018 08:53:57.848098 4097769 ast_gen.cc:159] ast gen: reduce body is after replace 0var_55[i, j, k, a] = cinn_max(var_55[i, j, k, a], pad_temp_0[i, j, ((k * 2) + kernel_idx), ((a * 2) + kernel_idx_0)])
I1018 08:53:57.848127 4097770 op_lowering_impl.cc:373] Do lower with Compute, op: reshape
I1018 08:53:57.848093 4097768 optimize.cc:60] After Optimize VectorizeLoops:{
  ScheduleBlock(root)
  {
    ScheduleBlock(var_45)
    {
      _flat_i = axis.bind(Ramp(0,1,64))
      var_45[Ramp(0,1,64)] = Broadcast(cinn_sycl_rsqrt_fp32(9.99999975e-06f),64)
    }
  }
}
I1018 08:53:57.848151 4097770 tensor.cc:259] name:batch_norm2d_0__w_1, domain: []->{ batch_norm2d_0__w_1[]:  }
I1018 08:53:57.848166 4097770 domain.cc:65] isl::set []->{ batch_norm2d_0__w_1[]:  }
I1018 08:53:57.848163 4097768 optimize.cc:74] After SimplifyBlocks:{
  ScheduleBlock(root)
  {
    ScheduleBlock(var_45)
    {
      _flat_i = axis.bind(Ramp(0,1,64))
      var_45[Ramp(0,1,64)] = Broadcast(cinn_sycl_rsqrt_fp32(9.99999975e-06f),64)
    }
  }
}
I1018 08:53:57.848191 4097769 ast_gen.cc:198] to replace : 4 6
I1018 08:53:57.848201 4097769 ast_gen.cc:201] reduce_block_vars[0] = i0_6
I1018 08:53:57.848210 4097769 ast_gen.cc:201] reduce_block_vars[1] = i1_3
I1018 08:53:57.848222 4097769 ast_gen.cc:201] reduce_block_vars[2] = i2_3
I1018 08:53:57.848196 4097768 ir_simplify.cc:467] Begin Simplify {
  ScheduleBlock(root)
  {
    ScheduleBlock(var_45)
    {
      _flat_i = axis.bind(Ramp(0,1,64))
      var_45[Ramp(0,1,64)] = Broadcast(cinn_sycl_rsqrt_fp32(9.99999975e-06f),64)
    }
  }
}
I1018 08:53:57.848232 4097769 ast_gen.cc:201] reduce_block_vars[3] = i3_3
I1018 08:53:57.848276 4097769 ast_gen.cc:201] reduce_block_vars[4] = i4
I1018 08:53:57.848285 4097769 ast_gen.cc:201] reduce_block_vars[5] = i5
I1018 08:53:57.848294 4097769 ast_gen.cc:204] reduce_axis[0] = kernel_idx
I1018 08:53:57.848286 4097770 elementwise.cc:992] A shape: 64, output_shapes: 1, 64, 1, 1
I1018 08:53:57.848304 4097769 ast_gen.cc:204] reduce_axis[1] = kernel_idx_0
I1018 08:53:57.848333 4097769 ast_gen.cc:206] before replace body: var_55[i0_6, i1_3, i2_3, i3_3] = cinn_max(var_55[i0_6, i1_3, i2_3, i3_3], pad_temp_0[i0_6, i1_3, ((i2_3 * 2) + kernel_idx), ((i3_3 * 2) + kernel_idx_0)])
I1018 08:53:57.848374 4097770 ir_simplify.cc:467] Begin Simplify 1
I1018 08:53:57.848389 4097770 ir_simplify.cc:467] Begin Simplify 64
I1018 08:53:57.848402 4097770 ir_simplify.cc:467] Begin Simplify 1
I1018 08:53:57.848412 4097770 ir_simplify.cc:467] Begin Simplify 1
I1018 08:53:57.848426 4097770 compute.cc:209] tensor var_27's domain is : 1, 64, 1, 1
I1018 08:53:57.848429 4097768 ir_simplify.cc:467] Begin Simplify Ramp(0,1,64)
I1018 08:53:57.848475 4097769 lower_tensor_group.cc:103] In store_exprs, its name is : pad_temp_0
I1018 08:53:57.848537 4097769 lower_tensor_group.cc:134] func_args is : _var_53
I1018 08:53:57.848546 4097769 lower_tensor_group.cc:134] func_args is : _var_55
I1018 08:53:57.848564 4097769 lower_tensor_group.cc:134] func_args is : _pad_temp_0
I1018 08:53:57.848518 4097770 tensor.cc:259] name:var_27, domain: []->{ var_27[i, j, k, a]: 0 <= i <= 0 and 0 <= j <= 63 and 0 <= k <= 0 and 0 <= a <= 0 }
I1018 08:53:57.848579 4097768 codegen_sycl_dev.cc:503] CodeGenSYCL visiting store op: var_45
I1018 08:53:57.848601 4097768 codegen_sycl_dev.cc:338] CodeGenSYCL visiting call op: cinn_sycl_rsqrt_fp32
I1018 08:53:57.848610 4097768 codegen_sycl_dev.cc:339] op->read_args.size(): 1
I1018 08:53:57.848618 4097768 codegen_sycl_dev.cc:340] op->write_args.size(): 0
I1018 08:53:57.848582 4097770 domain.cc:65] isl::set []->{ var_27[i, j, k, a]: 0 <= i <= 0 and 0 <= j <= 63 and 0 <= k <= 0 and 0 <= a <= 0 }
I1018 08:53:57.848678 4097769 lowered_func.cc:230] Function used 2 buffers
I1018 08:53:57.848681 4097768 lowered_func.cc:230] Function used 0 buffers
I1018 08:53:57.848793 4097769 lower_tensor_group.cc:103] In store_exprs, its name is : var_55__reduce_init
I1018 08:53:57.848798 4097769 lower_tensor_group.cc:103] In store_exprs, its name is : var_55
I1018 08:53:57.848816 4097768 ir_simplify.cc:467] Begin Simplify Ramp(0,1,64)
I1018 08:53:57.848837 4097769 lower_tensor_group.cc:131] Making func :fn_pool2d_20_1
I1018 08:53:57.848841 4097769 lower_tensor_group.cc:134] func_args is : _var_53
I1018 08:53:57.848845 4097769 lower_tensor_group.cc:134] func_args is : _var_55
I1018 08:53:57.848847 4097769 lower_tensor_group.cc:134] func_args is : _pad_temp_0
I1018 08:53:57.848903 4097769 lowered_func.cc:230] Function used 3 buffers
I1018 08:53:57.848958 4097769 op_lowering_impl.cc:420] Lower op: pool2d, get 2 LoweredFunc:
I1018 08:53:57.848965 4097768 codegen_sycl_dev.cc:503] CodeGenSYCL visiting store op: var_45
I1018 08:53:57.848974 4097768 codegen_sycl_dev.cc:338] CodeGenSYCL visiting call op: cinn_sycl_rsqrt_fp32
I1018 08:53:57.848977 4097768 codegen_sycl_dev.cc:339] op->read_args.size(): 1
I1018 08:53:57.848980 4097768 codegen_sycl_dev.cc:340] op->write_args.size(): 0
I1018 08:53:57.848991 4097768 parallel_compiler.cc:328] [SYCL]:
#include <sycl/sycl.hpp>
#include "cinn_sycl_runtime_source.h"
typedef sycl::half float16;
#ifdef __cplusplus
extern "C" {
#endif
// CodeGenSYCL: NOTE: Auto-generated packed function
void fn_fill_constant_4_fill_constant_8_pow_9_20_kernel(sycl::queue &Q, sycl::range<3> dimGrid, sycl::range<3> dimBlock, void** void_args) {
  float*  var_45 = (float* )(*(void **)(void_args[0]));
  Q.submit([&](sycl::handler &h) {
    h.parallel_for<class space0_fn_fill_constant_4_fill_constant_8_pow_9_20_kernel>(sycl::nd_range<3>(dimGrid * dimBlock, dimBlock), [=](sycl::nd_item<3> item) [[intel::kernel_args_restrict]][[intel::max_work_group_size(1, 1, 1)]]
    {
      cinn_sycl_store(var_45, IndexVec<64>::Ramp(0), cinn_sycl_rsqrt_fp32(9.99999975e-06f));
    });
  });
}

#ifdef __cplusplus
}
#endif
I1018 08:53:57.848963 4097769 op_lowering_impl.cc:423] function fn_pool2d_20 (_var_53, _var_55, _pad_temp_0)
{
  ScheduleBlock(root_4)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 114)
        {
          serial for (a, 0, 114)
          {
            ScheduleBlock(pad_temp_0)
            {
              i0_3, i1_0, i2_0, i3_0 = axis.bind(0, j, k, a)
              pad_temp_0[i0_3, i1_0, i2_0, i3_0] = select(((i3_0 < (1 + 112)) and ((i3_0 >= 1) and ((i2_0 < (1 + 112)) and (i2_0 >= 1)))), var_53[i0_3, i1_0, (i2_0 - 1), (i3_0 - 1)], -3.40282347e+38f)
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.849004 4097769 op_lowering_impl.cc:423] function fn_pool2d_20_1 (_var_53, _var_55, _pad_temp_0)
{
  ScheduleBlock(root_5)
  {
    serial for (j, 0, 64)
    {
      serial for (k, 0, 56)
      {
        serial for (a, 0, 56)
        {
          ScheduleBlock(var_55__reduce_init)
          {
            i0_5, i1_2, i2_2, i3_2 = axis.bind(0, j, k, a)
            var_55__reduce_init[i0_5, i1_2, i2_2, i3_2] = -3.40282347e+38f
          }
          serial for (kernel_idx, 0, 3)
          {
            serial for (kernel_idx_0, 0, 3)
            {
              ScheduleBlock(var_55)
              {
                i0_6, i1_3, i2_3, i3_3, i4, i5 = axis.bind(0, j, k, a, kernel_idx, kernel_idx_0)
                var_55[i0_6, i1_3, i2_3, i3_3] = cinn_max(var_55[i0_6, i1_3, i2_3, i3_3], pad_temp_0[i0_6, i1_3, ((i2_3 * 2) + i4), ((i3_3 * 2) + i5)])
              }
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.849041 4097769 op_lowering_impl.cc:450] Do op schedule
I1018 08:53:57.849054 4097769 base.cc:528] Before merging, exprs[0] is : {
  ScheduleBlock(root_4)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 114)
        {
          serial for (a, 0, 114)
          {
            ScheduleBlock(pad_temp_0)
            {
              i0_3, i1_0, i2_0, i3_0 = axis.bind(0, j, k, a)
              pad_temp_0[i0_3, i1_0, i2_0, i3_0] = select(((i3_0 < (1 + 112)) and ((i3_0 >= 1) and ((i2_0 < (1 + 112)) and (i2_0 >= 1)))), var_53[i0_3, i1_0, (i2_0 - 1), (i3_0 - 1)], -3.40282347e+38f)
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.849078 4097769 base.cc:546] in merged_block, it has serial for (i, 0, 1)
{
  serial for (j, 0, 64)
  {
    serial for (k, 0, 114)
    {
      serial for (a, 0, 114)
      {
        ScheduleBlock(pad_temp_0)
        {
          i0_3, i1_0, i2_0, i3_0 = axis.bind(0, j, k, a)
          pad_temp_0[i0_3, i1_0, i2_0, i3_0] = select(((i3_0 < (1 + 112)) and ((i3_0 >= 1) and ((i2_0 < (1 + 112)) and (i2_0 >= 1)))), var_53[i0_3, i1_0, (i2_0 - 1), (i3_0 - 1)], -3.40282347e+38f)
        }
      }
    }
  }
}
I1018 08:53:57.849097 4097769 base.cc:546] in merged_block, it has serial for (j, 0, 64)
{
  serial for (k, 0, 56)
  {
    serial for (a, 0, 56)
    {
      ScheduleBlock(var_55__reduce_init)
      {
        i0_5, i1_2, i2_2, i3_2 = axis.bind(0, j, k, a)
        var_55__reduce_init[i0_5, i1_2, i2_2, i3_2] = -3.40282347e+38f
      }
      serial for (kernel_idx, 0, 3)
      {
        serial for (kernel_idx_0, 0, 3)
        {
          ScheduleBlock(var_55)
          {
            i0_6, i1_3, i2_3, i3_3, i4, i5 = axis.bind(0, j, k, a, kernel_idx, kernel_idx_0)
            var_55[i0_6, i1_3, i2_3, i3_3] = cinn_max(var_55[i0_6, i1_3, i2_3, i3_3], pad_temp_0[i0_6, i1_3, ((i2_3 * 2) + i4), ((i3_3 * 2) + i5)])
          }
        }
      }
    }
  }
}
I1018 08:53:57.849117 4097768 context.cc:50] get runtime_include_dir from env: /home/wzy/.local/lib/python3.9/site-packages/paddle/libs
I1018 08:53:57.849118 4097769 base.cc:555] After merging, exprs[0] is : {
  ScheduleBlock(root_4)
  {
    {
      serial for (i, 0, 1)
      {
        serial for (j, 0, 64)
        {
          serial for (k, 0, 114)
          {
            serial for (a, 0, 114)
            {
              ScheduleBlock(pad_temp_0)
              {
                i0_3, i1_0, i2_0, i3_0 = axis.bind(0, j, k, a)
                pad_temp_0[i0_3, i1_0, i2_0, i3_0] = select(((i3_0 < (1 + 112)) and ((i3_0 >= 1) and ((i2_0 < (1 + 112)) and (i2_0 >= 1)))), var_53[i0_3, i1_0, (i2_0 - 1), (i3_0 - 1)], -3.40282347e+38f)
              }
            }
          }
        }
      }
      serial for (j, 0, 64)
      {
        serial for (k, 0, 56)
        {
          serial for (a, 0, 56)
          {
            ScheduleBlock(var_55__reduce_init)
            {
              i0_5, i1_2, i2_2, i3_2 = axis.bind(0, j, k, a)
              var_55__reduce_init[i0_5, i1_2, i2_2, i3_2] = -3.40282347e+38f
            }
            serial for (kernel_idx, 0, 3)
            {
              serial for (kernel_idx_0, 0, 3)
              {
                ScheduleBlock(var_55)
                {
                  i0_6, i1_3, i2_3, i3_3, i4, i5 = axis.bind(0, j, k, a, kernel_idx, kernel_idx_0)
                  var_55[i0_6, i1_3, i2_3, i3_3] = cinn_max(var_55[i0_6, i1_3, i2_3, i3_3], pad_temp_0[i0_6, i1_3, ((i2_3 * 2) + i4), ((i3_3 * 2) + i5)])
                }
              }
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.849248 4097768 compiler_sycl.cc:77] compile command: /home/wzy/sycl_workspace/llvm-mlu/build/bin/clang++ -I /home/wzy/.local/lib/python3.9/site-packages/paddle/libs -fsycl -fsycl-targets=mlisa-cambricon-bang -std=c++17 -fPIC -O1 -shared -ldl -fbracket-depth=1030 ./source/sycl_0.cc -o ./source/sycl_0.so
I1018 08:53:57.849313 4097769 op_lowering_impl.cc:463] After op schedule: {
  ScheduleBlock(root_4)
  {
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 56)
        {
          serial for (a, 0, 56)
          {
            ScheduleBlock(var_55__reduce_init)
            {
              i0_5, i1_2, i2_2, i3_2 = axis.bind(0, j, k, a)
              var_55__reduce_init[i0_5, i1_2, i2_2, i3_2] = -3.40282347e+38f
            }
            serial for (kernel_idx, 0, 3)
            {
              serial for (kernel_idx_0, 0, 3)
              {
                ScheduleBlock(var_55)
                {
                  i0_6, i1_3, i2_3, i3_3, i4, i5 = axis.bind(0, j, k, a, kernel_idx, kernel_idx_0)
                  var_55[i0_6, i1_3, i2_3, i3_3] = cinn_max(var_55[i0_6, i1_3, i2_3, i3_3], select(((((i3_3 * 2) + i5) < (1 + 112)) and ((((i3_3 * 2) + i5) >= 1) and ((((i2_3 * 2) + i4) < (1 + 112)) and (((i2_3 * 2) + i4) >= 1)))), var_53[i0_6, i1_3, (((i2_3 * 2) + i4) - 1), (((i3_3 * 2) + i5) - 1)], -3.40282347e+38f))
                }
              }
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.849382 4097769 op_lowering_impl.cc:127] After lower, ir is: 
{
  ScheduleBlock(root_4)
  {
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 56)
        {
          serial for (a, 0, 56)
          {
            ScheduleBlock(var_55__reduce_init)
            {
              i0_5, i1_2, i2_2, i3_2 = axis.bind(0, j, k, a)
              var_55__reduce_init[i0_5, i1_2, i2_2, i3_2] = -3.40282347e+38f
            }
            serial for (kernel_idx, 0, 3)
            {
              serial for (kernel_idx_0, 0, 3)
              {
                ScheduleBlock(var_55)
                {
                  i0_6, i1_3, i2_3, i3_3, i4, i5 = axis.bind(0, j, k, a, kernel_idx, kernel_idx_0)
                  var_55[i0_6, i1_3, i2_3, i3_3] = cinn_max(var_55[i0_6, i1_3, i2_3, i3_3], select(((((i3_3 * 2) + i5) < (1 + 112)) and ((((i3_3 * 2) + i5) >= 1) and ((((i2_3 * 2) + i4) < (1 + 112)) and (((i2_3 * 2) + i4) >= 1)))), var_53[i0_6, i1_3, (((i2_3 * 2) + i4) - 1), (((i3_3 * 2) + i5) - 1)], -3.40282347e+38f))
                }
              }
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.849428 4097769 transform_gpu_forloop.cc:429] Before Optimize Expr:
{
  ScheduleBlock(root_4)
  {
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 56)
        {
          serial for (a, 0, 56)
          {
            ScheduleBlock(var_55__reduce_init)
            {
              i0_5, i1_2, i2_2, i3_2 = axis.bind(0, j, k, a)
              var_55__reduce_init[i0_5, i1_2, i2_2, i3_2] = -3.40282347e+38f
            }
            serial for (kernel_idx, 0, 3)
            {
              serial for (kernel_idx_0, 0, 3)
              {
                ScheduleBlock(var_55)
                {
                  i0_6, i1_3, i2_3, i3_3, i4, i5 = axis.bind(0, j, k, a, kernel_idx, kernel_idx_0)
                  var_55[i0_6, i1_3, i2_3, i3_3] = cinn_max(var_55[i0_6, i1_3, i2_3, i3_3], select(((((i3_3 * 2) + i5) < (1 + 112)) and ((((i3_3 * 2) + i5) >= 1) and ((((i2_3 * 2) + i4) < (1 + 112)) and (((i2_3 * 2) + i4) >= 1)))), var_53[i0_6, i1_3, (((i2_3 * 2) + i4) - 1), (((i3_3 * 2) + i5) - 1)], -3.40282347e+38f))
                }
              }
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.849567 4097769 eliminate_common_factor_of_local_index.cc:289] Before EliminateCommonFactorOfLocalIndex, Expr = 
{
  ScheduleBlock(root_4)
  {
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 56)
        {
          serial for (a, 0, 56)
          {
            ScheduleBlock(var_55__reduce_init)
            {
              i0_5, i1_2, i2_2, i3_2 = axis.bind(0, j, k, a)
              var_55__reduce_init[0, j, k, a] = -3.40282347e+38f
            }
            serial for (kernel_idx, 0, 3)
            {
              serial for (kernel_idx_0, 0, 3)
              {
                ScheduleBlock(var_55)
                {
                  i0_6, i1_3, i2_3, i3_3, i4, i5 = axis.bind(0, j, k, a, kernel_idx, kernel_idx_0)
                  var_55[0, j, k, a] = cinn_max(var_55[0, j, k, a], select(((((a * 2) + kernel_idx_0) < (1 + 112)) and ((((a * 2) + kernel_idx_0) >= 1) and ((((k * 2) + kernel_idx) < (1 + 112)) and (((k * 2) + kernel_idx) >= 1)))), var_53[0, j, (((k * 2) + kernel_idx) - 1), (((a * 2) + kernel_idx_0) - 1)], -3.40282347e+38f))
                }
              }
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.849624 4097769 eliminate_common_factor_of_local_index.cc:301] After EliminateCommonFactorOfLocalIndex, Expr = 
{
  ScheduleBlock(root_4)
  {
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 56)
        {
          serial for (a, 0, 56)
          {
            ScheduleBlock(var_55__reduce_init)
            {
              i0_5, i1_2, i2_2, i3_2 = axis.bind(0, j, k, a)
              var_55__reduce_init[0, j, k, a] = -3.40282347e+38f
            }
            serial for (kernel_idx, 0, 3)
            {
              serial for (kernel_idx_0, 0, 3)
              {
                ScheduleBlock(var_55)
                {
                  i0_6, i1_3, i2_3, i3_3, i4, i5 = axis.bind(0, j, k, a, kernel_idx, kernel_idx_0)
                  var_55[0, j, k, a] = cinn_max(var_55[0, j, k, a], select(((((a * 2) + kernel_idx_0) < (1 + 112)) and ((((a * 2) + kernel_idx_0) >= 1) and ((((k * 2) + kernel_idx) < (1 + 112)) and (((k * 2) + kernel_idx) >= 1)))), var_53[0, j, (((k * 2) + kernel_idx) - 1), (((a * 2) + kernel_idx_0) - 1)], -3.40282347e+38f))
                }
              }
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.849977 4097770 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.849985 4097770 ir_util.cc:142] shape is : 1,64,1,1
I1018 08:53:57.849996 4097770 ir_util.cc:143] indices is : i,j,k,a
I1018 08:53:57.850071 4097769 transform_gpu_forloop.cc:461] After Optimize Expr: 
{
  ScheduleBlock(root_4)
  {
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 56)
        {
          serial for (a, 0, 56)
          {
            ScheduleBlock(var_55__reduce_init)
            {
              i0_5, i1_2, i2_2, i3_2 = axis.bind(0, j, k, a)
              var_55__reduce_init[0, j, k, a] = -3.40282347e+38f
            }
            serial for (kernel_idx, 0, 3)
            {
              serial for (kernel_idx_0, 0, 3)
              {
                ScheduleBlock(var_55)
                {
                  i0_6, i1_3, i2_3, i3_3, i4, i5 = axis.bind(0, j, k, a, kernel_idx, kernel_idx_0)
                  var_55[0, j, k, a] = cinn_max(var_55[0, j, k, a], select(((((a * 2) + kernel_idx_0) < (1 + 112)) and ((((a * 2) + kernel_idx_0) >= 1) and ((((k * 2) + kernel_idx) < (1 + 112)) and (((k * 2) + kernel_idx) >= 1)))), var_53[0, j, (-1 + ((2 * k) + kernel_idx)), (-1 + ((2 * a) + kernel_idx_0))], -3.40282347e+38f))
                }
              }
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.850162 4097770 lower_tensor_group.cc:103] In store_exprs, its name is : var_27
I1018 08:53:57.850185 4097770 lower_tensor_group.cc:134] func_args is : _batch_norm2d_0__w_1
I1018 08:53:57.850189 4097770 lower_tensor_group.cc:134] func_args is : _var_27
I1018 08:53:57.850226 4097770 lowered_func.cc:230] Function used 2 buffers
I1018 08:53:57.850277 4097770 op_lowering_impl.cc:420] Lower op: reshape, get 1 LoweredFunc:
I1018 08:53:57.850281 4097769 lowered_func.cc:230] Function used 3 buffers
I1018 08:53:57.850282 4097770 op_lowering_impl.cc:423] function fn_reshape_2 (_batch_norm2d_0__w_1, _var_27)
{
  ScheduleBlock(root_6)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 1)
        {
          serial for (a, 0, 1)
          {
            ScheduleBlock(var_27)
            {
              i0_7, i1_4, i2_4, i3_4 = axis.bind(0, j, 0, 0)
              var_27[i0_7, i1_4, i2_4, i3_4] = batch_norm2d_0__w_1[(((((i0_7 * 64) + (i1_4 * 1)) + (i2_4 * 1)) + (i3_4 * 1)) % 64)]
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.850318 4097770 op_lowering_impl.cc:450] Do op schedule
I1018 08:53:57.850327 4097770 ir_schedule_pe.cc:70] Before IRElementwiseSchedule, new ir is : {
  ScheduleBlock(root_6)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 1)
        {
          serial for (a, 0, 1)
          {
            ScheduleBlock(var_27)
            {
              i0_7, i1_4, i2_4, i3_4 = axis.bind(0, j, 0, 0)
              var_27[i0_7, i1_4, i2_4, i3_4] = batch_norm2d_0__w_1[(((((i0_7 * 64) + (i1_4 * 1)) + (i2_4 * 1)) + (i3_4 * 1)) % 64)]
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.850346 4097770 ir_analyzer.cc:101] it_expr is : {
  ScheduleBlock(root_6)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 1)
        {
          serial for (a, 0, 1)
          {
            ScheduleBlock(var_27)
            {
              i0_7, i1_4, i2_4, i3_4 = axis.bind(0, j, 0, 0)
              var_27[i0_7, i1_4, i2_4, i3_4] = batch_norm2d_0__w_1[(((((i0_7 * 64) + (i1_4 * 1)) + (i2_4 * 1)) + (i3_4 * 1)) % 64)]
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.850358 4097769 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.850368 4097770 loop_transformation.cc:688] Before FlattenLoops, ir is:
serial for (i, 0, 1)
{
  serial for (j, 0, 64)
  {
    serial for (k, 0, 1)
    {
      serial for (a, 0, 1)
      {
        ScheduleBlock(var_27)
        {
          i0_7, i1_4, i2_4, i3_4 = axis.bind(0, j, 0, 0)
          var_27[i0_7, i1_4, i2_4, i3_4] = batch_norm2d_0__w_1[(((((i0_7 * 64) + (i1_4 * 1)) + (i2_4 * 1)) + (i3_4 * 1)) % 64)]
        }
      }
    }
  }
}
I1018 08:53:57.850373 4097769 ir_util.cc:142] shape is : 1,64,56,56
I1018 08:53:57.850391 4097769 ir_util.cc:143] indices is : 0,j,k,a
I1018 08:53:57.850419 4097770 loop_transformation.cc:864] After FlattenLoops, ir is:
serial for (flat_i, 0, 64)
{
  ScheduleBlock(var_27)
  {
    _flat_i = axis.bind(flat_i)
    var_27[_flat_i] = batch_norm2d_0__w_1[_flat_i]
  }
}
I1018 08:53:57.850431 4097770 ir_schedule_pe.cc:109] After IRElementwiseSchedule, new ir is : {
  ScheduleBlock(root_6)
  {
    serial for (flat_i, 0, 64)
    {
      ScheduleBlock(var_27)
      {
        _flat_i = axis.bind(flat_i)
        var_27[_flat_i] = batch_norm2d_0__w_1[_flat_i]
      }
    }
  }
}
I1018 08:53:57.850445 4097770 op_lowering_impl.cc:463] After op schedule: {
  ScheduleBlock(root_6)
  {
    serial for (flat_i, 0, 64)
    {
      ScheduleBlock(var_27)
      {
        _flat_i = axis.bind(flat_i)
        var_27[_flat_i] = batch_norm2d_0__w_1[_flat_i]
      }
    }
  }
}
I1018 08:53:57.850484 4097770 op_lowering_impl.cc:373] Do lower with Compute, op: reshape
I1018 08:53:57.850494 4097770 tensor.cc:259] name:batch_norm2d_0__w_2, domain: []->{ batch_norm2d_0__w_2[]:  }
I1018 08:53:57.850500 4097770 domain.cc:65] isl::set []->{ batch_norm2d_0__w_2[]:  }
I1018 08:53:57.850551 4097770 elementwise.cc:992] A shape: 64, output_shapes: 1, 64, 1, 1
I1018 08:53:57.850586 4097770 ir_simplify.cc:467] Begin Simplify 1
I1018 08:53:57.850591 4097770 ir_simplify.cc:467] Begin Simplify 64
I1018 08:53:57.850596 4097770 ir_simplify.cc:467] Begin Simplify 1
I1018 08:53:57.850598 4097769 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.850601 4097770 ir_simplify.cc:467] Begin Simplify 1
I1018 08:53:57.850605 4097769 ir_util.cc:142] shape is : 1,64,56,56
I1018 08:53:57.850616 4097769 ir_util.cc:143] indices is : 0,j,k,a
I1018 08:53:57.850608 4097770 compute.cc:209] tensor var_31's domain is : 1, 64, 1, 1
I1018 08:53:57.850672 4097770 tensor.cc:259] name:var_31, domain: []->{ var_31[i, j, k, a]: 0 <= i <= 0 and 0 <= j <= 63 and 0 <= k <= 0 and 0 <= a <= 0 }
I1018 08:53:57.850700 4097770 domain.cc:65] isl::set []->{ var_31[i, j, k, a]: 0 <= i <= 0 and 0 <= j <= 63 and 0 <= k <= 0 and 0 <= a <= 0 }
I1018 08:53:57.850775 4097769 ir_simplify.cc:467] Begin Simplify function fn_pool2d_20_22 (_var_53, _var_55)
{
  ScheduleBlock(root_4)
  {
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 56)
        {
          serial for (a, 0, 56)
          {
            ScheduleBlock(var_55__reduce_init)
            {
              i0_5, i1_2, i2_2, i3_2 = axis.bind(0, j, k, a)
              var_55__reduce_init[0, j, k, a] = -3.40282347e+38f
            }
            serial for (kernel_idx, 0, 3)
            {
              serial for (kernel_idx_0, 0, 3)
              {
                ScheduleBlock(var_55)
                {
                  i0_6, i1_3, i2_3, i3_3, i4, i5 = axis.bind(0, j, k, a, kernel_idx, kernel_idx_0)
                  var_55[0, j, k, a] = cinn_max(var_55[0, j, k, a], select(((((a * 2) + kernel_idx_0) < (1 + 112)) and ((((a * 2) + kernel_idx_0) >= 1) and ((((k * 2) + kernel_idx) < (1 + 112)) and (((k * 2) + kernel_idx) >= 1)))), var_53[0, j, (-1 + ((2 * k) + kernel_idx)), (-1 + ((2 * a) + kernel_idx_0))], -3.40282347e+38f))
                }
              }
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.851302 4097769 optimize.cc:57] After Optimize UnrollLoop:function fn_pool2d_20_22 (_var_53, _var_55)
{
  ScheduleBlock(root_4)
  {
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 56)
        {
          serial for (a, 0, 56)
          {
            ScheduleBlock(var_55__reduce_init)
            {
              i0_5, i1_2, i2_2, i3_2 = axis.bind(0, j, k, a)
              var_55__reduce_init[0, j, k, a] = -3.40282347e+38f
            }
            serial for (kernel_idx, 0, 3)
            {
              serial for (kernel_idx_0, 0, 3)
              {
                ScheduleBlock(var_55)
                {
                  i0_6, i1_3, i2_3, i3_3, i4, i5 = axis.bind(0, j, k, a, kernel_idx, kernel_idx_0)
                  var_55[0, j, k, a] = cinn_max(var_55[0, j, k, a], select(((((a * 2) + kernel_idx_0) < (1 + 112)) and ((((a * 2) + kernel_idx_0) >= 1) and ((((k * 2) + kernel_idx) < (1 + 112)) and (((k * 2) + kernel_idx) >= 1)))), var_53[0, j, (-1 + ((2 * k) + kernel_idx)), (-1 + ((2 * a) + kernel_idx_0))], -3.40282347e+38f))
                }
              }
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.851361 4097769 ir_simplify.cc:467] Begin Simplify 0
I1018 08:53:57.851368 4097769 ir_simplify.cc:467] Begin Simplify j
I1018 08:53:57.851374 4097769 ir_simplify.cc:467] Begin Simplify k
I1018 08:53:57.851380 4097769 ir_simplify.cc:467] Begin Simplify a
I1018 08:53:57.851385 4097769 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.851389 4097769 ir_util.cc:142] shape is : 1,64,56,56
I1018 08:53:57.851398 4097769 ir_util.cc:143] indices is : 0,j,k,a
I1018 08:53:57.851460 4097770 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.851469 4097770 ir_util.cc:142] shape is : 1,64,1,1
I1018 08:53:57.851478 4097770 ir_util.cc:143] indices is : i,j,k,a
I1018 08:53:57.851500 4097769 ir_simplify.cc:467] Begin Simplify 0
I1018 08:53:57.851513 4097769 ir_simplify.cc:467] Begin Simplify j
I1018 08:53:57.851519 4097769 ir_simplify.cc:467] Begin Simplify k
I1018 08:53:57.851526 4097769 ir_simplify.cc:467] Begin Simplify a
I1018 08:53:57.851540 4097769 ir_simplify.cc:467] Begin Simplify 0
I1018 08:53:57.851547 4097769 ir_simplify.cc:467] Begin Simplify j
I1018 08:53:57.851595 4097769 ir_simplify.cc:467] Begin Simplify (-1 + ((2 * k) + kernel_idx))
I1018 08:53:57.851649 4097770 lower_tensor_group.cc:103] In store_exprs, its name is : var_31
I1018 08:53:57.851672 4097770 lower_tensor_group.cc:134] func_args is : _batch_norm2d_0__w_2
I1018 08:53:57.851675 4097770 lower_tensor_group.cc:134] func_args is : _var_31
I1018 08:53:57.851680 4097769 ir_simplify.cc:467] Begin Simplify (-1 + ((2 * a) + kernel_idx_0))
I1018 08:53:57.851713 4097770 lowered_func.cc:230] Function used 2 buffers
I1018 08:53:57.851733 4097769 ir_simplify.cc:467] Begin Simplify 0
I1018 08:53:57.851742 4097769 ir_simplify.cc:467] Begin Simplify j
I1018 08:53:57.851754 4097769 ir_simplify.cc:467] Begin Simplify k
I1018 08:53:57.851760 4097769 ir_simplify.cc:467] Begin Simplify a
I1018 08:53:57.851763 4097770 op_lowering_impl.cc:420] Lower op: reshape, get 1 LoweredFunc:
I1018 08:53:57.851766 4097769 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.851770 4097769 ir_util.cc:142] shape is : 1,64,56,56
I1018 08:53:57.851779 4097769 ir_util.cc:143] indices is : 0,j,k,a
I1018 08:53:57.851768 4097770 op_lowering_impl.cc:423] function fn_reshape_3 (_batch_norm2d_0__w_2, _var_31)
{
  ScheduleBlock(root_7)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 1)
        {
          serial for (a, 0, 1)
          {
            ScheduleBlock(var_31)
            {
              i0_8, i1_5, i2_5, i3_5 = axis.bind(0, j, 0, 0)
              var_31[i0_8, i1_5, i2_5, i3_5] = batch_norm2d_0__w_2[(((((i0_8 * 64) + (i1_5 * 1)) + (i2_5 * 1)) + (i3_5 * 1)) % 64)]
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.851797 4097770 op_lowering_impl.cc:450] Do op schedule
I1018 08:53:57.851807 4097770 ir_schedule_pe.cc:70] Before IRElementwiseSchedule, new ir is : {
  ScheduleBlock(root_7)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 1)
        {
          serial for (a, 0, 1)
          {
            ScheduleBlock(var_31)
            {
              i0_8, i1_5, i2_5, i3_5 = axis.bind(0, j, 0, 0)
              var_31[i0_8, i1_5, i2_5, i3_5] = batch_norm2d_0__w_2[(((((i0_8 * 64) + (i1_5 * 1)) + (i2_5 * 1)) + (i3_5 * 1)) % 64)]
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.851825 4097770 ir_analyzer.cc:101] it_expr is : {
  ScheduleBlock(root_7)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 1)
        {
          serial for (a, 0, 1)
          {
            ScheduleBlock(var_31)
            {
              i0_8, i1_5, i2_5, i3_5 = axis.bind(0, j, 0, 0)
              var_31[i0_8, i1_5, i2_5, i3_5] = batch_norm2d_0__w_2[(((((i0_8 * 64) + (i1_5 * 1)) + (i2_5 * 1)) + (i3_5 * 1)) % 64)]
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.851845 4097770 loop_transformation.cc:688] Before FlattenLoops, ir is:
serial for (i, 0, 1)
{
  serial for (j, 0, 64)
  {
    serial for (k, 0, 1)
    {
      serial for (a, 0, 1)
      {
        ScheduleBlock(var_31)
        {
          i0_8, i1_5, i2_5, i3_5 = axis.bind(0, j, 0, 0)
          var_31[i0_8, i1_5, i2_5, i3_5] = batch_norm2d_0__w_2[(((((i0_8 * 64) + (i1_5 * 1)) + (i2_5 * 1)) + (i3_5 * 1)) % 64)]
        }
      }
    }
  }
}
I1018 08:53:57.851895 4097770 loop_transformation.cc:864] After FlattenLoops, ir is:
serial for (flat_i, 0, 64)
{
  ScheduleBlock(var_31)
  {
    _flat_i = axis.bind(flat_i)
    var_31[_flat_i] = batch_norm2d_0__w_2[_flat_i]
  }
}
I1018 08:53:57.851872 4097769 optimize.cc:60] After Optimize VectorizeLoops:function fn_pool2d_20_22 (_var_53, _var_55)
{
  ScheduleBlock(root_4)
  {
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 56)
        {
          serial for (a, 0, 56)
          {
            ScheduleBlock(var_55__reduce_init)
            {
              i0_5, i1_2, i2_2, i3_2 = axis.bind(0, j, k, a)
              var_55__reduce_init[0, j, k, a] = -3.40282347e+38f
            }
            serial for (kernel_idx, 0, 3)
            {
              serial for (kernel_idx_0, 0, 3)
              {
                ScheduleBlock(var_55)
                {
                  i0_6, i1_3, i2_3, i3_3, i4, i5 = axis.bind(0, j, k, a, kernel_idx, kernel_idx_0)
                  var_55[0, j, k, a] = cinn_max(var_55[0, j, k, a], select(((((a * 2) + kernel_idx_0) < (1 + 112)) and ((((a * 2) + kernel_idx_0) >= 1) and ((((k * 2) + kernel_idx) < (1 + 112)) and (((k * 2) + kernel_idx) >= 1)))), var_53[0, j, (-1 + ((2 * k) + kernel_idx)), (-1 + ((2 * a) + kernel_idx_0))], -3.40282347e+38f))
                }
              }
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.851907 4097770 ir_schedule_pe.cc:109] After IRElementwiseSchedule, new ir is : {
  ScheduleBlock(root_7)
  {
    serial for (flat_i, 0, 64)
    {
      ScheduleBlock(var_31)
      {
        _flat_i = axis.bind(flat_i)
        var_31[_flat_i] = batch_norm2d_0__w_2[_flat_i]
      }
    }
  }
}
I1018 08:53:57.851935 4097770 op_lowering_impl.cc:463] After op schedule: {
  ScheduleBlock(root_7)
  {
    serial for (flat_i, 0, 64)
    {
      ScheduleBlock(var_31)
      {
        _flat_i = axis.bind(flat_i)
        var_31[_flat_i] = batch_norm2d_0__w_2[_flat_i]
      }
    }
  }
}
I1018 08:53:57.851977 4097770 broadcast.cc:267] broadcast out shape: 1, 64, 112, 112
I1018 08:53:57.851984 4097770 broadcast.cc:268] broadcast_axes shape: 0, 1, 2, 3
I1018 08:53:57.851992 4097770 op_lowering_impl.cc:373] Do lower with Compute, op: broadcast_to
I1018 08:53:57.852013 4097770 ir_simplify.cc:467] Begin Simplify 1
I1018 08:53:57.852020 4097770 ir_simplify.cc:467] Begin Simplify 64
I1018 08:53:57.852025 4097770 ir_simplify.cc:467] Begin Simplify 112
I1018 08:53:57.852030 4097770 ir_simplify.cc:467] Begin Simplify 112
I1018 08:53:57.852035 4097770 compute.cc:209] tensor var_101's domain is : 1, 64, 112, 112
I1018 08:53:57.852017 4097769 optimize.cc:74] After SimplifyBlocks:function fn_pool2d_20_22 (_var_53, _var_55)
{
  ScheduleBlock(root_4)
  {
    serial for (j, 0, 64)
    {
      serial for (k, 0, 56)
      {
        serial for (a, 0, 56)
        {
          ScheduleBlock(var_55__reduce_init)
          {
            i0_5, i1_2, i2_2, i3_2 = axis.bind(0, j, k, a)
            var_55__reduce_init[0, j, k, a] = -3.40282347e+38f
          }
          serial for (kernel_idx, 0, 3)
          {
            serial for (kernel_idx_0, 0, 3)
            {
              ScheduleBlock(var_55)
              {
                i0_6, i1_3, i2_3, i3_3, i4, i5 = axis.bind(0, j, k, a, kernel_idx, kernel_idx_0)
                var_55[0, j, k, a] = cinn_max(var_55[0, j, k, a], select(((((a * 2) + kernel_idx_0) < (1 + 112)) and ((((a * 2) + kernel_idx_0) >= 1) and ((((k * 2) + kernel_idx) < (1 + 112)) and (((k * 2) + kernel_idx) >= 1)))), var_53[0, j, (-1 + ((2 * k) + kernel_idx)), (-1 + ((2 * a) + kernel_idx_0))], -3.40282347e+38f))
              }
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.852066 4097769 ir_simplify.cc:467] Begin Simplify function fn_pool2d_20_22 (_var_53, _var_55)
{
  ScheduleBlock(root_4)
  {
    serial for (j, 0, 64)
    {
      serial for (k, 0, 56)
      {
        serial for (a, 0, 56)
        {
          ScheduleBlock(var_55__reduce_init)
          {
            i0_5, i1_2, i2_2, i3_2 = axis.bind(0, j, k, a)
            var_55__reduce_init[0, j, k, a] = -3.40282347e+38f
          }
          serial for (kernel_idx, 0, 3)
          {
            serial for (kernel_idx_0, 0, 3)
            {
              ScheduleBlock(var_55)
              {
                i0_6, i1_3, i2_3, i3_3, i4, i5 = axis.bind(0, j, k, a, kernel_idx, kernel_idx_0)
                var_55[0, j, k, a] = cinn_max(var_55[0, j, k, a], select(((((a * 2) + kernel_idx_0) < (1 + 112)) and ((((a * 2) + kernel_idx_0) >= 1) and ((((k * 2) + kernel_idx) < (1 + 112)) and (((k * 2) + kernel_idx) >= 1)))), var_53[0, j, (-1 + ((2 * k) + kernel_idx)), (-1 + ((2 * a) + kernel_idx_0))], -3.40282347e+38f))
              }
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.852073 4097770 tensor.cc:259] name:var_101, domain: []->{ var_101[i, j, k, a]: 0 <= i <= 0 and 0 <= j <= 63 and 0 <= k <= 111 and 0 <= a <= 111 }
I1018 08:53:57.852100 4097770 domain.cc:65] isl::set []->{ var_101[i, j, k, a]: 0 <= i <= 0 and 0 <= j <= 63 and 0 <= k <= 111 and 0 <= a <= 111 }
I1018 08:53:57.852566 4097769 parallel_compiler.cc:137] Start CodegenAndJit
I1018 08:53:57.852576 4097769 ir_simplify.cc:467] Begin Simplify {
  ScheduleBlock(root_4)
  {
    serial for (j, 0, 64)
    {
      serial for (k, 0, 56)
      {
        serial for (a, 0, 56)
        {
          ScheduleBlock(var_55__reduce_init)
          {
            i0_5, i1_2, i2_2, i3_2 = axis.bind(0, j, k, a)
            var_55__reduce_init[0, j, k, a] = -3.40282347e+38f
          }
          serial for (kernel_idx, 0, 3)
          {
            serial for (kernel_idx_0, 0, 3)
            {
              ScheduleBlock(var_55)
              {
                i0_6, i1_3, i2_3, i3_3, i4, i5 = axis.bind(0, j, k, a, kernel_idx, kernel_idx_0)
                var_55[0, j, k, a] = cinn_max(var_55[0, j, k, a], select(((((a * 2) + kernel_idx_0) < (1 + 112)) and ((((a * 2) + kernel_idx_0) >= 1) and ((((k * 2) + kernel_idx) < (1 + 112)) and (((k * 2) + kernel_idx) >= 1)))), var_53[0, j, (-1 + ((2 * k) + kernel_idx)), (-1 + ((2 * a) + kernel_idx_0))], -3.40282347e+38f))
              }
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.852860 4097770 tensor.cc:259] name:var_27, domain: []->{ var_27[]:  }
I1018 08:53:57.852869 4097770 domain.cc:65] isl::set []->{ var_27[]:  }
I1018 08:53:57.852936 4097770 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.852941 4097770 ir_util.cc:142] shape is : 1,64,112,112
I1018 08:53:57.852952 4097770 ir_util.cc:143] indices is : i,j,k,a
I1018 08:53:57.853109 4097769 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.853116 4097769 ir_util.cc:142] shape is : 1,64,56,56
I1018 08:53:57.853125 4097769 ir_util.cc:143] indices is : 0,j,k,a
I1018 08:53:57.853153 4097770 lower_tensor_group.cc:103] In store_exprs, its name is : var_101
I1018 08:53:57.853173 4097770 lower_tensor_group.cc:134] func_args is : _var_27
I1018 08:53:57.853178 4097770 lower_tensor_group.cc:134] func_args is : _var_101
I1018 08:53:57.853216 4097770 lowered_func.cc:230] Function used 2 buffers
I1018 08:53:57.853258 4097770 op_lowering_impl.cc:420] Lower op: broadcast_to, get 1 LoweredFunc:
I1018 08:53:57.853262 4097770 op_lowering_impl.cc:423] function fn_broadcast_to_5 (_var_27, _var_101)
{
  ScheduleBlock(root_8)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 112)
        {
          serial for (a, 0, 112)
          {
            ScheduleBlock(var_101)
            {
              i0_9, i1_6, i2_6, i3_6 = axis.bind(0, j, k, a)
              var_101[i0_9, i1_6, i2_6, i3_6] = var_27[0, i1_6, 0, 0]
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.853287 4097770 op_lowering_impl.cc:450] Do op schedule
I1018 08:53:57.853296 4097770 ir_schedule_pe.cc:116] Before IRInjectiveSchedule, new ir is : {
  ScheduleBlock(root_8)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 112)
        {
          serial for (a, 0, 112)
          {
            ScheduleBlock(var_101)
            {
              i0_9, i1_6, i2_6, i3_6 = axis.bind(0, j, k, a)
              var_101[i0_9, i1_6, i2_6, i3_6] = var_27[0, i1_6, 0, 0]
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.853325 4097769 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.853318 4097770 ir_analyzer.cc:101] it_expr is : {
  ScheduleBlock(root_8)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 112)
        {
          serial for (a, 0, 112)
          {
            ScheduleBlock(var_101)
            {
              i0_9, i1_6, i2_6, i3_6 = axis.bind(0, j, k, a)
              var_101[i0_9, i1_6, i2_6, i3_6] = var_27[0, i1_6, 0, 0]
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.853332 4097769 ir_util.cc:142] shape is : 1,64,56,56
I1018 08:53:57.853343 4097769 ir_util.cc:143] indices is : 0,j,k,a
I1018 08:53:57.853345 4097770 loop_transformation.cc:688] Before FlattenLoops, ir is:
serial for (i, 0, 1)
{
  serial for (j, 0, 64)
  {
    serial for (k, 0, 112)
    {
      serial for (a, 0, 112)
      {
        ScheduleBlock(var_101)
        {
          i0_9, i1_6, i2_6, i3_6 = axis.bind(0, j, k, a)
          var_101[i0_9, i1_6, i2_6, i3_6] = var_27[0, i1_6, 0, 0]
        }
      }
    }
  }
}
I1018 08:53:57.853386 4097770 ir.cc:629] Begin Load::index IndiceToAbsOffset of tensor: var_27
I1018 08:53:57.853390 4097770 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.853394 4097770 ir_util.cc:142] shape is : 1,64,1,1
I1018 08:53:57.853408 4097770 ir_util.cc:143] indices is : 0,((_flat_i % 802816) / 12544),0,0
I1018 08:53:57.853453 4097769 ir_simplify.cc:467] Begin Simplify {
  ScheduleBlock(root_4)
  {
    serial for (j, 0, 64)
    {
      serial for (k, 0, 56)
      {
        serial for (a, 0, 56)
        {
          ScheduleBlock(var_55__reduce_init)
          {
            i0_5, i1_2, i2_2, i3_2 = axis.bind(0, j, k, a)
            var_55__reduce_init[0, j, k, a] = -3.40282347e+38f
          }
          serial for (kernel_idx, 0, 3)
          {
            serial for (kernel_idx_0, 0, 3)
            {
              ScheduleBlock(var_55)
              {
                i0_6, i1_3, i2_3, i3_3, i4, i5 = axis.bind(0, j, k, a, kernel_idx, kernel_idx_0)
                var_55[0, j, k, a] = cinn_max(var_55[0, j, k, a], select(((((a * 2) + kernel_idx_0) < (1 + 112)) and ((((a * 2) + kernel_idx_0) >= 1) and ((((k * 2) + kernel_idx) < (1 + 112)) and (((k * 2) + kernel_idx) >= 1)))), var_53[0, j, (-1 + ((2 * k) + kernel_idx)), (-1 + ((2 * a) + kernel_idx_0))], -3.40282347e+38f))
              }
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.853498 4097770 loop_transformation.cc:864] After FlattenLoops, ir is:
serial for (flat_i, 0, 802816)
{
  ScheduleBlock(var_101)
  {
    _flat_i = axis.bind(flat_i)
    var_101[_flat_i] = var_27[((_flat_i % 802816) / 12544)]
  }
}
I1018 08:53:57.853514 4097770 ir_schedule_pe.cc:155] After IRInjectiveSchedule, new ir is : {
  ScheduleBlock(root_8)
  {
    serial for (flat_i, 0, 802816)
    {
      ScheduleBlock(var_101)
      {
        _flat_i = axis.bind(flat_i)
        var_101[_flat_i] = var_27[((_flat_i % 802816) / 12544)]
      }
    }
  }
}
I1018 08:53:57.853529 4097770 op_lowering_impl.cc:463] After op schedule: {
  ScheduleBlock(root_8)
  {
    serial for (flat_i, 0, 802816)
    {
      ScheduleBlock(var_101)
      {
        _flat_i = axis.bind(flat_i)
        var_101[_flat_i] = var_27[((_flat_i % 802816) / 12544)]
      }
    }
  }
}
I1018 08:53:57.853574 4097770 op_lowering_impl.cc:373] Do lower with Compute, op: scale
I1018 08:53:57.853595 4097770 ir_simplify.cc:467] Begin Simplify 1
I1018 08:53:57.853602 4097770 ir_simplify.cc:467] Begin Simplify 64
I1018 08:53:57.853606 4097770 ir_simplify.cc:467] Begin Simplify 1
I1018 08:53:57.853611 4097770 ir_simplify.cc:467] Begin Simplify 1
I1018 08:53:57.853616 4097770 compute.cc:209] tensor var_39's domain is : 1, 64, 1, 1
I1018 08:53:57.853650 4097770 tensor.cc:259] name:var_31, domain: []->{ var_31[]:  }
I1018 08:53:57.853657 4097770 domain.cc:65] isl::set []->{ var_31[]:  }
I1018 08:53:57.853714 4097770 tensor.cc:259] name:var_39, domain: []->{ var_39[i, j, k, a]: 0 <= i <= 0 and 0 <= j <= 63 and 0 <= k <= 0 and 0 <= a <= 0 }
I1018 08:53:57.853741 4097770 domain.cc:65] isl::set []->{ var_39[i, j, k, a]: 0 <= i <= 0 and 0 <= j <= 63 and 0 <= k <= 0 and 0 <= a <= 0 }
I1018 08:53:57.853933 4097769 optimize.cc:57] After Optimize UnrollLoop:{
  ScheduleBlock(root_4)
  {
    serial for (j, 0, 64)
    {
      serial for (k, 0, 56)
      {
        serial for (a, 0, 56)
        {
          ScheduleBlock(var_55__reduce_init)
          {
            i0_5, i1_2, i2_2, i3_2 = axis.bind(0, j, k, a)
            var_55__reduce_init[0, j, k, a] = -3.40282347e+38f
          }
          serial for (kernel_idx, 0, 3)
          {
            serial for (kernel_idx_0, 0, 3)
            {
              ScheduleBlock(var_55)
              {
                i0_6, i1_3, i2_3, i3_3, i4, i5 = axis.bind(0, j, k, a, kernel_idx, kernel_idx_0)
                var_55[0, j, k, a] = cinn_max(var_55[0, j, k, a], select(((((a * 2) + kernel_idx_0) < (1 + 112)) and ((((a * 2) + kernel_idx_0) >= 1) and ((((k * 2) + kernel_idx) < (1 + 112)) and (((k * 2) + kernel_idx) >= 1)))), var_53[0, j, (-1 + ((2 * k) + kernel_idx)), (-1 + ((2 * a) + kernel_idx_0))], -3.40282347e+38f))
              }
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.853991 4097769 ir_simplify.cc:467] Begin Simplify 0
I1018 08:53:57.853998 4097769 ir_simplify.cc:467] Begin Simplify j
I1018 08:53:57.854008 4097769 ir_simplify.cc:467] Begin Simplify k
I1018 08:53:57.854014 4097769 ir_simplify.cc:467] Begin Simplify a
I1018 08:53:57.854020 4097769 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.854023 4097769 ir_util.cc:142] shape is : 1,64,56,56
I1018 08:53:57.854032 4097769 ir_util.cc:143] indices is : 0,j,k,a
I1018 08:53:57.854132 4097769 ir_simplify.cc:467] Begin Simplify 0
I1018 08:53:57.854140 4097769 ir_simplify.cc:467] Begin Simplify j
I1018 08:53:57.854147 4097769 ir_simplify.cc:467] Begin Simplify k
I1018 08:53:57.854153 4097769 ir_simplify.cc:467] Begin Simplify a
I1018 08:53:57.854167 4097769 ir_simplify.cc:467] Begin Simplify 0
I1018 08:53:57.854174 4097769 ir_simplify.cc:467] Begin Simplify j
I1018 08:53:57.854221 4097769 ir_simplify.cc:467] Begin Simplify (-1 + ((2 * k) + kernel_idx))
I1018 08:53:57.854303 4097769 ir_simplify.cc:467] Begin Simplify (-1 + ((2 * a) + kernel_idx_0))
I1018 08:53:57.854354 4097769 ir_simplify.cc:467] Begin Simplify 0
I1018 08:53:57.854363 4097769 ir_simplify.cc:467] Begin Simplify j
I1018 08:53:57.854369 4097769 ir_simplify.cc:467] Begin Simplify k
I1018 08:53:57.854375 4097769 ir_simplify.cc:467] Begin Simplify a
I1018 08:53:57.854380 4097769 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.854384 4097769 ir_util.cc:142] shape is : 1,64,56,56
I1018 08:53:57.854393 4097769 ir_util.cc:143] indices is : 0,j,k,a
I1018 08:53:57.854471 4097770 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.854480 4097770 ir_util.cc:142] shape is : 1,64,1,1
I1018 08:53:57.854489 4097770 ir_util.cc:143] indices is : i,j,k,a
I1018 08:53:57.854485 4097769 optimize.cc:60] After Optimize VectorizeLoops:{
  ScheduleBlock(root_4)
  {
    serial for (j, 0, 64)
    {
      serial for (k, 0, 56)
      {
        serial for (a, 0, 56)
        {
          ScheduleBlock(var_55__reduce_init)
          {
            i0_5, i1_2, i2_2, i3_2 = axis.bind(0, j, k, a)
            var_55__reduce_init[0, j, k, a] = -3.40282347e+38f
          }
          serial for (kernel_idx, 0, 3)
          {
            serial for (kernel_idx_0, 0, 3)
            {
              ScheduleBlock(var_55)
              {
                i0_6, i1_3, i2_3, i3_3, i4, i5 = axis.bind(0, j, k, a, kernel_idx, kernel_idx_0)
                var_55[0, j, k, a] = cinn_max(var_55[0, j, k, a], select(((((a * 2) + kernel_idx_0) < (1 + 112)) and ((((a * 2) + kernel_idx_0) >= 1) and ((((k * 2) + kernel_idx) < (1 + 112)) and (((k * 2) + kernel_idx) >= 1)))), var_53[0, j, (-1 + ((2 * k) + kernel_idx)), (-1 + ((2 * a) + kernel_idx_0))], -3.40282347e+38f))
              }
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.854537 4097769 optimize.cc:74] After SimplifyBlocks:{
  ScheduleBlock(root_4)
  {
    serial for (j, 0, 64)
    {
      serial for (k, 0, 56)
      {
        serial for (a, 0, 56)
        {
          ScheduleBlock(var_55__reduce_init)
          {
            i0_5, i1_2, i2_2, i3_2 = axis.bind(0, j, k, a)
            var_55__reduce_init[0, j, k, a] = -3.40282347e+38f
          }
          serial for (kernel_idx, 0, 3)
          {
            serial for (kernel_idx_0, 0, 3)
            {
              ScheduleBlock(var_55)
              {
                i0_6, i1_3, i2_3, i3_3, i4, i5 = axis.bind(0, j, k, a, kernel_idx, kernel_idx_0)
                var_55[0, j, k, a] = cinn_max(var_55[0, j, k, a], select(((((a * 2) + kernel_idx_0) < (1 + 112)) and ((((a * 2) + kernel_idx_0) >= 1) and ((((k * 2) + kernel_idx) < (1 + 112)) and (((k * 2) + kernel_idx) >= 1)))), var_53[0, j, (-1 + ((2 * k) + kernel_idx)), (-1 + ((2 * a) + kernel_idx_0))], -3.40282347e+38f))
              }
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.854574 4097769 ir_simplify.cc:467] Begin Simplify {
  ScheduleBlock(root_4)
  {
    serial for (j, 0, 64)
    {
      serial for (k, 0, 56)
      {
        serial for (a, 0, 56)
        {
          ScheduleBlock(var_55__reduce_init)
          {
            i0_5, i1_2, i2_2, i3_2 = axis.bind(0, j, k, a)
            var_55__reduce_init[0, j, k, a] = -3.40282347e+38f
          }
          serial for (kernel_idx, 0, 3)
          {
            serial for (kernel_idx_0, 0, 3)
            {
              ScheduleBlock(var_55)
              {
                i0_6, i1_3, i2_3, i3_3, i4, i5 = axis.bind(0, j, k, a, kernel_idx, kernel_idx_0)
                var_55[0, j, k, a] = cinn_max(var_55[0, j, k, a], select(((((a * 2) + kernel_idx_0) < (1 + 112)) and ((((a * 2) + kernel_idx_0) >= 1) and ((((k * 2) + kernel_idx) < (1 + 112)) and (((k * 2) + kernel_idx) >= 1)))), var_53[0, j, (-1 + ((2 * k) + kernel_idx)), (-1 + ((2 * a) + kernel_idx_0))], -3.40282347e+38f))
              }
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.854655 4097770 lower_tensor_group.cc:103] In store_exprs, its name is : var_39
I1018 08:53:57.854676 4097770 lower_tensor_group.cc:134] func_args is : _var_31
I1018 08:53:57.854679 4097770 lower_tensor_group.cc:134] func_args is : _var_39
I1018 08:53:57.854712 4097770 lowered_func.cc:230] Function used 2 buffers
I1018 08:53:57.854756 4097770 op_lowering_impl.cc:420] Lower op: scale, get 1 LoweredFunc:
I1018 08:53:57.854761 4097770 op_lowering_impl.cc:423] function fn_scale_7 (_var_31, _var_39)
{
  ScheduleBlock(root_9)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 1)
        {
          serial for (a, 0, 1)
          {
            ScheduleBlock(var_39)
            {
              i0_10, i1_7, i2_7, i3_7 = axis.bind(0, j, 0, 0)
              var_39[i0_10, i1_7, i2_7, i3_7] = ((float32(1.00000000f) * var_31[i0_10, i1_7, i2_7, i3_7]) + float32(9.99999975e-06f))
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.854797 4097770 op_lowering_impl.cc:450] Do op schedule
I1018 08:53:57.854807 4097770 ir_schedule_pe.cc:70] Before IRElementwiseSchedule, new ir is : {
  ScheduleBlock(root_9)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 1)
        {
          serial for (a, 0, 1)
          {
            ScheduleBlock(var_39)
            {
              i0_10, i1_7, i2_7, i3_7 = axis.bind(0, j, 0, 0)
              var_39[i0_10, i1_7, i2_7, i3_7] = ((float32(1.00000000f) * var_31[i0_10, i1_7, i2_7, i3_7]) + float32(9.99999975e-06f))
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.854828 4097770 ir_analyzer.cc:101] it_expr is : {
  ScheduleBlock(root_9)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 1)
        {
          serial for (a, 0, 1)
          {
            ScheduleBlock(var_39)
            {
              i0_10, i1_7, i2_7, i3_7 = axis.bind(0, j, 0, 0)
              var_39[i0_10, i1_7, i2_7, i3_7] = ((float32(1.00000000f) * var_31[i0_10, i1_7, i2_7, i3_7]) + float32(9.99999975e-06f))
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.854849 4097770 loop_transformation.cc:688] Before FlattenLoops, ir is:
serial for (i, 0, 1)
{
  serial for (j, 0, 64)
  {
    serial for (k, 0, 1)
    {
      serial for (a, 0, 1)
      {
        ScheduleBlock(var_39)
        {
          i0_10, i1_7, i2_7, i3_7 = axis.bind(0, j, 0, 0)
          var_39[i0_10, i1_7, i2_7, i3_7] = ((float32(1.00000000f) * var_31[i0_10, i1_7, i2_7, i3_7]) + float32(9.99999975e-06f))
        }
      }
    }
  }
}
I1018 08:53:57.854897 4097770 loop_transformation.cc:864] After FlattenLoops, ir is:
serial for (flat_i, 0, 64)
{
  ScheduleBlock(var_39)
  {
    _flat_i = axis.bind(flat_i)
    var_39[_flat_i] = ((float32(1.00000000f) * var_31[_flat_i]) + float32(9.99999975e-06f))
  }
}
I1018 08:53:57.854916 4097770 ir_schedule_pe.cc:109] After IRElementwiseSchedule, new ir is : {
  ScheduleBlock(root_9)
  {
    serial for (flat_i, 0, 64)
    {
      ScheduleBlock(var_39)
      {
        _flat_i = axis.bind(flat_i)
        var_39[_flat_i] = ((float32(1.00000000f) * var_31[_flat_i]) + float32(9.99999975e-06f))
      }
    }
  }
}
I1018 08:53:57.854933 4097770 op_lowering_impl.cc:463] After op schedule: {
  ScheduleBlock(root_9)
  {
    serial for (flat_i, 0, 64)
    {
      ScheduleBlock(var_39)
      {
        _flat_i = axis.bind(flat_i)
        var_39[_flat_i] = ((float32(1.00000000f) * var_31[_flat_i]) + float32(9.99999975e-06f))
      }
    }
  }
}
I1018 08:53:57.854969 4097770 op_lowering_impl.cc:373] Do lower with Compute, op: fill_constant
I1018 08:53:57.854985 4097770 ir_simplify.cc:467] Begin Simplify 1
I1018 08:53:57.854991 4097770 ir_simplify.cc:467] Begin Simplify 64
I1018 08:53:57.854996 4097770 ir_simplify.cc:467] Begin Simplify 1
I1018 08:53:57.855001 4097770 ir_simplify.cc:467] Begin Simplify 1
I1018 08:53:57.855006 4097770 compute.cc:209] tensor var_103's domain is : 1, 64, 1, 1
I1018 08:53:57.855033 4097770 tensor.cc:259] name:var_103, domain: []->{ var_103[i, j, k, a]: 0 <= i <= 0 and 0 <= j <= 63 and 0 <= k <= 0 and 0 <= a <= 0 }
I1018 08:53:57.855059 4097770 domain.cc:65] isl::set []->{ var_103[i, j, k, a]: 0 <= i <= 0 and 0 <= j <= 63 and 0 <= k <= 0 and 0 <= a <= 0 }
I1018 08:53:57.855089 4097769 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.855096 4097769 ir_util.cc:142] shape is : 1,64,56,56
I1018 08:53:57.855106 4097769 ir_util.cc:143] indices is : 0,j,k,a
I1018 08:53:57.855295 4097769 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.855300 4097769 ir_util.cc:142] shape is : 1,64,56,56
I1018 08:53:57.855309 4097769 ir_util.cc:143] indices is : 0,j,k,a
I1018 08:53:57.855463 4097769 ir_simplify.cc:467] Begin Simplify 0
I1018 08:53:57.855473 4097769 ir_simplify.cc:467] Begin Simplify j
I1018 08:53:57.855479 4097769 ir_simplify.cc:467] Begin Simplify k
I1018 08:53:57.855485 4097769 ir_simplify.cc:467] Begin Simplify a
I1018 08:53:57.855490 4097769 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.855494 4097769 ir_util.cc:142] shape is : 1,64,56,56
I1018 08:53:57.855502 4097769 ir_util.cc:143] indices is : 0,j,k,a
I1018 08:53:57.855600 4097769 ir_simplify.cc:467] Begin Simplify 0
I1018 08:53:57.855608 4097769 ir_simplify.cc:467] Begin Simplify j
I1018 08:53:57.855615 4097769 ir_simplify.cc:467] Begin Simplify k
I1018 08:53:57.855623 4097769 ir_simplify.cc:467] Begin Simplify a
I1018 08:53:57.855635 4097769 ir_simplify.cc:467] Begin Simplify 0
I1018 08:53:57.855643 4097769 ir_simplify.cc:467] Begin Simplify j
I1018 08:53:57.855696 4097769 ir_simplify.cc:467] Begin Simplify (-1 + ((2 * k) + kernel_idx))
I1018 08:53:57.855777 4097769 ir_simplify.cc:467] Begin Simplify (-1 + ((2 * a) + kernel_idx_0))
I1018 08:53:57.855799 4097770 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.855806 4097770 ir_util.cc:142] shape is : 1,64,1,1
I1018 08:53:57.855818 4097770 ir_util.cc:143] indices is : i,j,k,a
I1018 08:53:57.855827 4097769 ir_simplify.cc:467] Begin Simplify 0
I1018 08:53:57.855836 4097769 ir_simplify.cc:467] Begin Simplify j
I1018 08:53:57.855844 4097769 ir_simplify.cc:467] Begin Simplify k
I1018 08:53:57.855849 4097769 ir_simplify.cc:467] Begin Simplify a
I1018 08:53:57.855854 4097769 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.855857 4097769 ir_util.cc:142] shape is : 1,64,56,56
I1018 08:53:57.855866 4097769 ir_util.cc:143] indices is : 0,j,k,a
I1018 08:53:57.855965 4097770 lower_tensor_group.cc:103] In store_exprs, its name is : var_103
I1018 08:53:57.855980 4097770 lower_tensor_group.cc:134] func_args is : _var_103
I1018 08:53:57.856007 4097770 lowered_func.cc:230] Function used 1 buffers
I1018 08:53:57.856043 4097770 op_lowering_impl.cc:420] Lower op: fill_constant, get 1 LoweredFunc:
I1018 08:53:57.856048 4097770 op_lowering_impl.cc:423] function fn_fill_constant_10 (_var_103)
{
  ScheduleBlock(root_10)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 1)
        {
          serial for (a, 0, 1)
          {
            ScheduleBlock(var_103)
            {
              i0_11, i1_8, i2_8, i3_8 = axis.bind(0, j, 0, 0)
              var_103[i0_11, i1_8, i2_8, i3_8] = float32(-0.50000000000000000)
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.856076 4097770 op_lowering_impl.cc:450] Do op schedule
I1018 08:53:57.856086 4097770 ir_schedule_pe.cc:70] Before IRElementwiseSchedule, new ir is : {
  ScheduleBlock(root_10)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 1)
        {
          serial for (a, 0, 1)
          {
            ScheduleBlock(var_103)
            {
              i0_11, i1_8, i2_8, i3_8 = axis.bind(0, j, 0, 0)
              var_103[i0_11, i1_8, i2_8, i3_8] = float32(-0.50000000000000000)
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.856109 4097770 ir_analyzer.cc:101] it_expr is : {
  ScheduleBlock(root_10)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 1)
        {
          serial for (a, 0, 1)
          {
            ScheduleBlock(var_103)
            {
              i0_11, i1_8, i2_8, i3_8 = axis.bind(0, j, 0, 0)
              var_103[i0_11, i1_8, i2_8, i3_8] = float32(-0.50000000000000000)
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.856128 4097770 loop_transformation.cc:688] Before FlattenLoops, ir is:
serial for (i, 0, 1)
{
  serial for (j, 0, 64)
  {
    serial for (k, 0, 1)
    {
      serial for (a, 0, 1)
      {
        ScheduleBlock(var_103)
        {
          i0_11, i1_8, i2_8, i3_8 = axis.bind(0, j, 0, 0)
          var_103[i0_11, i1_8, i2_8, i3_8] = float32(-0.50000000000000000)
        }
      }
    }
  }
}
I1018 08:53:57.856168 4097770 loop_transformation.cc:864] After FlattenLoops, ir is:
serial for (flat_i, 0, 64)
{
  ScheduleBlock(var_103)
  {
    _flat_i = axis.bind(flat_i)
    var_103[_flat_i] = float32(-0.50000000000000000)
  }
}
I1018 08:53:57.856184 4097770 ir_schedule_pe.cc:109] After IRElementwiseSchedule, new ir is : {
  ScheduleBlock(root_10)
  {
    serial for (flat_i, 0, 64)
    {
      ScheduleBlock(var_103)
      {
        _flat_i = axis.bind(flat_i)
        var_103[_flat_i] = float32(-0.50000000000000000)
      }
    }
  }
}
I1018 08:53:57.856199 4097770 op_lowering_impl.cc:463] After op schedule: {
  ScheduleBlock(root_10)
  {
    serial for (flat_i, 0, 64)
    {
      ScheduleBlock(var_103)
      {
        _flat_i = axis.bind(flat_i)
        var_103[_flat_i] = float32(-0.50000000000000000)
      }
    }
  }
}
I1018 08:53:57.856217 4097769 codegen_sycl_dev.cc:503] CodeGenSYCL visiting store op: var_55__reduce_init
I1018 08:53:57.856225 4097769 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.856227 4097769 ir_util.cc:142] shape is : 1,64,56,56
I1018 08:53:57.856237 4097769 ir_util.cc:143] indices is : 0,j,k,a
I1018 08:53:57.856257 4097770 op_lowering_impl.cc:373] Do lower with Compute, op: pow
I1018 08:53:57.856346 4097770 ir_simplify.cc:467] Begin Simplify 1
I1018 08:53:57.856353 4097770 ir_simplify.cc:467] Begin Simplify 64
I1018 08:53:57.856357 4097770 ir_simplify.cc:467] Begin Simplify 1
I1018 08:53:57.856360 4097769 codegen_sycl_dev.cc:503] CodeGenSYCL visiting store op: var_55
I1018 08:53:57.856362 4097770 ir_simplify.cc:467] Begin Simplify 1
I1018 08:53:57.856365 4097769 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.856369 4097770 compute.cc:209] tensor var_43's domain is : 1, 64, 1, 1
I1018 08:53:57.856370 4097769 ir_util.cc:142] shape is : 1,64,56,56
I1018 08:53:57.856380 4097769 ir_util.cc:143] indices is : 0,j,k,a
I1018 08:53:57.856413 4097770 tensor.cc:259] name:var_103, domain: []->{ var_103[]:  }
I1018 08:53:57.856420 4097770 domain.cc:65] isl::set []->{ var_103[]:  }
I1018 08:53:57.856470 4097770 tensor.cc:259] name:var_39, domain: []->{ var_39[]:  }
I1018 08:53:57.856475 4097769 ir.cc:629] Begin Load::index IndiceToAbsOffset of tensor: var_55
I1018 08:53:57.856483 4097769 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.856475 4097770 domain.cc:65] isl::set []->{ var_39[]:  }
I1018 08:53:57.856487 4097769 ir_util.cc:142] shape is : 1,64,56,56
I1018 08:53:57.856496 4097769 ir_util.cc:143] indices is : 0,j,k,a
I1018 08:53:57.856529 4097770 tensor.cc:259] name:var_43, domain: []->{ var_43[i, j, k, a]: 0 <= i <= 0 and 0 <= j <= 63 and 0 <= k <= 0 and 0 <= a <= 0 }
I1018 08:53:57.856556 4097770 domain.cc:65] isl::set []->{ var_43[i, j, k, a]: 0 <= i <= 0 and 0 <= j <= 63 and 0 <= k <= 0 and 0 <= a <= 0 }
I1018 08:53:57.856597 4097769 ir.cc:629] Begin Load::index IndiceToAbsOffset of tensor: var_53
I1018 08:53:57.856602 4097769 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.856606 4097769 ir_util.cc:142] shape is : 1,64,112,112
I1018 08:53:57.856614 4097769 ir_util.cc:143] indices is : 0,j,(-1 + ((2 * k) + kernel_idx)),(-1 + ((2 * a) + kernel_idx_0))
I1018 08:53:57.856863 4097769 lowered_func.cc:230] Function used 0 buffers
I1018 08:53:57.856936 4097769 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.856940 4097769 ir_util.cc:142] shape is : 1,64,56,56
I1018 08:53:57.856950 4097769 ir_util.cc:143] indices is : 0,j,k,a
I1018 08:53:57.857128 4097769 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.857133 4097769 ir_util.cc:142] shape is : 1,64,56,56
I1018 08:53:57.857143 4097769 ir_util.cc:143] indices is : 0,j,k,a
I1018 08:53:57.857321 4097769 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.857323 4097770 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.857327 4097769 ir_util.cc:142] shape is : 1,64,56,56
I1018 08:53:57.857333 4097770 ir_util.cc:142] shape is : 1,64,1,1
I1018 08:53:57.857338 4097769 ir_util.cc:143] indices is : 0,j,k,a
I1018 08:53:57.857347 4097770 ir_util.cc:143] indices is : i,j,k,a
I1018 08:53:57.857537 4097770 lower_tensor_group.cc:103] In store_exprs, its name is : var_43
I1018 08:53:57.857539 4097769 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.857544 4097769 ir_util.cc:142] shape is : 1,64,56,56
I1018 08:53:57.857554 4097769 ir_util.cc:143] indices is : 0,j,k,a
I1018 08:53:57.857559 4097770 lower_tensor_group.cc:134] func_args is : _var_39
I1018 08:53:57.857569 4097770 lower_tensor_group.cc:134] func_args is : _var_103
I1018 08:53:57.857573 4097770 lower_tensor_group.cc:134] func_args is : _var_43
I1018 08:53:57.857609 4097770 lowered_func.cc:230] Function used 3 buffers
I1018 08:53:57.857664 4097770 op_lowering_impl.cc:420] Lower op: pow, get 1 LoweredFunc:
I1018 08:53:57.857668 4097770 op_lowering_impl.cc:423] function fn_pow_11 (_var_39, _var_103, _var_43)
{
  ScheduleBlock(root_11)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 1)
        {
          serial for (a, 0, 1)
          {
            ScheduleBlock(var_43)
            {
              i0_12, i1_9, i2_9, i3_9 = axis.bind(0, j, 0, 0)
              var_43[i0_12, i1_9, i2_9, i3_9] = pow(var_39[i0_12, i1_9, i2_9, i3_9], var_103[i0_12, i1_9, i2_9, i3_9])
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.857699 4097770 op_lowering_impl.cc:450] Do op schedule
I1018 08:53:57.857712 4097769 ir_simplify.cc:467] Begin Simplify 0
I1018 08:53:57.857708 4097770 ir_schedule_pe.cc:116] Before IRInjectiveSchedule, new ir is : {
  ScheduleBlock(root_11)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 1)
        {
          serial for (a, 0, 1)
          {
            ScheduleBlock(var_43)
            {
              i0_12, i1_9, i2_9, i3_9 = axis.bind(0, j, 0, 0)
              var_43[i0_12, i1_9, i2_9, i3_9] = pow(var_39[i0_12, i1_9, i2_9, i3_9], var_103[i0_12, i1_9, i2_9, i3_9])
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.857722 4097769 ir_simplify.cc:467] Begin Simplify j
I1018 08:53:57.857728 4097769 ir_simplify.cc:467] Begin Simplify k
I1018 08:53:57.857724 4097770 ir_analyzer.cc:101] it_expr is : {
  ScheduleBlock(root_11)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 1)
        {
          serial for (a, 0, 1)
          {
            ScheduleBlock(var_43)
            {
              i0_12, i1_9, i2_9, i3_9 = axis.bind(0, j, 0, 0)
              var_43[i0_12, i1_9, i2_9, i3_9] = pow(var_39[i0_12, i1_9, i2_9, i3_9], var_103[i0_12, i1_9, i2_9, i3_9])
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.857734 4097769 ir_simplify.cc:467] Begin Simplify a
I1018 08:53:57.857748 4097769 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.857743 4097770 loop_transformation.cc:688] Before FlattenLoops, ir is:
serial for (i, 0, 1)
{
  serial for (j, 0, 64)
  {
    serial for (k, 0, 1)
    {
      serial for (a, 0, 1)
      {
        ScheduleBlock(var_43)
        {
          i0_12, i1_9, i2_9, i3_9 = axis.bind(0, j, 0, 0)
          var_43[i0_12, i1_9, i2_9, i3_9] = pow(var_39[i0_12, i1_9, i2_9, i3_9], var_103[i0_12, i1_9, i2_9, i3_9])
        }
      }
    }
  }
}
I1018 08:53:57.857757 4097769 ir_util.cc:142] shape is : 1,64,56,56
I1018 08:53:57.857766 4097769 ir_util.cc:143] indices is : 0,j,k,a
I1018 08:53:57.857797 4097770 loop_transformation.cc:864] After FlattenLoops, ir is:
serial for (flat_i, 0, 64)
{
  ScheduleBlock(var_43)
  {
    _flat_i = axis.bind(flat_i)
    var_43[_flat_i] = pow(var_39[_flat_i], var_103[_flat_i])
  }
}
I1018 08:53:57.857810 4097770 ir_schedule_pe.cc:155] After IRInjectiveSchedule, new ir is : {
  ScheduleBlock(root_11)
  {
    serial for (flat_i, 0, 64)
    {
      ScheduleBlock(var_43)
      {
        _flat_i = axis.bind(flat_i)
        var_43[_flat_i] = pow(var_39[_flat_i], var_103[_flat_i])
      }
    }
  }
}
I1018 08:53:57.857822 4097770 op_lowering_impl.cc:463] After op schedule: {
  ScheduleBlock(root_11)
  {
    serial for (flat_i, 0, 64)
    {
      ScheduleBlock(var_43)
      {
        _flat_i = axis.bind(flat_i)
        var_43[_flat_i] = pow(var_39[_flat_i], var_103[_flat_i])
      }
    }
  }
}
I1018 08:53:57.857869 4097769 ir_simplify.cc:467] Begin Simplify 0
I1018 08:53:57.857878 4097769 ir_simplify.cc:467] Begin Simplify j
I1018 08:53:57.857882 4097770 op_lowering_impl.cc:373] Do lower with Compute, op: subtract
I1018 08:53:57.857885 4097769 ir_simplify.cc:467] Begin Simplify k
I1018 08:53:57.857892 4097769 ir_simplify.cc:467] Begin Simplify a
I1018 08:53:57.857906 4097769 ir_simplify.cc:467] Begin Simplify 0
I1018 08:53:57.857913 4097769 ir_simplify.cc:467] Begin Simplify j
I1018 08:53:57.857960 4097769 ir_simplify.cc:467] Begin Simplify (-1 + ((2 * k) + kernel_idx))
I1018 08:53:57.857967 4097770 ir_simplify.cc:467] Begin Simplify 1
I1018 08:53:57.857975 4097770 ir_simplify.cc:467] Begin Simplify 64
I1018 08:53:57.857980 4097770 ir_simplify.cc:467] Begin Simplify 112
I1018 08:53:57.857985 4097770 ir_simplify.cc:467] Begin Simplify 112
I1018 08:53:57.857990 4097770 compute.cc:209] tensor var_37's domain is : 1, 64, 112, 112
I1018 08:53:57.858027 4097770 tensor.cc:259] name:conv2d_0__tmp_0, domain: []->{ conv2d_0__tmp_0[]:  }
I1018 08:53:57.858033 4097770 domain.cc:65] isl::set []->{ conv2d_0__tmp_0[]:  }
I1018 08:53:57.858042 4097769 ir_simplify.cc:467] Begin Simplify (-1 + ((2 * a) + kernel_idx_0))
I1018 08:53:57.858084 4097770 tensor.cc:259] name:var_101, domain: []->{ var_101[]:  }
I1018 08:53:57.858090 4097770 domain.cc:65] isl::set []->{ var_101[]:  }
I1018 08:53:57.858093 4097769 ir_simplify.cc:467] Begin Simplify 0
I1018 08:53:57.858103 4097769 ir_simplify.cc:467] Begin Simplify j
I1018 08:53:57.858108 4097769 ir_simplify.cc:467] Begin Simplify k
I1018 08:53:57.858114 4097769 ir_simplify.cc:467] Begin Simplify a
I1018 08:53:57.858120 4097769 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.858124 4097769 ir_util.cc:142] shape is : 1,64,56,56
I1018 08:53:57.858132 4097769 ir_util.cc:143] indices is : 0,j,k,a
I1018 08:53:57.858130 4097770 tensor.cc:259] name:var_37, domain: []->{ var_37[i, j, k, a]: 0 <= i <= 0 and 0 <= j <= 63 and 0 <= k <= 111 and 0 <= a <= 111 }
I1018 08:53:57.858157 4097770 domain.cc:65] isl::set []->{ var_37[i, j, k, a]: 0 <= i <= 0 and 0 <= j <= 63 and 0 <= k <= 111 and 0 <= a <= 111 }
I1018 08:53:57.858570 4097769 codegen_sycl_dev.cc:503] CodeGenSYCL visiting store op: var_55__reduce_init
I1018 08:53:57.858577 4097769 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.858580 4097769 ir_util.cc:142] shape is : 1,64,56,56
I1018 08:53:57.858589 4097769 ir_util.cc:143] indices is : 0,j,k,a
I1018 08:53:57.858695 4097769 codegen_sycl_dev.cc:503] CodeGenSYCL visiting store op: var_55
I1018 08:53:57.858700 4097769 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.858708 4097769 ir_util.cc:142] shape is : 1,64,56,56
I1018 08:53:57.858717 4097769 ir_util.cc:143] indices is : 0,j,k,a
I1018 08:53:57.858808 4097769 ir.cc:629] Begin Load::index IndiceToAbsOffset of tensor: var_55
I1018 08:53:57.858812 4097769 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.858815 4097769 ir_util.cc:142] shape is : 1,64,56,56
I1018 08:53:57.858824 4097769 ir_util.cc:143] indices is : 0,j,k,a
I1018 08:53:57.858922 4097769 ir.cc:629] Begin Load::index IndiceToAbsOffset of tensor: var_53
I1018 08:53:57.858924 4097770 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.858927 4097769 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.858934 4097770 ir_util.cc:142] shape is : 1,64,112,112
I1018 08:53:57.858945 4097770 ir_util.cc:143] indices is : i,j,k,a
I1018 08:53:57.858942 4097769 ir_util.cc:142] shape is : 1,64,112,112
I1018 08:53:57.858958 4097769 ir_util.cc:143] indices is : 0,j,(-1 + ((2 * k) + kernel_idx)),(-1 + ((2 * a) + kernel_idx_0))
I1018 08:53:57.859160 4097770 lower_tensor_group.cc:103] In store_exprs, its name is : var_37
I1018 08:53:57.859182 4097770 lower_tensor_group.cc:134] func_args is : _conv2d_0__tmp_0
I1018 08:53:57.859186 4097770 lower_tensor_group.cc:134] func_args is : _var_101
I1018 08:53:57.859189 4097770 lower_tensor_group.cc:134] func_args is : _var_37
I1018 08:53:57.859191 4097769 parallel_compiler.cc:328] [SYCL]:
#include <sycl/sycl.hpp>
#include "cinn_sycl_runtime_source.h"
typedef sycl::half float16;
#ifdef __cplusplus
extern "C" {
#endif
// CodeGenSYCL: NOTE: Auto-generated packed function
void fn_pool2d_20_22_kernel(sycl::queue &Q, sycl::range<3> dimGrid, sycl::range<3> dimBlock, void** void_args) {
  const float*  var_53 = (float* )(*(void **)(void_args[0]));
  float*  var_55 = (float* )(*(void **)(void_args[1]));
  Q.submit([&](sycl::handler &h) {
    h.parallel_for<class space1_fn_pool2d_20_22_kernel>(sycl::nd_range<3>(dimGrid * dimBlock, dimBlock), [=](sycl::nd_item<3> item) [[intel::kernel_args_restrict]][[intel::max_work_group_size(1, 1, 1)]]
    {
      float* var_55__reduce_init = var_55;
      for (int32_t j = 0; j < 64; j += 1) {
        for (int32_t k = 0; k < 56; k += 1) {
          for (int32_t a = 0; a < 56; a += 1) {
            cinn_sycl_store(var_55__reduce_init, ((3136 * j) + ((56 * k) + a)), -3.40282347e+38f);
            for (int32_t kernel_idx = 0; kernel_idx < 3; kernel_idx += 1) {
              for (int32_t kernel_idx_0 = 0; kernel_idx_0 < 3; kernel_idx_0 += 1) {
                cinn_sycl_store(var_55, ((3136 * j) + ((56 * k) + a)), cinn_sycl_max(var_55[((3136 * j) + ((56 * k) + a))], cinn_sycl_select(((((a * 2) + kernel_idx_0) < (1 + 112)) && ((((a * 2) + kernel_idx_0) >= 1) && ((((k * 2) + kernel_idx) < (1 + 112)) && (((k * 2) + kernel_idx) >= 1)))), var_53[(-113 + ((2 * a) + ((12544 * j) + ((224 * k) + ((112 * kernel_idx) + kernel_idx_0)))))], -3.40282347e+38f)));
              };
            };
          };
        };
      };
    });
  });
}

#ifdef __cplusplus
}
#endif
I1018 08:53:57.859225 4097770 lowered_func.cc:230] Function used 3 buffers
I1018 08:53:57.859279 4097770 op_lowering_impl.cc:420] Lower op: subtract, get 1 LoweredFunc:
I1018 08:53:57.859284 4097770 op_lowering_impl.cc:423] function fn_subtract_6 (_conv2d_0__tmp_0, _var_101, _var_37)
{
  ScheduleBlock(root_12)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 112)
        {
          serial for (a, 0, 112)
          {
            ScheduleBlock(var_37)
            {
              i0_13, i1_10, i2_10, i3_10 = axis.bind(0, j, k, a)
              var_37[i0_13, i1_10, i2_10, i3_10] = (conv2d_0__tmp_0[i0_13, i1_10, i2_10, i3_10] - var_101[i0_13, i1_10, i2_10, i3_10])
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.859305 4097769 compiler_sycl.cc:77] compile command: /home/wzy/sycl_workspace/llvm-mlu/build/bin/clang++ -I /home/wzy/.local/lib/python3.9/site-packages/paddle/libs -fsycl -fsycl-targets=mlisa-cambricon-bang -std=c++17 -fPIC -O1 -shared -ldl -fbracket-depth=1030 ./source/sycl_1.cc -o ./source/sycl_1.so
I1018 08:53:57.859311 4097770 op_lowering_impl.cc:450] Do op schedule
I1018 08:53:57.859330 4097770 ir_schedule_pe.cc:116] Before IRInjectiveSchedule, new ir is : {
  ScheduleBlock(root_12)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 112)
        {
          serial for (a, 0, 112)
          {
            ScheduleBlock(var_37)
            {
              i0_13, i1_10, i2_10, i3_10 = axis.bind(0, j, k, a)
              var_37[i0_13, i1_10, i2_10, i3_10] = (conv2d_0__tmp_0[i0_13, i1_10, i2_10, i3_10] - var_101[i0_13, i1_10, i2_10, i3_10])
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.859346 4097770 ir_analyzer.cc:101] it_expr is : {
  ScheduleBlock(root_12)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 112)
        {
          serial for (a, 0, 112)
          {
            ScheduleBlock(var_37)
            {
              i0_13, i1_10, i2_10, i3_10 = axis.bind(0, j, k, a)
              var_37[i0_13, i1_10, i2_10, i3_10] = (conv2d_0__tmp_0[i0_13, i1_10, i2_10, i3_10] - var_101[i0_13, i1_10, i2_10, i3_10])
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.859362 4097770 loop_transformation.cc:688] Before FlattenLoops, ir is:
serial for (i, 0, 1)
{
  serial for (j, 0, 64)
  {
    serial for (k, 0, 112)
    {
      serial for (a, 0, 112)
      {
        ScheduleBlock(var_37)
        {
          i0_13, i1_10, i2_10, i3_10 = axis.bind(0, j, k, a)
          var_37[i0_13, i1_10, i2_10, i3_10] = (conv2d_0__tmp_0[i0_13, i1_10, i2_10, i3_10] - var_101[i0_13, i1_10, i2_10, i3_10])
        }
      }
    }
  }
}
I1018 08:53:57.859416 4097770 loop_transformation.cc:864] After FlattenLoops, ir is:
serial for (flat_i, 0, 802816)
{
  ScheduleBlock(var_37)
  {
    _flat_i = axis.bind(flat_i)
    var_37[_flat_i] = (conv2d_0__tmp_0[_flat_i] - var_101[_flat_i])
  }
}
I1018 08:53:57.859429 4097770 ir_schedule_pe.cc:155] After IRInjectiveSchedule, new ir is : {
  ScheduleBlock(root_12)
  {
    serial for (flat_i, 0, 802816)
    {
      ScheduleBlock(var_37)
      {
        _flat_i = axis.bind(flat_i)
        var_37[_flat_i] = (conv2d_0__tmp_0[_flat_i] - var_101[_flat_i])
      }
    }
  }
}
I1018 08:53:57.859442 4097770 op_lowering_impl.cc:463] After op schedule: {
  ScheduleBlock(root_12)
  {
    serial for (flat_i, 0, 802816)
    {
      ScheduleBlock(var_37)
      {
        _flat_i = axis.bind(flat_i)
        var_37[_flat_i] = (conv2d_0__tmp_0[_flat_i] - var_101[_flat_i])
      }
    }
  }
}
I1018 08:53:57.859483 4097770 broadcast.cc:267] broadcast out shape: 1, 64, 112, 112
I1018 08:53:57.859488 4097770 broadcast.cc:268] broadcast_axes shape: 0, 1, 2, 3
I1018 08:53:57.859498 4097770 op_lowering_impl.cc:373] Do lower with Compute, op: broadcast_to
I1018 08:53:57.859519 4097770 ir_simplify.cc:467] Begin Simplify 1
I1018 08:53:57.859524 4097770 ir_simplify.cc:467] Begin Simplify 64
I1018 08:53:57.859529 4097770 ir_simplify.cc:467] Begin Simplify 112
I1018 08:53:57.859534 4097770 ir_simplify.cc:467] Begin Simplify 112
I1018 08:53:57.859539 4097770 compute.cc:209] tensor var_104's domain is : 1, 64, 112, 112
I1018 08:53:57.859578 4097770 tensor.cc:259] name:var_104, domain: []->{ var_104[i, j, k, a]: 0 <= i <= 0 and 0 <= j <= 63 and 0 <= k <= 111 and 0 <= a <= 111 }
I1018 08:53:57.859606 4097770 domain.cc:65] isl::set []->{ var_104[i, j, k, a]: 0 <= i <= 0 and 0 <= j <= 63 and 0 <= k <= 111 and 0 <= a <= 111 }
I1018 08:53:57.860340 4097770 tensor.cc:259] name:var_43, domain: []->{ var_43[]:  }
I1018 08:53:57.860349 4097770 domain.cc:65] isl::set []->{ var_43[]:  }
I1018 08:53:57.860415 4097770 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.860420 4097770 ir_util.cc:142] shape is : 1,64,112,112
I1018 08:53:57.860431 4097770 ir_util.cc:143] indices is : i,j,k,a
I1018 08:53:57.860639 4097770 lower_tensor_group.cc:103] In store_exprs, its name is : var_104
I1018 08:53:57.860659 4097770 lower_tensor_group.cc:134] func_args is : _var_43
I1018 08:53:57.860662 4097770 lower_tensor_group.cc:134] func_args is : _var_104
I1018 08:53:57.860702 4097770 lowered_func.cc:230] Function used 2 buffers
I1018 08:53:57.860742 4097770 op_lowering_impl.cc:420] Lower op: broadcast_to, get 1 LoweredFunc:
I1018 08:53:57.860746 4097770 op_lowering_impl.cc:423] function fn_broadcast_to_12 (_var_43, _var_104)
{
  ScheduleBlock(root_13)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 112)
        {
          serial for (a, 0, 112)
          {
            ScheduleBlock(var_104)
            {
              i0_14, i1_11, i2_11, i3_11 = axis.bind(0, j, k, a)
              var_104[i0_14, i1_11, i2_11, i3_11] = var_43[0, i1_11, 0, 0]
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.860771 4097770 op_lowering_impl.cc:450] Do op schedule
I1018 08:53:57.860780 4097770 ir_schedule_pe.cc:116] Before IRInjectiveSchedule, new ir is : {
  ScheduleBlock(root_13)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 112)
        {
          serial for (a, 0, 112)
          {
            ScheduleBlock(var_104)
            {
              i0_14, i1_11, i2_11, i3_11 = axis.bind(0, j, k, a)
              var_104[i0_14, i1_11, i2_11, i3_11] = var_43[0, i1_11, 0, 0]
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.860796 4097770 ir_analyzer.cc:101] it_expr is : {
  ScheduleBlock(root_13)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 112)
        {
          serial for (a, 0, 112)
          {
            ScheduleBlock(var_104)
            {
              i0_14, i1_11, i2_11, i3_11 = axis.bind(0, j, k, a)
              var_104[i0_14, i1_11, i2_11, i3_11] = var_43[0, i1_11, 0, 0]
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.860812 4097770 loop_transformation.cc:688] Before FlattenLoops, ir is:
serial for (i, 0, 1)
{
  serial for (j, 0, 64)
  {
    serial for (k, 0, 112)
    {
      serial for (a, 0, 112)
      {
        ScheduleBlock(var_104)
        {
          i0_14, i1_11, i2_11, i3_11 = axis.bind(0, j, k, a)
          var_104[i0_14, i1_11, i2_11, i3_11] = var_43[0, i1_11, 0, 0]
        }
      }
    }
  }
}
I1018 08:53:57.860852 4097770 ir.cc:629] Begin Load::index IndiceToAbsOffset of tensor: var_43
I1018 08:53:57.860855 4097770 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.860858 4097770 ir_util.cc:142] shape is : 1,64,1,1
I1018 08:53:57.860868 4097770 ir_util.cc:143] indices is : 0,((_flat_i % 802816) / 12544),0,0
I1018 08:53:57.860955 4097770 loop_transformation.cc:864] After FlattenLoops, ir is:
serial for (flat_i, 0, 802816)
{
  ScheduleBlock(var_104)
  {
    _flat_i = axis.bind(flat_i)
    var_104[_flat_i] = var_43[((_flat_i % 802816) / 12544)]
  }
}
I1018 08:53:57.860970 4097770 ir_schedule_pe.cc:155] After IRInjectiveSchedule, new ir is : {
  ScheduleBlock(root_13)
  {
    serial for (flat_i, 0, 802816)
    {
      ScheduleBlock(var_104)
      {
        _flat_i = axis.bind(flat_i)
        var_104[_flat_i] = var_43[((_flat_i % 802816) / 12544)]
      }
    }
  }
}
I1018 08:53:57.860985 4097770 op_lowering_impl.cc:463] After op schedule: {
  ScheduleBlock(root_13)
  {
    serial for (flat_i, 0, 802816)
    {
      ScheduleBlock(var_104)
      {
        _flat_i = axis.bind(flat_i)
        var_104[_flat_i] = var_43[((_flat_i % 802816) / 12544)]
      }
    }
  }
}
I1018 08:53:57.861043 4097770 op_lowering_impl.cc:373] Do lower with Compute, op: elementwise_mul
I1018 08:53:57.861131 4097770 ir_simplify.cc:467] Begin Simplify 1
I1018 08:53:57.861138 4097770 ir_simplify.cc:467] Begin Simplify 64
I1018 08:53:57.861143 4097770 ir_simplify.cc:467] Begin Simplify 112
I1018 08:53:57.861147 4097770 ir_simplify.cc:467] Begin Simplify 112
I1018 08:53:57.861153 4097770 compute.cc:209] tensor var_47's domain is : 1, 64, 112, 112
I1018 08:53:57.861200 4097770 tensor.cc:259] name:var_104, domain: []->{ var_104[]:  }
I1018 08:53:57.861215 4097770 domain.cc:65] isl::set []->{ var_104[]:  }
I1018 08:53:57.861266 4097770 tensor.cc:259] name:var_37, domain: []->{ var_37[]:  }
I1018 08:53:57.861277 4097770 domain.cc:65] isl::set []->{ var_37[]:  }
I1018 08:53:57.861320 4097770 tensor.cc:259] name:var_47, domain: []->{ var_47[i, j, k, a]: 0 <= i <= 0 and 0 <= j <= 63 and 0 <= k <= 111 and 0 <= a <= 111 }
I1018 08:53:57.861346 4097770 domain.cc:65] isl::set []->{ var_47[i, j, k, a]: 0 <= i <= 0 and 0 <= j <= 63 and 0 <= k <= 111 and 0 <= a <= 111 }
I1018 08:53:57.862103 4097770 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.862111 4097770 ir_util.cc:142] shape is : 1,64,112,112
I1018 08:53:57.862123 4097770 ir_util.cc:143] indices is : i,j,k,a
I1018 08:53:57.862341 4097770 lower_tensor_group.cc:103] In store_exprs, its name is : var_47
I1018 08:53:57.862363 4097770 lower_tensor_group.cc:134] func_args is : _var_37
I1018 08:53:57.862367 4097770 lower_tensor_group.cc:134] func_args is : _var_104
I1018 08:53:57.862370 4097770 lower_tensor_group.cc:134] func_args is : _var_47
I1018 08:53:57.862407 4097770 lowered_func.cc:230] Function used 3 buffers
I1018 08:53:57.862468 4097770 op_lowering_impl.cc:420] Lower op: elementwise_mul, get 1 LoweredFunc:
I1018 08:53:57.862473 4097770 op_lowering_impl.cc:423] function fn_elementwise_mul_13 (_var_37, _var_104, _var_47)
{
  ScheduleBlock(root_14)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 112)
        {
          serial for (a, 0, 112)
          {
            ScheduleBlock(var_47)
            {
              i0_15, i1_12, i2_12, i3_12 = axis.bind(0, j, k, a)
              var_47[i0_15, i1_12, i2_12, i3_12] = (var_37[i0_15, i1_12, i2_12, i3_12] * var_104[i0_15, i1_12, i2_12, i3_12])
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.862498 4097770 op_lowering_impl.cc:450] Do op schedule
I1018 08:53:57.862507 4097770 ir_schedule_pe.cc:116] Before IRInjectiveSchedule, new ir is : {
  ScheduleBlock(root_14)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 112)
        {
          serial for (a, 0, 112)
          {
            ScheduleBlock(var_47)
            {
              i0_15, i1_12, i2_12, i3_12 = axis.bind(0, j, k, a)
              var_47[i0_15, i1_12, i2_12, i3_12] = (var_37[i0_15, i1_12, i2_12, i3_12] * var_104[i0_15, i1_12, i2_12, i3_12])
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.862524 4097770 ir_analyzer.cc:101] it_expr is : {
  ScheduleBlock(root_14)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 112)
        {
          serial for (a, 0, 112)
          {
            ScheduleBlock(var_47)
            {
              i0_15, i1_12, i2_12, i3_12 = axis.bind(0, j, k, a)
              var_47[i0_15, i1_12, i2_12, i3_12] = (var_37[i0_15, i1_12, i2_12, i3_12] * var_104[i0_15, i1_12, i2_12, i3_12])
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.862540 4097770 loop_transformation.cc:688] Before FlattenLoops, ir is:
serial for (i, 0, 1)
{
  serial for (j, 0, 64)
  {
    serial for (k, 0, 112)
    {
      serial for (a, 0, 112)
      {
        ScheduleBlock(var_47)
        {
          i0_15, i1_12, i2_12, i3_12 = axis.bind(0, j, k, a)
          var_47[i0_15, i1_12, i2_12, i3_12] = (var_37[i0_15, i1_12, i2_12, i3_12] * var_104[i0_15, i1_12, i2_12, i3_12])
        }
      }
    }
  }
}
I1018 08:53:57.862588 4097770 loop_transformation.cc:864] After FlattenLoops, ir is:
serial for (flat_i, 0, 802816)
{
  ScheduleBlock(var_47)
  {
    _flat_i = axis.bind(flat_i)
    var_47[_flat_i] = (var_37[_flat_i] * var_104[_flat_i])
  }
}
I1018 08:53:57.862601 4097770 ir_schedule_pe.cc:155] After IRInjectiveSchedule, new ir is : {
  ScheduleBlock(root_14)
  {
    serial for (flat_i, 0, 802816)
    {
      ScheduleBlock(var_47)
      {
        _flat_i = axis.bind(flat_i)
        var_47[_flat_i] = (var_37[_flat_i] * var_104[_flat_i])
      }
    }
  }
}
I1018 08:53:57.862614 4097770 op_lowering_impl.cc:463] After op schedule: {
  ScheduleBlock(root_14)
  {
    serial for (flat_i, 0, 802816)
    {
      ScheduleBlock(var_47)
      {
        _flat_i = axis.bind(flat_i)
        var_47[_flat_i] = (var_37[_flat_i] * var_104[_flat_i])
      }
    }
  }
}
I1018 08:53:57.862666 4097770 broadcast.cc:267] broadcast out shape: 1, 64, 112, 112
I1018 08:53:57.862672 4097770 broadcast.cc:268] broadcast_axes shape: 0, 1, 2, 3
I1018 08:53:57.862681 4097770 op_lowering_impl.cc:373] Do lower with Compute, op: broadcast_to
I1018 08:53:57.862702 4097770 ir_simplify.cc:467] Begin Simplify 1
I1018 08:53:57.862708 4097770 ir_simplify.cc:467] Begin Simplify 64
I1018 08:53:57.862713 4097770 ir_simplify.cc:467] Begin Simplify 112
I1018 08:53:57.862717 4097770 ir_simplify.cc:467] Begin Simplify 112
I1018 08:53:57.862723 4097770 compute.cc:209] tensor var_105's domain is : 1, 64, 112, 112
I1018 08:53:57.862761 4097770 tensor.cc:259] name:var_105, domain: []->{ var_105[i, j, k, a]: 0 <= i <= 0 and 0 <= j <= 63 and 0 <= k <= 111 and 0 <= a <= 111 }
I1018 08:53:57.862789 4097770 domain.cc:65] isl::set []->{ var_105[i, j, k, a]: 0 <= i <= 0 and 0 <= j <= 63 and 0 <= k <= 111 and 0 <= a <= 111 }
I1018 08:53:57.863526 4097770 tensor.cc:259] name:var_23, domain: []->{ var_23[]:  }
I1018 08:53:57.863535 4097770 domain.cc:65] isl::set []->{ var_23[]:  }
I1018 08:53:57.863600 4097770 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.863606 4097770 ir_util.cc:142] shape is : 1,64,112,112
I1018 08:53:57.863616 4097770 ir_util.cc:143] indices is : i,j,k,a
I1018 08:53:57.863821 4097770 lower_tensor_group.cc:103] In store_exprs, its name is : var_105
I1018 08:53:57.863839 4097770 lower_tensor_group.cc:134] func_args is : _var_23
I1018 08:53:57.863842 4097770 lower_tensor_group.cc:134] func_args is : _var_105
I1018 08:53:57.863874 4097770 lowered_func.cc:230] Function used 2 buffers
I1018 08:53:57.863914 4097770 op_lowering_impl.cc:420] Lower op: broadcast_to, get 1 LoweredFunc:
I1018 08:53:57.863917 4097770 op_lowering_impl.cc:423] function fn_broadcast_to_14 (_var_23, _var_105)
{
  ScheduleBlock(root_15)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 112)
        {
          serial for (a, 0, 112)
          {
            ScheduleBlock(var_105)
            {
              i0_16, i1_13, i2_13, i3_13 = axis.bind(0, j, k, a)
              var_105[i0_16, i1_13, i2_13, i3_13] = var_23[0, i1_13, 0, 0]
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.863941 4097770 op_lowering_impl.cc:450] Do op schedule
I1018 08:53:57.863950 4097770 ir_schedule_pe.cc:116] Before IRInjectiveSchedule, new ir is : {
  ScheduleBlock(root_15)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 112)
        {
          serial for (a, 0, 112)
          {
            ScheduleBlock(var_105)
            {
              i0_16, i1_13, i2_13, i3_13 = axis.bind(0, j, k, a)
              var_105[i0_16, i1_13, i2_13, i3_13] = var_23[0, i1_13, 0, 0]
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.863965 4097770 ir_analyzer.cc:101] it_expr is : {
  ScheduleBlock(root_15)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 112)
        {
          serial for (a, 0, 112)
          {
            ScheduleBlock(var_105)
            {
              i0_16, i1_13, i2_13, i3_13 = axis.bind(0, j, k, a)
              var_105[i0_16, i1_13, i2_13, i3_13] = var_23[0, i1_13, 0, 0]
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.863982 4097770 loop_transformation.cc:688] Before FlattenLoops, ir is:
serial for (i, 0, 1)
{
  serial for (j, 0, 64)
  {
    serial for (k, 0, 112)
    {
      serial for (a, 0, 112)
      {
        ScheduleBlock(var_105)
        {
          i0_16, i1_13, i2_13, i3_13 = axis.bind(0, j, k, a)
          var_105[i0_16, i1_13, i2_13, i3_13] = var_23[0, i1_13, 0, 0]
        }
      }
    }
  }
}
I1018 08:53:57.864027 4097770 ir.cc:629] Begin Load::index IndiceToAbsOffset of tensor: var_23
I1018 08:53:57.864032 4097770 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.864034 4097770 ir_util.cc:142] shape is : 1,64,1,1
I1018 08:53:57.864049 4097770 ir_util.cc:143] indices is : 0,((_flat_i % 802816) / 12544),0,0
I1018 08:53:57.864133 4097770 loop_transformation.cc:864] After FlattenLoops, ir is:
serial for (flat_i, 0, 802816)
{
  ScheduleBlock(var_105)
  {
    _flat_i = axis.bind(flat_i)
    var_105[_flat_i] = var_23[((_flat_i % 802816) / 12544)]
  }
}
I1018 08:53:57.864149 4097770 ir_schedule_pe.cc:155] After IRInjectiveSchedule, new ir is : {
  ScheduleBlock(root_15)
  {
    serial for (flat_i, 0, 802816)
    {
      ScheduleBlock(var_105)
      {
        _flat_i = axis.bind(flat_i)
        var_105[_flat_i] = var_23[((_flat_i % 802816) / 12544)]
      }
    }
  }
}
I1018 08:53:57.864163 4097770 op_lowering_impl.cc:463] After op schedule: {
  ScheduleBlock(root_15)
  {
    serial for (flat_i, 0, 802816)
    {
      ScheduleBlock(var_105)
      {
        _flat_i = axis.bind(flat_i)
        var_105[_flat_i] = var_23[((_flat_i % 802816) / 12544)]
      }
    }
  }
}
I1018 08:53:57.864203 4097770 broadcast.cc:267] broadcast out shape: 1, 64, 112, 112
I1018 08:53:57.864208 4097770 broadcast.cc:268] broadcast_axes shape: 0, 1, 2, 3
I1018 08:53:57.864217 4097770 op_lowering_impl.cc:373] Do lower with Compute, op: broadcast_to
I1018 08:53:57.864238 4097770 ir_simplify.cc:467] Begin Simplify 1
I1018 08:53:57.864243 4097770 ir_simplify.cc:467] Begin Simplify 64
I1018 08:53:57.864248 4097770 ir_simplify.cc:467] Begin Simplify 112
I1018 08:53:57.864253 4097770 ir_simplify.cc:467] Begin Simplify 112
I1018 08:53:57.864259 4097770 compute.cc:209] tensor var_73's domain is : 1, 64, 112, 112
I1018 08:53:57.864288 4097770 tensor.cc:259] name:var_19, domain: []->{ var_19[]:  }
I1018 08:53:57.864295 4097770 domain.cc:65] isl::set []->{ var_19[]:  }
I1018 08:53:57.864351 4097770 tensor.cc:259] name:var_73, domain: []->{ var_73[i, j, k, a]: 0 <= i <= 0 and 0 <= j <= 63 and 0 <= k <= 111 and 0 <= a <= 111 }
I1018 08:53:57.864377 4097770 domain.cc:65] isl::set []->{ var_73[i, j, k, a]: 0 <= i <= 0 and 0 <= j <= 63 and 0 <= k <= 111 and 0 <= a <= 111 }
I1018 08:53:57.865118 4097770 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.865126 4097770 ir_util.cc:142] shape is : 1,64,112,112
I1018 08:53:57.865136 4097770 ir_util.cc:143] indices is : i,j,k,a
I1018 08:53:57.865337 4097770 lower_tensor_group.cc:103] In store_exprs, its name is : var_73
I1018 08:53:57.865357 4097770 lower_tensor_group.cc:134] func_args is : _var_19
I1018 08:53:57.865361 4097770 lower_tensor_group.cc:134] func_args is : _var_73
I1018 08:53:57.865391 4097770 lowered_func.cc:230] Function used 2 buffers
I1018 08:53:57.865439 4097770 op_lowering_impl.cc:420] Lower op: broadcast_to, get 1 LoweredFunc:
I1018 08:53:57.865443 4097770 op_lowering_impl.cc:423] function fn_broadcast_to_16 (_var_19, _var_73)
{
  ScheduleBlock(root_16)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 112)
        {
          serial for (a, 0, 112)
          {
            ScheduleBlock(var_73)
            {
              i0_17, i1_14, i2_14, i3_14 = axis.bind(0, j, k, a)
              var_73[i0_17, i1_14, i2_14, i3_14] = var_19[0, i1_14, 0, 0]
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.865468 4097770 op_lowering_impl.cc:450] Do op schedule
I1018 08:53:57.865478 4097770 ir_schedule_pe.cc:116] Before IRInjectiveSchedule, new ir is : {
  ScheduleBlock(root_16)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 112)
        {
          serial for (a, 0, 112)
          {
            ScheduleBlock(var_73)
            {
              i0_17, i1_14, i2_14, i3_14 = axis.bind(0, j, k, a)
              var_73[i0_17, i1_14, i2_14, i3_14] = var_19[0, i1_14, 0, 0]
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.865494 4097770 ir_analyzer.cc:101] it_expr is : {
  ScheduleBlock(root_16)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 112)
        {
          serial for (a, 0, 112)
          {
            ScheduleBlock(var_73)
            {
              i0_17, i1_14, i2_14, i3_14 = axis.bind(0, j, k, a)
              var_73[i0_17, i1_14, i2_14, i3_14] = var_19[0, i1_14, 0, 0]
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.865515 4097770 loop_transformation.cc:688] Before FlattenLoops, ir is:
serial for (i, 0, 1)
{
  serial for (j, 0, 64)
  {
    serial for (k, 0, 112)
    {
      serial for (a, 0, 112)
      {
        ScheduleBlock(var_73)
        {
          i0_17, i1_14, i2_14, i3_14 = axis.bind(0, j, k, a)
          var_73[i0_17, i1_14, i2_14, i3_14] = var_19[0, i1_14, 0, 0]
        }
      }
    }
  }
}
I1018 08:53:57.865558 4097770 ir.cc:629] Begin Load::index IndiceToAbsOffset of tensor: var_19
I1018 08:53:57.865563 4097770 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.865566 4097770 ir_util.cc:142] shape is : 1,64,1,1
I1018 08:53:57.865576 4097770 ir_util.cc:143] indices is : 0,((_flat_i % 802816) / 12544),0,0
I1018 08:53:57.865658 4097770 loop_transformation.cc:864] After FlattenLoops, ir is:
serial for (flat_i, 0, 802816)
{
  ScheduleBlock(var_73)
  {
    _flat_i = axis.bind(flat_i)
    var_73[_flat_i] = var_19[((_flat_i % 802816) / 12544)]
  }
}
I1018 08:53:57.865674 4097770 ir_schedule_pe.cc:155] After IRInjectiveSchedule, new ir is : {
  ScheduleBlock(root_16)
  {
    serial for (flat_i, 0, 802816)
    {
      ScheduleBlock(var_73)
      {
        _flat_i = axis.bind(flat_i)
        var_73[_flat_i] = var_19[((_flat_i % 802816) / 12544)]
      }
    }
  }
}
I1018 08:53:57.865689 4097770 op_lowering_impl.cc:463] After op schedule: {
  ScheduleBlock(root_16)
  {
    serial for (flat_i, 0, 802816)
    {
      ScheduleBlock(var_73)
      {
        _flat_i = axis.bind(flat_i)
        var_73[_flat_i] = var_19[((_flat_i % 802816) / 12544)]
      }
    }
  }
}
I1018 08:53:57.865746 4097770 op_lowering_impl.cc:373] Do lower with Compute, op: elementwise_mul
I1018 08:53:57.865834 4097770 ir_simplify.cc:467] Begin Simplify 1
I1018 08:53:57.865841 4097770 ir_simplify.cc:467] Begin Simplify 64
I1018 08:53:57.865846 4097770 ir_simplify.cc:467] Begin Simplify 112
I1018 08:53:57.865850 4097770 ir_simplify.cc:467] Begin Simplify 112
I1018 08:53:57.865856 4097770 compute.cc:209] tensor var_49's domain is : 1, 64, 112, 112
I1018 08:53:57.865898 4097770 tensor.cc:259] name:var_105, domain: []->{ var_105[]:  }
I1018 08:53:57.865904 4097770 domain.cc:65] isl::set []->{ var_105[]:  }
I1018 08:53:57.865955 4097770 tensor.cc:259] name:var_47, domain: []->{ var_47[]:  }
I1018 08:53:57.865960 4097770 domain.cc:65] isl::set []->{ var_47[]:  }
I1018 08:53:57.866005 4097770 tensor.cc:259] name:var_49, domain: []->{ var_49[i, j, k, a]: 0 <= i <= 0 and 0 <= j <= 63 and 0 <= k <= 111 and 0 <= a <= 111 }
I1018 08:53:57.866034 4097770 domain.cc:65] isl::set []->{ var_49[i, j, k, a]: 0 <= i <= 0 and 0 <= j <= 63 and 0 <= k <= 111 and 0 <= a <= 111 }
I1018 08:53:57.866791 4097770 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.866798 4097770 ir_util.cc:142] shape is : 1,64,112,112
I1018 08:53:57.866809 4097770 ir_util.cc:143] indices is : i,j,k,a
I1018 08:53:57.867028 4097770 lower_tensor_group.cc:103] In store_exprs, its name is : var_49
I1018 08:53:57.867050 4097770 lower_tensor_group.cc:134] func_args is : _var_105
I1018 08:53:57.867054 4097770 lower_tensor_group.cc:134] func_args is : _var_47
I1018 08:53:57.867058 4097770 lower_tensor_group.cc:134] func_args is : _var_49
I1018 08:53:57.867097 4097770 lowered_func.cc:230] Function used 3 buffers
I1018 08:53:57.867151 4097770 op_lowering_impl.cc:420] Lower op: elementwise_mul, get 1 LoweredFunc:
I1018 08:53:57.867154 4097770 op_lowering_impl.cc:423] function fn_elementwise_mul_15 (_var_105, _var_47, _var_49)
{
  ScheduleBlock(root_17)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 112)
        {
          serial for (a, 0, 112)
          {
            ScheduleBlock(var_49)
            {
              i0_18, i1_15, i2_15, i3_15 = axis.bind(0, j, k, a)
              var_49[i0_18, i1_15, i2_15, i3_15] = (var_105[i0_18, i1_15, i2_15, i3_15] * var_47[i0_18, i1_15, i2_15, i3_15])
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.867185 4097770 op_lowering_impl.cc:450] Do op schedule
I1018 08:53:57.867194 4097770 ir_schedule_pe.cc:116] Before IRInjectiveSchedule, new ir is : {
  ScheduleBlock(root_17)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 112)
        {
          serial for (a, 0, 112)
          {
            ScheduleBlock(var_49)
            {
              i0_18, i1_15, i2_15, i3_15 = axis.bind(0, j, k, a)
              var_49[i0_18, i1_15, i2_15, i3_15] = (var_105[i0_18, i1_15, i2_15, i3_15] * var_47[i0_18, i1_15, i2_15, i3_15])
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.867210 4097770 ir_analyzer.cc:101] it_expr is : {
  ScheduleBlock(root_17)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 112)
        {
          serial for (a, 0, 112)
          {
            ScheduleBlock(var_49)
            {
              i0_18, i1_15, i2_15, i3_15 = axis.bind(0, j, k, a)
              var_49[i0_18, i1_15, i2_15, i3_15] = (var_105[i0_18, i1_15, i2_15, i3_15] * var_47[i0_18, i1_15, i2_15, i3_15])
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.867226 4097770 loop_transformation.cc:688] Before FlattenLoops, ir is:
serial for (i, 0, 1)
{
  serial for (j, 0, 64)
  {
    serial for (k, 0, 112)
    {
      serial for (a, 0, 112)
      {
        ScheduleBlock(var_49)
        {
          i0_18, i1_15, i2_15, i3_15 = axis.bind(0, j, k, a)
          var_49[i0_18, i1_15, i2_15, i3_15] = (var_105[i0_18, i1_15, i2_15, i3_15] * var_47[i0_18, i1_15, i2_15, i3_15])
        }
      }
    }
  }
}
I1018 08:53:57.867276 4097770 loop_transformation.cc:864] After FlattenLoops, ir is:
serial for (flat_i, 0, 802816)
{
  ScheduleBlock(var_49)
  {
    _flat_i = axis.bind(flat_i)
    var_49[_flat_i] = (var_105[_flat_i] * var_47[_flat_i])
  }
}
I1018 08:53:57.867288 4097770 ir_schedule_pe.cc:155] After IRInjectiveSchedule, new ir is : {
  ScheduleBlock(root_17)
  {
    serial for (flat_i, 0, 802816)
    {
      ScheduleBlock(var_49)
      {
        _flat_i = axis.bind(flat_i)
        var_49[_flat_i] = (var_105[_flat_i] * var_47[_flat_i])
      }
    }
  }
}
I1018 08:53:57.867301 4097770 op_lowering_impl.cc:463] After op schedule: {
  ScheduleBlock(root_17)
  {
    serial for (flat_i, 0, 802816)
    {
      ScheduleBlock(var_49)
      {
        _flat_i = axis.bind(flat_i)
        var_49[_flat_i] = (var_105[_flat_i] * var_47[_flat_i])
      }
    }
  }
}
I1018 08:53:57.867364 4097770 op_lowering_impl.cc:373] Do lower with Compute, op: elementwise_add
I1018 08:53:57.867447 4097770 ir_simplify.cc:467] Begin Simplify 1
I1018 08:53:57.867455 4097770 ir_simplify.cc:467] Begin Simplify 64
I1018 08:53:57.867460 4097770 ir_simplify.cc:467] Begin Simplify 112
I1018 08:53:57.867465 4097770 ir_simplify.cc:467] Begin Simplify 112
I1018 08:53:57.867470 4097770 compute.cc:209] tensor var_51's domain is : 1, 64, 112, 112
I1018 08:53:57.867508 4097770 tensor.cc:259] name:var_49, domain: []->{ var_49[]:  }
I1018 08:53:57.867514 4097770 domain.cc:65] isl::set []->{ var_49[]:  }
I1018 08:53:57.867571 4097770 tensor.cc:259] name:var_51, domain: []->{ var_51[i, j, k, a]: 0 <= i <= 0 and 0 <= j <= 63 and 0 <= k <= 111 and 0 <= a <= 111 }
I1018 08:53:57.867599 4097770 domain.cc:65] isl::set []->{ var_51[i, j, k, a]: 0 <= i <= 0 and 0 <= j <= 63 and 0 <= k <= 111 and 0 <= a <= 111 }
I1018 08:53:57.868310 4097770 tensor.cc:259] name:var_73, domain: []->{ var_73[]:  }
I1018 08:53:57.868319 4097770 domain.cc:65] isl::set []->{ var_73[]:  }
I1018 08:53:57.868388 4097770 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.868394 4097770 ir_util.cc:142] shape is : 1,64,112,112
I1018 08:53:57.868404 4097770 ir_util.cc:143] indices is : i,j,k,a
I1018 08:53:57.868623 4097770 lower_tensor_group.cc:103] In store_exprs, its name is : var_51
I1018 08:53:57.868646 4097770 lower_tensor_group.cc:134] func_args is : _var_49
I1018 08:53:57.868655 4097770 lower_tensor_group.cc:134] func_args is : _var_73
I1018 08:53:57.868659 4097770 lower_tensor_group.cc:134] func_args is : _var_51
I1018 08:53:57.868695 4097770 lowered_func.cc:230] Function used 3 buffers
I1018 08:53:57.868752 4097770 op_lowering_impl.cc:420] Lower op: elementwise_add, get 1 LoweredFunc:
I1018 08:53:57.868757 4097770 op_lowering_impl.cc:423] function fn_elementwise_add_17 (_var_49, _var_73, _var_51)
{
  ScheduleBlock(root_18)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 112)
        {
          serial for (a, 0, 112)
          {
            ScheduleBlock(var_51)
            {
              i0_19, i1_16, i2_16, i3_16 = axis.bind(0, j, k, a)
              var_51[i0_19, i1_16, i2_16, i3_16] = (var_49[i0_19, i1_16, i2_16, i3_16] + var_73[i0_19, i1_16, i2_16, i3_16])
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.868781 4097770 op_lowering_impl.cc:450] Do op schedule
I1018 08:53:57.868790 4097770 ir_schedule_pe.cc:116] Before IRInjectiveSchedule, new ir is : {
  ScheduleBlock(root_18)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 112)
        {
          serial for (a, 0, 112)
          {
            ScheduleBlock(var_51)
            {
              i0_19, i1_16, i2_16, i3_16 = axis.bind(0, j, k, a)
              var_51[i0_19, i1_16, i2_16, i3_16] = (var_49[i0_19, i1_16, i2_16, i3_16] + var_73[i0_19, i1_16, i2_16, i3_16])
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.868805 4097770 ir_analyzer.cc:101] it_expr is : {
  ScheduleBlock(root_18)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 112)
        {
          serial for (a, 0, 112)
          {
            ScheduleBlock(var_51)
            {
              i0_19, i1_16, i2_16, i3_16 = axis.bind(0, j, k, a)
              var_51[i0_19, i1_16, i2_16, i3_16] = (var_49[i0_19, i1_16, i2_16, i3_16] + var_73[i0_19, i1_16, i2_16, i3_16])
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.868821 4097770 loop_transformation.cc:688] Before FlattenLoops, ir is:
serial for (i, 0, 1)
{
  serial for (j, 0, 64)
  {
    serial for (k, 0, 112)
    {
      serial for (a, 0, 112)
      {
        ScheduleBlock(var_51)
        {
          i0_19, i1_16, i2_16, i3_16 = axis.bind(0, j, k, a)
          var_51[i0_19, i1_16, i2_16, i3_16] = (var_49[i0_19, i1_16, i2_16, i3_16] + var_73[i0_19, i1_16, i2_16, i3_16])
        }
      }
    }
  }
}
I1018 08:53:57.868868 4097770 loop_transformation.cc:864] After FlattenLoops, ir is:
serial for (flat_i, 0, 802816)
{
  ScheduleBlock(var_51)
  {
    _flat_i = axis.bind(flat_i)
    var_51[_flat_i] = (var_49[_flat_i] + var_73[_flat_i])
  }
}
I1018 08:53:57.868881 4097770 ir_schedule_pe.cc:155] After IRInjectiveSchedule, new ir is : {
  ScheduleBlock(root_18)
  {
    serial for (flat_i, 0, 802816)
    {
      ScheduleBlock(var_51)
      {
        _flat_i = axis.bind(flat_i)
        var_51[_flat_i] = (var_49[_flat_i] + var_73[_flat_i])
      }
    }
  }
}
I1018 08:53:57.868893 4097770 op_lowering_impl.cc:463] After op schedule: {
  ScheduleBlock(root_18)
  {
    serial for (flat_i, 0, 802816)
    {
      ScheduleBlock(var_51)
      {
        _flat_i = axis.bind(flat_i)
        var_51[_flat_i] = (var_49[_flat_i] + var_73[_flat_i])
      }
    }
  }
}
I1018 08:53:57.868919 4097770 op_lowering_impl.cc:373] Do lower with Compute, op: fill_constant
I1018 08:53:57.868934 4097770 ir_simplify.cc:467] Begin Simplify 1
I1018 08:53:57.868940 4097770 ir_simplify.cc:467] Begin Simplify 64
I1018 08:53:57.868945 4097770 ir_simplify.cc:467] Begin Simplify 112
I1018 08:53:57.868950 4097770 ir_simplify.cc:467] Begin Simplify 112
I1018 08:53:57.868955 4097770 compute.cc:209] tensor var_106's domain is : 1, 64, 112, 112
I1018 08:53:57.868983 4097770 tensor.cc:259] name:var_106, domain: []->{ var_106[i, j, k, a]: 0 <= i <= 0 and 0 <= j <= 63 and 0 <= k <= 111 and 0 <= a <= 111 }
I1018 08:53:57.869009 4097770 domain.cc:65] isl::set []->{ var_106[i, j, k, a]: 0 <= i <= 0 and 0 <= j <= 63 and 0 <= k <= 111 and 0 <= a <= 111 }
I1018 08:53:57.869786 4097770 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.869796 4097770 ir_util.cc:142] shape is : 1,64,112,112
I1018 08:53:57.869807 4097770 ir_util.cc:143] indices is : i,j,k,a
I1018 08:53:57.869995 4097770 lower_tensor_group.cc:103] In store_exprs, its name is : var_106
I1018 08:53:57.870011 4097770 lower_tensor_group.cc:134] func_args is : _var_106
I1018 08:53:57.870038 4097770 lowered_func.cc:230] Function used 1 buffers
I1018 08:53:57.870072 4097770 op_lowering_impl.cc:420] Lower op: fill_constant, get 1 LoweredFunc:
I1018 08:53:57.870076 4097770 op_lowering_impl.cc:423] function fn_fill_constant_18 (_var_106)
{
  ScheduleBlock(root_19)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 112)
        {
          serial for (a, 0, 112)
          {
            ScheduleBlock(var_106)
            {
              i0_20, i1_17, i2_17, i3_17 = axis.bind(0, j, k, a)
              var_106[i0_20, i1_17, i2_17, i3_17] = float32(0.0000000000000000)
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.870105 4097770 op_lowering_impl.cc:450] Do op schedule
I1018 08:53:57.870113 4097770 ir_schedule_pe.cc:70] Before IRElementwiseSchedule, new ir is : {
  ScheduleBlock(root_19)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 112)
        {
          serial for (a, 0, 112)
          {
            ScheduleBlock(var_106)
            {
              i0_20, i1_17, i2_17, i3_17 = axis.bind(0, j, k, a)
              var_106[i0_20, i1_17, i2_17, i3_17] = float32(0.0000000000000000)
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.870131 4097770 ir_analyzer.cc:101] it_expr is : {
  ScheduleBlock(root_19)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 112)
        {
          serial for (a, 0, 112)
          {
            ScheduleBlock(var_106)
            {
              i0_20, i1_17, i2_17, i3_17 = axis.bind(0, j, k, a)
              var_106[i0_20, i1_17, i2_17, i3_17] = float32(0.0000000000000000)
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.870148 4097770 loop_transformation.cc:688] Before FlattenLoops, ir is:
serial for (i, 0, 1)
{
  serial for (j, 0, 64)
  {
    serial for (k, 0, 112)
    {
      serial for (a, 0, 112)
      {
        ScheduleBlock(var_106)
        {
          i0_20, i1_17, i2_17, i3_17 = axis.bind(0, j, k, a)
          var_106[i0_20, i1_17, i2_17, i3_17] = float32(0.0000000000000000)
        }
      }
    }
  }
}
I1018 08:53:57.870193 4097770 loop_transformation.cc:864] After FlattenLoops, ir is:
serial for (flat_i, 0, 802816)
{
  ScheduleBlock(var_106)
  {
    _flat_i = axis.bind(flat_i)
    var_106[_flat_i] = float32(0.0000000000000000)
  }
}
I1018 08:53:57.870208 4097770 ir_schedule_pe.cc:109] After IRElementwiseSchedule, new ir is : {
  ScheduleBlock(root_19)
  {
    serial for (flat_i, 0, 802816)
    {
      ScheduleBlock(var_106)
      {
        _flat_i = axis.bind(flat_i)
        var_106[_flat_i] = float32(0.0000000000000000)
      }
    }
  }
}
I1018 08:53:57.870222 4097770 op_lowering_impl.cc:463] After op schedule: {
  ScheduleBlock(root_19)
  {
    serial for (flat_i, 0, 802816)
    {
      ScheduleBlock(var_106)
      {
        _flat_i = axis.bind(flat_i)
        var_106[_flat_i] = float32(0.0000000000000000)
      }
    }
  }
}
I1018 08:53:57.870285 4097770 op_lowering_impl.cc:373] Do lower with Compute, op: max
I1018 08:53:57.870370 4097770 ir_simplify.cc:467] Begin Simplify 1
I1018 08:53:57.870378 4097770 ir_simplify.cc:467] Begin Simplify 64
I1018 08:53:57.870383 4097770 ir_simplify.cc:467] Begin Simplify 112
I1018 08:53:57.870386 4097770 ir_simplify.cc:467] Begin Simplify 112
I1018 08:53:57.870393 4097770 compute.cc:209] tensor var_53's domain is : 1, 64, 112, 112
I1018 08:53:57.870429 4097770 tensor.cc:259] name:var_106, domain: []->{ var_106[]:  }
I1018 08:53:57.870441 4097770 domain.cc:65] isl::set []->{ var_106[]:  }
I1018 08:53:57.870491 4097770 tensor.cc:259] name:var_51, domain: []->{ var_51[]:  }
I1018 08:53:57.870497 4097770 domain.cc:65] isl::set []->{ var_51[]:  }
I1018 08:53:57.870546 4097770 tensor.cc:259] name:var_53, domain: []->{ var_53[i, j, k, a]: 0 <= i <= 0 and 0 <= j <= 63 and 0 <= k <= 111 and 0 <= a <= 111 }
I1018 08:53:57.870573 4097770 domain.cc:65] isl::set []->{ var_53[i, j, k, a]: 0 <= i <= 0 and 0 <= j <= 63 and 0 <= k <= 111 and 0 <= a <= 111 }
I1018 08:53:57.871337 4097770 ir_util.cc:141] Begin IndiceToAbsOffset
I1018 08:53:57.871344 4097770 ir_util.cc:142] shape is : 1,64,112,112
I1018 08:53:57.871356 4097770 ir_util.cc:143] indices is : i,j,k,a
I1018 08:53:57.871570 4097770 lower_tensor_group.cc:103] In store_exprs, its name is : var_53
I1018 08:53:57.871593 4097770 lower_tensor_group.cc:134] func_args is : _var_51
I1018 08:53:57.871598 4097770 lower_tensor_group.cc:134] func_args is : _var_106
I1018 08:53:57.871600 4097770 lower_tensor_group.cc:134] func_args is : _var_53
I1018 08:53:57.871639 4097770 lowered_func.cc:230] Function used 3 buffers
I1018 08:53:57.871692 4097770 op_lowering_impl.cc:420] Lower op: max, get 1 LoweredFunc:
I1018 08:53:57.871696 4097770 op_lowering_impl.cc:423] function fn_max_19 (_var_51, _var_106, _var_53)
{
  ScheduleBlock(root_20)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 112)
        {
          serial for (a, 0, 112)
          {
            ScheduleBlock(var_53)
            {
              i0_21, i1_18, i2_18, i3_18 = axis.bind(0, j, k, a)
              var_53[i0_21, i1_18, i2_18, i3_18] = cinn_max(var_51[i0_21, i1_18, i2_18, i3_18], var_106[i0_21, i1_18, i2_18, i3_18])
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.871721 4097770 op_lowering_impl.cc:450] Do op schedule
I1018 08:53:57.871730 4097770 ir_schedule_pe.cc:116] Before IRInjectiveSchedule, new ir is : {
  ScheduleBlock(root_20)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 112)
        {
          serial for (a, 0, 112)
          {
            ScheduleBlock(var_53)
            {
              i0_21, i1_18, i2_18, i3_18 = axis.bind(0, j, k, a)
              var_53[i0_21, i1_18, i2_18, i3_18] = cinn_max(var_51[i0_21, i1_18, i2_18, i3_18], var_106[i0_21, i1_18, i2_18, i3_18])
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.871745 4097770 ir_analyzer.cc:101] it_expr is : {
  ScheduleBlock(root_20)
  {
    serial for (i, 0, 1)
    {
      serial for (j, 0, 64)
      {
        serial for (k, 0, 112)
        {
          serial for (a, 0, 112)
          {
            ScheduleBlock(var_53)
            {
              i0_21, i1_18, i2_18, i3_18 = axis.bind(0, j, k, a)
              var_53[i0_21, i1_18, i2_18, i3_18] = cinn_max(var_51[i0_21, i1_18, i2_18, i3_18], var_106[i0_21, i1_18, i2_18, i3_18])
            }
          }
        }
      }
    }
  }
}
I1018 08:53:57.871762 4097770 loop_transformation.cc:688] Before FlattenLoops, ir is:
serial for (i, 0, 1)
{
  serial for (j, 0, 64)
  {
    serial for (k, 0, 112)
    {
      serial for (a, 0, 112)
      {
        ScheduleBlock(var_53)
        {
          i0_21, i1_18, i2_18, i3_18 = axis.bind(0, j, k, a)
          var_53[i0_21, i1_18, i2_18, i3_18] = cinn_max(var_51[i0_21, i1_18, i2_18, i3_18], var_106[i0_21, i1_18, i2_18, i3_18])
        }
      }
    }
  }
}
I1018 08:53:57.871809 4097770 loop_transformation.cc:864] After FlattenLoops, ir is:
serial for (flat_i, 0, 802816)
{
  ScheduleBlock(var_53)
  {
    _flat_i = axis.bind(flat_i)
    var_53[_flat_i] = cinn_max(var_51[_flat_i], var_106[_flat_i])
  }
}
I1018 08:53:57.871822 4097770 ir_schedule_pe.cc:155] After IRInjectiveSchedule, new ir is : {
  ScheduleBlock(root_20)
  {
    serial for (flat_i, 0, 802816)
    {
      ScheduleBlock(var_53)
      {
        _flat_i = axis.bind(flat_i)
        var_53[_flat_i] = cinn_max(var_51[_flat_i], var_106[_flat_i])
      }
    }
  }
}
I1018 08:53:57.871834 4097770 op_lowering_impl.cc:463] After op schedule: {
  ScheduleBlock(root_20)
  {
    serial for (flat_i, 0, 802816)
    {
      ScheduleBlock(var_53)
      {
        _flat_i = axis.bind(flat_i)
        var_53[_flat_i] = cinn_max(var_51[_flat_i], var_106[_flat_i])
      }
    }
  }
}
I1018 08:53:57.871865 4097770 base.cc:528] Before merging, exprs[0] is : {
  ScheduleBlock(root_1)
  {
    serial for (flat_i, 0, 64)
    {
      ScheduleBlock(var_19)
      {
        _flat_i = axis.bind(flat_i)
        var_19[_flat_i] = batch_norm2d_0__b_0[_flat_i]
      }
    }
  }
}
I1018 08:53:57.871881 4097770 base.cc:546] in merged_block, it has serial for (flat_i, 0, 64)
{
  ScheduleBlock(var_19)
  {
    _flat_i = axis.bind(flat_i)
    var_19[_flat_i] = batch_norm2d_0__b_0[_flat_i]
  }
}
I1018 08:53:57.871888 4097770 base.cc:546] in merged_block, it has serial for (flat_i, 0, 64)
{
  ScheduleBlock(var_23)
  {
    _flat_i = axis.bind(flat_i)
    var_23[_flat_i] = batch_norm2d_0__w_0[_flat_i]
  }
}
I1018 08:53:57.871896 4097770 base.cc:546] in merged_block, it has serial for (flat_i, 0, 64)
{
  ScheduleBlock(var_27)
  {
    _flat_i = axis.bind(flat_i)
    var_27[_flat_i] = batch_norm2d_0__w_1[_flat_i]
  }
}
I1018 08:53:57.871902 4097770 base.cc:546] in merged_block, it has serial for (flat_i, 0, 64)
{
  ScheduleBlock(var_31)
  {
    _flat_i = axis.bind(flat_i)
    var_31[_flat_i] = batch_norm2d_0__w_2[_flat_i]
  }
}
I1018 08:53:57.871908 4097770 base.cc:546] in merged_block, it has serial for (flat_i, 0, 802816)
{
  ScheduleBlock(var_101)
  {
    _flat_i = axis.bind(flat_i)
    var_101[_flat_i] = var_27[((_flat_i % 802816) / 12544)]
  }
}
I1018 08:53:57.871917 4097770 base.cc:546] in merged_block, it has serial for (flat_i, 0, 64)
{
  ScheduleBlock(var_39)
  {
    _flat_i = axis.bind(flat_i)
    var_39[_flat_i] = ((float32(1.00000000f) * var_31[_flat_i]) + float32(9.99999975e-06f))
  }
}
I1018 08:53:57.871935 4097770 base.cc:546] in merged_block, it has serial for (flat_i, 0, 64)
{
  ScheduleBlock(var_103)
  {
    _flat_i = axis.bind(flat_i)
    var_103[_flat_i] = float32(-0.50000000000000000)
  }
}
I1018 08:53:57.871945 4097770 base.cc:546] in merged_block, it has serial for (flat_i, 0, 64)
{
  ScheduleBlock(var_43)
  {
    _flat_i = axis.bind(flat_i)
    var_43[_flat_i] = pow(var_39[_flat_i], var_103[_flat_i])
  }
}
I1018 08:53:57.871953 4097770 base.cc:546] in merged_block, it has serial for (flat_i, 0, 802816)
{
  ScheduleBlock(var_37)
  {
    _flat_i = axis.bind(flat_i)
    var_37[_flat_i] = (conv2d_0__tmp_0[_flat_i] - var_101[_flat_i])
  }
}
I1018 08:53:57.871960 4097770 base.cc:546] in merged_block, it has serial for (flat_i, 0, 802816)
{
  ScheduleBlock(var_104)
  {
    _flat_i = axis.bind(flat_i)
    var_104[_flat_i] = var_43[((_flat_i % 802816) / 12544)]
  }
}
I1018 08:53:57.871969 4097770 base.cc:546] in merged_block, it has serial for (flat_i, 0, 802816)
{
  ScheduleBlock(var_47)
  {
    _flat_i = axis.bind(flat_i)
    var_47[_flat_i] = (var_37[_flat_i] * var_104[_flat_i])
  }
}
I1018 08:53:57.871976 4097770 base.cc:546] in merged_block, it has serial for (flat_i, 0, 802816)
{
  ScheduleBlock(var_105)
  {
    _flat_i = axis.bind(flat_i)
    var_105[_flat_i] = var_23[((_flat_i % 802816) / 12544)]
  }
}
I1018 08:53:57.871984 4097770 base.cc:546] in merged_block, it has serial for (flat_i, 0, 802816)
{
  ScheduleBlock(var_73)
  {
    _flat_i = axis.bind(flat_i)
    var_73[_flat_i] = var_19[((_flat_i % 802816) / 12544)]
  }
}
I1018 08:53:57.871992 4097770 base.cc:546] in merged_block, it has serial for (flat_i, 0, 802816)
{
  ScheduleBlock(var_49)
  {
    _flat_i = axis.bind(flat_i)
    var_49[_flat_i] = (var_105[_flat_i] * var_47[_flat_i])
  }
}
I1018 08:53:57.871999 4097770 base.cc:546] in merged_block, it has serial for (flat_i, 0, 802816)
{
  ScheduleBlock(var_51)
  {
    _flat_i = axis.bind(flat_i)
    var_51[_flat_i] = (var_49[_flat_i] + var_73[_flat_i])
  }
}
I1018 08:53:57.872006 4097770 base.cc:546] in merged_block, it has serial for (flat_i, 0, 802816)
{
  ScheduleBlock(var_106)
  {
    _flat_i = axis.bind(flat_i)
    var_106[_flat_i] = float32(0.0000000000000000)
  }
}
I1018 08:53:57.872020 4097770 base.cc:546] in merged_block, it has serial for (flat_i, 0, 802816)
{
  ScheduleBlock(var_53)
  {
    _flat_i = axis.bind(flat_i)
    var_53[_flat_i] = cinn_max(var_51[_flat_i], var_106[_flat_i])
  }
}
I1018 08:53:57.872028 4097770 base.cc:555] After merging, exprs[0] is : {
  ScheduleBlock(root_1)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_19)
        {
          _flat_i = axis.bind(flat_i)
          var_19[_flat_i] = batch_norm2d_0__b_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_23)
        {
          _flat_i = axis.bind(flat_i)
          var_23[_flat_i] = batch_norm2d_0__w_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_27)
        {
          _flat_i = axis.bind(flat_i)
          var_27[_flat_i] = batch_norm2d_0__w_1[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_31)
        {
          _flat_i = axis.bind(flat_i)
          var_31[_flat_i] = batch_norm2d_0__w_2[_flat_i]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_101)
        {
          _flat_i = axis.bind(flat_i)
          var_101[_flat_i] = var_27[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_39)
        {
          _flat_i = axis.bind(flat_i)
          var_39[_flat_i] = ((float32(1.00000000f) * var_31[_flat_i]) + float32(9.99999975e-06f))
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_103)
        {
          _flat_i = axis.bind(flat_i)
          var_103[_flat_i] = float32(-0.50000000000000000)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_43)
        {
          _flat_i = axis.bind(flat_i)
          var_43[_flat_i] = pow(var_39[_flat_i], var_103[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_37)
        {
          _flat_i = axis.bind(flat_i)
          var_37[_flat_i] = (conv2d_0__tmp_0[_flat_i] - var_101[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_104)
        {
          _flat_i = axis.bind(flat_i)
          var_104[_flat_i] = var_43[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_47)
        {
          _flat_i = axis.bind(flat_i)
          var_47[_flat_i] = (var_37[_flat_i] * var_104[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_105)
        {
          _flat_i = axis.bind(flat_i)
          var_105[_flat_i] = var_23[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_73)
        {
          _flat_i = axis.bind(flat_i)
          var_73[_flat_i] = var_19[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_49)
        {
          _flat_i = axis.bind(flat_i)
          var_49[_flat_i] = (var_105[_flat_i] * var_47[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_51)
        {
          _flat_i = axis.bind(flat_i)
          var_51[_flat_i] = (var_49[_flat_i] + var_73[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_106)
        {
          _flat_i = axis.bind(flat_i)
          var_106[_flat_i] = float32(0.0000000000000000)
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max(var_51[_flat_i], var_106[_flat_i])
        }
      }
    }
  }
}
I1018 08:53:57.872098 4097770 op_lowering_impl.cc:127] After lower, ir is: 
{
  ScheduleBlock(root_1)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_19)
        {
          _flat_i = axis.bind(flat_i)
          var_19[_flat_i] = batch_norm2d_0__b_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_23)
        {
          _flat_i = axis.bind(flat_i)
          var_23[_flat_i] = batch_norm2d_0__w_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_27)
        {
          _flat_i = axis.bind(flat_i)
          var_27[_flat_i] = batch_norm2d_0__w_1[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_31)
        {
          _flat_i = axis.bind(flat_i)
          var_31[_flat_i] = batch_norm2d_0__w_2[_flat_i]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_101)
        {
          _flat_i = axis.bind(flat_i)
          var_101[_flat_i] = var_27[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_39)
        {
          _flat_i = axis.bind(flat_i)
          var_39[_flat_i] = ((float32(1.00000000f) * var_31[_flat_i]) + float32(9.99999975e-06f))
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_103)
        {
          _flat_i = axis.bind(flat_i)
          var_103[_flat_i] = float32(-0.50000000000000000)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_43)
        {
          _flat_i = axis.bind(flat_i)
          var_43[_flat_i] = pow(var_39[_flat_i], var_103[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_37)
        {
          _flat_i = axis.bind(flat_i)
          var_37[_flat_i] = (conv2d_0__tmp_0[_flat_i] - var_101[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_104)
        {
          _flat_i = axis.bind(flat_i)
          var_104[_flat_i] = var_43[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_47)
        {
          _flat_i = axis.bind(flat_i)
          var_47[_flat_i] = (var_37[_flat_i] * var_104[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_105)
        {
          _flat_i = axis.bind(flat_i)
          var_105[_flat_i] = var_23[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_73)
        {
          _flat_i = axis.bind(flat_i)
          var_73[_flat_i] = var_19[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_49)
        {
          _flat_i = axis.bind(flat_i)
          var_49[_flat_i] = (var_105[_flat_i] * var_47[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_51)
        {
          _flat_i = axis.bind(flat_i)
          var_51[_flat_i] = (var_49[_flat_i] + var_73[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_106)
        {
          _flat_i = axis.bind(flat_i)
          var_106[_flat_i] = float32(0.0000000000000000)
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max(var_51[_flat_i], var_106[_flat_i])
        }
      }
    }
  }
}
I1018 08:53:57.872218 4097770 op_lowering_impl.cc:503] Try FUSION max
I1018 08:53:57.872224 4097770 op_lowering_impl.cc:609] Before loop fusion, ir is:
{
  ScheduleBlock(root_1)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_19)
        {
          _flat_i = axis.bind(flat_i)
          var_19[_flat_i] = batch_norm2d_0__b_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_23)
        {
          _flat_i = axis.bind(flat_i)
          var_23[_flat_i] = batch_norm2d_0__w_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_27)
        {
          _flat_i = axis.bind(flat_i)
          var_27[_flat_i] = batch_norm2d_0__w_1[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_31)
        {
          _flat_i = axis.bind(flat_i)
          var_31[_flat_i] = batch_norm2d_0__w_2[_flat_i]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_101)
        {
          _flat_i = axis.bind(flat_i)
          var_101[_flat_i] = var_27[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_39)
        {
          _flat_i = axis.bind(flat_i)
          var_39[_flat_i] = ((float32(1.00000000f) * var_31[_flat_i]) + float32(9.99999975e-06f))
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_103)
        {
          _flat_i = axis.bind(flat_i)
          var_103[_flat_i] = float32(-0.50000000000000000)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_43)
        {
          _flat_i = axis.bind(flat_i)
          var_43[_flat_i] = pow(var_39[_flat_i], var_103[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_37)
        {
          _flat_i = axis.bind(flat_i)
          var_37[_flat_i] = (conv2d_0__tmp_0[_flat_i] - var_101[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_104)
        {
          _flat_i = axis.bind(flat_i)
          var_104[_flat_i] = var_43[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_47)
        {
          _flat_i = axis.bind(flat_i)
          var_47[_flat_i] = (var_37[_flat_i] * var_104[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_105)
        {
          _flat_i = axis.bind(flat_i)
          var_105[_flat_i] = var_23[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_73)
        {
          _flat_i = axis.bind(flat_i)
          var_73[_flat_i] = var_19[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_49)
        {
          _flat_i = axis.bind(flat_i)
          var_49[_flat_i] = (var_105[_flat_i] * var_47[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_51)
        {
          _flat_i = axis.bind(flat_i)
          var_51[_flat_i] = (var_49[_flat_i] + var_73[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_106)
        {
          _flat_i = axis.bind(flat_i)
          var_106[_flat_i] = float32(0.0000000000000000)
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max(var_51[_flat_i], var_106[_flat_i])
        }
      }
    }
  }
}
I1018 08:53:57.872284 4097770 op_lowering_impl.cc:618] After loop fusion, ir is:
{
  ScheduleBlock(root_1)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_19)
        {
          _flat_i = axis.bind(flat_i)
          var_19[_flat_i] = batch_norm2d_0__b_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_23)
        {
          _flat_i = axis.bind(flat_i)
          var_23[_flat_i] = batch_norm2d_0__w_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_27)
        {
          _flat_i = axis.bind(flat_i)
          var_27[_flat_i] = batch_norm2d_0__w_1[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_31)
        {
          _flat_i = axis.bind(flat_i)
          var_31[_flat_i] = batch_norm2d_0__w_2[_flat_i]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_101)
        {
          _flat_i = axis.bind(flat_i)
          var_101[_flat_i] = var_27[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_39)
        {
          _flat_i = axis.bind(flat_i)
          var_39[_flat_i] = ((float32(1.00000000f) * var_31[_flat_i]) + float32(9.99999975e-06f))
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_103)
        {
          _flat_i = axis.bind(flat_i)
          var_103[_flat_i] = float32(-0.50000000000000000)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_43)
        {
          _flat_i = axis.bind(flat_i)
          var_43[_flat_i] = pow(var_39[_flat_i], var_103[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_37)
        {
          _flat_i = axis.bind(flat_i)
          var_37[_flat_i] = (conv2d_0__tmp_0[_flat_i] - var_101[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_104)
        {
          _flat_i = axis.bind(flat_i)
          var_104[_flat_i] = var_43[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_47)
        {
          _flat_i = axis.bind(flat_i)
          var_47[_flat_i] = (var_37[_flat_i] * var_104[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_105)
        {
          _flat_i = axis.bind(flat_i)
          var_105[_flat_i] = var_23[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_73)
        {
          _flat_i = axis.bind(flat_i)
          var_73[_flat_i] = var_19[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_49)
        {
          _flat_i = axis.bind(flat_i)
          var_49[_flat_i] = (var_105[_flat_i] * var_47[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_51)
        {
          _flat_i = axis.bind(flat_i)
          var_51[_flat_i] = (var_49[_flat_i] + var_73[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_106)
        {
          _flat_i = axis.bind(flat_i)
          var_106[_flat_i] = float32(0.0000000000000000)
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max(var_51[_flat_i], var_106[_flat_i])
        }
      }
    }
  }
}
I1018 08:53:57.872340 4097770 op_lowering_impl.cc:503] Try FUSION fill_constant
I1018 08:53:57.872346 4097770 op_lowering_impl.cc:525] Before compute inline, ir is:
{
  ScheduleBlock(root_1)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_19)
        {
          _flat_i = axis.bind(flat_i)
          var_19[_flat_i] = batch_norm2d_0__b_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_23)
        {
          _flat_i = axis.bind(flat_i)
          var_23[_flat_i] = batch_norm2d_0__w_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_27)
        {
          _flat_i = axis.bind(flat_i)
          var_27[_flat_i] = batch_norm2d_0__w_1[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_31)
        {
          _flat_i = axis.bind(flat_i)
          var_31[_flat_i] = batch_norm2d_0__w_2[_flat_i]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_101)
        {
          _flat_i = axis.bind(flat_i)
          var_101[_flat_i] = var_27[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_39)
        {
          _flat_i = axis.bind(flat_i)
          var_39[_flat_i] = ((float32(1.00000000f) * var_31[_flat_i]) + float32(9.99999975e-06f))
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_103)
        {
          _flat_i = axis.bind(flat_i)
          var_103[_flat_i] = float32(-0.50000000000000000)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_43)
        {
          _flat_i = axis.bind(flat_i)
          var_43[_flat_i] = pow(var_39[_flat_i], var_103[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_37)
        {
          _flat_i = axis.bind(flat_i)
          var_37[_flat_i] = (conv2d_0__tmp_0[_flat_i] - var_101[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_104)
        {
          _flat_i = axis.bind(flat_i)
          var_104[_flat_i] = var_43[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_47)
        {
          _flat_i = axis.bind(flat_i)
          var_47[_flat_i] = (var_37[_flat_i] * var_104[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_105)
        {
          _flat_i = axis.bind(flat_i)
          var_105[_flat_i] = var_23[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_73)
        {
          _flat_i = axis.bind(flat_i)
          var_73[_flat_i] = var_19[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_49)
        {
          _flat_i = axis.bind(flat_i)
          var_49[_flat_i] = (var_105[_flat_i] * var_47[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_51)
        {
          _flat_i = axis.bind(flat_i)
          var_51[_flat_i] = (var_49[_flat_i] + var_73[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_106)
        {
          _flat_i = axis.bind(flat_i)
          var_106[_flat_i] = float32(0.0000000000000000)
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max(var_51[_flat_i], var_106[_flat_i])
        }
      }
    }
  }
}
I1018 08:53:57.872889 4097770 op_lowering_impl.cc:546] After compute inline, ir is:
{
  ScheduleBlock(root_1)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_19)
        {
          _flat_i = axis.bind(flat_i)
          var_19[_flat_i] = batch_norm2d_0__b_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_23)
        {
          _flat_i = axis.bind(flat_i)
          var_23[_flat_i] = batch_norm2d_0__w_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_27)
        {
          _flat_i = axis.bind(flat_i)
          var_27[_flat_i] = batch_norm2d_0__w_1[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_31)
        {
          _flat_i = axis.bind(flat_i)
          var_31[_flat_i] = batch_norm2d_0__w_2[_flat_i]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_101)
        {
          _flat_i = axis.bind(flat_i)
          var_101[_flat_i] = var_27[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_39)
        {
          _flat_i = axis.bind(flat_i)
          var_39[_flat_i] = ((float32(1.00000000f) * var_31[_flat_i]) + float32(9.99999975e-06f))
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_103)
        {
          _flat_i = axis.bind(flat_i)
          var_103[_flat_i] = float32(-0.50000000000000000)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_43)
        {
          _flat_i = axis.bind(flat_i)
          var_43[_flat_i] = pow(var_39[_flat_i], var_103[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_37)
        {
          _flat_i = axis.bind(flat_i)
          var_37[_flat_i] = (conv2d_0__tmp_0[_flat_i] - var_101[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_104)
        {
          _flat_i = axis.bind(flat_i)
          var_104[_flat_i] = var_43[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_47)
        {
          _flat_i = axis.bind(flat_i)
          var_47[_flat_i] = (var_37[_flat_i] * var_104[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_105)
        {
          _flat_i = axis.bind(flat_i)
          var_105[_flat_i] = var_23[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_73)
        {
          _flat_i = axis.bind(flat_i)
          var_73[_flat_i] = var_19[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_49)
        {
          _flat_i = axis.bind(flat_i)
          var_49[_flat_i] = (var_105[_flat_i] * var_47[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_51)
        {
          _flat_i = axis.bind(flat_i)
          var_51[_flat_i] = (var_49[_flat_i] + var_73[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max(var_51[_flat_i], float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.872962 4097770 op_lowering_impl.cc:503] Try FUSION elementwise_add
I1018 08:53:57.872967 4097770 op_lowering_impl.cc:525] Before compute inline, ir is:
{
  ScheduleBlock(root_1)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_19)
        {
          _flat_i = axis.bind(flat_i)
          var_19[_flat_i] = batch_norm2d_0__b_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_23)
        {
          _flat_i = axis.bind(flat_i)
          var_23[_flat_i] = batch_norm2d_0__w_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_27)
        {
          _flat_i = axis.bind(flat_i)
          var_27[_flat_i] = batch_norm2d_0__w_1[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_31)
        {
          _flat_i = axis.bind(flat_i)
          var_31[_flat_i] = batch_norm2d_0__w_2[_flat_i]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_101)
        {
          _flat_i = axis.bind(flat_i)
          var_101[_flat_i] = var_27[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_39)
        {
          _flat_i = axis.bind(flat_i)
          var_39[_flat_i] = ((float32(1.00000000f) * var_31[_flat_i]) + float32(9.99999975e-06f))
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_103)
        {
          _flat_i = axis.bind(flat_i)
          var_103[_flat_i] = float32(-0.50000000000000000)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_43)
        {
          _flat_i = axis.bind(flat_i)
          var_43[_flat_i] = pow(var_39[_flat_i], var_103[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_37)
        {
          _flat_i = axis.bind(flat_i)
          var_37[_flat_i] = (conv2d_0__tmp_0[_flat_i] - var_101[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_104)
        {
          _flat_i = axis.bind(flat_i)
          var_104[_flat_i] = var_43[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_47)
        {
          _flat_i = axis.bind(flat_i)
          var_47[_flat_i] = (var_37[_flat_i] * var_104[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_105)
        {
          _flat_i = axis.bind(flat_i)
          var_105[_flat_i] = var_23[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_73)
        {
          _flat_i = axis.bind(flat_i)
          var_73[_flat_i] = var_19[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_49)
        {
          _flat_i = axis.bind(flat_i)
          var_49[_flat_i] = (var_105[_flat_i] * var_47[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_51)
        {
          _flat_i = axis.bind(flat_i)
          var_51[_flat_i] = (var_49[_flat_i] + var_73[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max(var_51[_flat_i], float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.873459 4097770 op_lowering_impl.cc:546] After compute inline, ir is:
{
  ScheduleBlock(root_1)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_19)
        {
          _flat_i = axis.bind(flat_i)
          var_19[_flat_i] = batch_norm2d_0__b_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_23)
        {
          _flat_i = axis.bind(flat_i)
          var_23[_flat_i] = batch_norm2d_0__w_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_27)
        {
          _flat_i = axis.bind(flat_i)
          var_27[_flat_i] = batch_norm2d_0__w_1[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_31)
        {
          _flat_i = axis.bind(flat_i)
          var_31[_flat_i] = batch_norm2d_0__w_2[_flat_i]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_101)
        {
          _flat_i = axis.bind(flat_i)
          var_101[_flat_i] = var_27[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_39)
        {
          _flat_i = axis.bind(flat_i)
          var_39[_flat_i] = ((float32(1.00000000f) * var_31[_flat_i]) + float32(9.99999975e-06f))
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_103)
        {
          _flat_i = axis.bind(flat_i)
          var_103[_flat_i] = float32(-0.50000000000000000)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_43)
        {
          _flat_i = axis.bind(flat_i)
          var_43[_flat_i] = pow(var_39[_flat_i], var_103[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_37)
        {
          _flat_i = axis.bind(flat_i)
          var_37[_flat_i] = (conv2d_0__tmp_0[_flat_i] - var_101[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_104)
        {
          _flat_i = axis.bind(flat_i)
          var_104[_flat_i] = var_43[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_47)
        {
          _flat_i = axis.bind(flat_i)
          var_47[_flat_i] = (var_37[_flat_i] * var_104[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_105)
        {
          _flat_i = axis.bind(flat_i)
          var_105[_flat_i] = var_23[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_73)
        {
          _flat_i = axis.bind(flat_i)
          var_73[_flat_i] = var_19[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_49)
        {
          _flat_i = axis.bind(flat_i)
          var_49[_flat_i] = (var_105[_flat_i] * var_47[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max((var_49[_flat_i] + var_73[_flat_i]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.873533 4097770 op_lowering_impl.cc:503] Try FUSION broadcast_to
I1018 08:53:57.873540 4097770 op_lowering_impl.cc:525] Before compute inline, ir is:
{
  ScheduleBlock(root_1)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_19)
        {
          _flat_i = axis.bind(flat_i)
          var_19[_flat_i] = batch_norm2d_0__b_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_23)
        {
          _flat_i = axis.bind(flat_i)
          var_23[_flat_i] = batch_norm2d_0__w_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_27)
        {
          _flat_i = axis.bind(flat_i)
          var_27[_flat_i] = batch_norm2d_0__w_1[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_31)
        {
          _flat_i = axis.bind(flat_i)
          var_31[_flat_i] = batch_norm2d_0__w_2[_flat_i]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_101)
        {
          _flat_i = axis.bind(flat_i)
          var_101[_flat_i] = var_27[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_39)
        {
          _flat_i = axis.bind(flat_i)
          var_39[_flat_i] = ((float32(1.00000000f) * var_31[_flat_i]) + float32(9.99999975e-06f))
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_103)
        {
          _flat_i = axis.bind(flat_i)
          var_103[_flat_i] = float32(-0.50000000000000000)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_43)
        {
          _flat_i = axis.bind(flat_i)
          var_43[_flat_i] = pow(var_39[_flat_i], var_103[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_37)
        {
          _flat_i = axis.bind(flat_i)
          var_37[_flat_i] = (conv2d_0__tmp_0[_flat_i] - var_101[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_104)
        {
          _flat_i = axis.bind(flat_i)
          var_104[_flat_i] = var_43[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_47)
        {
          _flat_i = axis.bind(flat_i)
          var_47[_flat_i] = (var_37[_flat_i] * var_104[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_105)
        {
          _flat_i = axis.bind(flat_i)
          var_105[_flat_i] = var_23[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_73)
        {
          _flat_i = axis.bind(flat_i)
          var_73[_flat_i] = var_19[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_49)
        {
          _flat_i = axis.bind(flat_i)
          var_49[_flat_i] = (var_105[_flat_i] * var_47[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max((var_49[_flat_i] + var_73[_flat_i]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.873953 4097770 op_lowering_impl.cc:546] After compute inline, ir is:
{
  ScheduleBlock(root_1)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_19)
        {
          _flat_i = axis.bind(flat_i)
          var_19[_flat_i] = batch_norm2d_0__b_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_23)
        {
          _flat_i = axis.bind(flat_i)
          var_23[_flat_i] = batch_norm2d_0__w_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_27)
        {
          _flat_i = axis.bind(flat_i)
          var_27[_flat_i] = batch_norm2d_0__w_1[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_31)
        {
          _flat_i = axis.bind(flat_i)
          var_31[_flat_i] = batch_norm2d_0__w_2[_flat_i]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_101)
        {
          _flat_i = axis.bind(flat_i)
          var_101[_flat_i] = var_27[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_39)
        {
          _flat_i = axis.bind(flat_i)
          var_39[_flat_i] = ((float32(1.00000000f) * var_31[_flat_i]) + float32(9.99999975e-06f))
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_103)
        {
          _flat_i = axis.bind(flat_i)
          var_103[_flat_i] = float32(-0.50000000000000000)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_43)
        {
          _flat_i = axis.bind(flat_i)
          var_43[_flat_i] = pow(var_39[_flat_i], var_103[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_37)
        {
          _flat_i = axis.bind(flat_i)
          var_37[_flat_i] = (conv2d_0__tmp_0[_flat_i] - var_101[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_104)
        {
          _flat_i = axis.bind(flat_i)
          var_104[_flat_i] = var_43[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_47)
        {
          _flat_i = axis.bind(flat_i)
          var_47[_flat_i] = (var_37[_flat_i] * var_104[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_105)
        {
          _flat_i = axis.bind(flat_i)
          var_105[_flat_i] = var_23[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_49)
        {
          _flat_i = axis.bind(flat_i)
          var_49[_flat_i] = (var_105[_flat_i] * var_47[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max((var_49[_flat_i] + var_19[((_flat_i % 802816) / 12544)]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.874022 4097770 op_lowering_impl.cc:503] Try FUSION elementwise_mul
I1018 08:53:57.874028 4097770 op_lowering_impl.cc:525] Before compute inline, ir is:
{
  ScheduleBlock(root_1)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_19)
        {
          _flat_i = axis.bind(flat_i)
          var_19[_flat_i] = batch_norm2d_0__b_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_23)
        {
          _flat_i = axis.bind(flat_i)
          var_23[_flat_i] = batch_norm2d_0__w_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_27)
        {
          _flat_i = axis.bind(flat_i)
          var_27[_flat_i] = batch_norm2d_0__w_1[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_31)
        {
          _flat_i = axis.bind(flat_i)
          var_31[_flat_i] = batch_norm2d_0__w_2[_flat_i]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_101)
        {
          _flat_i = axis.bind(flat_i)
          var_101[_flat_i] = var_27[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_39)
        {
          _flat_i = axis.bind(flat_i)
          var_39[_flat_i] = ((float32(1.00000000f) * var_31[_flat_i]) + float32(9.99999975e-06f))
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_103)
        {
          _flat_i = axis.bind(flat_i)
          var_103[_flat_i] = float32(-0.50000000000000000)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_43)
        {
          _flat_i = axis.bind(flat_i)
          var_43[_flat_i] = pow(var_39[_flat_i], var_103[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_37)
        {
          _flat_i = axis.bind(flat_i)
          var_37[_flat_i] = (conv2d_0__tmp_0[_flat_i] - var_101[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_104)
        {
          _flat_i = axis.bind(flat_i)
          var_104[_flat_i] = var_43[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_47)
        {
          _flat_i = axis.bind(flat_i)
          var_47[_flat_i] = (var_37[_flat_i] * var_104[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_105)
        {
          _flat_i = axis.bind(flat_i)
          var_105[_flat_i] = var_23[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_49)
        {
          _flat_i = axis.bind(flat_i)
          var_49[_flat_i] = (var_105[_flat_i] * var_47[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max((var_49[_flat_i] + var_19[((_flat_i % 802816) / 12544)]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.874450 4097770 op_lowering_impl.cc:546] After compute inline, ir is:
{
  ScheduleBlock(root_1)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_19)
        {
          _flat_i = axis.bind(flat_i)
          var_19[_flat_i] = batch_norm2d_0__b_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_23)
        {
          _flat_i = axis.bind(flat_i)
          var_23[_flat_i] = batch_norm2d_0__w_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_27)
        {
          _flat_i = axis.bind(flat_i)
          var_27[_flat_i] = batch_norm2d_0__w_1[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_31)
        {
          _flat_i = axis.bind(flat_i)
          var_31[_flat_i] = batch_norm2d_0__w_2[_flat_i]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_101)
        {
          _flat_i = axis.bind(flat_i)
          var_101[_flat_i] = var_27[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_39)
        {
          _flat_i = axis.bind(flat_i)
          var_39[_flat_i] = ((float32(1.00000000f) * var_31[_flat_i]) + float32(9.99999975e-06f))
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_103)
        {
          _flat_i = axis.bind(flat_i)
          var_103[_flat_i] = float32(-0.50000000000000000)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_43)
        {
          _flat_i = axis.bind(flat_i)
          var_43[_flat_i] = pow(var_39[_flat_i], var_103[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_37)
        {
          _flat_i = axis.bind(flat_i)
          var_37[_flat_i] = (conv2d_0__tmp_0[_flat_i] - var_101[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_104)
        {
          _flat_i = axis.bind(flat_i)
          var_104[_flat_i] = var_43[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_47)
        {
          _flat_i = axis.bind(flat_i)
          var_47[_flat_i] = (var_37[_flat_i] * var_104[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_105)
        {
          _flat_i = axis.bind(flat_i)
          var_105[_flat_i] = var_23[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max(((var_105[_flat_i] * var_47[_flat_i]) + var_19[((_flat_i % 802816) / 12544)]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.874518 4097770 op_lowering_impl.cc:503] Try FUSION elementwise_mul
I1018 08:53:57.874526 4097770 op_lowering_impl.cc:525] Before compute inline, ir is:
{
  ScheduleBlock(root_1)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_19)
        {
          _flat_i = axis.bind(flat_i)
          var_19[_flat_i] = batch_norm2d_0__b_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_23)
        {
          _flat_i = axis.bind(flat_i)
          var_23[_flat_i] = batch_norm2d_0__w_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_27)
        {
          _flat_i = axis.bind(flat_i)
          var_27[_flat_i] = batch_norm2d_0__w_1[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_31)
        {
          _flat_i = axis.bind(flat_i)
          var_31[_flat_i] = batch_norm2d_0__w_2[_flat_i]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_101)
        {
          _flat_i = axis.bind(flat_i)
          var_101[_flat_i] = var_27[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_39)
        {
          _flat_i = axis.bind(flat_i)
          var_39[_flat_i] = ((float32(1.00000000f) * var_31[_flat_i]) + float32(9.99999975e-06f))
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_103)
        {
          _flat_i = axis.bind(flat_i)
          var_103[_flat_i] = float32(-0.50000000000000000)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_43)
        {
          _flat_i = axis.bind(flat_i)
          var_43[_flat_i] = pow(var_39[_flat_i], var_103[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_37)
        {
          _flat_i = axis.bind(flat_i)
          var_37[_flat_i] = (conv2d_0__tmp_0[_flat_i] - var_101[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_104)
        {
          _flat_i = axis.bind(flat_i)
          var_104[_flat_i] = var_43[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_47)
        {
          _flat_i = axis.bind(flat_i)
          var_47[_flat_i] = (var_37[_flat_i] * var_104[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_105)
        {
          _flat_i = axis.bind(flat_i)
          var_105[_flat_i] = var_23[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max(((var_105[_flat_i] * var_47[_flat_i]) + var_19[((_flat_i % 802816) / 12544)]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.874876 4097770 op_lowering_impl.cc:546] After compute inline, ir is:
{
  ScheduleBlock(root_1)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_19)
        {
          _flat_i = axis.bind(flat_i)
          var_19[_flat_i] = batch_norm2d_0__b_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_23)
        {
          _flat_i = axis.bind(flat_i)
          var_23[_flat_i] = batch_norm2d_0__w_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_27)
        {
          _flat_i = axis.bind(flat_i)
          var_27[_flat_i] = batch_norm2d_0__w_1[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_31)
        {
          _flat_i = axis.bind(flat_i)
          var_31[_flat_i] = batch_norm2d_0__w_2[_flat_i]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_101)
        {
          _flat_i = axis.bind(flat_i)
          var_101[_flat_i] = var_27[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_39)
        {
          _flat_i = axis.bind(flat_i)
          var_39[_flat_i] = ((float32(1.00000000f) * var_31[_flat_i]) + float32(9.99999975e-06f))
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_103)
        {
          _flat_i = axis.bind(flat_i)
          var_103[_flat_i] = float32(-0.50000000000000000)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_43)
        {
          _flat_i = axis.bind(flat_i)
          var_43[_flat_i] = pow(var_39[_flat_i], var_103[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_37)
        {
          _flat_i = axis.bind(flat_i)
          var_37[_flat_i] = (conv2d_0__tmp_0[_flat_i] - var_101[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_104)
        {
          _flat_i = axis.bind(flat_i)
          var_104[_flat_i] = var_43[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_105)
        {
          _flat_i = axis.bind(flat_i)
          var_105[_flat_i] = var_23[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max(((var_105[_flat_i] * (var_37[_flat_i] * var_104[_flat_i])) + var_19[((_flat_i % 802816) / 12544)]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.874935 4097770 op_lowering_impl.cc:503] Try FUSION broadcast_to
I1018 08:53:57.874941 4097770 op_lowering_impl.cc:525] Before compute inline, ir is:
{
  ScheduleBlock(root_1)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_19)
        {
          _flat_i = axis.bind(flat_i)
          var_19[_flat_i] = batch_norm2d_0__b_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_23)
        {
          _flat_i = axis.bind(flat_i)
          var_23[_flat_i] = batch_norm2d_0__w_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_27)
        {
          _flat_i = axis.bind(flat_i)
          var_27[_flat_i] = batch_norm2d_0__w_1[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_31)
        {
          _flat_i = axis.bind(flat_i)
          var_31[_flat_i] = batch_norm2d_0__w_2[_flat_i]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_101)
        {
          _flat_i = axis.bind(flat_i)
          var_101[_flat_i] = var_27[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_39)
        {
          _flat_i = axis.bind(flat_i)
          var_39[_flat_i] = ((float32(1.00000000f) * var_31[_flat_i]) + float32(9.99999975e-06f))
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_103)
        {
          _flat_i = axis.bind(flat_i)
          var_103[_flat_i] = float32(-0.50000000000000000)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_43)
        {
          _flat_i = axis.bind(flat_i)
          var_43[_flat_i] = pow(var_39[_flat_i], var_103[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_37)
        {
          _flat_i = axis.bind(flat_i)
          var_37[_flat_i] = (conv2d_0__tmp_0[_flat_i] - var_101[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_104)
        {
          _flat_i = axis.bind(flat_i)
          var_104[_flat_i] = var_43[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_105)
        {
          _flat_i = axis.bind(flat_i)
          var_105[_flat_i] = var_23[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max(((var_105[_flat_i] * (var_37[_flat_i] * var_104[_flat_i])) + var_19[((_flat_i % 802816) / 12544)]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.875263 4097770 op_lowering_impl.cc:546] After compute inline, ir is:
{
  ScheduleBlock(root_1)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_19)
        {
          _flat_i = axis.bind(flat_i)
          var_19[_flat_i] = batch_norm2d_0__b_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_23)
        {
          _flat_i = axis.bind(flat_i)
          var_23[_flat_i] = batch_norm2d_0__w_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_27)
        {
          _flat_i = axis.bind(flat_i)
          var_27[_flat_i] = batch_norm2d_0__w_1[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_31)
        {
          _flat_i = axis.bind(flat_i)
          var_31[_flat_i] = batch_norm2d_0__w_2[_flat_i]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_101)
        {
          _flat_i = axis.bind(flat_i)
          var_101[_flat_i] = var_27[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_39)
        {
          _flat_i = axis.bind(flat_i)
          var_39[_flat_i] = ((float32(1.00000000f) * var_31[_flat_i]) + float32(9.99999975e-06f))
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_103)
        {
          _flat_i = axis.bind(flat_i)
          var_103[_flat_i] = float32(-0.50000000000000000)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_43)
        {
          _flat_i = axis.bind(flat_i)
          var_43[_flat_i] = pow(var_39[_flat_i], var_103[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_37)
        {
          _flat_i = axis.bind(flat_i)
          var_37[_flat_i] = (conv2d_0__tmp_0[_flat_i] - var_101[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_104)
        {
          _flat_i = axis.bind(flat_i)
          var_104[_flat_i] = var_43[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max(((var_23[((_flat_i % 802816) / 12544)] * (var_37[_flat_i] * var_104[_flat_i])) + var_19[((_flat_i % 802816) / 12544)]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.875322 4097770 op_lowering_impl.cc:503] Try FUSION broadcast_to
I1018 08:53:57.875329 4097770 op_lowering_impl.cc:525] Before compute inline, ir is:
{
  ScheduleBlock(root_1)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_19)
        {
          _flat_i = axis.bind(flat_i)
          var_19[_flat_i] = batch_norm2d_0__b_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_23)
        {
          _flat_i = axis.bind(flat_i)
          var_23[_flat_i] = batch_norm2d_0__w_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_27)
        {
          _flat_i = axis.bind(flat_i)
          var_27[_flat_i] = batch_norm2d_0__w_1[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_31)
        {
          _flat_i = axis.bind(flat_i)
          var_31[_flat_i] = batch_norm2d_0__w_2[_flat_i]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_101)
        {
          _flat_i = axis.bind(flat_i)
          var_101[_flat_i] = var_27[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_39)
        {
          _flat_i = axis.bind(flat_i)
          var_39[_flat_i] = ((float32(1.00000000f) * var_31[_flat_i]) + float32(9.99999975e-06f))
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_103)
        {
          _flat_i = axis.bind(flat_i)
          var_103[_flat_i] = float32(-0.50000000000000000)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_43)
        {
          _flat_i = axis.bind(flat_i)
          var_43[_flat_i] = pow(var_39[_flat_i], var_103[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_37)
        {
          _flat_i = axis.bind(flat_i)
          var_37[_flat_i] = (conv2d_0__tmp_0[_flat_i] - var_101[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_104)
        {
          _flat_i = axis.bind(flat_i)
          var_104[_flat_i] = var_43[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max(((var_23[((_flat_i % 802816) / 12544)] * (var_37[_flat_i] * var_104[_flat_i])) + var_19[((_flat_i % 802816) / 12544)]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.875638 4097770 op_lowering_impl.cc:546] After compute inline, ir is:
{
  ScheduleBlock(root_1)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_19)
        {
          _flat_i = axis.bind(flat_i)
          var_19[_flat_i] = batch_norm2d_0__b_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_23)
        {
          _flat_i = axis.bind(flat_i)
          var_23[_flat_i] = batch_norm2d_0__w_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_27)
        {
          _flat_i = axis.bind(flat_i)
          var_27[_flat_i] = batch_norm2d_0__w_1[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_31)
        {
          _flat_i = axis.bind(flat_i)
          var_31[_flat_i] = batch_norm2d_0__w_2[_flat_i]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_101)
        {
          _flat_i = axis.bind(flat_i)
          var_101[_flat_i] = var_27[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_39)
        {
          _flat_i = axis.bind(flat_i)
          var_39[_flat_i] = ((float32(1.00000000f) * var_31[_flat_i]) + float32(9.99999975e-06f))
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_103)
        {
          _flat_i = axis.bind(flat_i)
          var_103[_flat_i] = float32(-0.50000000000000000)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_43)
        {
          _flat_i = axis.bind(flat_i)
          var_43[_flat_i] = pow(var_39[_flat_i], var_103[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_37)
        {
          _flat_i = axis.bind(flat_i)
          var_37[_flat_i] = (conv2d_0__tmp_0[_flat_i] - var_101[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max(((var_23[((_flat_i % 802816) / 12544)] * (var_37[_flat_i] * var_43[((_flat_i % 802816) / 12544)])) + var_19[((_flat_i % 802816) / 12544)]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.875689 4097770 op_lowering_impl.cc:503] Try FUSION subtract
I1018 08:53:57.875695 4097770 op_lowering_impl.cc:525] Before compute inline, ir is:
{
  ScheduleBlock(root_1)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_19)
        {
          _flat_i = axis.bind(flat_i)
          var_19[_flat_i] = batch_norm2d_0__b_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_23)
        {
          _flat_i = axis.bind(flat_i)
          var_23[_flat_i] = batch_norm2d_0__w_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_27)
        {
          _flat_i = axis.bind(flat_i)
          var_27[_flat_i] = batch_norm2d_0__w_1[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_31)
        {
          _flat_i = axis.bind(flat_i)
          var_31[_flat_i] = batch_norm2d_0__w_2[_flat_i]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_101)
        {
          _flat_i = axis.bind(flat_i)
          var_101[_flat_i] = var_27[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_39)
        {
          _flat_i = axis.bind(flat_i)
          var_39[_flat_i] = ((float32(1.00000000f) * var_31[_flat_i]) + float32(9.99999975e-06f))
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_103)
        {
          _flat_i = axis.bind(flat_i)
          var_103[_flat_i] = float32(-0.50000000000000000)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_43)
        {
          _flat_i = axis.bind(flat_i)
          var_43[_flat_i] = pow(var_39[_flat_i], var_103[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_37)
        {
          _flat_i = axis.bind(flat_i)
          var_37[_flat_i] = (conv2d_0__tmp_0[_flat_i] - var_101[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max(((var_23[((_flat_i % 802816) / 12544)] * (var_37[_flat_i] * var_43[((_flat_i % 802816) / 12544)])) + var_19[((_flat_i % 802816) / 12544)]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.875967 4097770 op_lowering_impl.cc:546] After compute inline, ir is:
{
  ScheduleBlock(root_1)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_19)
        {
          _flat_i = axis.bind(flat_i)
          var_19[_flat_i] = batch_norm2d_0__b_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_23)
        {
          _flat_i = axis.bind(flat_i)
          var_23[_flat_i] = batch_norm2d_0__w_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_27)
        {
          _flat_i = axis.bind(flat_i)
          var_27[_flat_i] = batch_norm2d_0__w_1[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_31)
        {
          _flat_i = axis.bind(flat_i)
          var_31[_flat_i] = batch_norm2d_0__w_2[_flat_i]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_101)
        {
          _flat_i = axis.bind(flat_i)
          var_101[_flat_i] = var_27[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_39)
        {
          _flat_i = axis.bind(flat_i)
          var_39[_flat_i] = ((float32(1.00000000f) * var_31[_flat_i]) + float32(9.99999975e-06f))
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_103)
        {
          _flat_i = axis.bind(flat_i)
          var_103[_flat_i] = float32(-0.50000000000000000)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_43)
        {
          _flat_i = axis.bind(flat_i)
          var_43[_flat_i] = pow(var_39[_flat_i], var_103[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max(((var_23[((_flat_i % 802816) / 12544)] * ((conv2d_0__tmp_0[_flat_i] - var_101[_flat_i]) * var_43[((_flat_i % 802816) / 12544)])) + var_19[((_flat_i % 802816) / 12544)]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.876016 4097770 op_lowering_impl.cc:503] Try FUSION broadcast_to
I1018 08:53:57.876026 4097770 op_lowering_impl.cc:525] Before compute inline, ir is:
{
  ScheduleBlock(root_1)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_19)
        {
          _flat_i = axis.bind(flat_i)
          var_19[_flat_i] = batch_norm2d_0__b_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_23)
        {
          _flat_i = axis.bind(flat_i)
          var_23[_flat_i] = batch_norm2d_0__w_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_27)
        {
          _flat_i = axis.bind(flat_i)
          var_27[_flat_i] = batch_norm2d_0__w_1[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_31)
        {
          _flat_i = axis.bind(flat_i)
          var_31[_flat_i] = batch_norm2d_0__w_2[_flat_i]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_101)
        {
          _flat_i = axis.bind(flat_i)
          var_101[_flat_i] = var_27[((_flat_i % 802816) / 12544)]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_39)
        {
          _flat_i = axis.bind(flat_i)
          var_39[_flat_i] = ((float32(1.00000000f) * var_31[_flat_i]) + float32(9.99999975e-06f))
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_103)
        {
          _flat_i = axis.bind(flat_i)
          var_103[_flat_i] = float32(-0.50000000000000000)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_43)
        {
          _flat_i = axis.bind(flat_i)
          var_43[_flat_i] = pow(var_39[_flat_i], var_103[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max(((var_23[((_flat_i % 802816) / 12544)] * ((conv2d_0__tmp_0[_flat_i] - var_101[_flat_i]) * var_43[((_flat_i % 802816) / 12544)])) + var_19[((_flat_i % 802816) / 12544)]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.876194 4097770 op_lowering_impl.cc:546] After compute inline, ir is:
{
  ScheduleBlock(root_1)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_19)
        {
          _flat_i = axis.bind(flat_i)
          var_19[_flat_i] = batch_norm2d_0__b_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_23)
        {
          _flat_i = axis.bind(flat_i)
          var_23[_flat_i] = batch_norm2d_0__w_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_27)
        {
          _flat_i = axis.bind(flat_i)
          var_27[_flat_i] = batch_norm2d_0__w_1[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_31)
        {
          _flat_i = axis.bind(flat_i)
          var_31[_flat_i] = batch_norm2d_0__w_2[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_39)
        {
          _flat_i = axis.bind(flat_i)
          var_39[_flat_i] = ((float32(1.00000000f) * var_31[_flat_i]) + float32(9.99999975e-06f))
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_103)
        {
          _flat_i = axis.bind(flat_i)
          var_103[_flat_i] = float32(-0.50000000000000000)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_43)
        {
          _flat_i = axis.bind(flat_i)
          var_43[_flat_i] = pow(var_39[_flat_i], var_103[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max(((var_23[((_flat_i % 802816) / 12544)] * ((conv2d_0__tmp_0[_flat_i] - var_27[((_flat_i % 802816) / 12544)]) * var_43[((_flat_i % 802816) / 12544)])) + var_19[((_flat_i % 802816) / 12544)]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.876240 4097770 op_lowering_impl.cc:503] Try FUSION reshape
I1018 08:53:57.876247 4097770 op_lowering_impl.cc:525] Before compute inline, ir is:
{
  ScheduleBlock(root_1)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_19)
        {
          _flat_i = axis.bind(flat_i)
          var_19[_flat_i] = batch_norm2d_0__b_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_23)
        {
          _flat_i = axis.bind(flat_i)
          var_23[_flat_i] = batch_norm2d_0__w_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_27)
        {
          _flat_i = axis.bind(flat_i)
          var_27[_flat_i] = batch_norm2d_0__w_1[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_31)
        {
          _flat_i = axis.bind(flat_i)
          var_31[_flat_i] = batch_norm2d_0__w_2[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_39)
        {
          _flat_i = axis.bind(flat_i)
          var_39[_flat_i] = ((float32(1.00000000f) * var_31[_flat_i]) + float32(9.99999975e-06f))
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_103)
        {
          _flat_i = axis.bind(flat_i)
          var_103[_flat_i] = float32(-0.50000000000000000)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_43)
        {
          _flat_i = axis.bind(flat_i)
          var_43[_flat_i] = pow(var_39[_flat_i], var_103[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max(((var_23[((_flat_i % 802816) / 12544)] * ((conv2d_0__tmp_0[_flat_i] - var_27[((_flat_i % 802816) / 12544)]) * var_43[((_flat_i % 802816) / 12544)])) + var_19[((_flat_i % 802816) / 12544)]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.876344 4097770 op_lowering_impl.cc:546] After compute inline, ir is:
{
  ScheduleBlock(root_1)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_23)
        {
          _flat_i = axis.bind(flat_i)
          var_23[_flat_i] = batch_norm2d_0__w_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_27)
        {
          _flat_i = axis.bind(flat_i)
          var_27[_flat_i] = batch_norm2d_0__w_1[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_31)
        {
          _flat_i = axis.bind(flat_i)
          var_31[_flat_i] = batch_norm2d_0__w_2[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_39)
        {
          _flat_i = axis.bind(flat_i)
          var_39[_flat_i] = ((float32(1.00000000f) * var_31[_flat_i]) + float32(9.99999975e-06f))
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_103)
        {
          _flat_i = axis.bind(flat_i)
          var_103[_flat_i] = float32(-0.50000000000000000)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_43)
        {
          _flat_i = axis.bind(flat_i)
          var_43[_flat_i] = pow(var_39[_flat_i], var_103[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max(((var_23[((_flat_i % 802816) / 12544)] * ((conv2d_0__tmp_0[_flat_i] - var_27[((_flat_i % 802816) / 12544)]) * var_43[((_flat_i % 802816) / 12544)])) + batch_norm2d_0__b_0[((_flat_i % 802816) / 12544)]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.876384 4097770 op_lowering_impl.cc:503] Try FUSION reshape
I1018 08:53:57.876390 4097770 op_lowering_impl.cc:525] Before compute inline, ir is:
{
  ScheduleBlock(root_1)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_23)
        {
          _flat_i = axis.bind(flat_i)
          var_23[_flat_i] = batch_norm2d_0__w_0[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_27)
        {
          _flat_i = axis.bind(flat_i)
          var_27[_flat_i] = batch_norm2d_0__w_1[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_31)
        {
          _flat_i = axis.bind(flat_i)
          var_31[_flat_i] = batch_norm2d_0__w_2[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_39)
        {
          _flat_i = axis.bind(flat_i)
          var_39[_flat_i] = ((float32(1.00000000f) * var_31[_flat_i]) + float32(9.99999975e-06f))
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_103)
        {
          _flat_i = axis.bind(flat_i)
          var_103[_flat_i] = float32(-0.50000000000000000)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_43)
        {
          _flat_i = axis.bind(flat_i)
          var_43[_flat_i] = pow(var_39[_flat_i], var_103[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max(((var_23[((_flat_i % 802816) / 12544)] * ((conv2d_0__tmp_0[_flat_i] - var_27[((_flat_i % 802816) / 12544)]) * var_43[((_flat_i % 802816) / 12544)])) + batch_norm2d_0__b_0[((_flat_i % 802816) / 12544)]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.876488 4097770 op_lowering_impl.cc:546] After compute inline, ir is:
{
  ScheduleBlock(root_1)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_27)
        {
          _flat_i = axis.bind(flat_i)
          var_27[_flat_i] = batch_norm2d_0__w_1[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_31)
        {
          _flat_i = axis.bind(flat_i)
          var_31[_flat_i] = batch_norm2d_0__w_2[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_39)
        {
          _flat_i = axis.bind(flat_i)
          var_39[_flat_i] = ((float32(1.00000000f) * var_31[_flat_i]) + float32(9.99999975e-06f))
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_103)
        {
          _flat_i = axis.bind(flat_i)
          var_103[_flat_i] = float32(-0.50000000000000000)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_43)
        {
          _flat_i = axis.bind(flat_i)
          var_43[_flat_i] = pow(var_39[_flat_i], var_103[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max(((batch_norm2d_0__w_0[((_flat_i % 802816) / 12544)] * ((conv2d_0__tmp_0[_flat_i] - var_27[((_flat_i % 802816) / 12544)]) * var_43[((_flat_i % 802816) / 12544)])) + batch_norm2d_0__b_0[((_flat_i % 802816) / 12544)]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.876529 4097770 op_lowering_impl.cc:503] Try FUSION reshape
I1018 08:53:57.876538 4097770 op_lowering_impl.cc:525] Before compute inline, ir is:
{
  ScheduleBlock(root_1)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_27)
        {
          _flat_i = axis.bind(flat_i)
          var_27[_flat_i] = batch_norm2d_0__w_1[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_31)
        {
          _flat_i = axis.bind(flat_i)
          var_31[_flat_i] = batch_norm2d_0__w_2[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_39)
        {
          _flat_i = axis.bind(flat_i)
          var_39[_flat_i] = ((float32(1.00000000f) * var_31[_flat_i]) + float32(9.99999975e-06f))
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_103)
        {
          _flat_i = axis.bind(flat_i)
          var_103[_flat_i] = float32(-0.50000000000000000)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_43)
        {
          _flat_i = axis.bind(flat_i)
          var_43[_flat_i] = pow(var_39[_flat_i], var_103[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max(((batch_norm2d_0__w_0[((_flat_i % 802816) / 12544)] * ((conv2d_0__tmp_0[_flat_i] - var_27[((_flat_i % 802816) / 12544)]) * var_43[((_flat_i % 802816) / 12544)])) + batch_norm2d_0__b_0[((_flat_i % 802816) / 12544)]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.876629 4097770 op_lowering_impl.cc:546] After compute inline, ir is:
{
  ScheduleBlock(root_1)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_31)
        {
          _flat_i = axis.bind(flat_i)
          var_31[_flat_i] = batch_norm2d_0__w_2[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_39)
        {
          _flat_i = axis.bind(flat_i)
          var_39[_flat_i] = ((float32(1.00000000f) * var_31[_flat_i]) + float32(9.99999975e-06f))
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_103)
        {
          _flat_i = axis.bind(flat_i)
          var_103[_flat_i] = float32(-0.50000000000000000)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_43)
        {
          _flat_i = axis.bind(flat_i)
          var_43[_flat_i] = pow(var_39[_flat_i], var_103[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max(((batch_norm2d_0__w_0[((_flat_i % 802816) / 12544)] * ((conv2d_0__tmp_0[_flat_i] - batch_norm2d_0__w_1[((_flat_i % 802816) / 12544)]) * var_43[((_flat_i % 802816) / 12544)])) + batch_norm2d_0__b_0[((_flat_i % 802816) / 12544)]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.876668 4097770 op_lowering_impl.cc:503] Try FUSION pow
I1018 08:53:57.876675 4097770 op_lowering_impl.cc:525] Before compute inline, ir is:
{
  ScheduleBlock(root_1)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_31)
        {
          _flat_i = axis.bind(flat_i)
          var_31[_flat_i] = batch_norm2d_0__w_2[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_39)
        {
          _flat_i = axis.bind(flat_i)
          var_39[_flat_i] = ((float32(1.00000000f) * var_31[_flat_i]) + float32(9.99999975e-06f))
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_103)
        {
          _flat_i = axis.bind(flat_i)
          var_103[_flat_i] = float32(-0.50000000000000000)
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_43)
        {
          _flat_i = axis.bind(flat_i)
          var_43[_flat_i] = pow(var_39[_flat_i], var_103[_flat_i])
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max(((batch_norm2d_0__w_0[((_flat_i % 802816) / 12544)] * ((conv2d_0__tmp_0[_flat_i] - batch_norm2d_0__w_1[((_flat_i % 802816) / 12544)]) * var_43[((_flat_i % 802816) / 12544)])) + batch_norm2d_0__b_0[((_flat_i % 802816) / 12544)]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.876857 4097770 op_lowering_impl.cc:546] After compute inline, ir is:
{
  ScheduleBlock(root_1)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_31)
        {
          _flat_i = axis.bind(flat_i)
          var_31[_flat_i] = batch_norm2d_0__w_2[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_39)
        {
          _flat_i = axis.bind(flat_i)
          var_39[_flat_i] = ((float32(1.00000000f) * var_31[_flat_i]) + float32(9.99999975e-06f))
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_103)
        {
          _flat_i = axis.bind(flat_i)
          var_103[_flat_i] = float32(-0.50000000000000000)
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max(((batch_norm2d_0__w_0[((_flat_i % 802816) / 12544)] * ((conv2d_0__tmp_0[_flat_i] - batch_norm2d_0__w_1[((_flat_i % 802816) / 12544)]) * pow(var_39[((_flat_i % 802816) / 12544)], var_103[((_flat_i % 802816) / 12544)]))) + batch_norm2d_0__b_0[((_flat_i % 802816) / 12544)]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.876900 4097770 op_lowering_impl.cc:503] Try FUSION fill_constant
I1018 08:53:57.876909 4097770 op_lowering_impl.cc:525] Before compute inline, ir is:
{
  ScheduleBlock(root_1)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_31)
        {
          _flat_i = axis.bind(flat_i)
          var_31[_flat_i] = batch_norm2d_0__w_2[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_39)
        {
          _flat_i = axis.bind(flat_i)
          var_39[_flat_i] = ((float32(1.00000000f) * var_31[_flat_i]) + float32(9.99999975e-06f))
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_103)
        {
          _flat_i = axis.bind(flat_i)
          var_103[_flat_i] = float32(-0.50000000000000000)
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max(((batch_norm2d_0__w_0[((_flat_i % 802816) / 12544)] * ((conv2d_0__tmp_0[_flat_i] - batch_norm2d_0__w_1[((_flat_i % 802816) / 12544)]) * pow(var_39[((_flat_i % 802816) / 12544)], var_103[((_flat_i % 802816) / 12544)]))) + batch_norm2d_0__b_0[((_flat_i % 802816) / 12544)]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.877018 4097770 op_lowering_impl.cc:546] After compute inline, ir is:
{
  ScheduleBlock(root_1)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_31)
        {
          _flat_i = axis.bind(flat_i)
          var_31[_flat_i] = batch_norm2d_0__w_2[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_39)
        {
          _flat_i = axis.bind(flat_i)
          var_39[_flat_i] = ((float32(1.00000000f) * var_31[_flat_i]) + float32(9.99999975e-06f))
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max(((batch_norm2d_0__w_0[((_flat_i % 802816) / 12544)] * ((conv2d_0__tmp_0[_flat_i] - batch_norm2d_0__w_1[((_flat_i % 802816) / 12544)]) * pow(var_39[((_flat_i % 802816) / 12544)], float32(-0.50000000000000000)))) + batch_norm2d_0__b_0[((_flat_i % 802816) / 12544)]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.877050 4097770 op_lowering_impl.cc:503] Try FUSION scale
I1018 08:53:57.877058 4097770 op_lowering_impl.cc:525] Before compute inline, ir is:
{
  ScheduleBlock(root_1)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_31)
        {
          _flat_i = axis.bind(flat_i)
          var_31[_flat_i] = batch_norm2d_0__w_2[_flat_i]
        }
      }
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_39)
        {
          _flat_i = axis.bind(flat_i)
          var_39[_flat_i] = ((float32(1.00000000f) * var_31[_flat_i]) + float32(9.99999975e-06f))
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max(((batch_norm2d_0__w_0[((_flat_i % 802816) / 12544)] * ((conv2d_0__tmp_0[_flat_i] - batch_norm2d_0__w_1[((_flat_i % 802816) / 12544)]) * pow(var_39[((_flat_i % 802816) / 12544)], float32(-0.50000000000000000)))) + batch_norm2d_0__b_0[((_flat_i % 802816) / 12544)]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.877163 4097770 op_lowering_impl.cc:546] After compute inline, ir is:
{
  ScheduleBlock(root_1)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_31)
        {
          _flat_i = axis.bind(flat_i)
          var_31[_flat_i] = batch_norm2d_0__w_2[_flat_i]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max(((batch_norm2d_0__w_0[((_flat_i % 802816) / 12544)] * ((conv2d_0__tmp_0[_flat_i] - batch_norm2d_0__w_1[((_flat_i % 802816) / 12544)]) * pow(((float32(1.00000000f) * var_31[((_flat_i % 802816) / 12544)]) + float32(9.99999975e-06f)), float32(-0.50000000000000000)))) + batch_norm2d_0__b_0[((_flat_i % 802816) / 12544)]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.877194 4097770 op_lowering_impl.cc:503] Try FUSION reshape
I1018 08:53:57.877202 4097770 op_lowering_impl.cc:525] Before compute inline, ir is:
{
  ScheduleBlock(root_1)
  {
    {
      serial for (flat_i, 0, 64)
      {
        ScheduleBlock(var_31)
        {
          _flat_i = axis.bind(flat_i)
          var_31[_flat_i] = batch_norm2d_0__w_2[_flat_i]
        }
      }
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max(((batch_norm2d_0__w_0[((_flat_i % 802816) / 12544)] * ((conv2d_0__tmp_0[_flat_i] - batch_norm2d_0__w_1[((_flat_i % 802816) / 12544)]) * pow(((float32(1.00000000f) * var_31[((_flat_i % 802816) / 12544)]) + float32(9.99999975e-06f)), float32(-0.50000000000000000)))) + batch_norm2d_0__b_0[((_flat_i % 802816) / 12544)]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.877282 4097770 op_lowering_impl.cc:546] After compute inline, ir is:
{
  ScheduleBlock(root_1)
  {
    {
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max(((batch_norm2d_0__w_0[((_flat_i % 802816) / 12544)] * ((conv2d_0__tmp_0[_flat_i] - batch_norm2d_0__w_1[((_flat_i % 802816) / 12544)]) * pow(((float32(1.00000000f) * batch_norm2d_0__w_2[((_flat_i % 802816) / 12544)]) + float32(9.99999975e-06f)), float32(-0.50000000000000000)))) + batch_norm2d_0__b_0[((_flat_i % 802816) / 12544)]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.877312 4097770 ir_analyzer.cc:101] it_expr is : {
  ScheduleBlock(root_1)
  {
    {
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max(((batch_norm2d_0__w_0[((_flat_i % 802816) / 12544)] * ((conv2d_0__tmp_0[_flat_i] - batch_norm2d_0__w_1[((_flat_i % 802816) / 12544)]) * pow(((float32(1.00000000f) * batch_norm2d_0__w_2[((_flat_i % 802816) / 12544)]) + float32(9.99999975e-06f)), float32(-0.50000000000000000)))) + batch_norm2d_0__b_0[((_flat_i % 802816) / 12544)]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.877337 4097770 op_lowering_impl.cc:624] Size of blocks: 1
I1018 08:53:57.877341 4097770 op_lowering_impl.cc:625] Op Pattern : 1
I1018 08:53:57.877347 4097770 op_lowering_impl.cc:690] Op Pattern : 1
I1018 08:53:57.877350 4097770 op_lowering_impl.cc:692] Before vectorize, ir is: 
{
  ScheduleBlock(root_1)
  {
    {
      serial for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max(((batch_norm2d_0__w_0[((_flat_i % 802816) / 12544)] * ((conv2d_0__tmp_0[_flat_i] - batch_norm2d_0__w_1[((_flat_i % 802816) / 12544)]) * pow(((float32(1.00000000f) * batch_norm2d_0__w_2[((_flat_i % 802816) / 12544)]) + float32(9.99999975e-06f)), float32(-0.50000000000000000)))) + batch_norm2d_0__b_0[((_flat_i % 802816) / 12544)]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.877379 4097770 op_lowering_impl.cc:706] var_53 dtype float32
I1018 08:53:57.877501 4097770 op_lowering_impl.cc:708] After vectorize, ir is: 
{
  ScheduleBlock(root_1)
  {
    {
      vectorize[8192] for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max(((batch_norm2d_0__w_0[((_flat_i % 802816) / 12544)] * ((conv2d_0__tmp_0[_flat_i] - batch_norm2d_0__w_1[((_flat_i % 802816) / 12544)]) * pow(((float32(1.00000000f) * batch_norm2d_0__w_2[((_flat_i % 802816) / 12544)]) + float32(9.99999975e-06f)), float32(-0.50000000000000000)))) + batch_norm2d_0__b_0[((_flat_i % 802816) / 12544)]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.877532 4097770 op_lowering_impl.cc:713] Before Sync IRLowerOp schedule, ir is: 
{
  ScheduleBlock(root_1)
  {
    {
      vectorize[8192] for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max(((batch_norm2d_0__w_0[((_flat_i % 802816) / 12544)] * ((conv2d_0__tmp_0[_flat_i] - batch_norm2d_0__w_1[((_flat_i % 802816) / 12544)]) * pow(((float32(1.00000000f) * batch_norm2d_0__w_2[((_flat_i % 802816) / 12544)]) + float32(9.99999975e-06f)), float32(-0.50000000000000000)))) + batch_norm2d_0__b_0[((_flat_i % 802816) / 12544)]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.877557 4097770 ir_analyzer.cc:101] it_expr is : {
  ScheduleBlock(root_1)
  {
    {
      vectorize[8192] for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max(((batch_norm2d_0__w_0[((_flat_i % 802816) / 12544)] * ((conv2d_0__tmp_0[_flat_i] - batch_norm2d_0__w_1[((_flat_i % 802816) / 12544)]) * pow(((float32(1.00000000f) * batch_norm2d_0__w_2[((_flat_i % 802816) / 12544)]) + float32(9.99999975e-06f)), float32(-0.50000000000000000)))) + batch_norm2d_0__b_0[((_flat_i % 802816) / 12544)]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.877589 4097770 op_lowering_impl.cc:717] After IRSchedule,  ir is: 
{
  ScheduleBlock(root_1)
  {
    {
      vectorize[8192] for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max(((batch_norm2d_0__w_0[((_flat_i % 802816) / 12544)] * ((conv2d_0__tmp_0[_flat_i] - batch_norm2d_0__w_1[((_flat_i % 802816) / 12544)]) * pow(((float32(1.00000000f) * batch_norm2d_0__w_2[((_flat_i % 802816) / 12544)]) + float32(9.99999975e-06f)), float32(-0.50000000000000000)))) + batch_norm2d_0__b_0[((_flat_i % 802816) / 12544)]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.877616 4097770 op_lowering_impl.cc:133] After group schedule, ir is: 
{
  ScheduleBlock(root_1)
  {
    {
      vectorize[8192] for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max(((batch_norm2d_0__w_0[((_flat_i % 802816) / 12544)] * ((conv2d_0__tmp_0[_flat_i] - batch_norm2d_0__w_1[((_flat_i % 802816) / 12544)]) * pow(((float32(1.00000000f) * batch_norm2d_0__w_2[((_flat_i % 802816) / 12544)]) + float32(9.99999975e-06f)), float32(-0.50000000000000000)))) + batch_norm2d_0__b_0[((_flat_i % 802816) / 12544)]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.877646 4097770 transform_gpu_forloop.cc:429] Before Optimize Expr:
{
  ScheduleBlock(root_1)
  {
    {
      vectorize[8192] for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[_flat_i] = cinn_max(((batch_norm2d_0__w_0[((_flat_i % 802816) / 12544)] * ((conv2d_0__tmp_0[_flat_i] - batch_norm2d_0__w_1[((_flat_i % 802816) / 12544)]) * pow(((float32(1.00000000f) * batch_norm2d_0__w_2[((_flat_i % 802816) / 12544)]) + float32(9.99999975e-06f)), float32(-0.50000000000000000)))) + batch_norm2d_0__b_0[((_flat_i % 802816) / 12544)]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.877717 4097770 eliminate_common_factor_of_local_index.cc:289] Before EliminateCommonFactorOfLocalIndex, Expr = 
{
  ScheduleBlock(root_1)
  {
    {
      vectorize[8192] for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[flat_i] = cinn_max(((batch_norm2d_0__w_0[((flat_i % 802816) / 12544)] * ((conv2d_0__tmp_0[flat_i] - batch_norm2d_0__w_1[((flat_i % 802816) / 12544)]) * pow(((float32(1.00000000f) * batch_norm2d_0__w_2[((flat_i % 802816) / 12544)]) + float32(9.99999975e-06f)), float32(-0.50000000000000000)))) + batch_norm2d_0__b_0[((flat_i % 802816) / 12544)]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.877756 4097770 eliminate_common_factor_of_local_index.cc:301] After EliminateCommonFactorOfLocalIndex, Expr = 
{
  ScheduleBlock(root_1)
  {
    {
      vectorize[8192] for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[flat_i] = cinn_max(((batch_norm2d_0__w_0[((flat_i % 802816) / 12544)] * ((conv2d_0__tmp_0[flat_i] - batch_norm2d_0__w_1[((flat_i % 802816) / 12544)]) * pow(((float32(1.00000000f) * batch_norm2d_0__w_2[((flat_i % 802816) / 12544)]) + float32(9.99999975e-06f)), float32(-0.50000000000000000)))) + batch_norm2d_0__b_0[((flat_i % 802816) / 12544)]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.877889 4097770 transform_gpu_forloop.cc:461] After Optimize Expr: 
{
  ScheduleBlock(root_1)
  {
    {
      vectorize[8192] for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[flat_i] = cinn_max(((batch_norm2d_0__w_0[((flat_i % 802816) / 12544)] * ((conv2d_0__tmp_0[flat_i] - batch_norm2d_0__w_1[((flat_i % 802816) / 12544)]) * pow(((float32(1.00000000f) * batch_norm2d_0__w_2[((flat_i % 802816) / 12544)]) + float32(9.99999975e-06f)), float32(-0.50000000000000000)))) + batch_norm2d_0__b_0[((flat_i % 802816) / 12544)]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.878086 4097770 lowered_func.cc:230] Function used 6 buffers
I1018 08:53:57.878381 4097770 ir_simplify.cc:467] Begin Simplify function fn_reshape_0_reshape_1_reshape_2_reshape_3_broadcast_to_5_scale_7_fill_constant_10_pow_11_subtract_6_broadcast_to_12_elementwise_mul_13_broadcast_to_14_broadcast_to_16_elementwise_mul_15_elementwise_add_17_fill_constant_18_max_19_21 (_batch_norm2d_0__b_0, _batch_norm2d_0__w_0, _batch_norm2d_0__w_1, _batch_norm2d_0__w_2, _conv2d_0__tmp_0, _var_53)
{
  ScheduleBlock(root_1)
  {
    {
      vectorize[8192] for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[flat_i] = cinn_max(((batch_norm2d_0__w_0[((flat_i % 802816) / 12544)] * ((conv2d_0__tmp_0[flat_i] - batch_norm2d_0__w_1[((flat_i % 802816) / 12544)]) * pow(((float32(1.00000000f) * batch_norm2d_0__w_2[((flat_i % 802816) / 12544)]) + float32(9.99999975e-06f)), float32(-0.50000000000000000)))) + batch_norm2d_0__b_0[((flat_i % 802816) / 12544)]), float32(0.0000000000000000))
        }
      }
    }
  }
}
I1018 08:53:57.879040 4097770 optimize.cc:57] After Optimize UnrollLoop:function fn_reshape_0_reshape_1_reshape_2_reshape_3_broadcast_to_5_scale_7_fill_constant_10_pow_11_subtract_6_broadcast_to_12_elementwise_mul_13_broadcast_to_14_broadcast_to_16_elementwise_mul_15_elementwise_add_17_fill_constant_18_max_19_21 (_batch_norm2d_0__b_0, _batch_norm2d_0__w_0, _batch_norm2d_0__w_1, _batch_norm2d_0__w_2, _conv2d_0__tmp_0, _var_53)
{
  ScheduleBlock(root_1)
  {
    {
      vectorize[8192] for (flat_i, 0, 802816)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(flat_i)
          var_53[flat_i] = cinn_max(((batch_norm2d_0__w_0[(flat_i / 12544)] * (pow((batch_norm2d_0__w_2[(flat_i / 12544)] + 9.99999975e-06f), -0.500000000f) * conv2d_0__tmp_0[flat_i])) + ((-1.00000000f * (batch_norm2d_0__w_0[(flat_i / 12544)] * (pow((batch_norm2d_0__w_2[(flat_i / 12544)] + 9.99999975e-06f), -0.500000000f) * batch_norm2d_0__w_1[(flat_i / 12544)]))) + batch_norm2d_0__b_0[(flat_i / 12544)])), 0.00000000f)
        }
      }
    }
  }
}
I1018 08:53:57.879101 4097770 ir_simplify.cc:467] Begin Simplify 802816
I1018 08:53:57.879122 4097770 ir_simplify.cc:467] Begin Simplify (flat_i / 12544)
I1018 08:53:57.879144 4097770 ir_simplify.cc:467] Begin Simplify flat_i
I1018 08:53:57.879163 4097770 ir_simplify.cc:467] Begin Simplify (flat_i / 12544)
I1018 08:53:57.879191 4097770 ir_simplify.cc:467] Begin Simplify (flat_i / 12544)
I1018 08:53:57.879220 4097770 ir_simplify.cc:467] Begin Simplify (flat_i / 12544)
I1018 08:53:57.879238 4097770 ir_simplify.cc:467] Begin Simplify flat_i
I1018 08:53:57.879329 4097770 vectorize_loops.cc:820] Vectorizing vi_0 extent 8192
I1018 08:53:57.879334 4097770 vectorize_loops.cc:822] before vectorize body:
{
  vectorize[8192] for (vi_0, 0, 8192)
  {
    ScheduleBlock(var_53)
    {
      _flat_i = axis.bind(((flat_i * 8192) + vi_0))
      var_53[((flat_i * 8192) + vi_0)] = cinn_max(((batch_norm2d_0__w_0[(((flat_i * 8192) + vi_0) / 12544)] * (pow((batch_norm2d_0__w_2[(((flat_i * 8192) + vi_0) / 12544)] + 9.99999975e-06f), -0.500000000f) * conv2d_0__tmp_0[((flat_i * 8192) + vi_0)])) + ((-1.00000000f * (batch_norm2d_0__w_0[(((flat_i * 8192) + vi_0) / 12544)] * (pow((batch_norm2d_0__w_2[(((flat_i * 8192) + vi_0) / 12544)] + 9.99999975e-06f), -0.500000000f) * batch_norm2d_0__w_1[(((flat_i * 8192) + vi_0) / 12544)]))) + batch_norm2d_0__b_0[(((flat_i * 8192) + vi_0) / 12544)])), 0.00000000f)
    }
  }
}
I1018 08:53:57.879385 4097770 vectorize_loops.cc:421] Before vectorize div: (((flat_i * 8192) + vi_0) / 12544)
I1018 08:53:57.879400 4097770 vectorize_loops.cc:423] After vectorize div: (Ramp(((flat_i * 8192) + 0),1,8192) / Broadcast(12544,8192))
I1018 08:53:57.879413 4097770 vectorize_loops.cc:421] Before vectorize div: (((flat_i * 8192) + vi_0) / 12544)
I1018 08:53:57.879426 4097770 vectorize_loops.cc:423] After vectorize div: (Ramp(((flat_i * 8192) + 0),1,8192) / Broadcast(12544,8192))
I1018 08:53:57.879441 4097770 vectorize_loops.cc:543] Vectorizer Call: pow is_changed: 1 lanes: 8192
I1018 08:53:57.879462 4097770 vectorize_loops.cc:421] Before vectorize div: (((flat_i * 8192) + vi_0) / 12544)
I1018 08:53:57.879475 4097770 vectorize_loops.cc:423] After vectorize div: (Ramp(((flat_i * 8192) + 0),1,8192) / Broadcast(12544,8192))
I1018 08:53:57.879487 4097770 vectorize_loops.cc:421] Before vectorize div: (((flat_i * 8192) + vi_0) / 12544)
I1018 08:53:57.879498 4097770 vectorize_loops.cc:423] After vectorize div: (Ramp(((flat_i * 8192) + 0),1,8192) / Broadcast(12544,8192))
I1018 08:53:57.879515 4097770 vectorize_loops.cc:543] Vectorizer Call: pow is_changed: 1 lanes: 8192
I1018 08:53:57.879523 4097770 vectorize_loops.cc:421] Before vectorize div: (((flat_i * 8192) + vi_0) / 12544)
I1018 08:53:57.879534 4097770 vectorize_loops.cc:423] After vectorize div: (Ramp(((flat_i * 8192) + 0),1,8192) / Broadcast(12544,8192))
I1018 08:53:57.879559 4097770 vectorize_loops.cc:421] Before vectorize div: (((flat_i * 8192) + vi_0) / 12544)
I1018 08:53:57.879571 4097770 vectorize_loops.cc:423] After vectorize div: (Ramp(((flat_i * 8192) + 0),1,8192) / Broadcast(12544,8192))
I1018 08:53:57.879599 4097770 vectorize_loops.cc:855] after vectorize body:
{
  vectorize[8192] for (vi_0, 0, 8192)
  {
    ScheduleBlock(var_53)
    {
      _flat_i = axis.bind(Ramp(((flat_i * 8192) + 0),1,8192))
      var_53[Ramp(((flat_i * 8192) + 0),1,8192)] = cinn_max(((batch_norm2d_0__w_0[(Ramp(((flat_i * 8192) + 0),1,8192) / Broadcast(12544,8192))] * (pow((batch_norm2d_0__w_2[(Ramp(((flat_i * 8192) + 0),1,8192) / Broadcast(12544,8192))] + Broadcast(9.99999975e-06f,8192)), Broadcast(-0.500000000f,8192)) * conv2d_0__tmp_0[Ramp(((flat_i * 8192) + 0),1,8192)])) + ((Broadcast(-1.00000000f,8192) * (batch_norm2d_0__w_0[(Ramp(((flat_i * 8192) + 0),1,8192) / Broadcast(12544,8192))] * (pow((batch_norm2d_0__w_2[(Ramp(((flat_i * 8192) + 0),1,8192) / Broadcast(12544,8192))] + Broadcast(9.99999975e-06f,8192)), Broadcast(-0.500000000f,8192)) * batch_norm2d_0__w_1[(Ramp(((flat_i * 8192) + 0),1,8192) / Broadcast(12544,8192))]))) + batch_norm2d_0__b_0[(Ramp(((flat_i * 8192) + 0),1,8192) / Broadcast(12544,8192))])), Broadcast(0.00000000f,8192))
    }
  }
}
I1018 08:53:57.879669 4097770 optimize.cc:60] After Optimize VectorizeLoops:function fn_reshape_0_reshape_1_reshape_2_reshape_3_broadcast_to_5_scale_7_fill_constant_10_pow_11_subtract_6_broadcast_to_12_elementwise_mul_13_broadcast_to_14_broadcast_to_16_elementwise_mul_15_elementwise_add_17_fill_constant_18_max_19_21 (_batch_norm2d_0__b_0, _batch_norm2d_0__w_0, _batch_norm2d_0__w_1, _batch_norm2d_0__w_2, _conv2d_0__tmp_0, _var_53)
{
  ScheduleBlock(root_1)
  {
    {
      serial for (flat_i, 0, 98)
      {
        ScheduleBlock(var_53)
        {
          _flat_i = axis.bind(Ramp(((flat_i * 8192) + 0),1,8192))
          var_53[Ramp(((flat_i * 8192) + 0),1,8192)] = cinn_max(((batch_norm2d_0__w_0[(Ramp(((flat_i * 8192) + 0),1,8192) / Broadcast(12544,8192))] * (pow((batch_norm2d_0__w_2[(Ramp(((flat_i * 8192) + 0),1,8192) / Broadcast(12544,8192))] + Broadcast(9.99999975e-06f,8192)), Broadcast(-0.500000000f,8192)) * conv2d_0__tmp_0[Ramp(((flat_i * 8192) + 0),1,8192)])) + ((Broadcast(-1.00000000f,8192) * (batch_norm2d_0__w_0[(Ramp(((flat_i * 8192) + 0),1,8192) / Broadcast(12544,8192))] * (pow((batch_norm2d_0__w_2[(Ramp(((flat_i * 8192) + 0),1,8192) / Broadcast(12544,8192))] + Broadcast(9.99999975e-06f,8192)), Broadcast(-0.500000000f,8192)) * batch_norm2d_0__w_1[(Ramp(((flat_i * 8192) + 0),1,8192) / Broadcast(12544,8192))]))) + batch_norm2d_0__b_0[(Ramp(((flat_i * 8192) + 0),1,8192) / Broadcast(12544,8192))])), Broadcast(0.00000000f,8192))
        }
      }
    }
  }
}
I1018 08:53:57.879807 4097770 optimize.cc:74] After SimplifyBlocks:function fn_reshape_0_reshape_1_reshape_2_reshape_3_broadcast_to_5_scale_7_fill_constant_10_pow_11_subtract_6_broadcast_to_12_elementwise_mul_13_broadcast_to_14_broadcast_to_16_elementwise_mul_15_elementwise_add_17_fill_constant_18_max_19_21 (_batch_norm2d_0__b_0, _batch_norm2d_0__w_0, _batch_norm2d_0__w_1, _batch_norm2d_0__w_2, _conv2d_0__tmp_0, _var_53)
{
  ScheduleBlock(root_1)
  {
    serial for (flat_i, 0, 98)
    {
      ScheduleBlock(var_53)
      {
        _flat_i = axis.bind(Ramp(((flat_i * 8192) + 0),1,8192))
        var_53[Ramp(((flat_i * 8192) + 0),1,8192)] = cinn_max(((batch_norm2d_0__w_0[(Ramp(((flat_i * 8192) + 0),1,8192) / Broadcast(12544,8192))] * (pow((batch_norm2d_0__w_2[(Ramp(((flat_i * 8192) + 0),1,8192) / Broadcast(12544,8192))] + Broadcast(9.99999975e-06f,8192)), Broadcast(-0.500000000f,8192)) * conv2d_0__tmp_0[Ramp(((flat_i * 8192) + 0),1,8192)])) + ((Broadcast(-1.00000000f,8192) * (batch_norm2d_0__w_0[(Ramp(((flat_i * 8192) + 0),1,8192) / Broadcast(12544,8192))] * (pow((batch_norm2d_0__w_2[(Ramp(((flat_i * 8192) + 0),1,8192) / Broadcast(12544,8192))] + Broadcast(9.99999975e-06f,8192)), Broadcast(-0.500000000f,8192)) * batch_norm2d_0__w_1[(Ramp(((flat_i * 8192) + 0),1,8192) / Broadcast(12544,8192))]))) + batch_norm2d_0__b_0[(Ramp(((flat_i * 8192) + 0),1,8192) / Broadcast(12544,8192))])), Broadcast(0.00000000f,8192))
      }
    }
  }
}
I1018 08:53:57.879877 4097770 ir_simplify.cc:467] Begin Simplify function fn_reshape_0_reshape_1_reshape_2_reshape_3_broadcast_to_5_scale_7_fill_constant_10_pow_11_subtract_6_broadcast_to_12_elementwise_mul_13_broadcast_to_14_broadcast_to_16_elementwise_mul_15_elementwise_add_17_fill_constant_18_max_19_21 (_batch_norm2d_0__b_0, _batch_norm2d_0__w_0, _batch_norm2d_0__w_1, _batch_norm2d_0__w_2, _conv2d_0__tmp_0, _var_53)
{
  ScheduleBlock(root_1)
  {
    serial for (flat_i, 0, 98)
    {
      ScheduleBlock(var_53)
      {
        _flat_i = axis.bind(Ramp(((flat_i * 8192) + 0),1,8192))
        var_53[Ramp(((flat_i * 8192) + 0),1,8192)] = cinn_max(((batch_norm2d_0__w_0[(Ramp(((flat_i * 8192) + 0),1,8192) / Broadcast(12544,8192))] * (cinn_sycl_rsqrt_fp32((batch_norm2d_0__w_2[(Ramp(((flat_i * 8192) + 0),1,8192) / Broadcast(12544,8192))] + Broadcast(9.99999975e-06f,8192))) * conv2d_0__tmp_0[Ramp(((flat_i * 8192) + 0),1,8192)])) + ((Broadcast(-1.00000000f,8192) * (batch_norm2d_0__w_0[(Ramp(((flat_i * 8192) + 0),1,8192) / Broadcast(12544,8192))] * (cinn_sycl_rsqrt_fp32((batch_norm2d_0__w_2[(Ramp(((flat_i * 8192) + 0),1,8192) / Broadcast(12544,8192))] + Broadcast(9.99999975e-06f,8192))) * batch_norm2d_0__w_1[(Ramp(((flat_i * 8192) + 0),1,8192) / Broadcast(12544,8192))]))) + batch_norm2d_0__b_0[(Ramp(((flat_i * 8192) + 0),1,8192) / Broadcast(12544,8192))])), Broadcast(0.00000000f,8192))
      }
    }
  }
}
I1018 08:53:57.880823 4097770 parallel_compiler.cc:137] Start CodegenAndJit
I1018 08:53:57.880836 4097770 ir_simplify.cc:467] Begin Simplify {
  ScheduleBlock(root_1)
  {
    serial for (flat_i, 0, 98)
    {
      ScheduleBlock(var_53)
      {
        _flat_i = axis.bind(Ramp((8192 * flat_i),1,8192))
        var_53[Ramp((8192 * flat_i),1,8192)] = cinn_max(((batch_norm2d_0__w_0[(Ramp((flat_i * 8192),1,8192) / Broadcast(12544,8192))] * (cinn_sycl_rsqrt_fp32((batch_norm2d_0__w_2[(Ramp((flat_i * 8192),1,8192) / Broadcast(12544,8192))] + Broadcast(9.99999975e-06f,8192))) * conv2d_0__tmp_0[Ramp((8192 * flat_i),1,8192)])) + ((Broadcast(-1.00000000f,8192) * (batch_norm2d_0__w_0[(Ramp((flat_i * 8192),1,8192) / Broadcast(12544,8192))] * (cinn_sycl_rsqrt_fp32((batch_norm2d_0__w_2[(Ramp((flat_i * 8192),1,8192) / Broadcast(12544,8192))] + Broadcast(9.99999975e-06f,8192))) * batch_norm2d_0__w_1[(Ramp((flat_i * 8192),1,8192) / Broadcast(12544,8192))]))) + batch_norm2d_0__b_0[(Ramp((flat_i * 8192),1,8192) / Broadcast(12544,8192))])), Broadcast(0.00000000f,8192))
      }
    }
  }
}
I1018 08:53:57.881800 4097770 ir_simplify.cc:467] Begin Simplify {
  ScheduleBlock(root_1)
  {
    serial for (flat_i, 0, 98)
    {
      ScheduleBlock(var_53)
      {
        _flat_i = axis.bind(Ramp((8192 * flat_i),1,8192))
        var_53[Ramp((8192 * flat_i),1,8192)] = cinn_max(((batch_norm2d_0__w_0[(Ramp((flat_i * 8192),1,8192) / Broadcast(12544,8192))] * (cinn_sycl_rsqrt_fp32((batch_norm2d_0__w_2[(Ramp((flat_i * 8192),1,8192) / Broadcast(12544,8192))] + Broadcast(9.99999975e-06f,8192))) * conv2d_0__tmp_0[Ramp((8192 * flat_i),1,8192)])) + ((Broadcast(-1.00000000f,8192) * (batch_norm2d_0__w_0[(Ramp((flat_i * 8192),1,8192) / Broadcast(12544,8192))] * (cinn_sycl_rsqrt_fp32((batch_norm2d_0__w_2[(Ramp((flat_i * 8192),1,8192) / Broadcast(12544,8192))] + Broadcast(9.99999975e-06f,8192))) * batch_norm2d_0__w_1[(Ramp((flat_i * 8192),1,8192) / Broadcast(12544,8192))]))) + batch_norm2d_0__b_0[(Ramp((flat_i * 8192),1,8192) / Broadcast(12544,8192))])), Broadcast(0.00000000f,8192))
      }
    }
  }
}
I1018 08:53:57.882598 4097770 optimize.cc:57] After Optimize UnrollLoop:{
  ScheduleBlock(root_1)
  {
    serial for (flat_i, 0, 98)
    {
      ScheduleBlock(var_53)
      {
        _flat_i = axis.bind(Ramp((8192 * flat_i),1,8192))
        var_53[Ramp((8192 * flat_i),1,8192)] = cinn_max(((batch_norm2d_0__w_0[(Ramp((flat_i * 8192),1,8192) / Broadcast(12544,8192))] * (cinn_sycl_rsqrt_fp32((batch_norm2d_0__w_2[(Ramp((flat_i * 8192),1,8192) / Broadcast(12544,8192))] + Broadcast(9.99999975e-06f,8192))) * conv2d_0__tmp_0[Ramp((8192 * flat_i),1,8192)])) + ((Broadcast(-1.00000000f,8192) * (batch_norm2d_0__w_0[(Ramp((flat_i * 8192),1,8192) / Broadcast(12544,8192))] * (cinn_sycl_rsqrt_fp32((batch_norm2d_0__w_2[(Ramp((flat_i * 8192),1,8192) / Broadcast(12544,8192))] + Broadcast(9.99999975e-06f,8192))) * batch_norm2d_0__w_1[(Ramp((flat_i * 8192),1,8192) / Broadcast(12544,8192))]))) + batch_norm2d_0__b_0[(Ramp((flat_i * 8192),1,8192) / Broadcast(12544,8192))])), Broadcast(0.00000000f,8192))
      }
    }
  }
}
I1018 08:53:57.882669 4097770 ir_simplify.cc:467] Begin Simplify (Ramp((flat_i * 8192),1,8192) / Broadcast(12544,8192))
I1018 08:53:57.882737 4097770 ir_simplify.cc:467] Begin Simplify Ramp((8192 * flat_i),1,8192)
I1018 08:53:57.882802 4097770 ir_simplify.cc:467] Begin Simplify (Ramp((flat_i * 8192),1,8192) / Broadcast(12544,8192))
I1018 08:53:57.882876 4097770 ir_simplify.cc:467] Begin Simplify (Ramp((flat_i * 8192),1,8192) / Broadcast(12544,8192))
I1018 08:53:57.882949 4097770 ir_simplify.cc:467] Begin Simplify (Ramp((flat_i * 8192),1,8192) / Broadcast(12544,8192))
I1018 08:53:57.883018 4097770 ir_simplify.cc:467] Begin Simplify Ramp((8192 * flat_i),1,8192)
I1018 08:53:57.883055 4097770 optimize.cc:60] After Optimize VectorizeLoops:{
  ScheduleBlock(root_1)
  {
    serial for (flat_i, 0, 98)
    {
      ScheduleBlock(var_53)
      {
        _flat_i = axis.bind(Ramp((8192 * flat_i),1,8192))
        var_53[Ramp((8192 * flat_i),1,8192)] = cinn_max(((batch_norm2d_0__w_0[(Ramp((8192 * flat_i),1,8192) / Broadcast(12544,8192))] * (cinn_sycl_rsqrt_fp32((batch_norm2d_0__w_2[(Ramp((flat_i * 8192),1,8192) / Broadcast(12544,8192))] + Broadcast(9.99999975e-06f,8192))) * conv2d_0__tmp_0[Ramp((8192 * flat_i),1,8192)])) + ((Broadcast(-1.00000000f,8192) * (batch_norm2d_0__w_0[(Ramp((8192 * flat_i),1,8192) / Broadcast(12544,8192))] * (cinn_sycl_rsqrt_fp32((batch_norm2d_0__w_2[(Ramp((flat_i * 8192),1,8192) / Broadcast(12544,8192))] + Broadcast(9.99999975e-06f,8192))) * batch_norm2d_0__w_1[(Ramp((8192 * flat_i),1,8192) / Broadcast(12544,8192))]))) + batch_norm2d_0__b_0[(Ramp((8192 * flat_i),1,8192) / Broadcast(12544,8192))])), Broadcast(0.00000000f,8192))
      }
    }
  }
}
I1018 08:53:57.883106 4097770 optimize.cc:74] After SimplifyBlocks:{
  ScheduleBlock(root_1)
  {
    serial for (flat_i, 0, 98)
    {
      ScheduleBlock(var_53)
      {
        _flat_i = axis.bind(Ramp((8192 * flat_i),1,8192))
        var_53[Ramp((8192 * flat_i),1,8192)] = cinn_max(((batch_norm2d_0__w_0[(Ramp((8192 * flat_i),1,8192) / Broadcast(12544,8192))] * (cinn_sycl_rsqrt_fp32((batch_norm2d_0__w_2[(Ramp((flat_i * 8192),1,8192) / Broadcast(12544,8192))] + Broadcast(9.99999975e-06f,8192))) * conv2d_0__tmp_0[Ramp((8192 * flat_i),1,8192)])) + ((Broadcast(-1.00000000f,8192) * (batch_norm2d_0__w_0[(Ramp((8192 * flat_i),1,8192) / Broadcast(12544,8192))] * (cinn_sycl_rsqrt_fp32((batch_norm2d_0__w_2[(Ramp((flat_i * 8192),1,8192) / Broadcast(12544,8192))] + Broadcast(9.99999975e-06f,8192))) * batch_norm2d_0__w_1[(Ramp((8192 * flat_i),1,8192) / Broadcast(12544,8192))]))) + batch_norm2d_0__b_0[(Ramp((8192 * flat_i),1,8192) / Broadcast(12544,8192))])), Broadcast(0.00000000f,8192))
      }
    }
  }
}
I1018 08:53:57.883147 4097770 ir_simplify.cc:467] Begin Simplify {
  ScheduleBlock(root_1)
  {
    serial for (flat_i, 0, 98)
    {
      ScheduleBlock(var_53)
      {
        _flat_i = axis.bind(Ramp((8192 * flat_i),1,8192))
        var_53[Ramp((8192 * flat_i),1,8192)] = cinn_max(((batch_norm2d_0__w_0[(Ramp((8192 * flat_i),1,8192) / Broadcast(12544,8192))] * (cinn_sycl_rsqrt_fp32((batch_norm2d_0__w_2[(Ramp((flat_i * 8192),1,8192) / Broadcast(12544,8192))] + Broadcast(9.99999975e-06f,8192))) * conv2d_0__tmp_0[Ramp((8192 * flat_i),1,8192)])) + ((Broadcast(-1.00000000f,8192) * (batch_norm2d_0__w_0[(Ramp((8192 * flat_i),1,8192) / Broadcast(12544,8192))] * (cinn_sycl_rsqrt_fp32((batch_norm2d_0__w_2[(Ramp((flat_i * 8192),1,8192) / Broadcast(12544,8192))] + Broadcast(9.99999975e-06f,8192))) * batch_norm2d_0__w_1[(Ramp((8192 * flat_i),1,8192) / Broadcast(12544,8192))]))) + batch_norm2d_0__b_0[(Ramp((8192 * flat_i),1,8192) / Broadcast(12544,8192))])), Broadcast(0.00000000f,8192))
      }
    }
  }
}
I1018 08:53:57.884195 4097770 ir_simplify.cc:467] Begin Simplify (Ramp((8192 * flat_i),1,8192) / Broadcast(12544,8192))
I1018 08:53:57.884266 4097770 ir_simplify.cc:467] Begin Simplify Ramp((8192 * flat_i),1,8192)
I1018 08:53:57.884330 4097770 ir_simplify.cc:467] Begin Simplify (Ramp((8192 * flat_i),1,8192) / Broadcast(12544,8192))
I1018 08:53:57.884402 4097770 ir_simplify.cc:467] Begin Simplify (Ramp((8192 * flat_i),1,8192) / Broadcast(12544,8192))
I1018 08:53:57.884474 4097770 ir_simplify.cc:467] Begin Simplify (Ramp((8192 * flat_i),1,8192) / Broadcast(12544,8192))
I1018 08:53:57.884536 4097770 ir_simplify.cc:467] Begin Simplify Ramp((8192 * flat_i),1,8192)
I1018 08:53:57.884863 4097770 codegen_sycl_dev.cc:503] CodeGenSYCL visiting store op: var_53
I1018 08:53:57.884873 4097770 ir.cc:629] Begin Load::index IndiceToAbsOffset of tensor: batch_norm2d_0__w_0
I1018 08:53:57.884884 4097770 codegen_sycl_dev.cc:338] CodeGenSYCL visiting call op: cinn_sycl_rsqrt_fp32
I1018 08:53:57.884893 4097770 codegen_sycl_dev.cc:339] op->read_args.size(): 1
I1018 08:53:57.884896 4097770 codegen_sycl_dev.cc:340] op->write_args.size(): 0
I1018 08:53:57.884900 4097770 ir.cc:629] Begin Load::index IndiceToAbsOffset of tensor: batch_norm2d_0__w_2
I1018 08:53:57.884913 4097770 ir.cc:629] Begin Load::index IndiceToAbsOffset of tensor: conv2d_0__tmp_0
I1018 08:53:57.884923 4097770 ir.cc:629] Begin Load::index IndiceToAbsOffset of tensor: batch_norm2d_0__w_0
I1018 08:53:57.884931 4097770 codegen_sycl_dev.cc:338] CodeGenSYCL visiting call op: cinn_sycl_rsqrt_fp32
I1018 08:53:57.884935 4097770 codegen_sycl_dev.cc:339] op->read_args.size(): 1
I1018 08:53:57.884938 4097770 codegen_sycl_dev.cc:340] op->write_args.size(): 0
I1018 08:53:57.884941 4097770 ir.cc:629] Begin Load::index IndiceToAbsOffset of tensor: batch_norm2d_0__w_2
I1018 08:53:57.884951 4097770 ir.cc:629] Begin Load::index IndiceToAbsOffset of tensor: batch_norm2d_0__w_1
I1018 08:53:57.884959 4097770 ir.cc:629] Begin Load::index IndiceToAbsOffset of tensor: batch_norm2d_0__b_0
I1018 08:53:57.884989 4097770 lowered_func.cc:230] Function used 0 buffers
I1018 08:53:57.885501 4097770 ir_simplify.cc:467] Begin Simplify (Ramp((8192 * flat_i),1,8192) / Broadcast(12544,8192))
I1018 08:53:57.885576 4097770 ir_simplify.cc:467] Begin Simplify Ramp((8192 * flat_i),1,8192)
I1018 08:53:57.885638 4097770 ir_simplify.cc:467] Begin Simplify (Ramp((8192 * flat_i),1,8192) / Broadcast(12544,8192))
I1018 08:53:57.885712 4097770 ir_simplify.cc:467] Begin Simplify (Ramp((8192 * flat_i),1,8192) / Broadcast(12544,8192))
I1018 08:53:57.885783 4097770 ir_simplify.cc:467] Begin Simplify (Ramp((8192 * flat_i),1,8192) / Broadcast(12544,8192))
I1018 08:53:57.885845 4097770 ir_simplify.cc:467] Begin Simplify Ramp((8192 * flat_i),1,8192)
I1018 08:53:57.886206 4097770 codegen_sycl_dev.cc:503] CodeGenSYCL visiting store op: var_53
I1018 08:53:57.886217 4097770 ir.cc:629] Begin Load::index IndiceToAbsOffset of tensor: batch_norm2d_0__w_0
I1018 08:53:57.886227 4097770 codegen_sycl_dev.cc:338] CodeGenSYCL visiting call op: cinn_sycl_rsqrt_fp32
I1018 08:53:57.886230 4097770 codegen_sycl_dev.cc:339] op->read_args.size(): 1
I1018 08:53:57.886233 4097770 codegen_sycl_dev.cc:340] op->write_args.size(): 0
I1018 08:53:57.886237 4097770 ir.cc:629] Begin Load::index IndiceToAbsOffset of tensor: batch_norm2d_0__w_2
I1018 08:53:57.886250 4097770 ir.cc:629] Begin Load::index IndiceToAbsOffset of tensor: conv2d_0__tmp_0
I1018 08:53:57.886260 4097770 ir.cc:629] Begin Load::index IndiceToAbsOffset of tensor: batch_norm2d_0__w_0
I1018 08:53:57.886267 4097770 codegen_sycl_dev.cc:338] CodeGenSYCL visiting call op: cinn_sycl_rsqrt_fp32
I1018 08:53:57.886270 4097770 codegen_sycl_dev.cc:339] op->read_args.size(): 1
I1018 08:53:57.886273 4097770 codegen_sycl_dev.cc:340] op->write_args.size(): 0
I1018 08:53:57.886277 4097770 ir.cc:629] Begin Load::index IndiceToAbsOffset of tensor: batch_norm2d_0__w_2
I1018 08:53:57.886287 4097770 ir.cc:629] Begin Load::index IndiceToAbsOffset of tensor: batch_norm2d_0__w_1
I1018 08:53:57.886296 4097770 ir.cc:629] Begin Load::index IndiceToAbsOffset of tensor: batch_norm2d_0__b_0
I1018 08:53:57.886307 4097770 parallel_compiler.cc:328] [SYCL]:
#include <sycl/sycl.hpp>
#include "cinn_sycl_runtime_source.h"
typedef sycl::half float16;
#ifdef __cplusplus
extern "C" {
#endif
// CodeGenSYCL: NOTE: Auto-generated packed function
void fn_reshape_0_reshape_1_reshape_2_reshape_3_broadcast_to_5_scale_7_fill_constant_10_pow_11_subtract_6_broadcast_to_12_elementwise_mul_13_broadcast_to_14_broadcast_to_16_elementwise_mul_15_elementwise_add_17_fill_constant_18_max_19_21_kernel(sycl::queue &Q, sycl::range<3> dimGrid, sycl::range<3> dimBlock, void** void_args) {
  const float*  batch_norm2d_0__b_0 = (float* )(*(void **)(void_args[0]));
  const float*  batch_norm2d_0__w_0 = (float* )(*(void **)(void_args[1]));
  const float*  batch_norm2d_0__w_1 = (float* )(*(void **)(void_args[2]));
  const float*  batch_norm2d_0__w_2 = (float* )(*(void **)(void_args[3]));
  const float*  conv2d_0__tmp_0 = (float* )(*(void **)(void_args[4]));
  float*  var_53 = (float* )(*(void **)(void_args[5]));
  Q.submit([&](sycl::handler &h) {
    h.parallel_for<class space2_fn_reshape_0_reshape_1_reshape_2_reshape_3_broadcast_to_5_scale_7_fill_constant_10_pow_11_subtract_6_broadcast_to_12_elementwise_mul_13_broadcast_to_14_broadcast_to_16_elementwise_mul_15_elementwise_add_17_fill_constant_18_max_19_21_kernel>(sycl::nd_range<3>(dimGrid * dimBlock, dimBlock), [=](sycl::nd_item<3> item) [[intel::kernel_args_restrict]][[intel::max_work_group_size(1, 1, 1)]]
    {
      for (int32_t flat_i = 0; flat_i < 98; flat_i += 1) {
        cinn_sycl_store(var_53, IndexVec<8192>::Ramp((8192 * flat_i)), cinn_sycl_max(((DataVec<float, 8192>::Load(batch_norm2d_0__w_0, (IndexVec<8192>::Ramp((8192 * flat_i)) / 12544)) * (cinn_sycl_rsqrt_fp32((DataVec<float, 8192>::Load(batch_norm2d_0__w_2, (IndexVec<8192>::Ramp((flat_i * 8192)) / 12544)) + 9.99999975e-06f)) * DataVec<float, 8192>::Load(conv2d_0__tmp_0, (8192 * flat_i)))) + ((-1.00000000f * (DataVec<float, 8192>::Load(batch_norm2d_0__w_0, (IndexVec<8192>::Ramp((8192 * flat_i)) / 12544)) * (cinn_sycl_rsqrt_fp32((DataVec<float, 8192>::Load(batch_norm2d_0__w_2, (IndexVec<8192>::Ramp((flat_i * 8192)) / 12544)) + 9.99999975e-06f)) * DataVec<float, 8192>::Load(batch_norm2d_0__w_1, (IndexVec<8192>::Ramp((8192 * flat_i)) / 12544))))) + DataVec<float, 8192>::Load(batch_norm2d_0__b_0, (IndexVec<8192>::Ramp((8192 * flat_i)) / 12544)))), 0.00000000f));
      };
    });
  });
}

#ifdef __cplusplus
}
#endif
I1018 08:53:57.886411 4097770 compiler_sycl.cc:77] compile command: /home/wzy/sycl_workspace/llvm-mlu/build/bin/clang++ -I /home/wzy/.local/lib/python3.9/site-packages/paddle/libs -fsycl -fsycl-targets=mlisa-cambricon-bang -std=c++17 -fPIC -O1 -shared -ldl -fbracket-depth=1030 ./source/sycl_2.cc -o ./source/sycl_2.so
I1018 08:54:04.991709 4097769 execution_engine.cc:117] ===================== Create CINN ExecutionEngine begin ====================
I1018 08:54:04.991760 4097769 execution_engine.cc:119] initialize llvm config
I1018 08:54:04.991770 4097769 execution_engine.cc:120] llvm version: 12.0.0git
I1018 08:54:04.991780 4097769 execution_engine.cc:121] llvm default target triple: x86_64-unknown-linux-gnu
I1018 08:54:04.995512 4097769 execution_engine.cc:153] create jit execution engine
I1018 08:54:04.996961 4097769 execution_engine.cc:134] create llvm compile layer
I1018 08:54:04.996975 4097769 execution_engine.cc:135] Target Name: x86-64
I1018 08:54:04.996984 4097769 execution_engine.cc:136] Target CPU: icelake-server
I1018 08:54:04.997359 4097769 execution_engine.cc:163] register runtime call symbols
I1018 08:54:04.999018 4097769 execution_engine.cc:167] ===================== Create CINN ExecutionEngine end ====================
I1018 08:54:05.009837 4097769 execution_engine.cc:181] ir_emitter->Compile(module) Begin
I1018 08:54:05.009868 4097769 codegen_llvm.cc:803] JIT Linking function [fn_pool2d_20_22]
I1018 08:54:05.009919 4097769 execution_engine.cc:183] ir_emitter->Compile(module) Succeed!
I1018 08:54:05.011528 4097769 llvm_optimizer.cc:77] llvm run function pass[Target Transform Information]
I1018 08:54:05.011545 4097769 llvm_optimizer.cc:82] llvm run module pass[Target Transform Information]
I1018 08:54:05.011569 4097769 llvm_optimizer.cc:77] llvm run function pass[Instrument function entry/exit with calls to e.g. mcount() (pre inlining)]
I1018 08:54:05.011579 4097769 llvm_optimizer.cc:77] llvm run function pass[Type-Based Alias Analysis]
I1018 08:54:05.011585 4097769 llvm_optimizer.cc:77] llvm run function pass[Scoped NoAlias Alias Analysis]
I1018 08:54:05.011592 4097769 llvm_optimizer.cc:77] llvm run function pass[Simplify the CFG]
I1018 08:54:05.011602 4097769 llvm_optimizer.cc:77] llvm run function pass[SROA]
I1018 08:54:05.011622 4097769 llvm_optimizer.cc:77] llvm run function pass[Early CSE]
I1018 08:54:05.011652 4097769 llvm_optimizer.cc:77] llvm run function pass[Lower 'expect' Intrinsics]
I1018 08:54:05.011658 4097769 llvm_optimizer.cc:82] llvm run module pass[Force set function attributes]
I1018 08:54:05.011677 4097769 llvm_optimizer.cc:82] llvm run module pass[Type-Based Alias Analysis]
I1018 08:54:05.011682 4097769 llvm_optimizer.cc:82] llvm run module pass[Scoped NoAlias Alias Analysis]
I1018 08:54:05.011687 4097769 llvm_optimizer.cc:82] llvm run module pass[Infer set function attributes]
I1018 08:54:05.011700 4097769 llvm_optimizer.cc:82] llvm run module pass[Call-site splitting]
I1018 08:54:05.011713 4097769 llvm_optimizer.cc:82] llvm run module pass[Interprocedural Sparse Conditional Constant Propagation]
I1018 08:54:05.011730 4097769 llvm_optimizer.cc:82] llvm run module pass[Called Value Propagation]
I1018 08:54:05.011736 4097769 llvm_optimizer.cc:82] llvm run module pass[Global Variable Optimizer]
I1018 08:54:05.011770 4097769 llvm_optimizer.cc:82] llvm run module pass[Promote Memory to Register]
I1018 08:54:05.011790 4097769 llvm_optimizer.cc:82] llvm run module pass[Dead Argument Elimination]
I1018 08:54:05.011795 4097769 llvm_optimizer.cc:82] llvm run module pass[Combine redundant instructions]
I1018 08:54:05.011845 4097769 llvm_optimizer.cc:82] llvm run module pass[Simplify the CFG]
I1018 08:54:05.011853 4097769 llvm_optimizer.cc:82] llvm run module pass[Globals Alias Analysis]
I1018 08:54:05.011862 4097769 llvm_optimizer.cc:82] llvm run module pass[Remove unused exception handling info]
I1018 08:54:05.011871 4097769 llvm_optimizer.cc:82] llvm run module pass[Function Integration/Inlining]
I1018 08:54:05.011885 4097769 llvm_optimizer.cc:82] llvm run module pass[OpenMP specific optimizations]
I1018 08:54:05.011891 4097769 llvm_optimizer.cc:82] llvm run module pass[Deduce function attributes]
I1018 08:54:05.011904 4097769 llvm_optimizer.cc:82] llvm run module pass[Promote 'by reference' arguments to scalars]
I1018 08:54:05.011914 4097769 llvm_optimizer.cc:82] llvm run module pass[SROA]
I1018 08:54:05.011929 4097769 llvm_optimizer.cc:82] llvm run module pass[Early CSE w/ MemorySSA]
I1018 08:54:05.011958 4097769 llvm_optimizer.cc:82] llvm run module pass[Speculatively execute instructions if target has divergent branches]
I1018 08:54:05.011969 4097769 llvm_optimizer.cc:82] llvm run module pass[Jump Threading]
I1018 08:54:05.011987 4097769 llvm_optimizer.cc:82] llvm run module pass[Value Propagation]
I1018 08:54:05.011998 4097769 llvm_optimizer.cc:82] llvm run module pass[Simplify the CFG]
I1018 08:54:05.012005 4097769 llvm_optimizer.cc:82] llvm run module pass[Combine pattern based expressions]
I1018 08:54:05.012020 4097769 llvm_optimizer.cc:82] llvm run module pass[Combine redundant instructions]
I1018 08:54:05.012065 4097769 llvm_optimizer.cc:82] llvm run module pass[Conditionally eliminate dead library calls]
I1018 08:54:05.012094 4097769 llvm_optimizer.cc:82] llvm run module pass[PGOMemOPSize]
I1018 08:54:05.012128 4097769 llvm_optimizer.cc:82] llvm run module pass[Tail Call Elimination]
I1018 08:54:05.012162 4097769 llvm_optimizer.cc:82] llvm run module pass[Simplify the CFG]
I1018 08:54:05.012176 4097769 llvm_optimizer.cc:82] llvm run module pass[Reassociate expressions]
I1018 08:54:05.012192 4097769 llvm_optimizer.cc:82] llvm run module pass[Rotate Loops]
I1018 08:54:05.012259 4097769 llvm_optimizer.cc:82] llvm run module pass[Loop Invariant Code Motion]
I1018 08:54:05.012286 4097769 llvm_optimizer.cc:82] llvm run module pass[Unswitch loops]
I1018 08:54:05.012305 4097769 llvm_optimizer.cc:82] llvm run module pass[Simplify the CFG]
I1018 08:54:05.012315 4097769 llvm_optimizer.cc:82] llvm run module pass[Combine redundant instructions]
I1018 08:54:05.012370 4097769 llvm_optimizer.cc:82] llvm run module pass[Induction Variable Simplification]
I1018 08:54:05.012430 4097769 llvm_optimizer.cc:82] llvm run module pass[Recognize loop idioms]
I1018 08:54:05.012450 4097769 llvm_optimizer.cc:82] llvm run module pass[Delete dead loops]
I1018 08:54:05.012467 4097769 llvm_optimizer.cc:82] llvm run module pass[Unroll loops]
I1018 08:54:05.012486 4097769 llvm_optimizer.cc:82] llvm run module pass[MergedLoadStoreMotion]
I1018 08:54:05.012502 4097769 llvm_optimizer.cc:82] llvm run module pass[Global Value Numbering]
I1018 08:54:05.012559 4097769 llvm_optimizer.cc:82] llvm run module pass[MemCpy Optimization]
I1018 08:54:05.012595 4097769 llvm_optimizer.cc:82] llvm run module pass[Sparse Conditional Constant Propagation]
I1018 08:54:05.012607 4097769 llvm_optimizer.cc:82] llvm run module pass[Bit-Tracking Dead Code Elimination]
I1018 08:54:05.012632 4097769 llvm_optimizer.cc:82] llvm run module pass[Combine redundant instructions]
I1018 08:54:05.012681 4097769 llvm_optimizer.cc:82] llvm run module pass[Jump Threading]
I1018 08:54:05.012694 4097769 llvm_optimizer.cc:82] llvm run module pass[Value Propagation]
I1018 08:54:05.012702 4097769 llvm_optimizer.cc:82] llvm run module pass[Dead Store Elimination]
I1018 08:54:05.012737 4097769 llvm_optimizer.cc:82] llvm run module pass[Loop Invariant Code Motion]
I1018 08:54:05.012810 4097769 llvm_optimizer.cc:82] llvm run module pass[Aggressive Dead Code Elimination]
I1018 08:54:05.012820 4097769 llvm_optimizer.cc:82] llvm run module pass[Simplify the CFG]
I1018 08:54:05.012831 4097769 llvm_optimizer.cc:82] llvm run module pass[Combine redundant instructions]
I1018 08:54:05.012888 4097769 llvm_optimizer.cc:82] llvm run module pass[A No-Op Barrier Pass]
I1018 08:54:05.012895 4097769 llvm_optimizer.cc:82] llvm run module pass[Eliminate Available Externally Globals]
I1018 08:54:05.012900 4097769 llvm_optimizer.cc:82] llvm run module pass[Deduce function attributes in RPO]
I1018 08:54:05.012913 4097769 llvm_optimizer.cc:82] llvm run module pass[Global Variable Optimizer]
I1018 08:54:05.012952 4097769 llvm_optimizer.cc:82] llvm run module pass[Dead Global Elimination]
I1018 08:54:05.012959 4097769 llvm_optimizer.cc:82] llvm run module pass[Globals Alias Analysis]
I1018 08:54:05.012969 4097769 llvm_optimizer.cc:82] llvm run module pass[Float to int]
I1018 08:54:05.012984 4097769 llvm_optimizer.cc:82] llvm run module pass[Lower constant intrinsics]
I1018 08:54:05.012989 4097769 llvm_optimizer.cc:82] llvm run module pass[Rotate Loops]
I1018 08:54:05.013064 4097769 llvm_optimizer.cc:82] llvm run module pass[Loop Distribution]
I1018 08:54:05.013118 4097769 llvm_optimizer.cc:82] llvm run module pass[Loop Vectorization]
I1018 08:54:05.013257 4097769 llvm_optimizer.cc:82] llvm run module pass[Loop Load Elimination]
I1018 08:54:05.013355 4097769 llvm_optimizer.cc:82] llvm run module pass[Combine redundant instructions]
I1018 08:54:05.013424 4097769 llvm_optimizer.cc:82] llvm run module pass[Simplify the CFG]
I1018 08:54:05.013433 4097769 llvm_optimizer.cc:82] llvm run module pass[SLP Vectorizer]
I1018 08:54:05.013543 4097769 llvm_optimizer.cc:82] llvm run module pass[Optimize scalar/vector ops]
I1018 08:54:05.013559 4097769 llvm_optimizer.cc:82] llvm run module pass[Combine redundant instructions]
I1018 08:54:05.013597 4097769 llvm_optimizer.cc:82] llvm run module pass[Unroll loops]
I1018 08:54:05.013670 4097769 llvm_optimizer.cc:82] llvm run module pass[Combine redundant instructions]
I1018 08:54:05.013727 4097769 llvm_optimizer.cc:82] llvm run module pass[Loop Invariant Code Motion]
I1018 08:54:05.013808 4097769 llvm_optimizer.cc:82] llvm run module pass[Warn about non-applied transformations]
I1018 08:54:05.013846 4097769 llvm_optimizer.cc:82] llvm run module pass[Alignment from assumptions]
I1018 08:54:05.013867 4097769 llvm_optimizer.cc:82] llvm run module pass[Strip Unused Function Prototypes]
I1018 08:54:05.013875 4097769 llvm_optimizer.cc:82] llvm run module pass[Dead Global Elimination]
I1018 08:54:05.013881 4097769 llvm_optimizer.cc:82] llvm run module pass[Merge Duplicate Global Constants]
I1018 08:54:05.013904 4097769 llvm_optimizer.cc:82] llvm run module pass[Call Graph Profile]
I1018 08:54:05.013944 4097769 llvm_optimizer.cc:82] llvm run module pass[Loop Sink]
I1018 08:54:05.014076 4097769 llvm_optimizer.cc:82] llvm run module pass[Remove redundant instructions]
I1018 08:54:05.014125 4097769 llvm_optimizer.cc:82] llvm run module pass[Hoist/decompose integer division and remainder]
I1018 08:54:05.014138 4097769 llvm_optimizer.cc:82] llvm run module pass[Simplify the CFG]
I1018 08:54:05.365314 4097769 parallel_compiler.cc:146] Start BuildInstruction
I1018 08:54:05.365345 4097769 parallel_compiler.cc:402] Start BuildInstruction of Group 2 at thread: 140640358967040
I1018 08:54:05.365437 4097769 execution_engine.cc:101] No object for <string> in cache. Compiling.
I1018 08:54:05.461488 4097768 execution_engine.cc:117] ===================== Create CINN ExecutionEngine begin ====================
I1018 08:54:05.461529 4097768 execution_engine.cc:119] initialize llvm config
I1018 08:54:05.461539 4097768 execution_engine.cc:120] llvm version: 12.0.0git
I1018 08:54:05.461547 4097768 execution_engine.cc:121] llvm default target triple: x86_64-unknown-linux-gnu
I1018 08:54:05.461558 4097768 execution_engine.cc:153] create jit execution engine
I1018 08:54:05.462848 4097768 execution_engine.cc:134] create llvm compile layer
I1018 08:54:05.462862 4097768 execution_engine.cc:135] Target Name: x86-64
I1018 08:54:05.462869 4097768 execution_engine.cc:136] Target CPU: icelake-server
I1018 08:54:05.463093 4097768 execution_engine.cc:163] register runtime call symbols
I1018 08:54:05.464444 4097768 execution_engine.cc:167] ===================== Create CINN ExecutionEngine end ====================
I1018 08:54:05.476693 4097768 execution_engine.cc:181] ir_emitter->Compile(module) Begin
I1018 08:54:05.476722 4097768 codegen_llvm.cc:803] JIT Linking function [fn_fill_constant_4_fill_constant_8_pow_9_20]
I1018 08:54:05.476768 4097768 execution_engine.cc:183] ir_emitter->Compile(module) Succeed!
I1018 08:54:05.478353 4097768 llvm_optimizer.cc:77] llvm run function pass[Target Transform Information]
I1018 08:54:05.478370 4097768 llvm_optimizer.cc:82] llvm run module pass[Target Transform Information]
I1018 08:54:05.478381 4097768 llvm_optimizer.cc:77] llvm run function pass[Instrument function entry/exit with calls to e.g. mcount() (pre inlining)]
I1018 08:54:05.478391 4097768 llvm_optimizer.cc:77] llvm run function pass[Type-Based Alias Analysis]
I1018 08:54:05.478396 4097768 llvm_optimizer.cc:77] llvm run function pass[Scoped NoAlias Alias Analysis]
I1018 08:54:05.478402 4097768 llvm_optimizer.cc:77] llvm run function pass[Simplify the CFG]
I1018 08:54:05.478411 4097768 llvm_optimizer.cc:77] llvm run function pass[SROA]
I1018 08:54:05.478431 4097768 llvm_optimizer.cc:77] llvm run function pass[Early CSE]
I1018 08:54:05.478463 4097768 llvm_optimizer.cc:77] llvm run function pass[Lower 'expect' Intrinsics]
I1018 08:54:05.478471 4097768 llvm_optimizer.cc:82] llvm run module pass[Force set function attributes]
I1018 08:54:05.478477 4097768 llvm_optimizer.cc:82] llvm run module pass[Type-Based Alias Analysis]
I1018 08:54:05.478480 4097768 llvm_optimizer.cc:82] llvm run module pass[Scoped NoAlias Alias Analysis]
I1018 08:54:05.478485 4097768 llvm_optimizer.cc:82] llvm run module pass[Infer set function attributes]
I1018 08:54:05.478498 4097768 llvm_optimizer.cc:82] llvm run module pass[Call-site splitting]
I1018 08:54:05.478510 4097768 llvm_optimizer.cc:82] llvm run module pass[Interprocedural Sparse Conditional Constant Propagation]
I1018 08:54:05.478536 4097768 llvm_optimizer.cc:82] llvm run module pass[Called Value Propagation]
I1018 08:54:05.478542 4097768 llvm_optimizer.cc:82] llvm run module pass[Global Variable Optimizer]
I1018 08:54:05.478574 4097768 llvm_optimizer.cc:82] llvm run module pass[Promote Memory to Register]
I1018 08:54:05.478592 4097768 llvm_optimizer.cc:82] llvm run module pass[Dead Argument Elimination]
I1018 08:54:05.478598 4097768 llvm_optimizer.cc:82] llvm run module pass[Combine redundant instructions]
I1018 08:54:05.478644 4097768 llvm_optimizer.cc:82] llvm run module pass[Simplify the CFG]
I1018 08:54:05.478652 4097768 llvm_optimizer.cc:82] llvm run module pass[Globals Alias Analysis]
I1018 08:54:05.478662 4097768 llvm_optimizer.cc:82] llvm run module pass[Remove unused exception handling info]
I1018 08:54:05.478669 4097768 llvm_optimizer.cc:82] llvm run module pass[Function Integration/Inlining]
I1018 08:54:05.478683 4097768 llvm_optimizer.cc:82] llvm run module pass[OpenMP specific optimizations]
I1018 08:54:05.478698 4097768 llvm_optimizer.cc:82] llvm run module pass[Deduce function attributes]
I1018 08:54:05.478711 4097768 llvm_optimizer.cc:82] llvm run module pass[Promote 'by reference' arguments to scalars]
I1018 08:54:05.478720 4097768 llvm_optimizer.cc:82] llvm run module pass[SROA]
I1018 08:54:05.478739 4097768 llvm_optimizer.cc:82] llvm run module pass[Early CSE w/ MemorySSA]
I1018 08:54:05.478768 4097768 llvm_optimizer.cc:82] llvm run module pass[Speculatively execute instructions if target has divergent branches]
I1018 08:54:05.478780 4097768 llvm_optimizer.cc:82] llvm run module pass[Jump Threading]
I1018 08:54:05.478797 4097768 llvm_optimizer.cc:82] llvm run module pass[Value Propagation]
I1018 08:54:05.478806 4097768 llvm_optimizer.cc:82] llvm run module pass[Simplify the CFG]
I1018 08:54:05.478813 4097768 llvm_optimizer.cc:82] llvm run module pass[Combine pattern based expressions]
I1018 08:54:05.478827 4097768 llvm_optimizer.cc:82] llvm run module pass[Combine redundant instructions]
I1018 08:54:05.478875 4097768 llvm_optimizer.cc:82] llvm run module pass[Conditionally eliminate dead library calls]
I1018 08:54:05.478883 4097768 llvm_optimizer.cc:82] llvm run module pass[PGOMemOPSize]
I1018 08:54:05.478917 4097768 llvm_optimizer.cc:82] llvm run module pass[Tail Call Elimination]
I1018 08:54:05.478950 4097768 llvm_optimizer.cc:82] llvm run module pass[Simplify the CFG]
I1018 08:54:05.478957 4097768 llvm_optimizer.cc:82] llvm run module pass[Reassociate expressions]
I1018 08:54:05.478968 4097768 llvm_optimizer.cc:82] llvm run module pass[Rotate Loops]
I1018 08:54:05.479036 4097768 llvm_optimizer.cc:82] llvm run module pass[Loop Invariant Code Motion]
I1018 08:54:05.479065 4097768 llvm_optimizer.cc:82] llvm run module pass[Unswitch loops]
I1018 08:54:05.479084 4097768 llvm_optimizer.cc:82] llvm run module pass[Simplify the CFG]
I1018 08:54:05.479091 4097768 llvm_optimizer.cc:82] llvm run module pass[Combine redundant instructions]
I1018 08:54:05.479149 4097768 llvm_optimizer.cc:82] llvm run module pass[Induction Variable Simplification]
I1018 08:54:05.479209 4097768 llvm_optimizer.cc:82] llvm run module pass[Recognize loop idioms]
I1018 08:54:05.479229 4097768 llvm_optimizer.cc:82] llvm run module pass[Delete dead loops]
I1018 08:54:05.479246 4097768 llvm_optimizer.cc:82] llvm run module pass[Unroll loops]
I1018 08:54:05.479265 4097768 llvm_optimizer.cc:82] llvm run module pass[MergedLoadStoreMotion]
I1018 08:54:05.479281 4097768 llvm_optimizer.cc:82] llvm run module pass[Global Value Numbering]
I1018 08:54:05.479333 4097768 llvm_optimizer.cc:82] llvm run module pass[MemCpy Optimization]
I1018 08:54:05.479369 4097768 llvm_optimizer.cc:82] llvm run module pass[Sparse Conditional Constant Propagation]
I1018 08:54:05.479382 4097768 llvm_optimizer.cc:82] llvm run module pass[Bit-Tracking Dead Code Elimination]
I1018 08:54:05.479399 4097768 llvm_optimizer.cc:82] llvm run module pass[Combine redundant instructions]
I1018 08:54:05.479449 4097768 llvm_optimizer.cc:82] llvm run module pass[Jump Threading]
I1018 08:54:05.479461 4097768 llvm_optimizer.cc:82] llvm run module pass[Value Propagation]
I1018 08:54:05.479470 4097768 llvm_optimizer.cc:82] llvm run module pass[Dead Store Elimination]
I1018 08:54:05.479504 4097768 llvm_optimizer.cc:82] llvm run module pass[Loop Invariant Code Motion]
I1018 08:54:05.479578 4097768 llvm_optimizer.cc:82] llvm run module pass[Aggressive Dead Code Elimination]
I1018 08:54:05.479589 4097768 llvm_optimizer.cc:82] llvm run module pass[Simplify the CFG]
I1018 08:54:05.479596 4097768 llvm_optimizer.cc:82] llvm run module pass[Combine redundant instructions]
I1018 08:54:05.479657 4097768 llvm_optimizer.cc:82] llvm run module pass[A No-Op Barrier Pass]
I1018 08:54:05.479664 4097768 llvm_optimizer.cc:82] llvm run module pass[Eliminate Available Externally Globals]
I1018 08:54:05.479671 4097768 llvm_optimizer.cc:82] llvm run module pass[Deduce function attributes in RPO]
I1018 08:54:05.479684 4097768 llvm_optimizer.cc:82] llvm run module pass[Global Variable Optimizer]
I1018 08:54:05.479727 4097768 llvm_optimizer.cc:82] llvm run module pass[Dead Global Elimination]
I1018 08:54:05.479732 4097768 llvm_optimizer.cc:82] llvm run module pass[Globals Alias Analysis]
I1018 08:54:05.479741 4097768 llvm_optimizer.cc:82] llvm run module pass[Float to int]
I1018 08:54:05.479756 4097768 llvm_optimizer.cc:82] llvm run module pass[Lower constant intrinsics]
I1018 08:54:05.479763 4097768 llvm_optimizer.cc:82] llvm run module pass[Rotate Loops]
I1018 08:54:05.479840 4097768 llvm_optimizer.cc:82] llvm run module pass[Loop Distribution]
I1018 08:54:05.479892 4097768 llvm_optimizer.cc:82] llvm run module pass[Loop Vectorization]
I1018 08:54:05.480017 4097768 llvm_optimizer.cc:82] llvm run module pass[Loop Load Elimination]
I1018 08:54:05.480114 4097768 llvm_optimizer.cc:82] llvm run module pass[Combine redundant instructions]
I1018 08:54:05.480185 4097768 llvm_optimizer.cc:82] llvm run module pass[Simplify the CFG]
I1018 08:54:05.480194 4097768 llvm_optimizer.cc:82] llvm run module pass[SLP Vectorizer]
I1018 08:54:05.480304 4097768 llvm_optimizer.cc:82] llvm run module pass[Optimize scalar/vector ops]
I1018 08:54:05.480319 4097768 llvm_optimizer.cc:82] llvm run module pass[Combine redundant instructions]
I1018 08:54:05.480357 4097768 llvm_optimizer.cc:82] llvm run module pass[Unroll loops]
I1018 08:54:05.480427 4097768 llvm_optimizer.cc:82] llvm run module pass[Combine redundant instructions]
I1018 08:54:05.480482 4097768 llvm_optimizer.cc:82] llvm run module pass[Loop Invariant Code Motion]
I1018 08:54:05.480561 4097768 llvm_optimizer.cc:82] llvm run module pass[Warn about non-applied transformations]
I1018 08:54:05.480600 4097768 llvm_optimizer.cc:82] llvm run module pass[Alignment from assumptions]
I1018 08:54:05.480621 4097768 llvm_optimizer.cc:82] llvm run module pass[Strip Unused Function Prototypes]
I1018 08:54:05.480629 4097768 llvm_optimizer.cc:82] llvm run module pass[Dead Global Elimination]
I1018 08:54:05.480635 4097768 llvm_optimizer.cc:82] llvm run module pass[Merge Duplicate Global Constants]
I1018 08:54:05.480641 4097768 llvm_optimizer.cc:82] llvm run module pass[Call Graph Profile]
I1018 08:54:05.480679 4097768 llvm_optimizer.cc:82] llvm run module pass[Loop Sink]
I1018 08:54:05.480810 4097768 llvm_optimizer.cc:82] llvm run module pass[Remove redundant instructions]
I1018 08:54:05.480858 4097768 llvm_optimizer.cc:82] llvm run module pass[Hoist/decompose integer division and remainder]
I1018 08:54:05.480872 4097768 llvm_optimizer.cc:82] llvm run module pass[Simplify the CFG]
I1018 08:54:05.551332 4097769 parallel_compiler.cc:155] Finish task 2 on thread: 140640358967040
F1018 08:54:05.590272 4097770 sycl_module.cc:47] Check failed: so_handler_ != nullptr ERROR:./source/sycl_2.so: undefined symbol: _ZN2cl10__host_std10vector_maxEmPfPKff
*** Check failure stack trace: ***
    @     0x7fec4f4d4063  google::LogMessage::Fail()
    @     0x7fec4f4d64e5  google::LogMessage::SendToLog()
    @     0x7fec4f4d3b41  google::LogMessage::Flush()
    @     0x7fec4f4d709f  google::LogMessageFatal::~LogMessageFatal()
    @     0x7fec46a2c353  cinn::runtime::sycl::SYCLModule::GetFunction()
    @     0x7fec46ce9a4b  cinn::hlir::framework::ParallelCompiler::Task::CodegenAndJit()
    @     0x7fec46cea70b  cinn::hlir::framework::ParallelCompiler::RunTask()
    @     0x7fec3f798df4  (unknown)
    @     0x7fec54eb0609  start_thread
    @     0x7fec54fea353  clone
    @              (nil)  (unknown)
